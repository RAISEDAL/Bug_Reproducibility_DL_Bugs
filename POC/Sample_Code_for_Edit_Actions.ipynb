{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Input Data Generation"
      ],
      "metadata": {
        "id": "umZlaMp1S-rf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T70N4P2vS7ej"
      },
      "outputs": [],
      "source": [
        "# Generating an Identity Matrix (3x3)\n",
        "identity_matrix = np.eye(3)\n",
        "\n",
        "# Generating an Array of Ones (2x4)\n",
        "ones_array = np.ones((2, 4))\n",
        "\n",
        "# Generating an Array of Zeros (3x2)\n",
        "zeros_array = np.zeros((3, 2))\n",
        "\n",
        "# Generating Random Data with Normal Distribution (2x3)\n",
        "mean = 0\n",
        "std_dev = 1\n",
        "shape = (2, 3)\n",
        "random_data = np.random.normal(mean, std_dev, shape)\n",
        "\n",
        "# Generating Random Integers (3x3)\n",
        "random_integers = np.random.randint(1, 10, (3, 3))\n",
        "\n",
        "# Generating Random Data from a Uniform Distribution (2x2)\n",
        "random_uniform_data = np.random.uniform(0, 1, (2, 2))\n",
        "\n",
        "# Printing the generated data\n",
        "print(\"Identity Matrix:\\n\", identity_matrix)\n",
        "print(\"Array of Ones:\\n\", ones_array)\n",
        "print(\"Array of Zeros:\\n\", zeros_array)\n",
        "print(\"Random Data with Normal Distribution:\\n\", random_data)\n",
        "print(\"Random Integers:\\n\", random_integers)\n",
        "print(\"Random Data from Uniform Distribution:\\n\", random_uniform_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network Construction"
      ],
      "metadata": {
        "id": "D1b_1UAHUTXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In one of the Stack Overflow question, it was mentioned that \"I am trying to write a simple multinomial logistic regression using mnist data\". So, using this information, we generate multiple regression models as following"
      ],
      "metadata": {
        "id": "m8zRVso3aHd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),  # Flatten the 28x28 images\n",
        "    keras.layers.Dense(10, activation='softmax')  # Output layer with 10 classes (digits 0-9)\n",
        "])"
      ],
      "metadata": {
        "id": "6NJ7p5yhUVVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n",
        "    keras.layers.Dense(64, activation='relu'),   # Additional hidden layer\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "s_MVVZ5SakJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D((2, 2)),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "Ub-zEJZTal5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Initialization"
      ],
      "metadata": {
        "id": "5mZINQUoUVzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Define hyperparameter search space\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "num_epochs = random.randint(10, 100)  # Random number of epochs between 10 and 100\n",
        "learning_rate = 10 ** random.uniform(-5, -2)  # Random learning rate in the range [1e-5, 1e-2]\n",
        "\n",
        "# Randomly select hyperparameters\n",
        "batch_size = random.choice(batch_sizes)\n",
        "\n",
        "# Print randomly generated hyperparameters\n",
        "print(\"Randomly Generated Hyperparameters:\")\n",
        "print(\"Batch Size:\", batch_size)\n",
        "print(\"Number of Epochs:\", num_epochs)\n",
        "print(\"Learning Rate:\", learning_rate)"
      ],
      "metadata": {
        "id": "c1MMFh4XUXWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Addition and Dependency Resolution"
      ],
      "metadata": {
        "id": "fj0g2rLaUXuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For adding the necessary imports and dependencies, we generate the dependencies using the pipreqs (https://pypi.org/project/pipreqs/), and then add the necessary imports in order to make the code compilable."
      ],
      "metadata": {
        "id": "MA-kXrgMawfG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logging"
      ],
      "metadata": {
        "id": "aDdVznodUbFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To facilitate logging, we either print the runtime values of various variables to verify the reproducibility of the bug or print the shape of the data or the summary of the neural network to ensure that we have set up the structure correctly and successfully reproduced the bug."
      ],
      "metadata": {
        "id": "6HOrkwkYa_Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_uniform_data = np.random.uniform(0, 1, (2, 2))\n",
        "print (random_uniform_data.shape)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n",
        "    keras.layers.Dense(64, activation='relu'),   # Additional hidden layer\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "print (model.summary())"
      ],
      "metadata": {
        "id": "2IlzzUhbUcBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Obsolete Parameter Removal"
      ],
      "metadata": {
        "id": "1Q3G_fKDUcko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this edit action, we remove all the obsolete parameters not supported by the newer library or framework version. This action is usually performed at runtime based on the developer's version of the library and framework and the reporter's version of the libraries and framework."
      ],
      "metadata": {
        "id": "pfDVwfo_buNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiler Error Resolution"
      ],
      "metadata": {
        "id": "OHmrvWRoUfMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this edit action, we debug and resolve compiler errors to reproduce the bug. To achieve this, we meticulously review the codebase, step through the program's execution, and use debugging tools to identify and rectify any compiler errors, allowing us to accurately reproduce the bug for effective troubleshooting and resolution."
      ],
      "metadata": {
        "id": "e0c4SvJBcgCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Procurement"
      ],
      "metadata": {
        "id": "aN9r8VC9UiJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some scenarios, we can use famous datasets such as MNIST and Iris to reproduce the bug. These datasets can sometimes be obtained by using code operations, as shown below."
      ],
      "metadata": {
        "id": "4ab-ZbnTcolX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "R4CnI5P4Uji2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, we might need to download the dataset from well-known sources such as Kaggle, or the dataset's original source.\n",
        "\n",
        "For example, Iris dataset can also be downloaded using the source (https://archive.ics.uci.edu/dataset/53/iris)"
      ],
      "metadata": {
        "id": "2Yku7VP4c8rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Models and Tokenizers"
      ],
      "metadata": {
        "id": "YqbaWeZIUj4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download the models and tokenizers, we usually use the Hugging Face Hub to procure the different language models and their specific tokenizers."
      ],
      "metadata": {
        "id": "x5A-5CdEduh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "\n",
        "# Specify the BERT model name\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForMaskedLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "MsYrbrD3Umz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version Migration"
      ],
      "metadata": {
        "id": "ZR9wywGEUnIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This edit action is used when we have to port the code from one framework version to another. In the example given below, the first cell of code is written using Tensorflow v1, and is incompatible with Tensorflow v2. So, in order to make the code compilable, we rewrite it using Tensorflow v2, as demonstrated in the second cell."
      ],
      "metadata": {
        "id": "s_QSA-7ZfCzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define a computation graph\n",
        "graph = tf.Graph()\n",
        "\n",
        "with graph.as_default():\n",
        "    input_data = tf.placeholder(tf.float32, shape=(None, input_size), name=\"input_data\")\n",
        "    target_data = tf.placeholder(tf.float32, shape=(None, output_size), name=\"target_data\")\n",
        "    hidden_layer = tf.layers.dense(inputs=input_data, units=64, activation=tf.nn.relu, name=\"hidden_layer\")\n",
        "    output_layer = tf.layers.dense(inputs=hidden_layer, units=output_size, name=\"output_layer\")\n",
        "\n",
        "    loss = tf.reduce_mean(tf.square(output_layer - target_data), name=\"loss\")\n",
        "\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "    train_op = optimizer.minimize(loss, name=\"train_op\")\n",
        "\n",
        "with tf.Session(graph=graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        _, current_loss = sess.run([train_op, loss], feed_dict={input_data: train_input, target_data: train_target})\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {current_loss}\")\n",
        "\n",
        "    predicted_output = sess.run(output_layer, feed_dict={input_data: test_input})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2SVZKM_DekAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the model architecture using the Sequential API\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(input_size,), name='hidden_layer'),\n",
        "    tf.keras.layers.Dense(output_size, name='output_layer')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_input, train_target, epochs=num_epochs, batch_size=batch_size, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "predicted_output = model.predict(test_input)\n",
        "\n",
        "# Print the training history\n",
        "print(\"Training history:\")\n",
        "print(history.history)\n"
      ],
      "metadata": {
        "id": "Qor8l0DNUoYv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}