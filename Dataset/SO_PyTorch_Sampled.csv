Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense
63582590,1,63869655.0,,2020-08-25 15:48:28,,61,63729,"<p><a href=""https://stackoverflow.com/questions/55466298/pytorch-cant-call-numpy-on-variable-that-requires-grad-use-var-detach-num"">It has been firmly established that <code>my_tensor.detach().numpy()</code> is the correct way to get a numpy array from a <code>torch</code> tensor.</a></p>
<p>I'm trying to get a better understanding of why.</p>
<p>In the <a href=""https://stackoverflow.com/a/57014852/1048186"">accepted answer</a> to the question just linked, Blupon states that:</p>
<blockquote>
<p>You need to convert your tensor to another tensor that isn't requiring a gradient in addition to its actual value definition.</p>
</blockquote>
<p>In the first discussion he links to, albanD states:</p>
<blockquote>
<p>This is expected behavior because moving to numpy will break the graph and so no gradient will be computed.</p>
<p>If you don’t actually need gradients, then you can explicitly .detach() the Tensor that requires grad to get a tensor with the same content that does not require grad. This other Tensor can then be converted to a numpy array.</p>
</blockquote>
<p>In the second discussion he links to, apaszke writes:</p>
<blockquote>
<p>Variable's can’t be transformed to numpy, because they’re wrappers around tensors that save the operation history, and numpy doesn’t have such objects. You can retrieve a tensor held by the Variable, using the .data attribute. Then, this should work: var.data.numpy().</p>
</blockquote>
<p>I have studied the internal workings of PyTorch's autodifferentiation library, and I'm still confused by these answers.  Why does it break the graph to to move to numpy? Is it because any operations on the numpy array will not be tracked in the autodiff graph?</p>
<p>What is a Variable? How does it relate to a tensor?</p>
<p>I feel that a thorough high-quality Stack-Overflow answer that explains the reason for this to new users of PyTorch who don't yet understand autodifferentiation is called for here.</p>
<p>In particular, I think it would be helpful to illustrate the graph through a figure and show how the disconnection occurs in this example:</p>
<blockquote>
<pre><code>import torch

tensor1 = torch.tensor([1.0,2.0],requires_grad=True)

print(tensor1)
print(type(tensor1))

tensor1 = tensor1.numpy()

print(tensor1)
print(type(tensor1))
</code></pre>
</blockquote>
",1048186.0,,1048186.0,,2020-09-12 22:06:54,2020-09-16 14:06:28,Why do we call .detach() before calling .numpy() on a Pytorch Tensor?,<numpy><pytorch><autodiff>,3,8,0.0,,,CC BY-SA 4.0
66091226,1,,,2021-02-07 17:54:33,,58,157324,"<p>I saved a checkpoint while training on gpu. After reloading the checkpoint and continue training I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;main.py&quot;, line 140, in &lt;module&gt;
    train(model,optimizer,train_loader,val_loader,criteria=args.criterion,epoch=epoch,batch=batch)
  File &quot;main.py&quot;, line 71, in train
    optimizer.step()
  File &quot;/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py&quot;, line 26, in decorate_context
    return func(*args, **kwargs)
  File &quot;/opt/conda/lib/python3.7/site-packages/torch/optim/sgd.py&quot;, line 106, in step
    buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
</code></pre>
<p>My training code is as follows:</p>
<pre><code>def train(model,optimizer,train_loader,val_loader,criteria,epoch=0,batch=0):
    batch_count = batch
    if criteria == 'l1':
        criterion = L1_imp_Loss()
    elif criteria == 'l2':
        criterion = L2_imp_Loss()
    if args.gpu and torch.cuda.is_available():
        model.cuda()
        criterion = criterion.cuda()

    print(f'{datetime.datetime.now().time().replace(microsecond=0)} Starting to train..')
    
    while epoch &lt;= args.epochs-1:
        print(f'********{datetime.datetime.now().time().replace(microsecond=0)} Epoch#: {epoch+1} / {args.epochs}')
        model.train()
        interval_loss, total_loss= 0,0
        for i , (input,target) in enumerate(train_loader):
            batch_count += 1
            if args.gpu and torch.cuda.is_available():
                input, target = input.cuda(), target.cuda()
            input, target = input.float(), target.float()
            pred = model(input)
            loss = criterion(pred,target)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            ....
</code></pre>
<p>The saving process happened after finishing each epoch.</p>
<pre><code>torch.save({'epoch': epoch,'batch':batch_count,'model_state_dict': model.state_dict(),'optimizer_state_dict':
                    optimizer.state_dict(),'loss': total_loss/len(train_loader),'train_set':args.train_set,'val_set':args.val_set,'args':args}, f'{args.weights_dir}/FastDepth_Final.pth')
</code></pre>
<p>I can't figure why I get this error.
<code>args.gpu == True</code>, and I'm passing the model, all data, and loss function to cuda, somehow there is still a tensor on cpu, could anyone figure out what's wrong?</p>
<p>Thanks.</p>
",13757501.0,,872328.0,,2022-11-21 04:13:30,2023-03-24 09:18:32,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! when resuming training",<python><deep-learning><pytorch><runtime-error>,10,2,0.0,,,CC BY-SA 4.0
68166721,1,68169616.0,,2021-06-28 16:12:46,,46,169169,"<p>I am trying to initialize a tensor on Google Colab with GPU enabled.</p>
<pre class=""lang-py prettyprint-override""><code>device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

t = torch.tensor([1,2], device=device)
</code></pre>
<p>But I am getting this strange error.</p>
<blockquote>
<p>RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1</p>
</blockquote>
<p>Even by setting that environment variable to 1 seems not showing any further details.<br />
Anyone ever had this issue?</p>
",5080195.0,,365102.0,,2022-03-28 12:32:56,2023-06-27 13:13:47,CUDA error: device-side assert triggered on Colab,<pytorch><google-colaboratory><tensor>,12,1,0.0,,,CC BY-SA 4.0
66992585,1,68626103.0,,2021-04-07 19:04:28,,43,52586,"<p>I was trying to use my current code with an A100 gpu but I get this error:</p>
<pre><code>---&gt; backend='nccl'
/home/miranda9/miniconda3/envs/metalearningpy1.7.1c10.2/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning: 
A100-SXM4-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.
The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.
If you want to use the A100-SXM4-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/
</code></pre>
<p>which is reather confusing because it points to the usual pytorch installation but doesn't tell me which combination of pytorch version + cuda version to use for my specific hardware (A100). What is the right way to install pytorch for an A100?</p>
<hr />
<p>These are some versions I've tried:</p>
<pre><code># conda install -y pytorch==1.8.0 torchvision cudatoolkit=10.2 -c pytorch
# conda install -y pytorch torchvision cudatoolkit=10.2 -c pytorch
#conda install -y pytorch==1.7.1 torchvision torchaudio cudatoolkit=10.2 -c pytorch -c conda-forge
# conda install -y pytorch==1.6.0 torchvision cudatoolkit=10.2 -c pytorch
#conda install -y pytorch==1.7.1 torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge

# conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch
# conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge
# conda install -y pytorch torchvision cudatoolkit=9.2 -c pytorch # For Nano, CC
# conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge
</code></pre>
<hr />
<p>note that this can be subtle because I've had this error with this machine + pytorch version in the past:</p>
<p><a href=""https://stackoverflow.com/questions/66807131/how-to-solve-the-famous-unhandled-cuda-error-nccl-version-2-7-8-error"">How to solve the famous `unhandled cuda error, NCCL version 2.7.8` error?</a></p>
<hr />
<h1>Bonus 1:</h1>
<p>I still have errors:</p>
<pre><code>ncclSystemError: System call (socket, malloc, munmap, etc) failed.
Traceback (most recent call last):
  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1423, in &lt;module&gt;
    main()
  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1365, in main
    train(args=args)
  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1385, in train
    args.opt = move_opt_to_cherry_opt_and_sync_params(args) if is_running_parallel(args.rank) else args.opt
  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/torch_uu/distributed.py&quot;, line 456, in move_opt_to_cherry_opt_and_sync_params
    args.opt = cherry.optim.Distributed(args.model.parameters(), opt=args.opt, sync=syn)
  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py&quot;, line 62, in __init__
    self.sync_parameters()
  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py&quot;, line 78, in sync_parameters
    dist.broadcast(p.data, src=root)
  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py&quot;, line 1090, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:911, unhandled system error, NCCL version 2.7.8
</code></pre>
<p>one of the answers suggested to have nvcca &amp; pytorch.version.cuda to match but they do not:</p>
<pre><code>(meta_learning_a100) [miranda9@hal-dgx ~]$ python -c &quot;import torch;print(torch.version.cuda)&quot;

11.1
(meta_learning_a100) [miranda9@hal-dgx ~]$ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Wed_Jul_22_19:09:09_PDT_2020
Cuda compilation tools, release 11.0, V11.0.221
Build cuda_11.0_bu.TC445_37.28845127_0
</code></pre>
<p>How do I match them? I this the error? Can someone display their pip, conda and nvcca version to see what set up works?</p>
<p>More error messages:</p>
<pre><code>hal-dgx:21797:21797 [0] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;
hal-dgx:21797:21797 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
hal-dgx:21797:21797 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;
hal-dgx:21797:21797 [0] NCCL INFO Using network IB
NCCL version 2.7.8+cuda11.1
hal-dgx:21805:21805 [2] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;
hal-dgx:21799:21799 [1] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;
hal-dgx:21805:21805 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
hal-dgx:21799:21799 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
hal-dgx:21811:21811 [3] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;
hal-dgx:21811:21811 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
hal-dgx:21811:21811 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;
hal-dgx:21811:21811 [3] NCCL INFO Using network IB
hal-dgx:21799:21799 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;
hal-dgx:21805:21805 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;
hal-dgx:21799:21799 [1] NCCL INFO Using network IB
hal-dgx:21805:21805 [2] NCCL INFO Using network IB

hal-dgx:21797:27906 [0] misc/ibvwrap.cc:280 NCCL WARN Call to ibv_create_qp failed
hal-dgx:21797:27906 [0] NCCL INFO transport/net_ib.cc:360 -&gt; 2
hal-dgx:21797:27906 [0] NCCL INFO transport/net_ib.cc:437 -&gt; 2
hal-dgx:21797:27906 [0] NCCL INFO include/net.h:21 -&gt; 2
hal-dgx:21797:27906 [0] NCCL INFO include/net.h:51 -&gt; 2
hal-dgx:21797:27906 [0] NCCL INFO init.cc:300 -&gt; 2
hal-dgx:21797:27906 [0] NCCL INFO init.cc:566 -&gt; 2
hal-dgx:21797:27906 [0] NCCL INFO init.cc:840 -&gt; 2
hal-dgx:21797:27906 [0] NCCL INFO group.cc:73 -&gt; 2 [Async thread]

hal-dgx:21811:27929 [3] misc/ibvwrap.cc:280 NCCL WARN Call to ibv_create_qp failed
hal-dgx:21811:27929 [3] NCCL INFO transport/net_ib.cc:360 -&gt; 2
hal-dgx:21811:27929 [3] NCCL INFO transport/net_ib.cc:437 -&gt; 2
hal-dgx:21811:27929 [3] NCCL INFO include/net.h:21 -&gt; 2
hal-dgx:21811:27929 [3] NCCL INFO include/net.h:51 -&gt; 2
hal-dgx:21811:27929 [3] NCCL INFO init.cc:300 -&gt; 2
hal-dgx:21811:27929 [3] NCCL INFO init.cc:566 -&gt; 2
hal-dgx:21811:27929 [3] NCCL INFO init.cc:840 -&gt; 2
hal-dgx:21811:27929 [3] NCCL INFO group.cc:73 -&gt; 2 [Async thread]
</code></pre>
<p>after putting</p>
<pre><code>import os
os.environ[&quot;NCCL_DEBUG&quot;] = &quot;INFO&quot;
</code></pre>
",1601580.0,,1601580.0,,2022-05-12 21:06:27,2022-05-18 02:24:26,How does one use Pytorch (+ cuda) with an A100 GPU?,<python><machine-learning><neural-network><pytorch>,6,3,0.0,,,CC BY-SA 4.0
73600481,1,73601542.0,,2022-09-04 15:10:31,,42,3500,"<hr />
<p><strong>TLDR</strong>:</p>
<p><em>A simple (single hidden-layer) feed-forward Pytorch model trained to predict the function <code>y = sin(X1) + sin(X2) + ... sin(X10)</code> substantially underperforms an identical model built/trained with Keras. Why is this so and what can be done to mitigate the difference in performance?</em></p>
<hr />
<p>In training a regression model, I noticed that PyTorch drastically underperforms an identical model built with Keras.</p>
<p><strong>This phenomenon has been observed and reported previously</strong>:</p>
<ul>
<li><p><a href=""https://discuss.pytorch.org/t/the-same-model-produces-worse-results-on-pytorch-than-on-tensorflow/5380"" rel=""noreferrer"">The same model produces worse results on pytorch than on tensorflow</a></p>
</li>
<li><p><a href=""https://discuss.pytorch.org/t/cnn-model-in-pytorch-giving-30-less-accuracy-to-tensoflowflow-model/85410"" rel=""noreferrer"">CNN model in pytorch giving 30% less accuracy to Tensoflowflow model</a>:</p>
</li>
<li><p><a href=""https://discuss.pytorch.org/t/pytorch-adam-vs-tensorflow-adam/74471"" rel=""noreferrer"">PyTorch Adam vs Tensorflow Adam</a></p>
</li>
<li><p><a href=""https://discuss.pytorch.org/t/suboptimal-convergence-when-compared-with-tensorflow-model/5099"" rel=""noreferrer"">Suboptimal convergence when compared with TensorFlow model</a></p>
</li>
<li><p><a href=""https://discuss.pytorch.org/t/rnn-and-adam-slower-convergence-than-keras/11278"" rel=""noreferrer"">RNN and Adam: slower convergence than Keras</a></p>
</li>
<li><p><a href=""https://discuss.pytorch.org/t/pytorch-comparable-but-worse-than-keras-on-a-simple-feed-forward-network/9928"" rel=""noreferrer"">PyTorch comparable but worse than keras on a simple feed forward network</a></p>
</li>
<li><p><a href=""https://www.reddit.com/r/pytorch/comments/ox0g4e/why_is_the_pytorch_model_doing_worse_than_the/"" rel=""noreferrer"">Why is the PyTorch model doing worse than the same model in Keras even with the same weight initialization?</a></p>
</li>
<li><p><a href=""https://stackoverflow.com/questions/59344571/why-keras-behave-better-than-pytorch-under-the-same-network-configuration"">Why Keras behave better than Pytorch under the same network configuration?</a></p>
</li>
</ul>
<p><strong>The following explanations and suggestions have been made previously as well</strong>:</p>
<ol>
<li><p>Using the same decimal precision (32 vs 64): <a href=""https://discuss.pytorch.org/t/the-same-model-produces-worse-results-on-pytorch-than-on-tensorflow/5380"" rel=""noreferrer"">1</a>, <a href=""https://www.reddit.com/r/MachineLearning/comments/7nw67c/d_pytorch_are_adam_and_rmsprop_okay/"" rel=""noreferrer"">2</a>,</p>
</li>
<li><p>Using a CPU instead of a GPU: <a href=""https://discuss.pytorch.org/t/the-same-model-produces-worse-results-on-pytorch-than-on-tensorflow/5380"" rel=""noreferrer"">1</a>,<a href=""https://discuss.pytorch.org/t/rnn-and-adam-slower-convergence-than-keras/11278/2?u=smth"" rel=""noreferrer"">2</a></p>
</li>
<li><p>Change <code>retain_graph=True</code> to <code>create_graph=True</code> in computing the 2nd derivative with <code>autograd.grad</code>: <a href=""https://discuss.pytorch.org/t/pytorch-adam-vs-tensorflow-adam/74471"" rel=""noreferrer"">1</a></p>
</li>
<li><p>Check if keras is using a regularizer, constraint, bias, or loss function in a different way from pytorch: <a href=""https://discuss.pytorch.org/t/suboptimal-convergence-when-compared-with-tensorflow-model/5099/2"" rel=""noreferrer"">1</a>,<a href=""https://discuss.pytorch.org/t/pytorch-comparable-but-worse-than-keras-on-a-simple-feed-forward-network/9928/4"" rel=""noreferrer"">2</a></p>
</li>
<li><p>Ensure you are computing the validation loss in the same way: <a href=""https://discuss.pytorch.org/t/suboptimal-convergence-when-compared-with-tensorflow-model/5099/3"" rel=""noreferrer"">1</a></p>
</li>
<li><p>Use the same initialization routine: <a href=""https://discuss.pytorch.org/t/suboptimal-convergence-when-compared-with-tensorflow-model/5099/3"" rel=""noreferrer"">1</a>,<a href=""https://stackoverflow.com/questions/59344571/why-keras-behave-better-than-pytorch-under-the-same-network-configuration"">2</a></p>
</li>
<li><p>Training the pytorch model for longer epochs: <a href=""https://discuss.pytorch.org/t/rnn-and-adam-slower-convergence-than-keras/11278?u=smth"" rel=""noreferrer"">1</a></p>
</li>
<li><p>Trying several random seeds: <a href=""https://discuss.pytorch.org/t/rnn-and-adam-slower-convergence-than-keras/11278/8?u=smth"" rel=""noreferrer"">1</a></p>
</li>
<li><p>Ensure that <code>model.eval()</code> is called in validation step when training pytorch model: <a href=""https://discuss.pytorch.org/t/pytorch-comparable-but-worse-than-keras-on-a-simple-feed-forward-network/9928"" rel=""noreferrer"">1</a></p>
</li>
<li><p>The main issue is with the Adam optimizer, not the initialization: <a href=""https://www.reddit.com/r/MachineLearning/comments/7nw67c/d_pytorch_are_adam_and_rmsprop_okay/"" rel=""noreferrer"">1</a></p>
</li>
</ol>
<p>To understand this issue, I trained a simple two-layer neural network (much simpler than my original model) in Keras and PyTorch, using the same hyperparameters and initialization routines, and following all the recommendations listed above. However, the PyTorch model results in a mean squared error (MSE) that is  400% higher than the MSE of the Keras model.</p>
<p><strong>Here is my code:</strong></p>
<p><strong>0. Imports</strong></p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from scipy.stats import pearsonr

from sklearn.preprocessing import MinMaxScaler
from sklearn import metrics

from torch.utils.data import Dataset, DataLoader

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
</code></pre>
<p><strong>1. Generate a reproducible dataset</strong></p>
<pre class=""lang-py prettyprint-override""><code>
def get_data():

    np.random.seed(0)
    Xtrain = np.random.normal(0, 1, size=(7000,10))
    Xval = np.random.normal(0, 1, size=(700,10))
    ytrain = np.sum(np.sin(Xtrain), axis=-1)
    yval = np.sum(np.sin(Xval), axis=-1)
    scaler = MinMaxScaler()
    ytrain = scaler.fit_transform(ytrain.reshape(-1,1)).reshape(-1)
    yval = scaler.transform(yval.reshape(-1,1)).reshape(-1) 

    return Xtrain, Xval, ytrain, yval



class XYData(Dataset):
    
    def __init__(self, X, y):
        
        super(XYData, self).__init__()
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)
        self.len = len(y)
         
    def __getitem__(self, index):
        
        return (self.X[index], self.y[index])


    def __len__(self):

        return self.len

# Data, dataset, and dataloader
Xtrain, Xval, ytrain, yval = get_data()
traindata = XYData(Xtrain, ytrain)
valdata = XYData(Xval, yval)
trainloader = DataLoader(dataset=traindata, shuffle=True, batch_size=32, drop_last=False)
valloader = DataLoader(dataset=valdata, shuffle=True, batch_size=32, drop_last=False)
</code></pre>
<p><strong>2. Build Keras and PyTorch models with identical hyperparameters and initialization methods</strong></p>
<pre class=""lang-py prettyprint-override""><code>class TorchLinearModel(nn.Module):
    
    def __init__(self, input_dim=10, random_seed=0):
        
        super(TorchLinearModel, self).__init__()
        _ = torch.manual_seed(random_seed)
        self.hidden_layer = nn.Linear(input_dim,100)
        self.initialize_layer(self.hidden_layer)        
        self.output_layer = nn.Linear(100, 1)
        self.initialize_layer(self.output_layer)

    def initialize_layer(self, layer):
        
        _ = torch.nn.init.xavier_normal_(layer.weight)
        #_ = torch.nn.init.xavier_uniform_(layer.weight)
        _ = torch.nn.init.constant(layer.bias,0)
        
    def forward(self, x):
        x = self.hidden_layer(x)
        x = self.output_layer(x)
        return x




def mean_squared_error(ytrue, ypred):
    
    return torch.mean(((ytrue - ypred) ** 2))


def build_torch_model():

    torch_model = TorchLinearModel()
    optimizer = optim.Adam(torch_model.parameters(), 
                           betas=(0.9,0.9999),
                           eps=1e-7,
                           lr=1e-3,
                           weight_decay=0)
    return torch_model, optimizer




def build_keras_model():
    
    x = layers.Input(shape=10)
    z = layers.Dense(units=100, activation=None, use_bias=True, kernel_regularizer=None, 
                     bias_regularizer=None)(x)
    y = layers.Dense(units=1, activation=None, use_bias=True, kernel_regularizer=None, 
                     bias_regularizer=None)(z)
    keras_model = Model(x, y, name='linear')
    optimizer = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.9999, epsilon=1e-7, 
                     amsgrad=False)
    
    keras_model.compile(optimizer=optimizer, loss='mean_squared_error')
    
    return keras_model




# Instantiate models
torch_model, optimizer = build_torch_model()
keras_model = build_keras_model()

</code></pre>
<p><strong>3. Train PyTorch model for 100 epochs:</strong></p>
<pre class=""lang-py prettyprint-override""><code>
torch_trainlosses, torch_vallosses = [], []

for epoch in range(100):

    # Training
    losses = []
    _ = torch_model.train()
    
    for i, (x,y) in enumerate(trainloader):
        optimizer.zero_grad()                          
        ypred = torch_model(x)
        loss = mean_squared_error(y, ypred) 
        _ = loss.backward()
        _ = optimizer.step()
        losses.append(loss.item())
    torch_trainlosses.append(np.mean(losses))
    
    # Validation
    losses = []
    _ = torch_model.eval()

    with torch.no_grad():
        for i, (x, y) in enumerate(valloader):
            ypred = torch_model(x)
            loss = mean_squared_error(y, ypred) 
            losses.append(loss.item())
    torch_vallosses.append(np.mean(losses))
    
    print(f&quot;epoch={epoch+1}, train_loss={torch_trainlosses[-1]:.4f}, val_loss={torch_vallosses[-1]:.4f}&quot;)
    
</code></pre>
<p><strong>4. Train Keras model for 100 epochs:</strong></p>
<pre class=""lang-py prettyprint-override""><code>history = keras_model.fit(Xtrain, ytrain, sample_weight=None, batch_size=32, epochs=100, 
                    validation_data=(Xval, yval))
</code></pre>
<p><strong>5. Loss in training history</strong></p>
<pre class=""lang-py prettyprint-override""><code>plt.plot(torch_trainlosses, color='blue', label='PyTorch Train')    
plt.plot(torch_vallosses, color='blue', linestyle='--', label='PyTorch Val')  
plt.plot(history.history['loss'], color='brown', label='Keras Train')
plt.plot(history.history['val_loss'], color='brown', linestyle='--', label='Keras Val')
plt.legend()
</code></pre>
<p><a href=""https://i.stack.imgur.com/RajJk.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/RajJk.png"" alt=""enter image description here"" /></a></p>
<p><em>Keras records a much lower error in the training. Since this may be due to a difference in how Keras computes the loss, I calculated the prediction error on the validation set with sklearn.metrics.mean_squared_error</em></p>
<p><strong>6. Validation error after training</strong></p>
<pre class=""lang-py prettyprint-override""><code>ypred_keras = keras_model.predict(Xval).reshape(-1)
ypred_torch = torch_model(torch.tensor(Xval, dtype=torch.float32))
ypred_torch = ypred_torch.detach().numpy().reshape(-1)


mse_keras = metrics.mean_squared_error(yval, ypred_keras)
mse_torch = metrics.mean_squared_error(yval, ypred_torch)
print('Percent error difference:', (mse_torch / mse_keras - 1) * 100) 

r_keras = pearsonr(yval, ypred_keras)[0] 
r_pytorch = pearsonr(yval, ypred_torch)[0]  
print(&quot;r_keras:&quot;, r_keras)
print(&quot;r_pytorch:&quot;, r_pytorch)

plt.scatter(ypred_keras, yval); plt.title('Keras'); plt.show(); plt.close()
plt.scatter(ypred_torch, yval); plt.title('Pytorch'); plt.show(); plt.close()
</code></pre>
<pre class=""lang-py prettyprint-override""><code>Percent error difference: 479.1312469426776
r_keras: 0.9115184443702814
r_pytorch: 0.21728812737220082
</code></pre>
<p><a href=""https://i.stack.imgur.com/y9y8x.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/y9y8x.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/KaLYZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/KaLYZ.png"" alt=""enter image description here"" /></a></p>
<p><em>The correlation of predicted values with ground truth is 0.912 for Keras but 0.217 for Pytorch, and the error for Pytorch is 479% higher!</em></p>
<p><strong>7. Other trials</strong>
I also tried:</p>
<ul>
<li>Lowering the learning rate for Pytorch (lr=1e-4), <strong>R increases from 0.217 to 0.576</strong>, but it's still much worse than Keras (r=0.912).</li>
<li>Increasing the learning rate for Pytorch (lr=1e-2), <strong>R is worse at 0.095</strong></li>
<li>Training numerous times with different random seeds. The <strong>performance is roughly the same</strong>, regardless.</li>
<li>Trained for longer than 100 epochs. No improvement was observed!</li>
<li>Used <code>torch.nn.init.xavier_uniform_</code> instead of <code>torch.nn.init.xavier_normal_</code> in the initialization of the weights. R <strong>improves from 0.217 to 0.639</strong>, but it's still worse than Keras (0.912).</li>
</ul>
<hr />
<p><strong>What can be done to ensure that the PyTorch model converges to a reasonable error comparable with the Keras model?</strong></p>
<hr />
",8437546.0,,8437546.0,,2022-09-04 15:33:05,2022-09-04 17:41:45,400% higher error with PyTorch compared with identical Keras model (with Adam optimizer),<tensorflow><keras><pytorch><adam>,1,2,0.0,,,CC BY-SA 4.0
64621585,1,64623695.0,,2020-10-31 12:11:46,,40,46901,"<p>Is there any difference between <code>torch.optim.Adam(weight_decay=0.01)</code> and <code>torch.optim.AdamW(weight_decay=0.01)</code>?</p>
<p>Link to the docs: <a href=""https://pytorch.org/docs/stable/optim.html"" rel=""noreferrer"">torch.optim</a></p>
",11235205.0,,11235205.0,,2023-02-22 16:39:56,2023-02-22 16:39:56,AdamW and Adam with weight decay,<python><pytorch>,1,0,0.0,,,CC BY-SA 4.0
66371130,1,,,2021-02-25 15:04:37,,38,53764,"<p>I was running a deep learning program on my Linux server and I suddenly got this error.</p>
<p><code>UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729096996/work/c10/cuda/CUDAFunctions.cpp:100.)</code></p>
<p>Earlier when I just created this conda environment, <code>torch.cuda.is_available()</code> returned <code>true</code> and I could use CUDA &amp; GPU. But all of a sudden I could not use CUDA and <code>torch.cuda.is_available()</code>returned <code>false</code>. What should I do?</p>
<p>ps. I use GeForce RTX 3080 and cuda 11.0 + pytorch 1.7.0. It worked before but now it doesn't.</p>
",15112093.0,,15112093.0,,2021-02-26 02:35:03,2023-06-30 04:07:25,CUDA initialization: Unexpected error from cudaGetDeviceCount(),<python><linux><pytorch>,4,0,0.0,,,CC BY-SA 4.0
65082243,1,,,2020-11-30 22:45:28,,38,29993,"<p>My code was working fine and when I tried to run it today without changing anything I got the following error:</p>
<pre><code>dropout(): argument 'input' (position 1) must be Tensor, not str
</code></pre>
<p>Would appreciate if help could be provided.<br />
Could be an issue with the data loader?</p>
",14738280.0,,6807769.0,,2021-11-07 08:51:19,2022-07-01 11:02:41,"dropout(): argument 'input' (position 1) must be Tensor, not str when using Bert with Huggingface",<pytorch><bert-language-model>,3,1,0.0,,,CC BY-SA 4.0
73747731,1,,,2022-09-16 16:18:51,,37,106578,"<p>I found this problem running a neural network on Colab Pro+ (with the high RAM option).</p>
<blockquote>
<p>RuntimeError: CUDA out of memory. Tried to allocate 8.00 GiB (GPU 0;
15.90 GiB total capacity; 12.04 GiB already allocated; 2.72 GiB free; 12.27 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.
See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</p>
</blockquote>
<p>I have already decreased the batch size to 2. I upload the data using the <code>h5py</code> format.</p>
<p>At this point, I assume the only thing I can try is setting the <code>max_split_size_mb</code>.
I could not find anything about how I can implement the <code>max_split_size_mb</code>. The <a href=""https://pytorch.org/docs/stable/notes/cuda.html"" rel=""noreferrer"">PyTorch documentation</a> was not clear to me.</p>
",19509665.0,,681865.0,,2023-05-04 01:38:39,2023-05-04 01:38:39,RuntimeError: CUDA out of memory. How setting max_split_size_mb?,<memory><pytorch><runtime-error><google-colaboratory>,2,0,,,,CC BY-SA 4.0
64089854,1,64145989.0,,2020-09-27 14:59:36,,33,63308,"<p>Which is the <strong>command</strong> to see the &quot;correct&quot; CUDA Version that <code>pytorch</code> in conda env is seeing?  <a href=""https://stackoverflow.com/questions/58005297/pytorch-having-trouble-detecting-cuda"">This</a>, is a similar question, but doesn't get me far.</p>
<ol>
<li><p><code>nvidia-smi</code> says I have cuda version <code>10.1</code></p>
</li>
<li><p><code>conda list</code> tells me cudatoolkit version is <code>10.2.89</code></p>
</li>
<li><p><code>torch.cuda.is_available()</code> shows FALSE, so it sees <code>No CUDA</code>?</p>
</li>
<li><p><code>print(torch.cuda.current_device())</code>, I get <code>10.0.10</code> (10010??) (it
looks like):</p>
<blockquote>
<p>AssertionError: The NVIDIA driver on your system is too old
(found version 10010)</p>
</blockquote>
</li>
<li><p><code>print(torch._C._cuda_getCompiledVersion(), 'cuda compiled version')</code> tells me my version is <code>10.0.20</code> (10020??)?</p>
<blockquote>
<p>10020 cuda compiled version</p>
</blockquote>
</li>
</ol>
<p>Why are there so many different versions? What am I missing?</p>
<p>P.S
I have Nvidia driver <code>430</code> on Ubuntu 16.04 with Geforce 1050. It comes
with <code>libcuda1-430</code> when I installed the driver from <code>additional drivers</code> tab in ubuntu (<code>Software and Updates</code>). I installed <code>pytorch</code>
with <code>conda</code> which also installed the <code>cudatoolkit</code> using <code>conda install -c fastai -c pytorch -c anaconda fastai</code></p>
",5986651.0,,3867406.0,,2022-06-24 09:05:08,2022-06-24 09:05:08,Pytorch detection of CUDA,<python><pytorch>,1,8,0.0,,,CC BY-SA 4.0
63008040,1,,,2020-07-21 05:36:05,,32,59669,"<p>I'm using a laptop which has Intel Corporation HD Graphics 5500 (rev 09), and AMD Radeon r5 m255 graphics card.</p>
<p>Does anyone know how to it set up for Deep Learning, specifically fastai/Pytorch?</p>
",13954488.0,,,,,2023-02-09 11:04:16,How to use AMD GPU for fastai/pytorch?,<pytorch><gpu><amd><fast-ai>,2,0,0.0,,,CC BY-SA 4.0
65279115,1,65875359.0,,2020-12-13 18:23:01,,29,54768,"<p>I am trying to train a pretrained roberta model using <em>3</em> inputs, <em>3</em> input_masks and a label as tensors of my training dataset.</p>
<p>I do this using the following code:</p>
<pre><code>from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
batch_size = 32
# Create the DataLoader for our training set.
train_data = TensorDataset(train_AT, train_BT, train_CT, train_maskAT, train_maskBT, train_maskCT, labels_trainT)
train_dataloader = DataLoader(train_data, batch_size=batch_size)

# Create the Dataloader for our validation set.
validation_data = TensorDataset(val_AT, val_BT, val_CT, val_maskAT, val_maskBT, val_maskCT, labels_valT)
val_dataloader = DataLoader(validation_data, batch_size=batch_size)

# Pytorch Training
training_args = TrainingArguments(
    output_dir='C:/Users/samvd/Documents/Master/AppliedMachineLearning/FinalProject/results',          # output directory
    num_train_epochs=1,              # total # of training epochs
    per_device_train_batch_size=32,  # batch size per device during training
    per_device_eval_batch_size=32,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='C:/Users/samvd/Documents/Master/AppliedMachineLearning/FinalProject/logs',            # directory for storing logs
)

trainer = Trainer(
    model=model,                          # the instantiated 🤗 Transformers model to be trained
    args=training_args,                   # training arguments, defined above
    train_dataset = train_data,           # training dataset
    eval_dataset = validation_data,       # evaluation dataset
)

trainer.train()
</code></pre>
<p>However this gives me the following error:</p>
<blockquote>
<p>TypeError: vars() argument must have <strong>dict</strong> attribute</p>
</blockquote>
<p>Now I have found out that it is probably because I don't use <code>collate_fn</code> when using <code>DataLoader</code>, but I can't really find a source that helps me define this correctly so the trainer understands the different tensors I put in.</p>
<p>Can anyone point me in the right direction?</p>
",14320652.0,,6331369.0,,2020-12-13 18:34:31,2021-01-24 20:07:03,How to use 'collate_fn' with dataloaders?,<python><pytorch><huggingface-transformers><dataloader>,1,2,0.0,,,CC BY-SA 4.0
66857471,1,,,2021-03-29 15:57:34,,28,36273,"<p>I am trying to install torch with CUDA support.</p>
<p>Here is the result of my <code>collect_env.py</code> script:</p>
<pre><code>PyTorch version: 1.7.1+cu101
Is debug build: False
CUDA used to build PyTorch: 10.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.9 (64-bit runtime)
Is CUDA available: False
CUDA runtime version: 10.1.243
GPU models and configuration: GPU 0: GeForce GTX 1080
Nvidia driver version: 460.39
cuDNN version: Could not collect
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] torch==1.7.1+cu101
[pip3] torchaudio==0.7.2
[pip3] torchvision==0.8.2+cu101
[conda] blas                      1.0                         mkl  
[conda] cudatoolkit               10.1.243             h6bb024c_0  
[conda] mkl                       2020.2                      256  
[conda] mkl-service               2.3.0            py39he8ac12f_0  
[conda] mkl_fft                   1.3.0            py39h54f3939_0  
[conda] mkl_random                1.0.2            py39h63df603_0  
[conda] numpy                     1.19.2           py39h89c1606_0  
[conda] numpy-base                1.19.2           py39h2ae0177_0  
[conda] torch                     1.7.1+cu101              pypi_0    pypi
[conda] torchaudio                0.7.2                    pypi_0    pypi
[conda] torchvision               0.8.2+cu101              pypi_0    pypi

Process finished with exit code 0
</code></pre>
<p>Here is the output of <code>nvcc - V</code></p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
</code></pre>
<p>Finally, here is the output of <code>nvidia-smi</code></p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1080    Off  | 00000000:01:00.0  On |                  N/A |
|  0%   52C    P0    46W / 180W |    624MiB /  8116MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       873      G   /usr/lib/xorg/Xorg                101MiB |
|    0   N/A  N/A      1407      G   /usr/lib/xorg/Xorg                419MiB |
|    0   N/A  N/A      2029      G   ...AAAAAAAAA= --shared-files       90MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>However, when I try to run</p>
<pre><code>print(torch.cuda.is_available())
</code></pre>
<p>I get the following error:</p>
<pre><code>UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)
  return torch._C._cuda_getDeviceCount() &gt; 0
</code></pre>
<p>I have performed a reboot, and I have followed the post-installation steps as detailed in <a href=""https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#mandatory-post"" rel=""noreferrer"">here</a></p>
",1290607.0,,681865.0,,2021-03-29 20:20:24,2023-05-29 01:06:57,CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment,<python><pytorch>,4,4,0.0,,,CC BY-SA 4.0
64776822,1,64776823.0,,2020-11-10 21:10:41,,28,42185,"<p>I know I can access the current GPU using <code>torch.cuda.current_device()</code>, but how can I get a list of all the currently available GPUs?</p>
",5446749.0,,,,,2023-05-16 12:45:18,How do I list all currently available GPUs with pytorch?,<python><pytorch><gpu>,4,1,0.0,,,CC BY-SA 4.0
74394695,1,,,2022-11-10 20:00:55,,28,52169,"<p>I get this error with a pytorch import <code>python -c &quot;import torch&quot;</code>:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/afs/cs.stanford.edu/u/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/__init__.py&quot;, line 13, in &lt;module&gt;
    import torch
  File &quot;/dfs/scratch0/brando9/miniconda/envs/metalearning_gpu/lib/python3.9/site-packages/torch/__init__.py&quot;, line 191, in &lt;module&gt;
    _load_global_deps()
  File &quot;/dfs/scratch0/brando9/miniconda/envs/metalearning_gpu/lib/python3.9/site-packages/torch/__init__.py&quot;, line 153, in _load_global_deps
    ctypes.CDLL(lib_path, mode=ctypes.RTLD_GLOBAL)
  File &quot;/dfs/scratch0/brando9/miniconda/envs/metalearning_gpu/lib/python3.9/ctypes/__init__.py&quot;, line 382, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: /dfs/scratch0/brando9/miniconda/envs/metalearning_gpu/lib/python3.9/site-packages/torch/lib/../../nvidia/cublas/lib/libcublas.so.11: symbol cublasLtHSHMatmulAlgoInit, version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference
</code></pre>
<p>how does one fix it?</p>
<p>related:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/70268140/could-not-load-dynamic-library-libcublaslt-so-11-dlerror-libcublaslt-so-11"">Could not load dynamic library &#39;libcublasLt.so.11&#39;; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory</a></li>
<li><a href=""https://github.com/pytorch/pytorch/issues/51080"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/51080</a></li>
</ul>
",1601580.0,,,,,2023-05-06 17:39:41,"how does one fix when torch can't find cuda, error: version libcublasLt.so.11 not defined in file libcublasLt.so.11 with link time reference?",<pytorch><cuda>,6,0,,,,CC BY-SA 4.0
71998978,1,71999355.0,,2022-04-25 11:42:30,,27,30267,"<p>I tried to implement an early stopping function to avoid my neural network model overfit. I'm pretty sure that the logic is fine, but for some reason, it doesn't work.
I want that when the validation loss is greater than the training loss over some epochs, the early stopping function returns True. But it returns False all the time, even though the validation loss becomes a lot greater than the training loss. Could you see where is the problem, please?</p>
<h1>early stopping function</h1>
<pre><code>def early_stopping(train_loss, validation_loss, min_delta, tolerance):

    counter = 0
    if (validation_loss - train_loss) &gt; min_delta:
        counter +=1
        if counter &gt;= tolerance:
          return True
</code></pre>
<h1>calling the function during the training</h1>
<pre><code>for i in range(epochs):
    
    print(f&quot;Epoch {i+1}&quot;)
    epoch_train_loss, pred = train_one_epoch(model, train_dataloader, loss_func, optimiser, device)
    train_loss.append(epoch_train_loss)

    # validation 

    with torch.no_grad(): 
       epoch_validate_loss = validate_one_epoch(model, validate_dataloader, loss_func, device)
       validation_loss.append(epoch_validate_loss)
    
    # early stopping
    if early_stopping(epoch_train_loss, epoch_validate_loss, min_delta=10, tolerance = 20):
      print(&quot;We are at epoch:&quot;, i)
      break
</code></pre>
<p>EDIT:
The train and validation loss:
<a href=""https://i.stack.imgur.com/CmARg.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/CmARg.png"" alt=""enter image description here"" /></a>
<a href=""https://i.stack.imgur.com/ADCSL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ADCSL.png"" alt=""enter image description here"" /></a></p>
<p>EDIT2:</p>
<pre><code>def train_validate (model, train_dataloader, validate_dataloader, loss_func, optimiser, device, epochs):
    preds = []
    train_loss =  []
    validation_loss = []
    min_delta = 5
    

    for e in range(epochs):
        
        print(f&quot;Epoch {e+1}&quot;)
        epoch_train_loss, pred = train_one_epoch(model, train_dataloader, loss_func, optimiser, device)
        train_loss.append(epoch_train_loss)

        # validation 
        with torch.no_grad(): 
           epoch_validate_loss = validate_one_epoch(model, validate_dataloader, loss_func, device)
           validation_loss.append(epoch_validate_loss)
        
        # early stopping
        early_stopping = EarlyStopping(tolerance=2, min_delta=5)
        early_stopping(epoch_train_loss, epoch_validate_loss)
        if early_stopping.early_stop:
            print(&quot;We are at epoch:&quot;, e)
            break

    return train_loss, validation_loss
</code></pre>
",15959591.0,,15959591.0,,2022-04-26 10:32:20,2022-12-18 05:53:18,early stopping in PyTorch,<python><deep-learning><neural-network><pytorch><early-stopping>,2,0,0.0,,,CC BY-SA 4.0
66516388,1,66517960.0,,2021-03-07 12:12:05,,27,40825,"<p>I want to run a git <a href=""https://github.com/fastnlp/style-transformer"" rel=""noreferrer"">project</a> used pytorch and torchtext but when I run it, it raise error:</p>
<pre><code>  File &quot;main.py&quot;, line 60, in &lt;module&gt;
    main()
  File &quot;main.py&quot;, line 50, in main
    train_iters, dev_iters, test_iters, vocab = load_dataset(config)
  File &quot;/home/esmailza/style transfer/style-transformer/data.py&quot;, line 23, in load_dataset
    TEXT = data.Field(batch_first=True, eos_token='&lt;eos&gt;')
AttributeError: module 'torchtext.data' has no attribute 'Field'
</code></pre>
<p>torch version  = 1.8.0
torchtext version = 0.9</p>
<pre><code>
def load_dataset(config, train_pos='train.pos', train_neg='train.neg',
                 dev_pos='dev.pos', dev_neg='dev.neg',
                 test_pos='test.pos', test_neg='test.neg'):

    root = config.data_path
    TEXT = data.Field(batch_first=True, eos_token='&lt;eos&gt;')
    
    dataset_fn = lambda name: data.TabularDataset(
        path=root + name,
        format='tsv',
        fields=[('text', TEXT)]
    )
</code></pre>
",4110056.0,,4110056.0,,2021-03-07 12:27:15,2021-09-26 11:07:12,AttributeError: module 'torchtext.data' has no attribute 'Field',<python><pytorch>,2,2,0.0,,,CC BY-SA 4.0
64837376,1,69489193.0,,2020-11-14 18:42:22,,26,20715,"<p><strong>Background</strong></p>
<p>I have a very small network which I want to test with different random seeds.
The network barely uses 1% of my GPUs compute power so i could in theory run 50 processes at once to try many different seeds at once.</p>
<p><strong>Problem</strong></p>
<p>Unfortunately i can't even import pytorch in multiple processes. When the <strong>nr of processes</strong> exceeds <strong>4</strong> I get a Traceback regarding a too small paging file.</p>
<p><strong>Minimal reproducable code§ - dispatcher.py</strong></p>
<pre><code>from subprocess import Popen
import sys

procs = []
for seed in range(50):
    procs.append(Popen([sys.executable, &quot;ml_model.py&quot;, str(seed)]))

for proc in procs:
    proc.wait()
</code></pre>
<p>§I increased the number of seeds so people with better machines can also reproduce this.</p>
<p><strong>Minimal reproducable code - ml_model.py</strong></p>
<pre><code>import torch
import time
time.sleep(10)
</code></pre>
<pre><code> 
 Traceback (most recent call last):
  File &quot;ml_model.py&quot;, line 1, in &lt;module&gt;
    import torch
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\__init__.py&quot;, line 117, in &lt;module&gt;
    import torch
  File &quot;C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\__init__.py&quot;, line 117, in &lt;module&gt;
    raise err
 OSError: [WinError 1455] The paging file is too small for this operation to complete. Error loading &quot;C:\Users\user\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\lib\cudnn_cnn_infer64_8.dll&quot; or one of its dependencies.
    raise err

</code></pre>
<p><strong>Further Investigation</strong></p>
<p>I noticed that each process loads a lot of dll's into RAM. And when i close all other programs which use a lot of RAM i can get up to 10 procesess instead of 4. So it seems like a resource constraint.</p>
<p><strong>Questions</strong></p>
<p>Is there a workaround ?</p>
<p>What's the recommended way to train many small networks with pytorch on a single gpu ?</p>
<p>Should i write my own CUDA Kernel instead, or use a different framework to achieve this ?</p>
<p>My goal would be to run around 50 processes at once (on a 16GB RAM Machine, 8GB GPU RAM)</p>
",5130800.0,,,,,2022-10-26 23:10:28,How to efficiently run multiple Pytorch Processes / Models at once ? Traceback: The paging file is too small for this operation to complete,<python><pytorch><python-multiprocessing>,7,4,0.0,,,CC BY-SA 4.0
71498324,1,,,2022-03-16 13:53:45,,25,78200,"<p>While training the model, I encountered the following problem:</p>
<p><code>RuntimeError: CUDA out of memory. Tried to allocate 304.00 MiB (GPU 0; 8.00 GiB total capacity; 142.76 MiB already allocated; 6.32 GiB free; 158.00 MiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>
<p>As we can see, the error occurs when trying to allocate 304 MiB of memory, while 6.32 GiB is free! What is the problem? As I can see, the suggested option is to set max_split_size_mb to avoid fragmentation. Will it help and how to do it correctly?</p>
<p>This is my version of PyTorch:</p>
<p>torch==1.10.2+cu113</p>
<p>torchvision==0.11.3+cu113</p>
<p>torchaudio===0.10.2+cu113</p>
",15790497.0,,681865.0,,2022-03-16 17:53:15,2023-03-21 15:57:33,Pytorch RuntimeError: CUDA out of memory with a huge amount of free memory,<python><machine-learning><pytorch><computer-vision>,4,3,,,,CC BY-SA 4.0
64663385,1,,,2020-11-03 13:03:34,,24,20642,"<p>I am trying to reload a fine-tuned DistilBertForTokenClassification model. I am using transformers 3.4.0 and pytorch version 1.6.0+cu101. After using the Trainer to train the downloaded model, I save the model with trainer.save_model() and in my trouble shooting I save in a different directory via model.save_pretrained(). I am using Google Colab and saving the model to my Google drive. After testing the model I also evaluated the model on my test getting great results, however, when I return to the notebook (or Factory restart the colab notebook) and try to reload the model, the predictions are terrible. Upon checking the directories, the config.json file is there as is the pytorch_mode.bin. Below is the full code.</p>
<pre><code>from transformers import DistilBertForTokenClassification

# load the pretrained model from huggingface
#model = DistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(uniq_labels))
model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased', num_labels=len(uniq_labels)) 

model.to('cuda');

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir = model_dir +  'mitmovie_pt_distilbert_uncased/results',          # output directory
    #overwrite_output_dir = True,
    evaluation_strategy='epoch',
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir = model_dir +  'mitmovie_pt_distilbert_uncased/logs',            # directory for storing logs
    logging_steps=10,
    load_best_model_at_end = True
)

trainer = Trainer(
    model = model,                         # the instantiated 🤗 Transformers model to be trained
    args = training_args,                  # training arguments, defined above
    train_dataset = train_dataset,         # training dataset
    eval_dataset = test_dataset             # evaluation dataset
)

trainer.train()

trainer.evaluate()

model_dir = '/content/drive/My Drive/Colab Notebooks/models/'
trainer.save_model(model_dir + 'mitmovie_pt_distilbert_uncased/model')

# alternative saving method and folder
model.save_pretrained(model_dir + 'distilbert_testing')
</code></pre>
<p>Coming back to the notebook after restarting...</p>
<pre><code>from transformers import DistilBertForTokenClassification, DistilBertConfig, AutoModelForTokenClassification

# retreive the saved model 
model = DistilBertForTokenClassification.from_pretrained(model_dir + 'mitmovie_pt_distilbert_uncased/model', 
                                                        local_files_only=True)

model.to('cuda')
</code></pre>
<p>Model predictions are terrible now from either directory, however, the model does work and outputs the number of classes I would expect, it appears that the actual trained weights have not been saved or are somehow not getting loaded.</p>
",4283043.0,,3890632.0,,2020-11-03 16:15:04,2022-08-30 17:54:08,Saving and reload huggingface fine-tuned transformer,<python><pytorch><huggingface-transformers>,2,4,0.0,,,CC BY-SA 4.0
63539809,1,64829570.0,,2020-08-22 18:37:46,,24,6957,"<p>Looks like the previous paradigm of declaring Fields, Examples and using BucketIterator is deprecated and will move to legacy in 0.8. However, I don't seem to be able to find an example of the new paradigm for custom datasets (as in, not the ones included in torch.datasets) that doesn't use Field. Can anyone point me at an up-to-date example?</p>
<p>Reference for deprecation:</p>
<p><a href=""https://github.com/pytorch/text/releases"" rel=""noreferrer"">https://github.com/pytorch/text/releases</a></p>
",8340222.0,,,,,2022-04-26 10:34:27,Torchtext 0.7 shows Field is being deprecated. What is the alternative?,<pytorch><torchtext>,4,0,0.0,,,CC BY-SA 4.0
66588715,1,,,2021-03-11 18:50:21,,24,58980,"<p>I am trying to run a simple pytorch sample code. It's works fine using CPU. But when using GPU, i get this error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py&quot;, line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py&quot;, line 260, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED
</code></pre>
<p>The code i am trying to run is the following:</p>
<pre><code>import torch
from torch import nn
m = nn.Conv1d(16, 33, 3, stride=2)
m=m.to('cuda')
input = torch.randn(20, 16, 50)
input=input.to('cuda')
output = m(input)
</code></pre>
<p>I am running this code in a NVIDIA docker with CUDA version 10.2 and my GPU is a RTX 2070</p>
",15377810.0,,681865.0,,2021-03-11 22:48:06,2022-12-09 09:23:08,RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED using pytorch,<python><pytorch><gpu>,7,4,0.0,,,CC BY-SA 4.0
65470807,1,65470808.0,,2020-12-27 21:49:31,,23,35959,"<p>In NumPy, I would do</p>
<pre class=""lang-py prettyprint-override""><code>a = np.zeros((4, 5, 6))
a = a[:, :, np.newaxis, :]
assert a.shape == (4, 5, 1, 6)
</code></pre>
<p>How to do the same in PyTorch?</p>
",913098.0,,1165181.0,,2022-04-07 21:32:42,2022-04-07 21:32:42,How to add a new dimension to a PyTorch tensor?,<python><pytorch>,3,0,,,,CC BY-SA 4.0
64156202,1,64156912.0,,2020-10-01 13:16:01,,22,16006,"<p>I want to add a dense layer on top of the bare BERT Model transformer outputting raw hidden-states, and then fine tune the resulting model. Specifically, I am using <a href=""https://huggingface.co/dbmdz/bert-base-italian-xxl-cased"" rel=""noreferrer"">this</a> base model. This is what the model should do:</p>
<ol>
<li>Encode the sentence (a vector with 768 elements for each token of the sentence)</li>
<li>Keep only the first vector (related to the first token)</li>
<li>Add a dense layer on top of this vector, to get the desired transformation</li>
</ol>
<p>So far, I have successfully encoded the sentences:</p>
<pre><code>from sklearn.neural_network import MLPRegressor

import torch

from transformers import AutoModel, AutoTokenizer

# List of strings
sentences = [...]
# List of numbers
labels = [...]

tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/bert-base-italian-xxl-cased&quot;)
model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-italian-xxl-cased&quot;)

# 2D array, one line per sentence containing the embedding of the first token
encoded_sentences = torch.stack([model(**tokenizer(s, return_tensors='pt'))[0][0][0]
                                 for s in sentences]).detach().numpy()

regr = MLPRegressor()
regr.fit(encoded_sentences, labels)
</code></pre>
<p>In this way I can train a neural network by feeding it with the encoded sentences. However, this approach clearly does not fine tune the base BERT model. Can anybody help me? How can I build a model (possibly in pytorch or using the Huggingface library) that can be entirely fine tuned?</p>
",5296106.0,,,,,2023-03-23 12:13:18,Add dense layer on top of Huggingface BERT model,<python><python-3.x><neural-network><pytorch><huggingface-transformers>,3,0,0.0,,,CC BY-SA 4.0
65246703,1,65255500.0,,2020-12-11 06:26:28,,22,31834,"<p>I am working with Text Classification problem where I want to use the BERT model as the base followed by Dense layers. I want to know how does the 3 arguments work? For example, if I have 3 sentences as:</p>
<pre><code>'My name is slim shade and I am an aspiring AI Engineer',
'I am an aspiring AI Engineer',
'My name is Slim'
</code></pre>
<p>SO what will these 3 arguments do? What I think is as follows:</p>
<ol>
<li><code>max_length=5</code> will keep all the sentences as of length 5 strictly</li>
<li><code>padding=max_length</code> will add a padding of 1 to the third sentence</li>
<li><code>truncate=True</code> will truncate the first and second sentence so that their length will be strictly 5.</li>
</ol>
<p>Please correct me if I am wrong.</p>
<p>Below is my code which I have used.</p>
<pre><code>! pip install transformers==3.5.1

from transformers import BertTokenizerFast

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

tokens = tokenizer.batch_encode_plus(text,max_length=5,padding='max_length', truncation=True)
  
text_seq = torch.tensor(tokens['input_ids'])
text_mask = torch.tensor(tokens['attention_mask'])
</code></pre>
",11725056.0,,4108803.0,,2022-11-24 22:00:04,2022-11-24 22:00:04,"How does max_length, padding and truncation arguments work in HuggingFace' BertTokenizerFast.from_pretrained('bert-base-uncased')?",<python><deep-learning><pytorch><bert-language-model><huggingface-tokenizers>,1,0,0.0,,,CC BY-SA 4.0
63522955,1,,,2020-08-21 12:29:24,,22,5109,"<p>When running a PyTorch training program with <code>num_workers=32</code> for <code>DataLoader</code>, <code>htop</code> shows 33 python process each with 32 GB  of <code>VIRT</code> and 15 GB of <code>RES</code>.</p>
<p>Does this mean that the PyTorch training is using 33 processes X 15 GB = 495 GB of memory? <code>htop</code> shows only about 50 GB of RAM and 20 GB of swap is being used on the entire machine with 128 GB of RAM. So, how do we explain the discrepancy?</p>
<p>Is there a more accurate way of calculating the total amount of RAM being used by the main PyTorch program and all its child DataLoader worker processes?</p>
<p>Thank you</p>
",3023615.0,,3023615.0,,2020-08-21 12:49:22,2021-07-29 15:31:18,Understanding Memory Usage by PyTorch DataLoader Workers,<python><python-3.x><ubuntu><deep-learning><pytorch>,2,2,0.0,,,CC BY-SA 4.0
63061779,1,,,2020-07-23 19:27:57,,22,37272,"<p>I am new to Pytorch, but it seems pretty nice. My only question was when to use <code>tensor.to(device)</code> or <code>Module.nn.to(device)</code>.</p>
<p>I was reading the <a href=""https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to"" rel=""noreferrer"">documentation</a> on this topic, and it indicates that this method will move the tensor or model to the specified device. But I was not clear for what operations this is necessary, and what kind of errors I will get if I don't use <code>.to()</code> at the right time?</p>
<p>For example, if I just create a tensor, I imagine that the tensor is stored in CPU accessible memory until I move the tensor to the GPU. Once the tensor is on the GPU, then the GPU will execute any mathematical operations on that tensor.</p>
<p>However, do I have to worry about accidentally transferring the data tensor to the GPU while not transferring the model to the GPU? Will this just give me straight errors, or will it engage in a lot of expensive data transfer behind the scenes. This example is easy enough for me to test, but I was just wondering about other cases where it might not be so obvious.</p>
<p>Any guidance would be helpful.</p>
",1610428.0,,,,,2020-07-24 14:52:21,pytorch when do I need to use `.to(device)` on a model or tensor?,<python><pytorch><gpu><tensor>,1,2,0.0,,,CC BY-SA 4.0
64772335,1,,,2020-11-10 15:59:14,,21,11230,"<p>I'm trying to use a pre-trained template on my image set by following the tutorial right here :
<a href=""https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"" rel=""noreferrer"">https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html</a></p>
<p>Only I always get this &quot;error&quot; when I run my code and the console locks up :</p>
<p><code>[W ParallelNative.cpp:206] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads) </code></p>
<p>Thank you in advance for your help,</p>
",14448931.0,,14448931.0,,2020-11-10 16:22:57,2021-02-22 11:17:47,Pytorch : W ParallelNative.cpp:206,<parallel-processing><pytorch><gpu><cpu><conv-neural-network>,1,1,0.0,,,CC BY-SA 4.0
74289077,1,74331018.0,,2022-11-02 12:20:55,,21,26348,"<p>I am trying to load the dataset using <code>Torch Dataset and DataLoader</code>, but I got the following error:</p>
<pre><code>AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'
</code></pre>
<p>the code I use is:</p>
<pre><code>class WineDataset(Dataset):

    def __init__(self):
        # Initialize data, download, etc.
        # read with numpy or pandas
        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)
        self.n_samples = xy.shape[0]

        # here the first column is the class label, the rest are the features
        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]
        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]

    # support indexing such that dataset[i] can be used to get i-th sample
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # we can call len(dataset) to return the size
    def __len__(self):
        return self.n_samples

    dataset = WineDataset()
        
    train_loader = DataLoader(dataset=dataset,
                              batch_size=4,
                              shuffle=True,
                              num_workers=2)
</code></pre>
<p>I tried to make the num_workers=0, still have the same error.</p>
<pre><code>Python version 3.8.9
PyTorch version 1.13.0
</code></pre>
",4825376.0,,3650983.0,,2023-01-31 15:07:24,2023-04-13 04:24:46,AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next',<python><pytorch><torch><pytorch-dataloader>,3,0,,,,CC BY-SA 4.0
70508960,1,,,2021-12-28 15:13:41,,21,26149,"<p>I have a list of sentences I'm trying to calculate perplexity for, using several models using this code:</p>
<pre><code>from transformers import AutoModelForMaskedLM, AutoTokenizer
import torch
import numpy as np
model_name = 'cointegrated/rubert-tiny'
model = AutoModelForMaskedLM.from_pretrained(model_name).cuda()
tokenizer = AutoTokenizer.from_pretrained(model_name)

def score(model, tokenizer, sentence):
    tensor_input = tokenizer.encode(sentence, return_tensors='pt')
    repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)
    mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]
    masked_input = repeat_input.masked_fill(mask == 1, tokenizer.mask_token_id)
    labels = repeat_input.masked_fill( masked_input != tokenizer.mask_token_id, -100)
    with torch.inference_mode():
        loss = model(masked_input.cuda(), labels=labels.cuda()).loss
    return np.exp(loss.item())


print(score(sentence='London is the capital of Great Britain.', model=model, tokenizer=tokenizer)) 
# 4.541251105675365
</code></pre>
<p>Most models work well, but some sentences seem to throw an error:</p>
<p><code>RuntimeError: CUDA out of memory. Tried to allocate 10.34 GiB (GPU 0; 23.69 GiB total capacity; 10.97 GiB already allocated; 6.94 GiB free; 14.69 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>
<p>Which makes sense because some are very long. So what I did was to add something like <code>try, except RuntimeError, pass</code>.</p>
<p>This seemed to work until around 210 sentences, and then it just outputs the error:</p>
<p><code>CUDA error: an illegal memory access was encountered CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1.</code></p>
<p>I found <a href=""https://github.com/pytorch/pytorch/issues/21819"" rel=""noreferrer"">this</a> which had a lot of discussions and ideas, some were regarding potential faulty GPUs? But I know that my GPU works as this exact code works for other models. There's also talk about batch size <a href=""https://github.com/pytorch/pytorch/issues/46974"" rel=""noreferrer"">here</a>, which is why I thought it potentially relates to freeing up memory.</p>
<p>I tried running <code>torch.cuda.empty_cache()</code> to free the memory like in <a href=""https://stackoverflow.com/questions/68106457/pytorch-cuda-error-an-illegal-memory-access-was-encountered"">here</a> after every some epochs but it didn't work (threw the same error).</p>
<p><strong>Update:</strong>
I filtered sentences with length over 550 and this seems to remove the <code>CUDA error: an illegal memory access was encountered CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1.</code> error.</p>
",14735451.0,,14735451.0,,2022-01-06 05:31:54,2022-01-06 11:05:39,How to free GPU memory in PyTorch,<python><memory><pytorch><huggingface-transformers>,2,3,0.0,,,CC BY-SA 4.0
64914598,1,,,2020-11-19 15:17:26,,21,29280,"<p>I am running a BERT model on torch. It's a multi-class sentiment classification task with about 30,000 rows. I have already put everything on cuda, but not sure why I'm getting the following run time error. Here is my code:</p>
<pre><code>for epoch in tqdm(range(1, epochs+1)):
    
    model.train()
    
    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }       

        outputs = model(**inputs)
        
        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})
         
        
    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')
        
    tqdm.write(f'\nEpoch {epoch}')
    
    loss_train_avg = loss_train_total/len(dataloader_train)            
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-67-9306225bb55a&gt; in &lt;module&gt;()
     17                  }       
     18 
---&gt; 19         outputs = model(**inputs)
     20 
     21         loss = outputs[0]

8 frames
/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1850         # remove once script supports set_grad_enabled
   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1853 
   1854 

RuntimeError: Input, output and indices must be on the current device
</code></pre>
<p>Any suggestions would be appreciated. Thanks!</p>
",12210377.0,,12210377.0,,2020-11-19 15:22:42,2021-01-06 20:46:30,"PyTorch: RuntimeError: Input, output and indices must be on the current device",<python><nlp><pytorch><bert-language-model>,1,2,0.0,,,CC BY-SA 4.0
65192475,1,65193236.0,,2020-12-08 03:00:12,,20,23762,"<p>I understand that PyTorch's LogSoftmax function is basically just a more numerically stable way to compute <code>Log(Softmax(x))</code>. Softmax lets you convert the output from a Linear layer into a categorical probability distribution.</p>
<p>The <a href=""https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"" rel=""noreferrer"">pytorch documentation</a> says that CrossEntropyLoss combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.</p>
<p>Looking at <code>NLLLoss</code>, I'm still confused...Are there 2 logs being used? I think of negative log as information content of an event. (As in <a href=""https://en.wikipedia.org/wiki/Entropy_(information_theory)"" rel=""noreferrer"">entropy</a>)</p>
<p>After a bit more looking, I think that <code>NLLLoss</code> assumes that you're actually passing in log probabilities instead of just probabilities. Is this correct? It's kind of weird if so...</p>
",600791.0,,,,,2023-01-06 08:20:29,PyTorch LogSoftmax vs Softmax for CrossEntropyLoss,<pytorch><cross-entropy>,1,0,0.0,,,CC BY-SA 4.0
67263288,1,,,2021-04-26 08:31:19,,20,14093,"<p>I am trying to use the roberta transformer and a pre-trained model but I keep getting this error:</p>
<pre><code>    ImportError: 
AutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment. Checkout the instructions on the
installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
</code></pre>
<p>Here's my code:</p>
<pre><code># Tasks:
# emoji, emotion, hate, irony, offensive, sentiment
# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary

task='sentiment'
MODEL = f&quot;cardiffnlp/twitter-roberta-base-{task}&quot;

tokenizer = AutoTokenizer.from_pretrained(MODEL)
# download label mapping
labels=[]
mapping_link = f&quot;https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt&quot;
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)
labels=[]
mapping_link = f&quot;https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt&quot;
with urllib.request.urlopen(mapping_link) as f:
    html = f.read().decode('utf-8').split(&quot;\n&quot;)
    csvreader = csv.reader(html, delimiter='\t')
labels = [row[1] for row in csvreader if len(row) &gt; 1]
</code></pre>
<p>I made sure that PyTorch is installed and working:</p>
<p><a href=""https://i.stack.imgur.com/yLhj0.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yLhj0.png"" alt=""enter image description here"" /></a></p>
",5743155.0,,,,,2021-05-08 14:02:51,AutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment,<python><pytorch><roberta>,2,2,0.0,,,CC BY-SA 4.0
70110429,1,70123569.0,,2021-11-25 11:37:52,,20,36503,"<p>I have a model which looks as follows:</p>
<pre class=""lang-py prettyprint-override""><code>IMG_WIDTH = IMG_HEIGHT = 224

class AlexNet(nn.Module):
  def __init__(self, output_dim):
    super(AlexNet, self).__init__()
    self._to_linear = None
    self.x = torch.randn(3, IMG_WIDTH, IMG_HEIGHT).view(-1, 3, IMG_WIDTH, IMG_HEIGHT)
    self.features = nn.Sequential(
        nn.Conv2d(3, 64, 3, 2, 1), # in_channels, out_channels, kernel_size, stride, padding
        nn.MaxPool2d(2),
        nn.ReLU(inplace=True),
        nn.Conv2d(64, 192, 3, padding=1),
        nn.MaxPool2d(2),
        nn.ReLU(inplace=True), 
        nn.Conv2d(192, 384, 3, padding=1),
        nn.MaxPool2d(2),
        nn.ReLU(inplace=True), 
        nn.Conv2d(384, 256, 3, padding=1),
        nn.MaxPool2d(2),
        nn.ReLU(inplace=True),
        nn.Conv2d(256, 512, 3, padding=1),
        nn.ReLU(inplace=True),
        nn.Conv2d(512, 256, 3, padding=1),
        nn.MaxPool2d(2),
        nn.ReLU(inplace=True)
  )
    self.conv(self.x)
    self.classifier = nn.Sequential(
        nn.Dropout(.5),
        nn.Linear(self._to_linear, 4096),
        nn.ReLU(inplace=True),
        nn.Dropout(.5),
        nn.Linear(4096, 4096),
        nn.ReLU(inplace=True),
        nn.Linear(4096, output_dim),
    )

  def conv(self, x):
    x = self.features(x)
    if self._to_linear is None:
        self._to_linear = x.shape[1] * x.shape[2] * x.shape[3]
    return x

  def forward(self, x):
    x = self.conv(x)
    h = x.view(x.shape[0], -1)
    x = self.classifier(h)
    return x, h
</code></pre>
<p>Here is my optimizer and loss functions:</p>
<pre class=""lang-py prettyprint-override""><code>optimizer = torch.optim.Adam(model.parameters())
criterion = nn.BCEWithLogitsLoss().to(device)
</code></pre>
<p>Here is my <code>train</code> and <code>evaluate</code> functions:</p>
<pre class=""lang-py prettyprint-override""><code>def train(model, iterator, optimizer, criterion, device):
  epoch_loss, epoch_acc = 0, 0
  model.train()
  for (x, y) in iterator:
    # features and labels to the device
    x = x.to(device)
    y = y.to(device).long()
    # Zero the gradients
    optimizer.zero_grad()
    y_pred, _ = model(x)
  
    # Calculate the loss and accuracy
    loss = criterion(y_pred.squeeze(), y)
    acc = binary_accuracy(y_pred, y)
    # Backward propagate
    loss.backward()
    # Update the weights
    optimizer.step()

    epoch_loss +=loss.item()
    epoch_acc += acc.item()

  return epoch_loss/len(iterator), epoch_acc/len(iterator)

def evaluate(model, iterator, criterion, device):
  epoch_loss, epoch_acc = 0, 0
  model.eval()
  with torch.no_grad():
    for (x, y) in iterator:
      x = x.to(device)
      y = y.to(device).long()
      y_pred, _ = model(x)
      loss = criterion(y_pred, y)
      acc = binary_accuracy(y_pred, y)

      epoch_loss += loss.item()
      epoch_acc += acc.item()
  return epoch_loss/len(iterator), epoch_acc/len(iterator)
</code></pre>
<p>This is the error that I'm getting:</p>
<pre><code>RuntimeError: result type Float can't be cast to the desired output type Long
</code></pre>
<p>What may be possibly my problem because I have tried to convert my labels to <code>long</code> tensors as follows:</p>
<pre><code>y = y.to(device).long()
</code></pre>
<p>But it seems not to work.</p>
",12925831.0,,,,,2022-08-04 14:00:00,Pytorch: RuntimeError: result type Float can't be cast to the desired output type Long,<python><deep-learning><pytorch>,2,2,,,,CC BY-SA 4.0
66600362,1,66850736.0,,2021-03-12 12:58:21,,20,67649,"<p>I'm working on the CNN with one-dimensional signal. It works totally fine with CPU device. However, when I training model in GPU, CUDA error occurred. I set <code>os.environ['CUDA_LAUNCH_BLOCKING'] = &quot;1&quot;</code> command after I got <code>RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED</code> when calling <code>cublasCreate(handle)</code>. With doing this, a <code>cublasSgemm</code> error occurred instead of <code>cublasCreate</code> error.
Though the nvidia document doubt the hardware problem, I can training other CNN with images without any error. Below is my code for the data loading and set data in training model.</p>
<pre><code>    idx = np.arange(len(dataset))  # dataset &amp; label shuffle in once
    np.random.shuffle(idx)

    dataset = dataset[idx]
    sdnn = np.array(sdnn)[idx.astype(int)]        

    train_data, val_data = dataset[:int(0.8 * len(dataset))], dataset[int(0.8 * len(dataset)):]
    train_label, val_label = sdnn[:int(0.8 * len(sdnn))], sdnn[int(0.8 * len(sdnn)):]
    train_set = DataLoader(dataset=train_data, batch_size=opt.batch_size, num_workers=opt.workers)

    for i, data in enumerate(train_set, 0):  # data.shape = [batch_size, 3000(len(signal)), 1(channel)] tensor

        x = data.transpose(1, 2)
        label = torch.Tensor(train_label[i * opt.batch_size:i * opt.batch_size + opt.batch_size])
        x = x.to(device, non_blocking=True)
        label = label.to(device, non_blocking=True) # [batch size]
        label = label.view([len(label), 1])
        optim.zero_grad()

        # Feature of signal extract
        y_predict = model(x) # [batch size, fc3 output] # Error occurred HERE
        loss = mse(y_predict, label)
</code></pre>
<p>Below is the error message from this code.</p>
<pre><code>File C:/Users/Me/Desktop/Me/Study/Project/Analysis/Regression/main.py&quot;, line 217, in Processing
    y_predict = model(x) # [batch size, fc3 output]
  File &quot;C:\Anaconda\envs\torch\lib\site-packages\torch\nn\modules\module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\ME\Desktop\ME\Study\Project\Analysis\Regression\cnn.py&quot;, line 104, in forward
    x = self.fc1(x)
  File &quot;C:\Anaconda\envs\torch\lib\site-packages\torch\nn\modules\module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Anaconda\envs\torch\lib\site-packages\torch\nn\modules\linear.py&quot;, line 91, in forward
    return F.linear(input, self.weight, self.bias)
  File &quot;C:\Anaconda\envs\torch\lib\site-packages\torch\nn\functional.py&quot;, line 1674, in linear
    ret = torch.addmm(bias, input, weight.t())
RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &amp;alpha, a, lda, b, ldb, &amp;beta, c, ldc)`
</code></pre>
<p>I've tried to solve this error for weeks but can't find the solution. If you can see anything wrong here, please let me know.</p>
",10621921.0,,681865.0,,2021-03-12 13:37:40,2023-05-24 20:03:54,RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle)` with GPU only,<python><pytorch><gpu><conv-neural-network>,6,3,0.0,,,CC BY-SA 4.0
62907815,1,62908532.0,,2020-07-15 04:53:05,,20,12446,"<p>In PyTorch, what is the difference between the following two methods in sending a tensor (or model) to GPU:</p>
<p>Setup:</p>
<pre><code>X = np.array([[1, 3, 2, 3], [2, 3, 5, 6], [1, 2, 3, 4]]) # X = model()
X = torch.DoubleTensor(X)
</code></pre>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Method 1</th>
<th>Method 2</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>X.cuda()</code></td>
<td><code>device = torch.device(&quot;cuda:0&quot;)</code><br><code>X = X.to(device)</code></td>
</tr>
</tbody>
</table>
</div><hr />
<p>(I don't really need a detailed explanation of what is happening in the backend, just want to know if they are both essentially doing the same thing)</p>
",12462568.0,,9067615.0,,2021-04-15 06:56:37,2021-04-15 07:00:17,"PyTorch: What is the difference between tensor.cuda() and tensor.to(torch.device(""cuda:0""))?",<python><pytorch><gpu>,2,0,0.0,,,CC BY-SA 4.0
63263292,1,63263351.0,,2020-08-05 10:32:58,,20,13974,"<p>What is the difference between <code>Tensor.size</code> and <code>Tensor.shape</code> in Pytorch?
I want to get the number of elements and the dimensions of Tensor. For example for a tensor with the dimensions of 2 by 3 by 4 I expect 24 for number of elements and (2,3,4) for dimension.
Thanks.</p>
",11922226.0,,4685471.0,,2020-08-05 11:28:12,2022-05-03 03:18:37,What is the difference between Tensor.size and Tensor.shape in PyTorch?,<pytorch>,2,3,0.0,,,CC BY-SA 4.0
65381244,1,65381245.0,,2020-12-20 14:58:25,,19,30602,"<p>I have a tensor</p>
<p><code>t = torch.zeros((4, 5, 6))</code></p>
<p>How to check if it is on gpu or not, and send it to gpu and back?</p>
",913098.0,,913098.0,,2022-10-25 17:19:00,2022-11-05 01:47:40,How to check if a tensor is on cuda or send it to cuda in Pytorch?,<python><pytorch><gpu><tensor>,2,0,0.0,,,CC BY-SA 4.0
65344578,1,65344579.0,,2020-12-17 16:27:34,,19,6472,"<p>How to check from within a model if it is currently in train or eval mode?</p>
",913098.0,,913098.0,,2020-12-17 22:01:10,2020-12-17 22:01:10,How to check if a model is in train or eval mode in Pytorch?,<python><machine-learning><deep-learning><pytorch>,1,0,0.0,,,CC BY-SA 4.0
66226135,1,,,2021-02-16 14:18:43,,19,2053,"<p>I want to parallelize over single examples or batch of example (in my situation is that I only have cpus, I have up to 112). I tried it but I get a bug that the losses cannot have the gradient out of separate processes (which entirely ruins my attempt). I still want to do it and it essential that after the multiproessing happens that I can do an optimizer step. How do I get around it? I made a totally self contained example:</p>
<pre><code>
import torch
import torch.nn as nn
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Dataset, DataLoader

from torch.multiprocessing import Pool

class SimpleDataSet(Dataset):

    def __init__(self, Din, num_examples=23):
        self.x_dataset = [torch.randn(Din) for _ in range(num_examples)]
        # target function is x*x
        self.y_dataset = [x**2 for x in self.x_dataset]

    def __len__(self):
        return len(self.x_dataset)

    def __getitem__(self, idx):
        return self.x_dataset[idx], self.y_dataset[idx]

def get_loss(args):
    x, y, model = args
    y_pred = model(x)
    criterion = nn.MSELoss()
    loss = criterion(y_pred, y)
    return loss

def get_dataloader(D, num_workers, batch_size):
    ds = SimpleDataSet(D)
    dl = DataLoader(ds, batch_size=batch_size, num_workers=num_workers)
    return dl

def train_fake_data():
    num_workers = 2
    Din, Dout = 3, 1
    model = nn.Linear(Din, Dout).share_memory()

    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)

    batch_size = 2
    num_epochs = 10
    # num_batches = 5
    num_procs = 5
    dataloader = get_dataloader(Din, num_workers, batch_size)
    scheduler = StepLR(optimizer, step_size=1, gamma=0.7)
    for epoch in range(num_epochs):
        for _, batch in enumerate(dataloader):
            batch = [(torch.randn(Din), torch.randn(Dout), model) for _ in batch]
            with Pool(num_procs) as pool:
                optimizer.zero_grad()

                losses = pool.map(get_loss, batch)
                loss = torch.mean(losses)
                loss.backward()

                optimizer.step()
            # scheduler
            scheduler.step()


if __name__ == '__main__':
    # start = time.time()
    # train()
    train_fake_data()
    # print(f'execution time: {time.time() - start}')
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/Users/brando/anaconda3/envs/coq_gym/lib/python3.7/site-packages/IPython/core/interactiveshell.py&quot;, line 3427, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File &quot;&lt;ipython-input-2-ea57e03ba088&gt;&quot;, line 1, in &lt;module&gt;
    runfile('/Users/brando/ML4Coq/playground/multiprocessing_playground/multiprocessing_cpu_pytorch.py', wdir='/Users/brando/ML4Coq/playground/multiprocessing_playground')
  File &quot;/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_bundle/pydev_umd.py&quot;, line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File &quot;/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydev_imps/_pydev_execfile.py&quot;, line 18, in execfile
    exec(compile(contents+&quot;\n&quot;, file, 'exec'), glob, loc)
  File &quot;/Users/brando/ML4Coq/playground/multiprocessing_playground/multiprocessing_cpu_pytorch.py&quot;, line 95, in &lt;module&gt;
    train_fake_data()
  File &quot;/Users/brando/ML4Coq/playground/multiprocessing_playground/multiprocessing_cpu_pytorch.py&quot;, line 83, in train_fake_data
    losses = pool.map(get_loss, batch)
  File &quot;/Users/brando/anaconda3/envs/coq_gym/lib/python3.7/multiprocessing/pool.py&quot;, line 290, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File &quot;/Users/brando/anaconda3/envs/coq_gym/lib/python3.7/multiprocessing/pool.py&quot;, line 683, in get
    raise self._value
multiprocessing.pool.MaybeEncodingError: Error sending result: '[tensor(0.5237, grad_fn=&lt;MseLossBackward&gt;)]'. Reason: 'RuntimeError('Cowardly refusing to serialize non-leaf tensor which requires_grad, since autograd does not support crossing process boundaries.  If you just want to transfer the data, call detach() on the tensor before serializing (e.g., putting it on the queue).')'
</code></pre>
<p>I am sure I want to do this. How should I be doing this?</p>
<hr />
<h1>New attempt using DDP</h1>
<pre><code>&quot;&quot;&quot;
Based on: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html

Note: as opposed to the multiprocessing (torch.multiprocessing) package, processes can use
different communication backends and are not restricted to being executed on the same machine.
&quot;&quot;&quot;
import torch
from torch import nn, optim
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP

import os

num_epochs = 5
batch_size = 8
Din, Dout = 10, 5
data_x = torch.randn(batch_size, Din)
data_y = torch.randn(batch_size, Dout)
data = [(i*data_x, i*data_y) for i in range(num_epochs)]

class OneDeviceModel(nn.Module):
    &quot;&quot;&quot;
    Toy example for a model ran in parallel but not distributed accross gpus
    (only processes with their own gpu or hardware)
    &quot;&quot;&quot;
    def __init__(self):
        super().__init__()
        self.net1 = nn.Linear(Din, Din)
        self.relu = nn.ReLU()
        self.net2 = nn.Linear(Din, Dout)

    def forward(self, x):
        return self.net2(self.relu(self.net1(x)))

def setup_process(rank, world_size, backend='gloo'):
    &quot;&quot;&quot;
    Initialize the distributed environment (for each process).

    gloo: is a collective communications library (https://github.com/facebookincubator/gloo). My understanding is that
    it's a library/API for process to communicate/coordinate with each other/master. It's a backend library.
    &quot;&quot;&quot;
    # set up the master's ip address so this child process can coordinate
    # os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'

    # - use NCCL if you are using gpus: https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends
    if torch.cuda.is_available():
        backend = 'nccl'
    # Initializes the default distributed process group, and this will also initialize the distributed package.
    dist.init_process_group(backend, rank=rank, world_size=world_size)

def cleanup():
    &quot;&quot;&quot; Destroy a given process group, and deinitialize the distributed package &quot;&quot;&quot;
    dist.destroy_process_group()

def run_parallel_training_loop(rank, world_size):
    &quot;&quot;&quot;
    Distributed function to be implemented later.

    This is the function that is actually ran in each distributed process.

    Note: as DDP broadcasts model states from rank 0 process to all other processes in the DDP constructor,
    you don’t need to worry about different DDP processes start from different model parameter initial values.
    &quot;&quot;&quot;
    print()
    print(f&quot;Start running DDP with model parallel example on rank: {rank}.&quot;)
    print(f'current process: {mp.current_process()}')
    print(f'pid: {os.getpid()}')
    setup_process(rank, world_size)

    # create model and move it to GPU with id rank
    model = OneDeviceModel().to(rank) if torch.cuda.is_available() else OneDeviceModel().share_memory()
    # ddp_model = DDP(model, device_ids=[rank])
    ddp_model = DDP(model)

    for batch_idx, batch in enumerate(data):
        x, y = batch
        loss_fn = nn.MSELoss()
        optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

        optimizer.zero_grad()
        outputs = ddp_model(x)
        labels = y.to(rank) if torch.cuda.is_available() else y
        # Gradient synchronization communications take place during the backward pass and overlap with the backward computation.
        loss_fn(outputs, labels).backward()  # When the backward() returns, param.grad already contains the synchronized gradient tensor.
        optimizer.step()  # TODO how does the optimizer know to do the gradient step only once?

    print()
    print(f&quot;Start running DDP with model parallel example on rank: {rank}.&quot;)
    print(f'current process: {mp.current_process()}')
    print(f'pid: {os.getpid()}')
    # Destroy a given process group, and deinitialize the distributed package
    cleanup()

def main():
    print()
    print('running main()')
    print(f'current process: {mp.current_process()}')
    print(f'pid: {os.getpid()}')
    # args
    world_size = mp.cpu_count()
    mp.spawn(run_parallel_training_loop, args=(world_size,), nprocs=world_size)

if __name__ == &quot;__main__&quot;:
    print('starting __main__')
    main()
    print('Done!\a\n')
</code></pre>
<p>it seems it works but my question is in line 74 do I need to do this</p>
<pre><code>    model = OneDeviceModel().to(rank) if torch.cuda.is_available() else OneDeviceModel().share_memory()
</code></pre>
<p>or</p>
<pre><code>    model = OneDeviceModel().to(rank) if torch.cuda.is_available() else OneDeviceModel()
</code></pre>
<p>for it to work properly in multiple CPUs?</p>
<hr />
<h1>Serial is faster than parallel even if I have 112 cpu cores?</h1>
<p>My current issue is that the cpu parallel job is slower than the serially running one when only cpus are available.</p>
<hr />
<p>I want to know how to set up python and parallel cpus. e.g. if I have X cpus how many processes should I be running...X? or what? How do I choose this number, even if its heursitics rough.</p>
<hr />
<p>related links from research:</p>
<ul>
<li><a href=""https://discuss.pytorch.org/t/multiprocessing-for-loop-on-cpu/59836"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/multiprocessing-for-loop-on-cpu/59836</a></li>
<li><a href=""https://stackoverflow.com/questions/56174874/how-to-use-multiprocessing-in-pytorch"">How to use multiprocessing in PyTorch?</a></li>
<li><a href=""https://discuss.pytorch.org/t/how-to-parallelize-a-loop-over-the-samples-of-a-batch/32698/7"" rel=""nofollow noreferrer"">https://discuss.pytorch.org/t/how-to-parallelize-a-loop-over-the-samples-of-a-batch/32698/7</a></li>
<li><a href=""https://www.reddit.com/r/pytorch/comments/sm073v/how_to_parallelize_a_training_loop_ever_samples/"" rel=""nofollow noreferrer"">https://www.reddit.com/r/pytorch/comments/sm073v/how_to_parallelize_a_training_loop_ever_samples/</a></li>
</ul>
",1601580.0,,1601580.0,,2022-02-06 16:01:45,2022-02-13 11:11:20,How to parallelize a training loop ever samples of a batch when CPU is only available in pytorch?,<machine-learning><deep-learning><multiprocessing><pytorch><conv-neural-network>,1,8,0.0,,,CC BY-SA 4.0
66626700,1,66626867.0,,2021-03-14 16:11:59,,18,27650,"<p>I have a quick (and possibly silly) question about how Tensorflow defines its Linear layer. Within PyTorch, a Linear (or Dense) layer is defined as, y = x A^T + b where A and b are the weight matrix and bias vector for a Linear layer (see <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear"" rel=""noreferrer"">here</a>).</p>
<p>However, I can't precisely find an equivalent equation for Tensorflow! Is it the same as PyTorch or is it just y = x A + b ?</p>
<p>Thank you in advance!</p>
",6300467.0,,,,,2021-03-14 17:36:41,Difference between Tensorflow's tf.keras.layers.Dense and PyTorch's torch.nn.Linear?,<tensorflow><pytorch>,2,0,0.0,,,CC BY-SA 4.0
68820453,1,72293634.0,,2021-08-17 15:50:13,,18,33442,"<p>I tried to train a model using PyTorch on my Macbook pro. It uses the new generation apple M1 CPU. However, PyTorch couldn't recognize my GPUs.</p>
<pre><code>GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
</code></pre>
<p>Does anyone know any solution?</p>
<p>I have updated all the libraries to the latest versions.</p>
",3904529.0,,8372853.0,,2022-05-24 00:50:48,2023-04-20 19:40:58,How to run Pytorch on Macbook pro (M1) GPU?,<pytorch><gpu><apple-m1>,3,2,0.0,,,CC BY-SA 4.0
66488807,1,66562064.0,,2021-03-05 07:59:52,,18,40900,"<p>I loaded a custom PyTorch model and I want to find out its input shape. Something like this:</p>
<pre><code>model.input_shape
</code></pre>
<p>Is it possible to get this information?</p>
<hr />
<p><strong>Update:</strong> <code>print()</code> and <code>summary()</code> don't show this model's input shape, so they are not what I'm looking for.</p>
",6013016.0,,6013016.0,,2021-03-26 03:30:54,2022-01-26 15:40:16,PyTorch model input shape,<python><deep-learning><pytorch><conv-neural-network>,3,8,0.0,,,CC BY-SA 4.0
74219480,1,,,2022-10-27 09:10:15,,18,18986,"<p>I recently upgraded to Python 3.11 and proceeded to install the libraries I typically use for 3.11. I went through my list one by one with <code>pip</code>.</p>
<p>When I tried to install PyTorch, I got an error which says:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch
</code></pre>
<p>None of the suggestions I could find on pytorch.com resolved the issue.</p>
<p>What went wrong, and what do I need to do in order to install PyTorch properly?</p>
",20347179.0,,523612.0,,2023-01-26 11:23:53,2023-03-13 13:59:10,Cannot install PyTorch with Python 3.11 (Windows),<python><pip><pytorch><python-3.11>,3,1,,,,CC BY-SA 4.0
65343377,1,65344276.0,,2020-12-17 15:12:56,,17,30352,"<p>In the paper <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""noreferrer"">Attention is all you need</a>, under section 5.3, the authors suggested to increase the learning rate linearly and then decrease proportionally to the inverse square root of steps.</p>
<p><a href=""https://i.stack.imgur.com/GQurA.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GQurA.png"" alt=""Paper SS"" /></a></p>
<p>How do we implement this in PyTorch with <a href=""https://pytorch.org/docs/stable/optim.html#torch.optim.Adam"" rel=""noreferrer"">Adam</a> optimizer? Preferably without additional packages.</p>
",2611189.0,,,,,2023-03-17 18:47:11,Adam optimizer with warmup on PyTorch,<python><machine-learning><pytorch>,5,1,,,,CC BY-SA 4.0
64047261,1,,,2020-09-24 13:13:53,,17,13389,"<p>I'm trying to train a model using a Trainer, according to the documentation (<a href=""https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer"" rel=""noreferrer"">https://huggingface.co/transformers/master/main_classes/trainer.html#transformers.Trainer</a>) I can specify a tokenizer:</p>
<blockquote>
<p>tokenizer (PreTrainedTokenizerBase, optional) – The tokenizer used to
preprocess the data. If provided, will be used to automatically pad
the inputs the maximum length when batching inputs, and it will be
saved along the model to make it easier to rerun an interrupted
training or reuse the fine-tuned model.</p>
</blockquote>
<p>So padding should be handled automatically, but when trying to run it I get this error:</p>
<blockquote>
<p>ValueError: Unable to create tensor, you should probably activate
truncation and/or padding with 'padding=True' 'truncation=True' to
have batched tensors with the same length.</p>
</blockquote>
<p>The tokenizer is created this way:</p>
<pre><code>tokenizer = BertTokenizerFast.from_pretrained(pretrained_model)
</code></pre>
<p>And the Trainer like that:</p>
<pre><code>trainer = Trainer(
    tokenizer=tokenizer,
    model=model,
    args=training_args,
    train_dataset=train,
    eval_dataset=dev,
    compute_metrics=compute_metrics
)
</code></pre>
<p>I've tried putting the <code>padding</code> and <code>truncation</code> parameters in the tokenizer, in the Trainer, and in the training_args. Nothing does. Any idea?</p>
",4657751.0,,,,,2023-05-09 12:33:16,How to make a Trainer pad inputs in a batch with huggingface-transformers?,<python><pytorch><huggingface-transformers>,4,2,0.0,,,CC BY-SA 4.0
64334033,1,64334237.0,,2020-10-13 11:06:02,,17,52671,"<p>I'm trying to run this code. I don't know what is wrong with it, but this code is not running. and I don't know how to solve this problem.</p>
<pre><code>import cv2
from facial_emotion_recognition import EmotionRecognition

emotion_detector = EmotionRecognition(device='gpu', gpu_id=1)
camera = cv2.VideoCapture(0)

while True:
    image = camera.read()[1]
    image = emotion_detector.recognise_emotion(image, return_type='BGR')
    cv2.imshow('Camera', image)

    key = cv2.waitKey(1)
    if key == 27:
        break

camera.release()
cv2.destroyAllWindows()
</code></pre>
<p>but I'm getting this error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/fahim/Documents/Python_projects/Python tutorials/pantech AI Master/Computer_Vision/Day 8 Face emotion recognition/emotion.py&quot;, line 4, in &lt;module&gt;
    emotion_detector = EmotionRecognition(device='gpu', gpu_id=1)
  File &quot;/home/fahim/anaconda3/envs/Computer_Vision/lib/python3.7/site-packages/facial_emotion_recognition/facial_emotion_recognition.py&quot;, line 25, in __init__
    self.network = NetworkV2(in_c=1, nl=32, out_f=7).to(self.device)
  File &quot;/home/fahim/anaconda3/envs/Computer_Vision/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 607, in to
    return self._apply(convert)
  File &quot;/home/fahim/anaconda3/envs/Computer_Vision/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 354, in _apply
    module._apply(fn)
  File &quot;/home/fahim/anaconda3/envs/Computer_Vision/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 354, in _apply
    module._apply(fn)
  File &quot;/home/fahim/anaconda3/envs/Computer_Vision/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 376, in _apply
    param_applied = fn(param)
  File &quot;/home/fahim/anaconda3/envs/Computer_Vision/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 605, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA error: invalid device ordinal

Process finished with exit code 1
</code></pre>
<p>This is my the configuration of my computer:
GPU: NVIDIA GeForce MX130
CPU: Intel i5-10210U (8) @ 4.200GHz
Help me to solve this please.</p>
",11642901.0,,,,,2022-12-06 23:02:07,"How to solve ""RuntimeError: CUDA error: invalid device ordinal""?",<python><opencv><pytorch>,3,0,,,,CC BY-SA 4.0
63751883,1,64422522.0,,2020-09-05 08:22:05,,17,26067,"<p>I'm trying to use GPU from inside my docker container. I'm using docker with version 19.03 on Ubuntu 18.04.</p>
<p>Outside the docker container if I run <code>nvidia-smi</code> I get the below output:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   30C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>If I run the same thing inside the container created from <code>nvidia/cuda</code> docker image, I get the same output as above and everything is running smoothly. <code>torch.cuda.is_available()</code> returns <code>True</code>.</p>
<p>But if I run the same <code>nvidia-smi</code> command inside any other docker container, it gives the following output where you can see that the CUDA Version is coming as <strong>N/A</strong>. Inside the containers <code>torch.cuda.is_available()</code> also returns <code>False</code>.</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: N/A      |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |
| N/A   30C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>I have installed <code>nvidia-container-toolkit</code> using the following commands.</p>
<pre><code>curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu18.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
sudo apt-get install nvidia-container-toolkit
sudo systemctl restart docker
</code></pre>
<p>I started my containers using the following commands</p>
<pre><code>sudo docker run --rm --gpus all nvidia/cuda nvidia-smi
sudo docker run -it --rm --gpus all ubuntu nvidia-smi
</code></pre>
",6733421.0,,681865.0,,2023-04-02 05:58:37,2023-04-02 05:58:37,Using GPU inside docker container - CUDA Version: N/A and torch.cuda.is_available returns False,<docker><docker-compose><pytorch><nvidia-docker>,2,7,0.0,,,CC BY-SA 4.0
64593792,1,,,2020-10-29 15:03:00,,17,20862,"<p>I'm using a laptop which has Intel Corporation HD Graphics 520.
Does anyone know how to it set up for Deep Learning, specifically Pytorch? I have seen if you have Nvidia graphics I can install cuda but what to do when you have intel GPU?</p>
",12860141.0,,2736559.0,,2022-04-25 10:09:29,2023-01-19 08:59:14,How to make Intel GPU available for processing through pytorch?,<deep-learning><pytorch><gpu><intel>,2,1,0.0,,,CC BY-SA 4.0
66807131,1,,,2021-03-25 20:28:40,,17,27039,"<p>I've seen multiple issue about the:</p>
<pre><code>RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1614378083779/work/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled cuda error, NCCL version 2.7.8
ncclUnhandledCudaError: Call to CUDA function failed.
</code></pre>
<p>but none seem to fix it for me:</p>
<ul>
<li><a href=""https://github.com/pytorch/pytorch/issues/54550"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/54550</a></li>
<li><a href=""https://github.com/pytorch/pytorch/issues/47885"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/47885</a></li>
<li><a href=""https://github.com/pytorch/pytorch/issues/50921"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/50921</a></li>
<li><a href=""https://github.com/pytorch/pytorch/issues/54823"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/54823</a></li>
</ul>
<p>I've tried to do <code>torch.cuda.set_device(device)</code> manually at the beginning of every script. That didn't seem to work for me. I've tried different GPUS. I've tried downgrading pytorch version and cuda version. Different combinations of 1.6.0, 1.7.1, 1.8.0 and cuda 10.2, 11.0, 11.1. I am unsure what else to do. What did people do to solve this issue?</p>
<hr />
<p>very related perhaps?</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/61075390/pytorch-nccl-error-unhandled-system-error-nccl-version-2-4-8"">Pytorch &quot;NCCL error&quot;: unhandled system error, NCCL version 2.4.8&quot;</a></li>
</ul>
<hr />
<p>More complete error message:</p>
<pre><code>('jobid', 4852)
('slurm_jobid', -1)
('slurm_array_task_id', -1)
('condor_jobid', 4852)
('current_time', 'Mar25_16-27-35')
('tb_dir', PosixPath('/home/miranda9/data/logs/logs_Mar25_16-27-35_jobid_4852/tb'))
('gpu_name', 'GeForce GTX TITAN X')
('PID', '30688')
torch.cuda.device_count()=2

opts.world_size=2

ABOUT TO SPAWN WORKERS
done setting sharing strategy...next mp.spawn
INFO:root:Added key: store_based_barrier_key:1 to store for rank: 1
INFO:root:Added key: store_based_barrier_key:1 to store for rank: 0
rank=0
mp.current_process()=&lt;SpawnProcess name='SpawnProcess-1' parent=30688 started&gt;
os.getpid()=30704
setting up rank=0 (with world_size=2)
MASTER_ADDR='127.0.0.1'
59264
backend='nccl'
--&gt; done setting up rank=0
setup process done for rank=0
Traceback (most recent call last):
  File &quot;/home/miranda9/ML4Coq/ml4coq-proj/embeddings_zoo/tree_nns/main_brando.py&quot;, line 279, in &lt;module&gt;
    main_distributed()
  File &quot;/home/miranda9/ML4Coq/ml4coq-proj/embeddings_zoo/tree_nns/main_brando.py&quot;, line 188, in main_distributed
    spawn_return = mp.spawn(fn=train, args=(opts,), nprocs=opts.world_size)
  File &quot;/home/miranda9/miniconda3/envs/metalearning11.1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File &quot;/home/miranda9/miniconda3/envs/metalearning11.1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 188, in start_processes
    while not context.join():
  File &quot;/home/miranda9/miniconda3/envs/metalearning11.1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File &quot;/home/miranda9/miniconda3/envs/metalearning11.1/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 59, in _wrap
    fn(i, *args)
  File &quot;/home/miranda9/ML4Coq/ml4coq-proj/embeddings_zoo/tree_nns/main_brando.py&quot;, line 212, in train
    tactic_predictor = move_to_ddp(rank, opts, tactic_predictor)
  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-project/uutils/torch/distributed.py&quot;, line 162, in move_to_ddp
    model = DistributedDataParallel(model, find_unused_parameters=True, device_ids=[opts.gpu])
  File &quot;/home/miranda9/miniconda3/envs/metalearning11.1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py&quot;, line 446, in __init__
    self._sync_params_and_buffers(authoritative_rank=0)
  File &quot;/home/miranda9/miniconda3/envs/metalearning11.1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py&quot;, line 457, in _sync_params_and_buffers
    self._distributed_broadcast_coalesced(
  File &quot;/home/miranda9/miniconda3/envs/metalearning11.1/lib/python3.8/site-packages/torch/nn/parallel/distributed.py&quot;, line 1155, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(
RuntimeError: NCCL error in: /opt/conda/conda-bld/pytorch_1616554793803/work/torch/lib/c10d/ProcessGroupNCCL.cpp:825, unhandled cuda error, NCCL version 2.7.8
ncclUnhandledCudaError: Call to CUDA function failed.
</code></pre>
<hr />
<h1>Bonus 1:</h1>
<p>I still have errors:</p>
<pre><code>ncclSystemError: System call (socket, malloc, munmap, etc) failed.
Traceback (most recent call last):
  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1423, in &lt;module&gt;
    main()
  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1365, in main
    train(args=args)
  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1385, in train
    args.opt = move_opt_to_cherry_opt_and_sync_params(args) if is_running_parallel(args.rank) else args.opt
  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/torch_uu/distributed.py&quot;, line 456, in move_opt_to_cherry_opt_and_sync_params
    args.opt = cherry.optim.Distributed(args.model.parameters(), opt=args.opt, sync=syn)
  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py&quot;, line 62, in __init__
    self.sync_parameters()
  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py&quot;, line 78, in sync_parameters
    dist.broadcast(p.data, src=root)
  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py&quot;, line 1090, in broadcast
    work = default_pg.broadcast([tensor], opts)
RuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:911, unhandled system error, NCCL version 2.7.8
</code></pre>
<p>one of the answers suggested to have nvcca &amp; pytorch.version.cuda to match but they do not:</p>
<pre><code>(meta_learning_a100) [miranda9@hal-dgx ~]$ python -c &quot;import torch;print(torch.version.cuda)&quot;

11.1
(meta_learning_a100) [miranda9@hal-dgx ~]$ nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Wed_Jul_22_19:09:09_PDT_2020
Cuda compilation tools, release 11.0, V11.0.221
Build cuda_11.0_bu.TC445_37.28845127_0
</code></pre>
<p>How do I match them?</p>
",1601580.0,,1601580.0,,2022-05-12 20:51:38,2022-09-21 11:33:15,"How to solve the famous `unhandled cuda error, NCCL version 2.7.8` error?",<pytorch><distributed-computing><distributed-system>,4,3,0.0,,,CC BY-SA 4.0
64856195,1,67591848.0,,2020-11-16 10:18:53,,16,4490,"<p>I understand <code>autograd</code> is used to imply automatic differentiation. But what exactly is <code>tape-based autograd</code> in <code>Pytorch</code> and why there are so many discussions that affirm or deny it.</p>
<p>For example:</p>
<p><a href=""https://discuss.pytorch.org/t/is-pytorch-autograd-tape-based/13992"" rel=""noreferrer"">this</a></p>
<blockquote>
<p>In pytorch, there is no traditional sense of tape</p>
</blockquote>
<p>and <a href=""https://discuss.pytorch.org/t/get-the-gradient-tape/62886"" rel=""noreferrer"">this</a></p>
<blockquote>
<p>We don’t really build gradient tapes per se. But graphs.</p>
</blockquote>
<p>but not <a href=""https://pytorch.org/tutorials/beginner/former_torchies/autograd_tutorial.html"" rel=""noreferrer"">this</a></p>
<blockquote>
<p>Autograd is now a core torch package for automatic differentiation. It
uses a tape based system for automatic differentiation.</p>
</blockquote>
<p>And for further reference, please compare it with <code>GradientTape</code> in <code>Tensorflow</code>.</p>
",1511274.0,,9215780.0,,2021-05-17 12:19:44,2021-10-26 00:55:08,What is tape-based autograd in Pytorch?,<python><tensorflow><machine-learning><pytorch><tensorflow2.0>,2,0,,,,CC BY-SA 4.0
67683406,1,67683891.0,,2021-05-25 07:20:42,,15,8129,"<p>what is the difference between &quot;torch.utils.data.TensorDataset&quot; and &quot;torch.utils.data.Dataset&quot; - the docs are not clear about that and I could not find any answers on google.</p>
",7972317.0,,,,,2021-05-25 07:57:28,difference between Dataset and TensorDataset in pyTorch,<deep-learning><pytorch>,1,0,0.0,,,CC BY-SA 4.0
66538407,1,,,2021-03-08 22:53:23,,15,33619,"<p>I have done the following:</p>
<pre><code>!pip install pytorch_lightning -qqq
import pytorch_lightning
</code></pre>
<p>But get the following error:</p>
<pre><code>ImportError                               Traceback (most recent call last)
&lt;ipython-input-7-d883b15aac58&gt; in &lt;module&gt;()
----&gt; 1 import pytorch_lightning

----------------------------------9 frames------------------------------------------------
/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/apply_func.py in &lt;module&gt;()
     26 
     27 if _TORCHTEXT_AVAILABLE:
---&gt; 28     from torchtext.data import Batch
     29 else:
     30     Batch = type(None)

ImportError: cannot import name 'Batch' from 'torchtext.data' (/usr/local/lib/python3.7/dist-packages/torchtext/data/__init__.py)
</code></pre>
<p>What could the issue be?</p>
",15357068.0,,14203913.0,,2021-03-09 05:04:35,2022-12-22 09:56:26,Unable to import pytorch_lightning on google colab,<python><pytorch><google-colaboratory><pytorch-lightning>,7,5,,,,CC BY-SA 4.0
63654232,1,63729964.0,,2020-08-30 05:26:04,,15,10824,"<p>When I create a PyTorch DataLoader and start iterating -- I get an extremely slow first epoch (x10--x30 slower then all next epochs). Moreover, this problem occurs only with the train dataset from the Google landmark recognition 2020 from Kaggle. I can't reproduce this on synthetic images, also, I tried to create a folder with 500k images from GLR2020, and everything worked well. Found few similar problems in the PyTorch forum without any solutions.</p>
<pre><code>import argparse
import pandas as pd
import numpy as np
import os, sys
import multiprocessing, ray
import time
import cv2
import logging
import albumentations as albu
from torch.utils.data import Dataset, DataLoader

samples = 50000 # count of samples to speed up test
bs = 64 # batch size
dir = '/hdd0/datasets/ggl_landmark_recognition_2020/train' # directory with train data
all_files = pd.read_csv('/hdd0/datasets/ggl_landmark_recognition_2020/train.csv')
files = np.random.choice(all_files.id.values, 50000)
files = [os.path.join(_[0], _[1], _[2], _+'.jpg') for _ in files]

# augmentations
aug =  albu.Compose([albu.Resize(400, 400),
        albu.Rotate(limit=15),
        albu.ChannelDropout(p=0.1),
        albu.Normalize(),])

class ImgDataset:
    def __init__(self, path, files, augmentation = None):
        self.path = path
        self.files = {k:v for k, v in enumerate(files)}
        self.augmentation = augmentation

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        img_name = self.files[idx]
        img = np.array(cv2.imread(os.path.join(self.path, img_name)))
        if self.augmentation is not None:
            return self.augmentation(image=img)['image']


dtset = ImgDataset(dir,files, aug)
torchloader = DataLoader(dataset= dtset, batch_size=64, num_worker=16, shuffle=True)
for _ in range(3):
   t1 = time.time()
   for idx, val in enumerate(torchloader):
       pass
   t2 = time.time()
   print(str(t2-t1) +' sec')
</code></pre>
<p>Here are some examples of execution speed with different <code>num_workers</code> in DataLoader</p>
<pre><code>#num_workers=0
273.1584792137146 sec
83.15653467178345 sec
83.67923021316528 sec

# num_workers = 8 
165.62366938591003 sec
10.405716896057129 sec
10.495309114456177 sec

# num_workers = 16
156.60744667053223 sec
8.051618099212646 sec
7.922858238220215 sec
</code></pre>
<p>Looks like the problem is not with DataLoader, but with dataset. When I delete and reinitialise DataLoader object after first &quot;long&quot; iteration, everything still works fine. When I reinitialise dataset -- long first iteration appears again.
Moreover, I tracked my cpu utilisation via <code>htop</code> during this epochs with <code>num_workers</code> setted to 32, and during the first epoch, utilisation is really low; only 1-2 of 32 cores are working, during other epochs ~all cores are working.</p>
",4479664.0,,12984567.0,,2020-10-18 21:44:00,2022-04-29 13:45:52,pytorch DataLoader extremely slow first epoch,<python><multiprocessing><pytorch><dataloader>,2,4,0.0,,,CC BY-SA 4.0
69742930,1,,,2021-10-27 17:16:32,,14,40322,"<p>So, I was trying to code a chatbot using Pytorch following this <a href=""https://www.youtube.com/watch?v=Da-iHgrmHYg"" rel=""noreferrer"">tutorial</a>.</p>
<p>Code: (Minimal, Reproducible one)</p>
<pre><code>tags = []
for intent in intents['intents']:
    tag = intent['tag']
    tags.append(tag)

tags = sorted(set(tags))

X_train = []
X_train = np.array(X_train)

class ChatDataset(Dataset):
    def __init__(self):
        self.n_sample = len(X_train)
        self.x_data = X_train

#Hyperparameter
batch_size = 8
hidden_size = 47
output_size = len(tags)
input_size = len(X_train[0])
learning_rate = 0.001
num_epochs = 1000


dataset = ChatDataset()
train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=0)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # using gpu
model = NeuralNet(input_size, hidden_size, output_size).to(device)

# loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    for (words, labels) in train_loader:
        words = words.to(device)
        labels = labels.to(device)

        #forward
        outputs = model(words)
        loss = criterion(outputs, labels) #the line where it is showing the problem

        #backward and optimizer step
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    if (epoch +1) % 100 == 0:
        print(f'epoch {epoch+1}/{num_epochs}, loss={loss.item():.4f}')

print(f'final loss, loss={loss.item():.4f}')
</code></pre>
<p><a href=""https://gist.github.com/lepotatoguy/7247bbfe80004a699c00150ad6488983"" rel=""noreferrer"">Full Code</a> (if needed)</p>
<p>I am getting this error while trying to get the loss function.</p>
<p><code>RuntimeError: &quot;nll_loss_forward_reduce_cuda_kernel_2d_index&quot; not implemented for 'Int'</code></p>
<p>Traceback:</p>
<p><code>Traceback (most recent call last): File &quot;train.py&quot;, line 91, in &lt;module&gt; loss = criterion(outputs, labels) File &quot;C:\Users\PC\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl return forward_call(*input, **kwargs) File &quot;C:\Users\PC\anaconda3\lib\site-packages\torch\nn\modules\loss.py&quot;, line 1150, in forward return F.cross_entropy(input, target, weight=self.weight, File &quot;C:\Users\PC\anaconda3\lib\site-packages\torch\nn\functional.py&quot;, line 2846, in cross_entropy return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing) RuntimeError: &quot;nll_loss_forward_reduce_cuda_kernel_2d_index&quot; not implemented for 'Int'</code></p>
<p>But looking into the tutorial, it seems to work perfectly there whereas it is not in my case.</p>
<p>What to do now?</p>
<p>Thanks.</p>
",9848043.0,,9848043.0,,2022-01-17 18:37:30,2022-03-29 19:09:37,"RuntimeError: ""nll_loss_forward_reduce_cuda_kernel_2d_index"" not implemented for 'Int': Pytorch",<python><pytorch><nltk>,5,0,,,,CC BY-SA 4.0
62800189,1,62836623.0,,2020-07-08 17:19:00,,14,13956,"<p>I would like to create a new tensor in a <code>validation_epoch_end</code> method of a <code>LightningModule</code>. From the official <a href=""https://pytorch-lightning.readthedocs.io/_/downloads/en/latest/pdf/"" rel=""noreferrer"">docs</a> (page 48) it is stated that we should avoid direct <code>.cuda()</code> or <code>.to(device)</code> calls:</p>
<blockquote>
<p>There are no .cuda() or .to() calls. . . Lightning does these for you.</p>
</blockquote>
<p>and we are encouraged to use <code>type_as</code> method to transfer to the correct device.</p>
<p><code>new_x = new_x.type_as(x.type())</code></p>
<p>However, in a step <code>validation_epoch_end</code> I do not have any tensor to copy device from(by <code>type_as</code> method) in a clean way.</p>
<p>My question is what should I do if I want to create a new tensor in this method and transfer it to the device where is the model?</p>
<p>The only thing I can think of is to find a tensor in the <code>outputs</code> dictionary but it feels kinda messy:</p>
<pre><code>avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()
output = self(self.__test_input.type_as(avg_loss))
</code></pre>
<p>Is there any clean way to achieve that?</p>
",1368155.0,,1368155.0,,2020-07-08 17:39:53,2020-07-10 14:37:42,PyTorch Lightning move tensor to correct device in validation_epoch_end,<python><pytorch><pytorch-lightning>,1,0,0.0,,,CC BY-SA 4.0
66402331,1,66402452.0,,2021-02-27 18:53:05,,14,9675,"<p>In PyTorch, the <code>Tensor</code> class has a <code>grad_fn</code> attribute. This <em>references</em> the operation used to obtain the tensor: for instance, if <code>a = b + 2</code>, <code>a.grad_fn</code> will be <code>AddBackward0</code>. But what does &quot;reference&quot; mean exactly?</p>
<p>Inspecting <code>AddBackward0</code> using <code>inspect.getmro(type(a.grad_fn))</code> will state that the only base class of <code>AddBackward0</code> is <code>object</code>. Additionally, the source code for this class (and in fact, any other class which might be encountered in <code>grad_fn</code>) is nowhere to be found in the <a href=""https://github.com/pytorch/pytorch"" rel=""noreferrer"">source code</a>!</p>
<p>All of this leads me to the following questions:</p>
<ol>
<li>What precisely is stored in <code>grad_fn</code> and how is it called during back-propagation?</li>
<li>How come the objects that get stored in <code>grad_fn</code> do not have some sort of common super class, and why is there no source code for them on GitHub?</li>
</ol>
",5556711.0,,,,,2021-02-27 19:42:10,"In PyTorch, what exactly does the grad_fn attribute store and how is it used?",<python><oop><pytorch>,1,0,,,,CC BY-SA 4.0
67456368,1,67456436.0,,2021-05-09 09:53:33,,14,38890,"<p>I am trying to implement a neural net in PyTorch but it doesn't seem to work. The problem seems to be in the training loop. I've spend several hours into this but can't get it right. Please help, thanks.</p>
<p>I haven't added the data preprocessing parts.</p>
<pre><code># importing libraries
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
</code></pre>
<pre><code># get x function (dataset related stuff)
def Getx(idx):
    sample = samples[idx]
    vector = Calculating_bottom(sample)
    vector = torch.as_tensor(vector, dtype = torch.float64)
    
    return vector

# get y function (dataset related stuff)
def Gety(idx):
    y = np.array(train.iloc[idx, 4], dtype = np.float64)
    y = torch.as_tensor(y, dtype = torch.float64)
    
    return y
</code></pre>
<pre><code># dataset
class mydataset(Dataset):

    def __init__(self):
        super().__init__()

    def __getitem__(self, index):
        x = Getx(index)
        y = Gety(index)
        
        return x, y

    def __len__(self):
        return len(train)
    
dataset = mydataset()
</code></pre>
<pre><code># sample dataset value
print(dataset.__getitem__(0))
</code></pre>
<p>(tensor([ 5.,  5.,  8., 14.], dtype=torch.float64), tensor(-0.3403, dtype=torch.float64))</p>
<pre><code># data-loader
dataloader = DataLoader(dataset, batch_size = 1, shuffle = True)
</code></pre>
<pre><code># nn architecture
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(4, 4)
        self.fc2 = nn.Linear(4, 2)
        self.fc3 = nn.Linear(2, 1)

    def forward(self, x):
        x = x.float()
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = Net()
</code></pre>
<pre><code># device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)
</code></pre>
<pre><code># hyper-parameters
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
</code></pre>
<pre><code># training loop

for epoch in range(5):
    
    for batch in dataloader:
        
        # unpacking
        x, y = batch
        x.to(device)
        y.to(device)
        
        # reset gradients
        optimizer.zero_grad()
        
        # forward propagation through the network
        out = model(x)
        
        # calculate the loss
        loss = criterion(out, y)
        
        # backpropagation
        loss.backward()
        
        # update the parameters
        optimizer.step()
</code></pre>
<p>Error:</p>
<pre><code>/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-18-3f68fcee9ff3&gt; in &lt;module&gt;
     20 
     21         # backpropagation
---&gt; 22         loss.backward()
     23 
     24         # update the parameters

/opt/conda/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    219                 retain_graph=retain_graph,
    220                 create_graph=create_graph)
--&gt; 221         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    222 
    223     def register_hook(self, hook):

/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
    130     Variable._execution_engine.run_backward(
    131         tensors, grad_tensors_, retain_graph, create_graph,
--&gt; 132         allow_unreachable=True)  # allow_unreachable flag
    133 
    134 

RuntimeError: Found dtype Double but expected Float
</code></pre>
",15166885.0,,913098.0,,2022-09-12 16:36:49,2022-09-12 16:36:49,Pytorch getting RuntimeError: Found dtype Double but expected Float,<python><deep-learning><casting><pytorch><precision>,2,0,0.0,,,CC BY-SA 4.0
64635630,1,64638044.0,,2020-11-01 18:45:34,,14,36432,"<p>I am working on the classic example with digits. I want to create a my first neural network that predict the labels of digit images {0,1,2,3,4,5,6,7,8,9}. So the first column of <code>train.txt</code> has the labels and all the other columns are the features of each label. I have defined a class to import my data:</p>
<pre><code>class DigitDataset(Dataset):
    &quot;&quot;&quot;Digit dataset.&quot;&quot;&quot;

    def __init__(self, file_path, transform=None):
        &quot;&quot;&quot;
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        &quot;&quot;&quot;
        self.data = pd.read_csv(file_path, header = None, sep =&quot; &quot;)
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        labels = self.data.iloc[idx,0]
        images = self.data.iloc[idx,1:-1].values.astype(np.uint8).reshape((1,16,16))

        if self.transform is not None:
            sample = self.transform(sample)
        return images, labels
</code></pre>
<p>And then I am running these commands to split my dataset into batches, to define a model and a loss:</p>
<pre><code>train_dataset = DigitDataset(&quot;train.txt&quot;)
train_loader = DataLoader(train_dataset, batch_size=64,
                        shuffle=True, num_workers=4)

# Model creation with neural net Sequential model
model=nn.Sequential(nn.Linear(256, 128), # 1 layer:- 256 input 128 o/p
                    nn.ReLU(),          # Defining Regular linear unit as activation
                    nn.Linear(128,64),  # 2 Layer:- 128 Input and 64 O/p
                    nn.Tanh(),          # Defining Regular linear unit as activation
                    nn.Linear(64,10),   # 3 Layer:- 64 Input and 10 O/P as (0-9)
                    nn.LogSoftmax(dim=1) # Defining the log softmax to find the probablities 
for the last output unit 
                  ) 

# defining the negative log-likelihood loss for calculating loss
criterion = nn.NLLLoss()

images, labels = next(iter(train_loader))
images = images.view(images.shape[0], -1)

logps = model(images) #log probabilities
loss = criterion(logps, labels) #calculate the NLL-loss
</code></pre>
<p>And I take the error:</p>
<pre><code>---------------------------------------------------------------------------
   RuntimeError                              Traceback (most recent call last) 
    &lt;ipython-input-2-7f4160c1f086&gt; in &lt;module&gt;
     47 images = images.view(images.shape[0], -1)
     48 
---&gt; 49 logps = model(images) #log probabilities
     50 loss = criterion(logps, labels) #calculate the NLL-loss

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, 
*input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py in forward(self, input)
    115     def forward(self, input):
    116         for module in self:
--&gt; 117             input = module(input)
    118         return input
    119 

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, 
*input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

 ~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py in forward(self, input)
     91 
     92     def forward(self, input: Tensor) -&gt; Tensor:
---&gt; 93         return F.linear(input, self.weight, self.bias)
     94 
     95     def extra_repr(self) -&gt; str:

 ~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py in linear(input, weight, bias)
   1688     if input.dim() == 2 and bias is not None:
   1689         # fused op is marginally faster
-&gt; 1690         ret = torch.addmm(bias, input, weight.t())
   1691     else:
   1692         output = input.matmul(weight.t())

RuntimeError: expected scalar type Float but found Byte
</code></pre>
<p>Do you know what is wrong? Thank you for your patience and help!</p>
",12383160.0,,,,,2022-03-07 18:18:57,Pytorch RuntimeError: expected scalar type Float but found Byte,<python><python-3.x><machine-learning><pytorch><pattern-recognition>,3,0,0.0,,,CC BY-SA 4.0
63081486,1,,,2020-07-24 21:08:55,,14,1815,"<p>I've encountered a mysterious bug while trying to implement Hogwild with torch.multiprocessing. In particular, one version of the code runs fine, but when I add in a seemingly unrelated bit of code before the multiprocessing step, this somehow causes an error during the multiprocessing step: <code>RuntimeError: Unable to handle autograd's threading in combination with fork-based multiprocessing. See https://github.com/pytorch/pytorch/wiki/Autograd-and-Fork</code></p>
<p>I reproduced the error in a minimal code sample, pasted below. If I comment out the two lines of code <code>m0 = Model(); train(m0)</code> which carry out a non-parallel training run on a separate model instance, then everything runs fine. I can't figure out how these lines could be causing a problem.</p>
<p>I'm running PyTorch 1.5.1 and Python 3.7.6 on a Linux machine, training on CPU only.</p>
<pre class=""lang-python prettyprint-override""><code>import torch
import torch.multiprocessing as mp
from torch import nn

def train(model):
    opt = torch.optim.Adam(model.parameters(), lr=1e-5)
    for _ in range(10000):
        opt.zero_grad()
        # We train the model to output the value 4 (arbitrarily)
        loss = (model(0) - 4)**2
        loss.backward()
        opt.step()

# Toy model with one parameter tensor of size 3.
# Output is always the sum of the elements in the tensor,
# independent of the input
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.x = nn.Parameter(torch.ones(3))

    def forward(self, x):
        return torch.sum(self.x)

############################################
# Create a separate Model instance and run
# a non-parallel training run.
# For some reason, this code causes the 
# subsequent parallel run to fail.
m0 = Model()
train(m0)
print ('Done with preliminary run')
############################################

num_processes = 2
model = Model()
model.share_memory()
processes = []
for rank in range(num_processes):
    p = mp.Process(target=train, args=(model,))
    p.start()
    processes.append(p)
for p in processes:
    p.join()
    
print(model.x)
</code></pre>
",9314540.0,,2230844.0,,2020-08-07 02:47:14,2020-09-28 06:50:59,PyTorch multiprocessing error with Hogwild,<python><machine-learning><deep-learning><multiprocessing><pytorch>,2,7,0.0,,,CC BY-SA 4.0
64809370,1,75737969.0,,2020-11-12 18:06:58,,14,2996,"<p>I have a <code>MelSpectrogram</code> generated from:</p>
<pre><code>eval_seq_specgram = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=256)(eval_audio_data).transpose(1, 2)
</code></pre>
<p>So <code>eval_seq_specgram</code> now has a <code>size</code> of <code>torch.Size([1, 128, 499])</code>, where 499 is the number of timesteps and 128 is the <code>n_mels</code>.</p>
<p>I'm trying to invert it, so I'm trying to use <code>GriffinLim</code>, but before doing that, I think I need to invert the <code>melscale</code>, so I have:</p>
<pre><code>inverse_mel_pred = torchaudio.transforms.InverseMelScale(sample_rate=sample_rate, n_stft=256)(eval_seq_specgram)
</code></pre>
<p><code>inverse_mel_pred</code> has a <code>size</code> of <code>torch.Size([1, 256, 499])</code></p>
<p>Then I'm trying to use <code>GriffinLim</code>:</p>
<pre><code>pred_audio = torchaudio.transforms.GriffinLim(n_fft=256)(inverse_mel_pred)
</code></pre>
<p>but I get an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;evaluate_spect.py&quot;, line 63, in &lt;module&gt;
    main()
  File &quot;evaluate_spect.py&quot;, line 51, in main
    pred_audio = torchaudio.transforms.GriffinLim(n_fft=256)(inverse_mel_pred)
  File &quot;/home/shamoon/.local/share/virtualenvs/speech-reconstruction-7HMT9fTW/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/shamoon/.local/share/virtualenvs/speech-reconstruction-7HMT9fTW/lib/python3.8/site-packages/torchaudio/transforms.py&quot;, line 169, in forward
    return F.griffinlim(specgram, self.window, self.n_fft, self.hop_length, self.win_length, self.power,
  File &quot;/home/shamoon/.local/share/virtualenvs/speech-reconstruction-7HMT9fTW/lib/python3.8/site-packages/torchaudio/functional.py&quot;, line 179, in griffinlim
    inverse = torch.istft(specgram * angles,
RuntimeError: The size of tensor a (256) must match the size of tensor b (129) at non-singleton dimension 1
</code></pre>
<p>Not sure what I'm doing wrong or how to resolve this.</p>
",239879.0,,,,,2023-03-14 20:16:15,How can I invert a MelSpectrogram with torchaudio and get an audio waveform?,<python><pytorch><torchaudio>,4,1,,,,CC BY-SA 4.0
65492490,1,65495386.0,,2020-12-29 12:55:38,,14,19375,"<p>I am trying to build a Docker container on a server within which a conda environment is built. All the other requirements are satisfied except for CUDA enabled PyTorch (I can get PyTorch working without CUDA however, no problem). How do I make sure PyTorch is using CUDA?</p>
<p>This is the <code>Dockerfile</code> :</p>
<pre><code># Use nvidia/cuda image
FROM nvidia/cuda:10.2-cudnn7-devel-ubuntu18.04

# set bash as current shell
RUN chsh -s /bin/bash

# install anaconda
RUN apt-get update
RUN apt-get install -y wget bzip2 ca-certificates libglib2.0-0 libxext6 libsm6 libxrender1 git mercurial subversion &amp;&amp; \
        apt-get clean
RUN wget --quiet https://repo.anaconda.com/archive/Anaconda3-2020.02-Linux-x86_64.sh -O ~/anaconda.sh &amp;&amp; \
        /bin/bash ~/anaconda.sh -b -p /opt/conda &amp;&amp; \
        rm ~/anaconda.sh &amp;&amp; \
        ln -s /opt/conda/etc/profile.d/conda.sh /etc/profile.d/conda.sh &amp;&amp; \
        echo &quot;. /opt/conda/etc/profile.d/conda.sh&quot; &gt;&gt; ~/.bashrc &amp;&amp; \
        find /opt/conda/ -follow -type f -name '*.a' -delete &amp;&amp; \
        find /opt/conda/ -follow -type f -name '*.js.map' -delete &amp;&amp; \
        /opt/conda/bin/conda clean -afy

# set path to conda
ENV PATH /opt/conda/bin:$PATH


# setup conda virtual environment
COPY ./requirements.yaml /tmp/requirements.yaml
RUN conda update conda \
    &amp;&amp; conda env create --name camera-seg -f /tmp/requirements.yaml \
    &amp;&amp; conda install -y -c conda-forge -n camera-seg flake8

# From the pythonspeed tutorial; Make RUN commands use the new environment
SHELL [&quot;conda&quot;, &quot;run&quot;, &quot;-n&quot;, &quot;camera-seg&quot;, &quot;/bin/bash&quot;, &quot;-c&quot;]

# PyTorch with CUDA 10.2
RUN conda activate camera-seg &amp;&amp; conda install pytorch torchvision cudatoolkit=10.2 -c pytorch

RUN echo &quot;conda activate camera-seg&quot; &gt; ~/.bashrc
ENV PATH /opt/conda/envs/camera-seg/bin:$PATH
</code></pre>
<p>This gives me the following error when I try to build this container ( <code>docker build -t camera-seg .</code> ):</p>
<pre><code>.....

Step 10/12 : RUN conda activate camera-seg &amp;&amp; conda install pytorch torchvision cudatoolkit=10.2 -c pytorch
 ---&gt; Running in e0dd3e648f7b
ERROR conda.cli.main_run:execute(34): Subprocess for 'conda run ['/bin/bash', '-c', 'conda activate camera-seg &amp;&amp; conda install pytorch torchvision cudatoolkit=10.2 -c pytorch']' command failed.  (See above for error)

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init &lt;SHELL_NAME&gt;

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.



The command 'conda run -n camera-seg /bin/bash -c conda activate camera-seg &amp;&amp; conda install pytorch torchvision cudatoolkit=10.2 -c pytorch' returned a non-zero code: 1
</code></pre>
<p>This is the <code>requirements.yaml</code>:</p>
<pre><code>name: camera-seg
channels:
  - defaults
  - conda-forge
dependencies:
  - python=3.6
  - numpy
  - pillow
  - yaml
  - pyyaml
  - matplotlib
  - jupyter
  - notebook
  - tensorboardx
  - tensorboard
  - protobuf
  - tqdm
</code></pre>
<p>When I put <code>pytorch</code>, <code>torchvision</code> and <code>cudatoolkit=10.2</code> within the <code>requirements.yaml</code>, then PyTorch is successfully installed but it cannot recognize CUDA ( <code>torch.cuda.is_available()</code> returns <code>False</code> ).</p>
<p>I have tried various solutions, for example, <a href=""https://pythonspeed.com/articles/activate-conda-dockerfile/"" rel=""noreferrer"">this</a>, <a href=""https://stackoverflow.com/questions/55123637/activate-conda-environment-in-docker/62674910#62674910"">this</a> and <a href=""https://stackoverflow.com/questions/55123637/activate-conda-environment-in-docker/62674910#62674910"">this</a> and some different combinations of them but all to no avail.</p>
<p>Any help is much appreciated. Thanks.</p>
",5847147.0,,4240413.0,,2022-01-04 19:16:56,2022-10-10 07:55:16,How to conda install CUDA enabled PyTorch in a Docker container?,<python-3.x><docker><anaconda><pytorch>,2,2,0.0,,,CC BY-SA 4.0
65445174,1,65448744.0,,2020-12-25 04:02:03,,14,5758,"<p>I am reading the &quot;Deep Learning for Coders with fastai &amp; PyTorch&quot; book. I'm still a bit confused as to what the Embedding module does. It seems like a short and simple network, except I can't seem to wrap my head around what Embedding does differently than Linear without a bias. I know it does some faster computational version of a dot product where one of the matrices is a one-hot encoded matrix and the other is the embedding matrix. It does this to in effect select a piece of data? Please point out where I am wrong. Here is one of the simple networks shown in the book.</p>
<pre><code>class DotProduct(Module):
    def __init__(self, n_users, n_movies, n_factors):
        self.user_factors = Embedding(n_users, n_factors)
        self.movie_factors = Embedding(n_movies, n_factors)
        
    def forward(self, x):
        users = self.user_factors(x[:,0])
        movies = self.movie_factors(x[:,1])
        return (users * movies).sum(dim=1)
</code></pre>
",13751442.0,,,,,2023-03-20 12:45:56,What is the difference between an Embedding Layer with a bias immediately afterwards and a Linear Layer in PyTorch,<python><oop><deep-learning><pytorch><fast-ai>,2,0,0.0,,,CC BY-SA 4.0
63426545,1,63427058.0,,2020-08-15 13:27:01,,14,31204,"<p>How to use tqdm for data_loader ?</p>
<p>is this the correct way?</p>
<pre><code>for i,j in enumerate(data_loader,total = 100):
           pass
</code></pre>
",11939308.0,,4685471.0,,2020-08-15 17:37:16,2023-03-02 13:42:56,best way of tqdm for data loader,<python><pytorch><tqdm>,2,2,,,,CC BY-SA 4.0
65988678,1,66008320.0,,2021-02-01 07:28:39,,13,10790,"<p>Recently,I need to install pytorch ,when I check out the website :</p>
<p><a href=""https://pytorch.org/get-started/locally/#supported-linux-distributions"" rel=""noreferrer"">install pytorch website</a></p>
<p>It shows four different version 9.2,10.1,10.2,11.0 to choose ,And I have cuda version 10.0 and driver version 450 installed on my computer,I thought it would fail to enable gpu when using pytorch
,After I choose 10.1 and  try <code>torch.cuda.is_available()</code> and it returns <code>True</code></p>
<p>I have two questions:</p>
<ol>
<li><p>Why does everything turn out to be working even my cuda version is not the same as any of one I mentioned ?</p>
</li>
<li><p>What's the difference between choosing cuda verison 9.2,10.1,10.2,11.0 ?</p>
</li>
</ol>
",10689760.0,,6331369.0,,2021-02-01 08:16:30,2021-02-02 13:04:30,"Difference between versions 9.2,10.1,10.2,11.0 of cuda for PyTorch 1.7",<pytorch>,1,4,0.0,,,CC BY-SA 4.0
63347149,1,,,2020-08-10 19:56:09,,13,5520,"<p>A map-style dataset in Pytorch has the <code>__getitem__()</code> and <code>__len__()</code> and iterable-style datasets has <code>__iter__()</code> protocol. If we use map-style, we can access the data with <code>dataset[idx]</code> which is great, however with the iterable dataset we can't.</p>
<p>My question is why this distinction was necessary? What makes the data random read so expensive or even improbable?</p>
",3907250.0,,,,,2020-12-25 20:40:39,pytorch dataset map-style vs iterable-style,<pytorch>,2,0,,,,CC BY-SA 4.0
67546911,1,,,2021-05-15 12:50:32,,13,24629,"<p>I am creating an entity extraction model in PyTorch using <code>bert-base-uncased</code> but when I try to run the model I get this error:</p>
<h2>Error:</h2>
<pre><code>Some weights of the model checkpoint at D:\Transformers\bert-entity-extraction\input\bert-base-uncased_L-12_H-768_A-12 were not used when initializing BertModel:    
['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight',   'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias',  
 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight',  
 'cls.predictions.bias']  
    - This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
    - This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>I have downloaded the bert model from <a href=""https://github.com/google-research/bert"" rel=""noreferrer"">here</a> and the additional files from <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""noreferrer"">here</a></p>
<h2>Code</h2>
<p>Following is the code for my model:</p>
<pre><code>import config
import torch
import transformers
import torch.nn as nn

def loss_fn(output, target, mask, num_labels):

    lfn = nn.CrossEntropyLoss()
    active_loss = mask.view(-1) == 1
    active_logits = output.view(-1, num_labels)
    active_labels = torch.where(
        active_loss,
        target.view(-1),
        torch.tensor(lfn.ignore_index).type_as(target)
    )
    loss = lfn(active_logits, active_labels)
    return loss

class EntityModel(nn.Module):
    def __init__(self, num_tag, num_pos):
        super(EntityModel, self).__init__()

        self.num_tag = num_tag
        self.num_pos = num_pos
        self.bert = transformers.BertModel.from_pretrained(config.BASE_MODEL_PATH)
        self.bert_drop_1 = nn.Dropout(p = 0.3)
        self.bert_drop_2 = nn.Dropout(p = 0.3)
        self.out_tag = nn.Linear(768, self.num_tag)
        self.out_pos = nn.Linear(768, self.num_pos)

    def forward(self, ids, mask, token_type_ids, target_pos, target_tag):
        o1, _ = self.bert(ids, 
                          attention_mask = mask,
                          token_type_ids = token_type_ids)

        bo_tag = self.bert_drop_1(o1)
        bo_pos = self.bert_drop_2(o1)

        tag = self.out_tag(bo_tag)
        pos = self.out_pos(bo_pos)

        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)
        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)

        loss = (loss_tag + loss_pos) / 2

        return tag, pos, loss 

print(&quot;model.py run success!&quot;)
</code></pre>
",13540652.0,,,,,2023-02-02 12:33:51,Python: BERT Error - Some weights of the model checkpoint at were not used when initializing BertModel,<python><nlp><pytorch><bert-language-model><huggingface-transformers>,3,5,,,,CC BY-SA 4.0
63785319,1,63791977.0,,2020-09-07 23:23:32,,13,10558,"<p>I'm following a <a href=""https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb"" rel=""noreferrer"">PyTorch tutorial</a> which uses the BERT NLP model (feature extractor) from the Huggingface Transformers library. There are two pieces of interrelated code for gradient updates that I don't understand.</p>
<p>(1) <code>torch.no_grad()</code></p>
<p>The tutorial has a class where the <code>forward()</code> function creates a <code>torch.no_grad()</code> block around a call to the BERT feature extractor, like this:</p>
<pre class=""lang-py prettyprint-override""><code>bert = BertModel.from_pretrained('bert-base-uncased')

class BERTGRUSentiment(nn.Module):
    
    def __init__(self, bert):
        super().__init__()
        self.bert = bert
        
    def forward(self, text):
        with torch.no_grad():
            embedded = self.bert(text)[0]
</code></pre>
<p>(2) <code>param.requires_grad = False</code></p>
<p>There is another portion in the same tutorial where the BERT parameters are frozen.</p>
<pre class=""lang-py prettyprint-override""><code>for name, param in model.named_parameters():                
    if name.startswith('bert'):
        param.requires_grad = False
</code></pre>
<p><strong>When would I need (1) and/or (2)?</strong></p>
<ul>
<li>If I want to train with a frozen BERT, would I need to enable both?</li>
<li>If I want to train to let BERT be updated, would I need to disable both?</li>
</ul>
<p>Additionaly, I ran all four combinations and found:</p>
<pre><code>   with torch.no_grad   requires_grad = False  Parameters  Ran
   ------------------   ---------------------  ----------  ---
a. Yes                  Yes                      3M        Successfully
b. Yes                  No                     112M        Successfully
c. No                   Yes                      3M        Successfully
d. No                   No                     112M        CUDA out of memory
</code></pre>
<p><strong>Can someone please explain what's going on?</strong> Why am I getting <code>CUDA out of memory</code> for (d) but not (b)? Both have 112M learnable parameters.</p>
",4561314.0,,4561314.0,,2020-09-08 16:54:24,2020-09-08 16:54:24,PyTorch torch.no_grad() versus requires_grad=False,<python><machine-learning><pytorch><bert-language-model><huggingface-transformers>,1,0,0.0,,,CC BY-SA 4.0
72431190,1,,,2022-05-30 08:19:14,,13,8965,"<p>I'm having trouble with using Pytorch and CUDA. Sometimes it works fine, other times it tells me <code>RuntimeError: CUDA out of memory.</code> However, I am confused because checking <code>nvidia-smi</code> shows that the used memory of my card is <code>563MiB / 6144 MiB</code>, which should in theory leave over 5GiB available.
<a href=""https://i.stack.imgur.com/0cMqI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0cMqI.png"" alt=""output of nvidia-smi"" /></a></p>
<p>However, upon running my program, I am greeted with the message:
<code>RuntimeError: CUDA out of memory. Tried to allocate 578.00 MiB (GPU 0; 5.81 GiB total capacity; 670.69 MiB already allocated; 624.31 MiB free; 898.00 MiB reserved in total by PyTorch)</code></p>
<p>It looks like Pytorch is reserving 1GiB, knows that ~700MiB are allocated, and is trying to assign ~600MiB to the program—but claims that the GPU is out of memory. How can this be? There should be plenty of GPU memory left given these numbers.</p>
",10703696.0,,681865.0,,2022-05-30 12:05:30,2023-04-14 18:55:10,CUDA Out of memory when there is plenty available,<python><pytorch><nvidia>,2,4,0.0,,,CC BY-SA 4.0
71617570,1,,,2022-03-25 13:15:06,,13,37208,"<p>I am trying to learn pytorch from a book, but it seems not a straight line for me.
I coped the code below and pasted in my jupyter notebook for running but it gave me an error I am not able to interpret at my level!</p>
<pre><code>from torchvision import models
model = models.alexnet(pretrained=True)

# set the device
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Device: {device}')

model.eval()
model.to(device)
y = model(batch.to(device))
print(y.shape)
</code></pre>
<p>The error is as below</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-50-03488007067c&gt; in &lt;module&gt;
      1 from torchvision import models
----&gt; 2 model = models.alexnet(pretrained=True)
      3 
      4 # set the device
      5 device = 'cuda' if torch.cuda.is_available() else 'cpu'

~\anaconda3\lib\site-packages\torchvision\models\alexnet.py in alexnet(pretrained, progress, **kwargs)
     61     model = AlexNet(**kwargs)
     62     if pretrained:
---&gt; 63         state_dict = load_state_dict_from_url(model_urls['alexnet'],
     64                                               progress=progress)
     65         model.load_state_dict(state_dict)

~\anaconda3\lib\site-packages\torch\hub.py in load_state_dict_from_url(url, model_dir, map_location, progress, check_hash, file_name)
    555     if _is_legacy_zip_format(cached_file):
    556         return _legacy_zip_load(cached_file, model_dir, map_location)
--&gt; 557     return torch.load(cached_file, map_location=map_location)

~\anaconda3\lib\site-packages\torch\serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    598             # reset back to the original position.
    599             orig_position = opened_file.tell()
--&gt; 600             with _open_zipfile_reader(opened_file) as opened_zipfile:
    601                 if _is_torchscript_zip(opened_zipfile):
    602                     warnings.warn(&quot;'torch.load' received a zip file that looks like a TorchScript archive&quot;

~\anaconda3\lib\site-packages\torch\serialization.py in __init__(self, name_or_buffer)
    240 class _open_zipfile_reader(_opener):
    241     def __init__(self, name_or_buffer) -&gt; None:
--&gt; 242         super(_open_zipfile_reader, self).__init__(torch._C.PyTorchFileReader(name_or_buffer))
    243 
    244 

RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
</code></pre>
<p>someone help me understand this please.
Thank you.</p>
",14334349.0,,,,,2023-03-19 09:15:23,PytorchStreamReader failed reading zip archive: failed finding central directory,<python-3.x><pytorch>,3,1,0.0,,,CC BY-SA 4.0
67852880,1,,,2021-06-05 18:45:03,,13,12927,"<p>I'm trying to build a datasetDictionary object to train a QA model on PyTorch. I have these two different datasets:</p>
<pre><code>test_dataset

Dataset({
    features: ['answer_text', 'answer_start', 'title', 'context', 'question', 'answers', 'id'],
    num_rows: 21489
})
</code></pre>
<p>and</p>
<pre><code>train_dataset

Dataset({
    features: ['answer_text', 'answer_start', 'title', 'context', 'question', 'answers', 'id'],
    num_rows: 54159
})
</code></pre>
<p>In the dataset's <a href=""https://dataset.readthedocs.io/en/latest/index.html#"" rel=""noreferrer"">documentation</a> I didn't find anything. I'm quite a noob, thus the solution may be really easy. What I wish to obtain is something like this:</p>
<pre><code>dataset

DatasetDict({
    train: Dataset({
        features: ['answer_text', 'answer_start', 'title', 'context', 'question', 'answers', 'id'],
        num_rows: 54159
    })
    test: Dataset({
        features: ['answer_text', 'answer_start', 'title', 'context', 'question', 'answers', 'id'],
        num_rows: 21489
    })
})
</code></pre>
<p>I really don't find how to use two datasets to create a dataserDict or how to set the keys. Moreover, I wish to &quot;cut&quot; the train set in two: train and validation sets, but also this passage is hard for me to handle. The final result should be something like this:</p>
<pre><code>dataset

DatasetDict({
    train: Dataset({
        features: ['answer_text', 'answer_start', 'title', 'context', 'question', 'answers', 'id'],
        num_rows: 54159 - x
    })
    validation: Dataset({
        features: ['answer_text', 'answer_start', 'title', 'context', 'question', 'answers', 'id'],
        num_rows: x
    })
    test: Dataset({
        features: ['answer_text', 'answer_start', 'title', 'context', 'question', 'answers', 'id'],
        num_rows: 21489
    })
})
</code></pre>
<p>Thank you in advance and pardon me for being a noob :)</p>
",15261008.0,,12943692.0,,2021-06-06 18:08:49,2023-03-16 11:52:27,How can I handle this datasets to create a datasetDict?,<python><deep-learning><pytorch><dataset><nlp-question-answering>,3,0,0.0,,,CC BY-SA 4.0
64850321,1,64850654.0,,2020-11-15 22:09:35,,13,4967,"<p>I am currently trying to install PyTorch (using the installation commands posted on PyTorch.org for pip) and when I run the command, my computer completely freezes.</p>
<p>I tried this multiple times with the same result. I had to restart the computer a few times as well. On my current try, I put &quot;-v&quot; when trying to install and the pip seems to be stuck on
&quot;Looking up  in the cache&quot;.</p>
<p>I do not know how to proceed. As I mentioned, I've already tried this method multiple times. It worked the first time but did not install PyTorch as it gave me an error for not using &quot;--user&quot;.</p>
<p>Are there any solutions to this?</p>
<p>EDIT: I did want to add that I have Python 3.8.6 (64bit)</p>
",14644683.0,,,,,2023-06-26 20:53:34,Windows keeps crashing when trying to install PyTorch via pip,<python><pip><pytorch>,2,0,0.0,,,CC BY-SA 4.0
63460538,1,,,2020-08-18 01:14:12,,13,8828,"<p>I am looking into prefetching data into the GPU from the CPU when the model is being trained on the GPU. Overlapping CPU-to-GPU data transfer with GPU model training appears to require both</p>
<ol>
<li>Transferring data to GPU using <code>data = data.cuda(non_blocking=True)</code></li>
<li>Pin data to CPU memory using <code>train_loader = DataLoader(..., pin_memory=True)</code></li>
</ol>
<p>However, I cannot understand how non-blocking transfer is being performed in this <a href=""https://github.com/pytorch/examples/blob/master/imagenet/main.py#L280-L291"" rel=""noreferrer"">official PyTorch example</a>, specifically this code block:</p>
<pre><code>for i, (images, target) in enumerate(train_loader):
        # measure data loading time
        data_time.update(time.time() - end)

        if args.gpu is not None:
            images = images.cuda(args.gpu, non_blocking=True)
        if torch.cuda.is_available():
            target = target.cuda(args.gpu, non_blocking=True)

        # compute output
        output = model(images)
        loss = criterion(output, target)
</code></pre>
<p>Won't <code>images.cuda(non_blocking=True)</code> and <code>target.cuda(non_blocking=True)</code> have to be completed before <code>output = model(images)</code> is executed. Since this is a synchronization point, <code>images</code> must be first fully transferred to the CUDA device, so the data transfer steps are effectively no longer non-blocking.</p>
<p>Since <code>output = model(images)</code> is blocking, <code>images.cuda()</code> and <code>target.cuda()</code> in the next <code>i</code> iteration of the <code>for</code> loop will not occur until the model output is computed, meaning no prefetching in the next loop iteration.</p>
<p>If this is correct, what is the correct way to perform data prefetching to the GPU?</p>
",3023615.0,,681865.0,,2020-08-18 02:33:14,2021-11-10 01:15:07,Proper Usage of PyTorch's non_blocking=True for Data Prefetching,<python><python-3.x><deep-learning><pytorch>,2,0,0.0,,,CC BY-SA 4.0
65710713,1,65710714.0,,2021-01-13 22:28:56,,13,17726,"<p>While running smdataparallel, I see following error</p>
<pre><code># python
Python 3.6.10 |Anaconda, Inc.| (default, May  8 2020, 02:54:21)
[GCC 7.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import smdistributed.dataparallel.torch.distributed as dist
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/opt/conda/lib/python3.6/site-packages/smdistributed/dataparallel/__init__.py&quot;, line 16, in &lt;module&gt;
    import smddpcommon as hc
ImportError: libc10.so: cannot open shared object file: No such file or directory
</code></pre>
",5157515.0,,,,,2022-12-17 14:22:53,ImportError: libc10.so: cannot open shared object file: No such file or directory,<python><pytorch>,2,2,0.0,,,CC BY-SA 4.0
65739700,1,,,2021-01-15 16:21:29,,13,40585,"<p>If I run the following:</p>
<pre><code>import torch
import sys
print('A', sys.version)
print('B', torch.__version__)
print('C', torch.cuda.is_available())
print('D', torch.backends.cudnn.enabled)
device = torch.device('cuda')
print('E', torch.cuda.get_device_properties(device))
print('F', torch.tensor([1.0, 2.0]).cuda())
</code></pre>
<p>I get this:</p>
<pre><code>A 3.7.5 (default, Nov  7 2019, 10:50:52) 
[GCC 8.3.0]
B 1.8.0.dev20210115+cu110
C True
D True
E _CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24267MB, multi_processor_count=82)
F 
&lt;stacktrace&gt;
CUDA error: no kernel image is available for execution on the device
</code></pre>
<p>More info about my system:</p>
<ul>
<li>Nvidia version: NVIDIA-SMI 455.38 Driver Version: 455.38 CUDA Version: 11.1</li>
<li>python 3.7, Ubuntu 18.04</li>
</ul>
",311744.0,,681865.0,,2021-01-15 23:44:47,2022-05-05 04:40:48,Pytorch CUDA error: no kernel image is available for execution on the device on RTX 3090 with cuda 11.1,<python><pytorch><nvidia>,2,0,0.0,,,CC BY-SA 4.0
72779926,1,75308606.0,,2022-06-28 01:53:38,,12,13155,"<p>I am creating an inference service with torch, gunicorn and flask that should use CUDA. To reduce resource requirements, I use the preload option of gunicorn, so the model is shared between the worker processes. However, this leads to an issue with CUDA. The following code snipped shows a minimal reproducing example:</p>
<pre><code>from flask import Flask, request
import torch

app = Flask('dummy')

model = torch.rand(500)
model = model.to('cuda:0')


@app.route('/', methods=['POST'])
def f():
    data = request.get_json()
    x = torch.rand((data['number'], 500))
    x = x.to('cuda:0')
    res = x * model
    return {
        &quot;result&quot;: res.sum().item()
    }
</code></pre>
<p>Starting the server with <code>CUDA_VISIBLE_DEVICES=1 gunicorn -w 3 -b $HOST_IP:8080 --preload run_server:app</code> lets the service start successfully. However, once doing the first request (<code>curl -X POST -d '{&quot;number&quot;: 1}'</code>), the worker throws the following error:</p>
<pre><code>[2022-06-28 09:42:00,378] ERROR in app: Exception on / [POST]
Traceback (most recent call last):
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/_compat.py&quot;, line 39, in reraise
    raise value
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File &quot;/home/user/.local/lib/python3.6/site-packages/flask/app.py&quot;, line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File &quot;/home/user/project/run_server.py&quot;, line 14, in f
    x = x.to('cuda:0')
  File &quot;/home/user/.local/lib/python3.6/site-packages/torch/cuda/__init__.py&quot;, line 195, in _lazy_init
    &quot;Cannot re-initialize CUDA in forked subprocess. &quot; + msg)
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
</code></pre>
<p>I load the model in the parent process and it's accessible to each forked worker process. The problem occurs when creating a CUDA-backed tensor in the worker process. This re-initializes the CUDA context in the worker process, which fails because it was already initialized in the parent process. If we set <code>x = data['number']</code> and remove <code>x = x.to('cuda:0')</code>, the inference succeeds.</p>
<p>Adding <code>torch.multiprocessing.set_start_method('spawn')</code> or <code>multiprocessing.set_start_method('spawn')</code> won't change anything, probably because gunicorn will definitely use <code>fork</code> when being started with the <code>--preload</code> option.</p>
<p>A solution could be not using the <code>--preload</code> option, which leads to multiple copies of the model in memory/GPU. But this is what I am trying to avoid.</p>
<p>Is there any possibility to overcome this issue <em>without</em> loading the model separately in each worker process?</p>
",2989330.0,,681865.0,,2022-06-28 02:00:49,2023-02-01 10:14:57,GUnicorn + CUDA: Cannot re-initialize CUDA in forked subprocess,<python><pytorch><python-multiprocessing><gunicorn>,2,7,0.0,,,CC BY-SA 4.0
67678874,1,,,2021-05-24 20:56:01,,12,805,"<p>My program for training a model in PyTorch converges worse than the TensorFlow implementation. When I switch to SGD instead of Adam, the losses are identical. With Adam, the losses are different starting at the very first epoch. I believe I'm using the same settings in both programs. Any thoughts on how to debug this would be helpful!</p>
<p><strong>Losses using SGD</strong></p>
<p>PyTorch</p>
<pre><code>0.1504615843296051
0.10858417302370071
0.08603279292583466
</code></pre>
<p>TensorFlow</p>
<pre><code>0.15046157
0.108584
0.08603277
</code></pre>
<p><strong>Losses using Adam</strong></p>
<p>PyTorch</p>
<pre><code>0.0031117501202970743
0.0020642257295548916
0.0019268309697508812
0.0016333406092599034
0.0017334128497168422
0.0014430736191570759
0.0010424457723274827
0.0012145100627094507
0.0011195113183930516
0.0009501167223788798
0.0009987876983359456
0.0007953296881169081
0.00075263757025823
0.0008374055614694953
0.000735406531020999
</code></pre>
<p>TensorFlow:</p>
<pre><code>0.0036667113
0.0032563617
0.0021536187
0.0015266595
0.0013580231
0.0013878695
0.0011856346
0.0011136091
0.00091276
0.000890126
0.00088381825
0.0007283067
0.00081382995
0.0006670901
0.00046282331
</code></pre>
<p><strong>Adam optimizer settings</strong></p>
<p>TF 1.15.3:</p>
<pre><code>adam_optimizer = tf.train.AdamOptimizer(learning_rate=5e-5)

# default parameters from the documentation at https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/python/training/adam.py#L32-L235:
# learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, use_locking=False, name=&quot;Adam&quot;)
</code></pre>
<p>PyTorch</p>
<pre><code>torch.optim.Adam(params=model.parameters(), lr=5e-5, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.0)
</code></pre>
<p><strong>Training</strong></p>
<ul>
<li>I loaded identical weights from file to initialize the two models.</li>
<li>I trained and tested on a single data sample, also loaded from file. I used 1000 iterations for training and 1 for test, batch size 1.</li>
</ul>
<p><strong>Prior debugging</strong></p>
<ul>
<li>As above, I used identical parameters and data</li>
<li>I ran a single forward-backward pass using the Adam optimizer and saved the data and gradients at each layer. I plotted the results. All looked the same and were within 1e-6 to 1e-10 of each other. The loss was also identical within rounding error.</li>
</ul>
<p><strong>Saving and loading the PyTorch model</strong></p>
<pre><code>def train(...):
    ...
    checkpoint = torch.load(checkpoint_file, map_location=device)
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    ...
    counter = 0
    while run:
            counter += 1
            if counter &gt; 1000:
                break

            in = np.load(&quot;debug_data/in.npy&quot;)
            out1 = np.load(&quot;debug_data/out1.npy&quot;)
            out2 = np.load(&quot;debug_data/out2.npy&quot;)

            # adjust from TF
            in = in.squeeze(3)
            in = np.expand_dims(in, axis=0)
            ... do the same for out1 and out2

        in, out1, out2 = \
                torch.from_numpy(in).to(device), \
                torch.from_numpy(out1).to(device), \
                torch.from_numpy(out2).to(device)

        optimizer.zero_grad()
        out1_hat, out2_hat = model(in)

        train_loss = loss_fn(out1_hat, out1) + loss_fn(out2_hat, out2)
        train_loss.backward()

        optimizer.step()

    save_checkpoint({'state_dict': model.state_dict(),
                    'optimizer': optimizer.state_dict()},
                    latest_filename=latest_checkpoint_path)
</code></pre>
<p><strong>Saving and loading the TensorFlow model</strong></p>
<pre><code>sess.run(tf.global_variables_initializer())
writer = tf.summary.FileWriter(my_path, graph=sess.graph)

restorer = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)
restorer.restore(sess, load_path)

saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)

counter = 0
while run:
    counter += 1
    if counter &gt; 1000:
        break

    in = np.load(&quot;&quot;)
    out1 = np.load(&quot;&quot;)
    out2 = np.load(&quot;&quot;)
    out1 = out1[0, :, :, :]
    out1 = out1[:, :, :, np.newaxis]
    out2 = out2[0, :, :, :]
    out2 = out2[:, :, :, np.newaxis]
    in = in[0, :, :, :]
    in = in[:, :, :, np.newaxis]
    _, _loss = sess.run([optimizer, loss],
    feed_dict={in: in, out1: out1, out2: out2})

save_path = saver.save(sess, my_save_path, global_step=int(_global_step))

sess.close()
tf.reset_default_graph()
</code></pre>
",4008884.0,,,,,2021-05-24 20:56:01,Suboptimal convergence in PyTorch compared to TensorFlow when using Adam optimizer,<tensorflow><deep-learning><pytorch><gradient-descent>,0,3,0.0,,,CC BY-SA 4.0
69148622,1,70919569.0,,2021-09-12 05:08:38,,12,5327,"<p>Suppose I have my custom loss function and I want to fit the solution of some differential equation with help of my neural network. So in each forward pass, I am calculating the output of my neural net and then calculating the loss by taking the MSE with the expected equation to which I want to fit my perceptron.</p>
<p>Now my doubt is: should I use <code>grad(loss)</code> or should I do <code>loss.backward()</code> for backpropagation to calculate and update my gradients?</p>
<p>I understand that while using loss.backward() I have to wrap my tensors with Variable and have to set the requires_grad = True for the variables w.r.t which I want to take the gradient of my loss.</p>
<p>So my questions are :</p>
<ul>
<li>Does <code>grad(loss)</code> also requires any such explicit parameter to identify the variables for gradient computation?</li>
<li>How does it actually compute the gradients?</li>
<li>Which approach is better?</li>
<li>what is the main difference between the two in a practical scenario.</li>
</ul>
<p>It would be better if you could explain the practical implications of both approaches because whenever I try to find it online I am just bombarded with a lot of stuff that isn't much relevant to my project.</p>
",16890158.0,,2956066.0,,2023-06-04 00:25:52,2023-06-04 00:25:52,Difference between autograd.grad and autograd.backward?,<pytorch><gradient><gradient-descent><backpropagation><autograd>,2,0,0.0,,,CC BY-SA 4.0
66577151,1,,,2021-03-11 05:53:06,,12,11272,"<p>I am using Google Colab for training a LeNet-300-100 fully-connected neural network on MNIST using Python3 and PyTorch 1.8.</p>
<p>To apply the transformations and download the MNIST dataset, the following code is being used:</p>
<pre><code># MNIST dataset statistics:
# mean = tensor([0.1307]) &amp; std dev = tensor([0.3081])
mean = np.array([0.1307])
std_dev = np.array([0.3081])

transforms_apply = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean = mean, std = std_dev)
    ])
</code></pre>
<p>which gives the error:</p>
<blockquote>
<p>Downloading
<a href=""http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz"" rel=""noreferrer"">http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz</a> to
./data/MNIST/raw/train-images-idx3-ubyte.gz
--------------------------------------------------------------------------- HTTPError                                 Traceback (most recent call
last)  in ()
2 train_dataset = torchvision.datasets.MNIST(
3         root = './data', train = True,
----&gt; 4         transform = transforms_apply, download = True
5         )
6</p>
<p>11 frames /usr/lib/python3.7/urllib/request.py in
http_error_default(self, req, fp, code, msg, hdrs)
647 class HTTPDefaultErrorHandler(BaseHandler):
648     def http_error_default(self, req, fp, code, msg, hdrs):
--&gt; 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)
650
651 class HTTPRedirectHandler(BaseHandler):</p>
<p>HTTPError: HTTP Error 503: Service Unavailable</p>
</blockquote>
<p>What's wrong?</p>
",3616293.0,,4685471.0,,2021-03-17 02:17:30,2021-08-03 14:50:53,HTTP Error when trying to download MNIST data,<python><pytorch>,7,3,0.0,,,CC BY-SA 4.0
67518928,1,68369031.0,,2021-05-13 12:04:43,,12,25039,"<p>I'm reseaching <a href=""https://github.com/facebookresearch/swav"" rel=""noreferrer"">self-supervised muchine learning code</a>.</p>
<p>And I have wanted to debug the code with <code>python debugger</code> not <code>pdb.set_trace()</code>.
This is python command for ubuntu terminal.</p>
<pre class=""lang-sh prettyprint-override""><code>python -m torch.distributed.launch --nproc_per_node=1 main_swav.py \
--data_path /dataset/imagenet/train \
--epochs 400 \
--base_lr 0.6 \
--final_lr 0.0006 \
--warmup_epochs 0 \
--batch_size 8 \
--size_crops 224 96 \
--nmb_crops 2 6 \
--min_scale_crops 0.14 0.05 \
--max_scale_crops 1. 0.14 \
--use_fp16 true \
--freeze_prototypes_niters 5005 \
--queue_length 380 \
--epoch_queue_starts 15\
--workers 10
</code></pre>
<p>In order to debug the code with VScode, I tried to revise launch.json like below as referring <a href=""https://stackoverflow.com/questions/46102228/how-to-debug-a-python-module-in-visual-studio-codes-launch-json"">stackoverflow -question</a></p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;version&quot;: &quot;0.2.0&quot;,
    &quot;configurations&quot;: [
        {
            &quot;name&quot;: &quot;Python: Current File&quot;,
            &quot;type&quot;: &quot;python&quot;,
            &quot;module&quot;: &quot;torch.distributed.launch --nproc_per_node=1 main_swav.py&quot;,
            &quot;request&quot;: &quot;launch&quot;,
            &quot;console&quot;: &quot;integratedTerminal&quot;,
            &quot;args&quot;: [&quot;--data_path&quot;, &quot;/dataset/imagenet/train&quot;]
        }
    ]
}
</code></pre>
<p>I knew this would not work... TT</p>
<p>Could you give me some advice?</p>
<p>Thank you for your time.</p>
",12906108.0,,,,,2023-02-20 04:45:45,How to make VScode launch.json for a Python module,<python><visual-studio-code><pytorch><vscode-debugger>,2,1,,,,CC BY-SA 4.0
67117097,1,,,2021-04-15 23:11:50,,12,34275,"<p>What is the reason for this error and how can I fix it? I am running the code from this repo: <a href=""https://github.com/facebookresearch/frankmocap"" rel=""noreferrer"">https://github.com/facebookresearch/frankmocap</a></p>
<pre><code>(frank) mona@goku:~/research/code/frankmocap$ python -m demo.demo_frankmocap --input_path ./sample_data/han_short.mp4 --out_dir ./mocap_output
Traceback (most recent call last):
  File &quot;/usr/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/usr/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/home/mona/research/code/frankmocap/demo/demo_frankmocap.py&quot;, line 25, in &lt;module&gt;
    from handmocap.hand_bbox_detector import HandBboxDetector
  File &quot;/home/mona/research/code/frankmocap/handmocap/hand_bbox_detector.py&quot;, line 33, in &lt;module&gt;
    from detectors.hand_object_detector.lib.model.roi_layers import nms # might raise segmentation fault at the end of program
  File &quot;/home/mona/research/code/frankmocap/detectors/hand_object_detector/lib/model/roi_layers/__init__.py&quot;, line 3, in &lt;module&gt;
    from .nms import nms
  File &quot;/home/mona/research/code/frankmocap/detectors/hand_object_detector/lib/model/roi_layers/nms.py&quot;, line 3, in &lt;module&gt;
    from model import _C
ImportError: /home/mona/research/code/frankmocap/detectors/hand_object_detector/lib/model/_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceIdEEPKNS_6detail12TypeMetaDataEv
</code></pre>
<p>I have:</p>
<pre><code>$ lsb_release -a
LSB Version:    core-11.1.0ubuntu2-noarch:security-11.1.0ubuntu2-noarch
Distributor ID: Ubuntu
Description:    Ubuntu 20.04.2 LTS
Release:    20.04
Codename:   focal
</code></pre>
<p>and</p>
<pre><code>$ python
Python 3.8.5 (default, Jan 27 2021, 15:41:15) 
[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.__version__
'1.8.1+cu111'
&gt;&gt;&gt; import detectron2
&gt;&gt;&gt; detectron2.__version__
'0.4'
&gt;&gt;&gt; from detectron2 import _C
</code></pre>
<p>and:</p>
<pre><code>$ python -m detectron2.utils.collect_env
/home/mona/venv/frank/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() &gt; 0
No CUDA runtime is found, using CUDA_HOME='/usr'
---------------------  --------------------------------------------------------------------------
sys.platform           linux
Python                 3.8.5 (default, Jan 27 2021, 15:41:15) [GCC 9.3.0]
numpy                  1.19.5
detectron2             0.4 @/home/mona/venv/frank/lib/python3.8/site-packages/detectron2
Compiler               GCC 7.3
CUDA compiler          CUDA 11.1
DETECTRON2_ENV_MODULE  &lt;not set&gt;
PyTorch                1.8.1+cu111 @/home/mona/venv/frank/lib/python3.8/site-packages/torch
PyTorch debug build    False
GPU available          False
Pillow                 8.1.0
torchvision            0.9.1+cu111 @/home/mona/venv/frank/lib/python3.8/site-packages/torchvision
fvcore                 0.1.3.post20210311
cv2                    4.5.1
---------------------  --------------------------------------------------------------------------
PyTorch built with:
  - GCC 7.3
  - C++ Version: 201402
  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v1.7.0 (Git Hash 7aed236906b1f7a05c0917e5257a1af05e9ff683)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.8.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, 
</code></pre>
",2414957.0,,,,,2023-05-23 15:05:47,_C.cpython-38-x86_64-linux-gnu.so: undefined symbol: _ZN6caffe28TypeMeta21_typeMetaDataInstanceIdEEPKNS_6detail12TypeMetaDataEv,<python><c><ubuntu><pytorch><detectron>,4,2,,,,CC BY-SA 4.0
64141188,1,,,2020-09-30 15:46:17,,12,9877,"<p>As I outlined here I am stuck using old versions of pytorch and torchvision due to hardware e.g. using ppc64le IBM architectures.</p>
<p>For this reason, I am having issues when sending and receiving checkpoints between different computers, clusters and my personal mac. I wonder if there is any way to load models in a way to avoid this issue? e.g. perhaps saving models in with a old and new format when using 1.6.x. Of course for the 1.3.1 to 1.6.x is impossible but at leat I was hoping something would work.</p>
<p>Any advice? Of course my ideal solution is that I don't have to worry about it and I can always load and save my checkpoints and everything I usually pickle uniformly across all my hardware.</p>
<hr />
<p>The first error I got was a zip jit error:</p>
<pre><code>RuntimeError: /home/miranda9/data/f.pt is a zip archive (did you mean to use torch.jit.load()?)
</code></pre>
<p>so I used that (and other pickle libraries):</p>
<pre><code># %%
import torch
from pathlib import Path


def load(path):
    import torch
    import pickle
    import dill

    path = str(path)
    try:
        db = torch.load(path)
        f = db['f']
    except Exception as e:
        db = torch.jit.load(path)
        f = db['f']
        #with open():
        # db = pickle.load(open(path, &quot;r+&quot;))
        # db = dill.load(open(path, &quot;r+&quot;))
        #raise ValueError(f'FAILED: {e}')
    return db, f

p = &quot;~/data/f.pt&quot;
path = Path(p).expanduser()

db, f = load(path)

Din, nb_examples = 1, 5
x = torch.distributions.Normal(loc=0.0, scale=1.0).sample(sample_shape=(nb_examples, Din))

y = f(x)

print(y)
print('Success!\a')
</code></pre>
<p>but I get complains of different pytorch versions which I am forced to use:</p>
<pre><code>Traceback (most recent call last):
  File &quot;hal_pg.py&quot;, line 27, in &lt;module&gt;
    db, f = load(path)
  File &quot;hal_pg.py&quot;, line 16, in load
    db = torch.jit.load(path)
  File &quot;/home/miranda9/.conda/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/jit/__init__.py&quot;, line 239, in load
    cpp_module = torch._C.import_ir_module(cu, f, map_location, _extra_files)
RuntimeError: version_number &lt;= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /opt/anaconda/conda-bld/pytorch-base_1581395437985/work/caffe2/serialize/inline_container.cc:131, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 1. Your PyTorch installation may be too old. (init at /opt/anaconda/conda-bld/pytorch-base_1581395437985/work/caffe2/serialize/inline_container.cc:131)
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0xbc (0x7fff7b527b9c in /home/miranda9/.conda/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: caffe2::serialize::PyTorchStreamReader::init() + 0x1d98 (0x7fff1d293c78 in /home/miranda9/.conda/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch.so)
frame #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0x88 (0x7fff1d2950d8 in /home/miranda9/.conda/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch.so)
frame #3: torch::jit::import_ir_module(std::shared_ptr&lt;torch::jit::script::CompilationUnit&gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, c10::optional&lt;c10::Device&gt;, std::unordered_map&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::hash&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::equal_to&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; &gt;&amp;) + 0x64 (0x7fff1e624664 in /home/miranda9/.conda/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch.so)
frame #4: &lt;unknown function&gt; + 0x70e210 (0x7fff7c0ae210 in /home/miranda9/.conda/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #5: &lt;unknown function&gt; + 0x28efc4 (0x7fff7bc2efc4 in /home/miranda9/.conda/envs/wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
&lt;omitting python frames&gt;
frame #26: &lt;unknown function&gt; + 0x25280 (0x7fff84b35280 in /lib64/libc.so.6)
frame #27: __libc_start_main + 0xc4 (0x7fff84b35474 in /lib64/libc.so.6)
</code></pre>
<p>any ideas how to make everything consistent across the clusters? I can't even open the pickle files.</p>
<hr />
<p>maybe this is just impossible with the current pytorch version I am forced to use :(</p>
<pre><code>RuntimeError: version_number &lt;= kMaxSupportedFileFormatVersion INTERNAL ASSERT FAILED at /opt/anaconda/conda-bld/pytorch-base_1581395437985/work/caffe2/serialize/inline_container.cc:131, please report a bug to PyTorch. Attempted to read a PyTorch file with version 3, but the maximum supported version for reading is 1. Your PyTorch installation may be too old. (init at /opt/anaconda/conda-bld/pytorch-base_1581395437985/work/caffe2/serialize/inline_container.cc:131)
frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0xbc (0x7fff83ba7b9c in /home/miranda9/.conda/envs/automl-meta-learning_wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: caffe2::serialize::PyTorchStreamReader::init() + 0x1d98 (0x7fff25993c78 in /home/miranda9/.conda/envs/automl-meta-learning_wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch.so)
frame #2: caffe2::serialize::PyTorchStreamReader::PyTorchStreamReader(std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;) + 0x88 (0x7fff259950d8 in /home/miranda9/.conda/envs/automl-meta-learning_wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch.so)
frame #3: torch::jit::import_ir_module(std::shared_ptr&lt;torch::jit::script::CompilationUnit&gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const&amp;, c10::optional&lt;c10::Device&gt;, std::unordered_map&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt;, std::hash&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::equal_to&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt;, std::allocator&lt;std::pair&lt;std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; const, std::__cxx11::basic_string&lt;char, std::char_traits&lt;char&gt;, std::allocator&lt;char&gt; &gt; &gt; &gt; &gt;&amp;) + 0x64 (0x7fff26d24664 in /home/miranda9/.conda/envs/automl-meta-learning_wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch.so)
frame #4: &lt;unknown function&gt; + 0x70e210 (0x7fff8472e210 in /home/miranda9/.conda/envs/automl-meta-learning_wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
frame #5: &lt;unknown function&gt; + 0x28efc4 (0x7fff842aefc4 in /home/miranda9/.conda/envs/automl-meta-learning_wmlce-v1.7.0-py3.7/lib/python3.7/site-packages/torch/lib/libtorch_python.so)
&lt;omitting python frames&gt;
frame #23: &lt;unknown function&gt; + 0x25280 (0x7fff8d335280 in /lib64/libc.so.6)
frame #24: __libc_start_main + 0xc4 (0x7fff8d335474 in /lib64/libc.so.6)

</code></pre>
<p>using code:</p>
<pre><code>from pathlib import Path

import torch

path = '/home/miranda9/data/dataset/'
path = Path(path).expanduser() / 'fi_db.pt'
path = str(path)

# db = torch.load(path)
# torch.jit.load(path)
db = torch.jit.load(str(path))

print(db)
</code></pre>
<hr />
<p>related links:</p>
<ul>
<li><a href=""https://stackoverflow.com/questions/64141188/how-to-load-checkpoints-across-different-versions-of-pytorch-1-3-1-and-1-6-x-u"">How to load checkpoints across different versions of pytorch (1.3.1 and 1.6.x) using ppc64le and x86?</a></li>
<li><a href=""https://discuss.pytorch.org/t/how-to-load-checkpoints-across-different-versions-of-pytorch-1-3-1-and-1-6-x-using-ppc64le-and-x86/97829"" rel=""noreferrer"">https://discuss.pytorch.org/t/how-to-load-checkpoints-across-different-versions-of-pytorch-1-3-1-and-1-6-x-using-ppc64le-and-x86/97829</a></li>
<li>related gitissue: <a href=""https://github.com/pytorch/pytorch/issues/43766"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/43766</a></li>
<li>reddit: <a href=""https://www.reddit.com/r/pytorch/comments/jvza7v/how_to_load_checkpoints_across_different_versions/"" rel=""noreferrer"">https://www.reddit.com/r/pytorch/comments/jvza7v/how_to_load_checkpoints_across_different_versions/</a></li>
</ul>
",1601580.0,,1601580.0,,2020-11-17 19:28:06,2021-09-14 03:42:40,How to load checkpoints across different versions of pytorch (1.3.1 and 1.6.x) using ppc64le and x86?,<pytorch><ppc64le>,4,2,,,,CC BY-SA 4.0
63423463,1,,,2020-08-15 07:00:17,,11,32530,"<p>I am using MacBook Pro (16-inch, 2019, macOS 10.15.5 (19F96))</p>
<p>GPU</p>
<ul>
<li>AMD Radeon Pro 5300M</li>
<li>Intel UHD Graphics 630</li>
</ul>
<p>I am trying to use Pytorch with Cuda on my mac.</p>
<p>All of the guides I saw assume that i have Nvidia graphic card.</p>
<p>I found this: <a href=""https://github.com/pytorch/pytorch/issues/10657"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/10657</a> issue, but it looks like I need to install ROCm, and according to their <a href=""https://github.com/pytorch/pytorch/issues/10657"" rel=""noreferrer"">Supported Operating Systems</a>, it only supports Linux.</p>
<p>Is it possible to run Pytorch on GPU using mac and AMD Graphic card?</p>
",12918011.0,,681865.0,,2020-08-17 14:30:15,2022-06-15 13:20:43,Using pytorch Cuda on MacBook Pro,<macos><pytorch>,4,0,0.0,,,CC BY-SA 4.0
65932328,1,66012521.0,,2021-01-28 06:56:53,,11,24191,"<p>If we use a combination of the <code>Dataset</code> and <code>Dataloader</code> classes (as shown below), I have to explicitly load the data onto the <strong>GPU</strong> using <code>.to()</code> or <code>.cuda()</code>. Is there a way to instruct the dataloader to do it automatically/implicitly?</p>
<p>Code to understand/reproduce the scenario:</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Dataset, DataLoader
import numpy as np

class DemoData(Dataset):
    def __init__(self, limit):
        super(DemoData, self).__init__()
        self.data = np.arange(limit)

    def __len__(self):
        return self.data.shape[0]

    def __getitem__(self, idx):
        return (self.data[idx], self.data[idx]*100)

demo = DemoData(100)

loader = DataLoader(demo, batch_size=50, shuffle=True)

for i, (i1, i2) in enumerate(loader):
    print('Batch Index: {}'.format(i))
    print('Shape of data item 1: {}; shape of data item 2: {}'.format(i1.shape, i2.shape))
    # i1, i2 = i1.to('cuda:0'), i2.to('cuda:0')
    print('Device of data item 1: {}; device of data item 2: {}\n'.format(i1.device, i2.device))
</code></pre>
<p>Which will output the following; note - without explicit device transfer instruction, the data is loaded onto <strong>CPU</strong>:</p>
<pre class=""lang-bash prettyprint-override""><code>Batch Index: 0
Shape of data item 1: torch.Size([50]); shape of data item 2: torch.Size([50])
Device of data item 1: cpu; device of data item 2: cpu

Batch Index: 1
Shape of data item 1: torch.Size([50]); shape of data item 2: torch.Size([50])
Device of data item 1: cpu; device of data item 2: cpu
</code></pre>
<p>A possible solution is at <a href=""https://github.com/pytorch/pytorch/issues/11372#issuecomment-589981738"" rel=""noreferrer"">this PyTorch GitHub repo. Issue</a>(<em>still open at the time this question was posted</em>), but, I am unable to make it to work when the dataloader has to return multiple data-items!</p>
",14739759.0,,6331369.0,,2021-02-05 12:04:57,2021-02-08 08:13:15,"PyTorch: while loading batched data using Dataloader, how to transfer the data to GPU automatically",<python-3.x><pytorch><gpu><pytorch-dataloader>,1,2,0.0,,,CC BY-SA 4.0
65205506,1,65314347.0,,2020-12-08 19:20:27,,11,2354,"<h3>TLDR:</h3>
<p>Autoencoder underfits timeseries reconstruction and just predicts average value.</p>
<h3>Question Set-up:</h3>
<p>Here is a summary of my attempt at a sequence-to-sequence autoencoder. This image was taken from this paper: <a href=""https://arxiv.org/pdf/1607.00148.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1607.00148.pdf</a>
<a href=""https://i.stack.imgur.com/RbmCF.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/RbmCF.png"" alt=""enter image description here"" /></a></p>
<p><strong>Encoder:</strong> Standard LSTM layer. Input sequence is encoded in the final hidden state.</p>
<p><strong>Decoder:</strong> LSTM Cell (I think!). Reconstruct the sequence one element at a time, starting with the last element <code>x[N]</code>.</p>
<p>Decoder algorithm is as follows for a sequence of length <code>N</code>:</p>
<ol>
<li>Get Decoder initial hidden state <code>hs[N]</code>: Just use encoder final hidden state.</li>
<li>Reconstruct last element in the sequence: <code>x[N]= w.dot(hs[N]) + b</code>.</li>
<li>Same pattern for other elements: <code>x[i]= w.dot(hs[i]) + b</code></li>
<li>use <code>x[i]</code> and <code>hs[i]</code> as inputs to <code>LSTMCell</code> to get <code>x[i-1]</code> and <code>hs[i-1]</code></li>
</ol>
<h3>Minimum Working Example:</h3>
<p>Here is my implementation, starting with the encoder:</p>
<pre><code>class SeqEncoderLSTM(nn.Module):
    def __init__(self, n_features, latent_size):
        super(SeqEncoderLSTM, self).__init__()
        
        self.lstm = nn.LSTM(
            n_features, 
            latent_size, 
            batch_first=True)
        
    def forward(self, x):
        _, hs = self.lstm(x)
        return hs
</code></pre>
<p>Decoder class:</p>
<pre><code>class SeqDecoderLSTM(nn.Module):
    def __init__(self, emb_size, n_features):
        super(SeqDecoderLSTM, self).__init__()
        
        self.cell = nn.LSTMCell(n_features, emb_size)
        self.dense = nn.Linear(emb_size, n_features)
        
    def forward(self, hs_0, seq_len):
        
        x = torch.tensor([])
        
        # Final hidden and cell state from encoder
        hs_i, cs_i = hs_0
        
        # reconstruct first element with encoder output
        x_i = self.dense(hs_i)
        x = torch.cat([x, x_i])
        
        # reconstruct remaining elements
        for i in range(1, seq_len):
            hs_i, cs_i = self.cell(x_i, (hs_i, cs_i))
            x_i = self.dense(hs_i)
            x = torch.cat([x, x_i])
        return x
</code></pre>
<p>Bringing the two together:</p>
<pre><code>class LSTMEncoderDecoder(nn.Module):
    def __init__(self, n_features, emb_size):
        super(LSTMEncoderDecoder, self).__init__()
        self.n_features = n_features
        self.hidden_size = emb_size

        self.encoder = SeqEncoderLSTM(n_features, emb_size)
        self.decoder = SeqDecoderLSTM(emb_size, n_features)
    
    def forward(self, x):
        seq_len = x.shape[1]
        hs = self.encoder(x)
        hs = tuple([h.squeeze(0) for h in hs])
        out = self.decoder(hs, seq_len)
        return out.unsqueeze(0)        
</code></pre>
<p>And here's my training function:</p>
<pre><code>def train_encoder(model, epochs, trainload, testload=None, criterion=nn.MSELoss(), optimizer=optim.Adam, lr=1e-6,  reverse=False):

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f'Training model on {device}')
    model = model.to(device)
    opt = optimizer(model.parameters(), lr)

    train_loss = []
    valid_loss = []

    for e in tqdm(range(epochs)):
        running_tl = 0
        running_vl = 0
        for x in trainload:
            x = x.to(device).float()
            opt.zero_grad()
            x_hat = model(x)
            if reverse:
                x = torch.flip(x, [1])
            loss = criterion(x_hat, x)
            loss.backward()
            opt.step()
            running_tl += loss.item()

        if testload is not None:
            model.eval()
            with torch.no_grad():
                for x in testload:
                    x = x.to(device).float()
                    loss = criterion(model(x), x)
                    running_vl += loss.item()
                valid_loss.append(running_vl / len(testload))
            model.train()
            
        train_loss.append(running_tl / len(trainload))
    
    return train_loss, valid_loss
</code></pre>
<h3>Data:</h3>
<p>Large dataset of events scraped from the news (ICEWS). Various categories exist that describe each event. I initially one-hot encoded these variables, expanding the data to 274 dimensions. However, in order to debug the model, I've cut it down to a single sequence that is 14 timesteps long and only contains 5 variables. Here is the sequence I'm trying to overfit:</p>
<pre><code>tensor([[0.5122, 0.0360, 0.7027, 0.0721, 0.1892],
        [0.5177, 0.0833, 0.6574, 0.1204, 0.1389],
        [0.4643, 0.0364, 0.6242, 0.1576, 0.1818],
        [0.4375, 0.0133, 0.5733, 0.1867, 0.2267],
        [0.4838, 0.0625, 0.6042, 0.1771, 0.1562],
        [0.4804, 0.0175, 0.6798, 0.1053, 0.1974],
        [0.5030, 0.0445, 0.6712, 0.1438, 0.1404],
        [0.4987, 0.0490, 0.6699, 0.1536, 0.1275],
        [0.4898, 0.0388, 0.6704, 0.1330, 0.1579],
        [0.4711, 0.0390, 0.5877, 0.1532, 0.2201],
        [0.4627, 0.0484, 0.5269, 0.1882, 0.2366],
        [0.5043, 0.0807, 0.6646, 0.1429, 0.1118],
        [0.4852, 0.0606, 0.6364, 0.1515, 0.1515],
        [0.5279, 0.0629, 0.6886, 0.1514, 0.0971]], dtype=torch.float64)
</code></pre>
<p>And here is the custom <code>Dataset</code> class:</p>
<pre><code>class TimeseriesDataSet(Dataset):
    def __init__(self, data, window, n_features, overlap=0):
        super().__init__()
        if isinstance(data, (np.ndarray)):
            data = torch.tensor(data)
        elif isinstance(data, (pd.Series, pd.DataFrame)):
            data = torch.tensor(data.copy().to_numpy())
        else: 
            raise TypeError(f&quot;Data should be ndarray, series or dataframe. Found {type(data)}.&quot;)
        
        self.n_features = n_features
        self.seqs = torch.split(data, window)
        
    def __len__(self):
        return len(self.seqs)
    
    def __getitem__(self, idx):
        try:    
            return self.seqs[idx].view(-1, self.n_features)
        except TypeError:
            raise TypeError(&quot;Dataset only accepts integer index/slices, not lists/arrays.&quot;)
</code></pre>
<h3>Problem:</h3>
<p>The model only learns the average, no matter how complex I make the model or now long I train it.</p>
<p>Predicted/Reconstruction:
<a href=""https://i.stack.imgur.com/vVCJS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vVCJS.png"" alt=""enter image description here"" /></a></p>
<p>Actual:</p>
<p><a href=""https://i.stack.imgur.com/7Z1cB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/7Z1cB.png"" alt=""enter image description here"" /></a></p>
<h3>My research:</h3>
<p>This problem is identical to the one discussed in this question: <a href=""https://stackoverflow.com/questions/54411662/lstm-autoencoder-always-returns-the-average-of-the-input-sequence"">LSTM autoencoder always returns the average of the input sequence</a></p>
<p>The problem in that case ended up being that the objective function was averaging the target timeseries before calculating loss. This was due to some broadcasting errors because the author didn't have the right sized inputs to the objective function.</p>
<p>In my case, I do not see this being the issue. I have checked and double checked that all of my dimensions/sizes line up. I am at a loss.</p>
<h3>Other Things I've Tried</h3>
<ol>
<li>I've tried this with varied sequence lengths from 7 timesteps to 100 time steps.</li>
<li>I've tried with varied number of variables in the time series. I've tried with univariate all the way to all 274 variables that the data contains.</li>
<li>I've tried with various <code>reduction</code> parameters on the <code>nn.MSELoss</code> module. The paper calls for <code>sum</code>, but I've tried both <code>sum</code> and <code>mean</code>. No difference.</li>
<li>The paper calls for reconstructing the sequence in reverse order (see graphic above). I have tried this method using the <code>flipud</code> on the original input (after training but before calculating loss). This makes no difference.</li>
<li>I tried making the model more complex by adding an extra LSTM layer in the encoder.</li>
<li>I've tried playing with the latent space. I've tried from 50% of the input number of features to 150%.</li>
<li>I've tried overfitting a single sequence (provided in the <strong>Data</strong> section above).</li>
</ol>
<h3>Question:</h3>
<p>What is causing my model to predict the average and how do I fix it?</p>
",3696204.0,,3696204.0,,2020-12-15 20:32:20,2020-12-17 10:13:16,LSTM Autoencoder problems,<python><neural-network><pytorch><lstm><autoencoder>,1,1,0.0,,,CC BY-SA 4.0
66750391,1,,,2021-03-22 16:50:12,,11,18681,"<p>I am using Pytorch. I got this RuntimeError while evaluating a model. Any idea how to solve this?</p>
",15454018.0,,,,,2022-02-20 12:08:46,RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces),<python><deep-learning><computer-vision><pytorch>,2,1,0.0,,,CC BY-SA 4.0
66074684,1,66075139.0,,2021-02-06 07:51:23,,11,16526,"<p>I just begin to learn Pytorch and create my first CNN. The dataset contains 3360 RGB images and I converted them to a <code>[3360, 3, 224, 224]</code> tensor. The data and label are in the <code>dataset(torch.utils.data.TensorDataset)</code>. Below is the training code.</p>
<pre class=""lang-py prettyprint-override""><code>    def train_net():
        dataset = ld.load()
        data_iter = Data.DataLoader(dataset, batch_size=168, shuffle=True)
        net = model.VGG_19()
        summary(net, (3, 224, 224), device=&quot;cpu&quot;)
        loss_func = nn.CrossEntropyLoss()
        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, dampening=0.1)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)
        for epoch in range(5):
            print(&quot;epoch:&quot;, epoch + 1)
            train_loss = 0
            for i, data in enumerate(data_iter, 0):
                x, y = data
                print(x.dtype)
                optimizer.zero_grad()
                out = net(x)
                loss = loss_func(out, y)
                loss.backward()
                optimizer.step()
                train_loss += loss.item()
                if i % 100 == 99:
                    print(&quot;loss:&quot;, train_loss / 100)
                    train_loss = 0.0
        print(&quot;finish train&quot;)
</code></pre>
<p>Then I have this error:</p>
<pre><code>    Traceback (most recent call last):
              File &quot;D:/python/DeepLearning/VGG/train.py&quot;, line 52, in &lt;module&gt;
                train_net()
              File &quot;D:/python/DeepLearning/VGG/train.py&quot;, line 29, in train_net
                out = net(x)
              File &quot;D:\python\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
                result = self.forward(*input, **kwargs)
              File &quot;D:\python\DeepLearning\VGG\model.py&quot;, line 37, in forward
                out = self.conv3_64(x)
              File &quot;D:\python\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
                result = self.forward(*input, **kwargs)
              File &quot;D:\python\lib\site-packages\torch\nn\modules\container.py&quot;, line 117, in forward
                input = module(input)
              File &quot;D:\python\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
                result = self.forward(*input, **kwargs)
              File &quot;D:\python\lib\site-packages\torch\nn\modules\conv.py&quot;, line 423, in forward
                return self._conv_forward(input, self.weight)
              File &quot;D:\python\lib\site-packages\torch\nn\modules\conv.py&quot;, line 419, in _conv_forward
                return F.conv2d(input, weight, self.bias, self.stride,
            RuntimeError: expected scalar type Double but found Float
</code></pre>
<p>I think there is something wrong with x and I print its type by <code>print(x.dtype)</code>:</p>
<pre><code>torch.float64
</code></pre>
<p>which is double instead of float. Do you know what`s wrong? Thanks for your help!</p>
",15156880.0,,7013263.0,,2021-02-06 10:11:26,2021-07-27 07:30:38,"""RuntimeError: expected scalar type Double but found Float"" in Pytorch CNN training",<python><deep-learning><pytorch><tensor><scalar>,1,1,0.0,,,CC BY-SA 4.0
74724120,1,,,2022-12-07 23:59:21,,11,4845,"<p>I'm training a model in PyTorch 1.13.0 (I have also tried this on the nightly build torch-1.14.0.dev20221207 to no avail) on my M1 Mac and would like to use MPS hardware acceleration. I have the following relevant code in my project to send the model and input tensors to MPS:</p>
<pre><code>device = torch.device(&quot;mps&quot; if torch.backends.mps.is_available() else &quot;cpu&quot;) # This always results in MPS

model.to(device)
</code></pre>
<p>... And in my Dataset subclass:</p>
<pre><code>class MyDataset(Dataset):
    def __init__(self, df, window_size):
        self.df = df
        self.window_size = window_size
        self.data = []
        self.labels = []
        for i in range(len(df) - window_size):
            x = torch.tensor(df.iloc[i:i+window_size].values, dtype=torch.float, device=device)
            y = torch.tensor(df.iloc[i+window_size].values, dtype=torch.float, device=device)
            self.data.append(x)
            self.labels.append(y)
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]
</code></pre>
<p>This results in the following traceback during my first training step:</p>
<pre><code>Traceback (most recent call last):
  File &quot;lstm_model.py&quot;, line 263, in &lt;module&gt;
    train_losses, val_losses = train_model(model, criterion, optimizer, train_loader, val_loader, epochs=100)
  File &quot;lstm_model.py&quot;, line 212, in train_model
    train_loss += train_step(model, criterion, optimizer, x, y)
  File &quot;lstm_model.py&quot;, line 191, in train_step
    y_pred = model(x)
  File &quot;miniconda3/envs/pytenv/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;lstm_model.py&quot;, line 182, in forward
    out, _ = self.lstm(x, (h0, c0))
  File &quot;miniconda3/envs/pytenv/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;miniconda3/envs/pytenv/lib/python3.10/site-packages/torch/nn/modules/rnn.py&quot;, line 774, in forward
    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
RuntimeError: Placeholder storage has not been allocated on MPS device!
</code></pre>
<p>I've tried creating tensors in my Dataset subclass without a device specified and then calling <code>.to(device)</code> on them:</p>
<pre><code>x = torch.tensor(df.iloc[i:i+window_size].values, dtype=torch.float)
x = x.to(device)
y = torch.tensor(df.iloc[i+window_size].values, dtype=torch.float)
y = y.to(device)
</code></pre>
<p>I've also tried creating the tensors without a device specified in my Dataset subclass and sending tensors to <code>device</code> in both the <code>forward</code> method of my model and in my <code>train_step</code> function.</p>
<p>How can I resolve my error?</p>
",9967715.0,,9967715.0,,2022-12-08 02:33:06,2023-03-14 08:15:58,PyTorch on M1 Mac: RuntimeError: Placeholder storage has not been allocated on MPS device,<pytorch><gpu><apple-m1>,2,3,,,,CC BY-SA 4.0
69994731,1,72484905.0,,2021-11-16 19:01:04,,11,5268,"<p>According to <a href=""https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/"" rel=""noreferrer"">https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/</a></p>
<p>We can use:</p>
<pre><code>   with torch.cuda.amp.autocast():
      loss = model(data)
</code></pre>
<p>In order to casts operations to mixed precision.</p>
<p>Another thing is that we can use  <code>model.half()</code> to convert all the model weights to half precision.</p>
<ol>
<li>What is the difference between these 2 commands ?</li>
<li>If I want to take advantage of <code>FP16</code> (in order to create larger models and shorter training time), what do I need ?
Do I need to use  <code>model.half()</code> or using <code>torch.cuda.amp</code> (according the link above) ?</li>
</ol>
",3668129.0,,1695960.0,,2021-11-16 19:03:00,2022-06-03 09:18:02,What is the difference between cuda.amp and model.half()?,<pytorch><nvidia>,1,0,0.0,,,CC BY-SA 4.0
68606661,1,68609343.0,,2021-08-01 00:25:48,,11,10751,"<p>I am just learning to use PyTorch as a beginner. If anyone is familiar with PyTorch, would you tell me the difference between <code>nn.Module</code> and <code>nn.Sequential</code>?</p>
<p>My questions are</p>
<ol>
<li><p>What is the advantage to use <code>nn.Module</code> instead of <code>nn.Sequential</code>?</p>
</li>
<li><p>Which is regularly utilised to build the model?</p>
</li>
<li><p>How we should select <code>nn.Module</code> or <code>nn.Sequential</code>?</p>
</li>
</ol>
",16532921.0,,16532921.0,,2021-08-02 11:23:05,2022-01-24 09:49:06,What is difference between nn.Module and nn.Sequential,<python><pytorch>,1,0,0.0,,,CC BY-SA 4.0
66389707,1,74993496.0,,2021-02-26 16:45:13,,11,3208,"<p>I am learning the Transformer. Here is the pytorch document for <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention"" rel=""noreferrer"">MultiheadAttention</a>. In their <a href=""https://github.com/pytorch/pytorch/blob/7a178a8a523d4653a3a2fa10c573b71e7fab1b9a/torch/nn/modules/activation.py#L874"" rel=""noreferrer"">implementation</a>, I saw there is a constraint:</p>
<pre><code> assert self.head_dim * num_heads == self.embed_dim, &quot;embed_dim must be divisible by num_heads&quot;
</code></pre>
<p>Why require the constraint: <code>embed_dim must be divisible by num_heads?</code>  If we go back to the equation</p>
<p><a href=""https://i.stack.imgur.com/T0niJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/T0niJ.png"" alt=""MultiHead(Q,K,V)=Concat(head1​,…,headh​)WOwhereheadi​=Attention(QWiQ​,KWiK​,VWiV​)"" /></a></p>
<p>Assume:
<code>Q</code>, <code>K</code>,<code>V</code> are <code>n x emded_dim</code> matrices; all the weight matrices <code>W</code> is <code>emded_dim x head_dim</code>,</p>
<p>Then, the concat <code>[head_i, ..., head_h]</code> will be a <code>n x (num_heads*head_dim)</code> matrix;</p>
<p><code>W^O</code> with size <code>(num_heads*head_dim) x embed_dim</code></p>
<p><code>[head_i, ..., head_h] * W^O</code> will become a <code>n x embed_dim</code> output</p>
<p>I don't know why we require <code>embed_dim must be divisible by num_heads</code>.</p>
<p>Let say we have <code>num_heads=10000</code>, the resuts are the same, since the matrix-matrix product will absort this information.</p>
",5865579.0,,,,,2023-01-03 13:02:30,Why embed dimemsion must be divisible by num of heads in MultiheadAttention?,<python-3.x><pytorch><transformer-model><attention-model>,2,0,0.0,,,CC BY-SA 4.0
66857829,1,,,2021-03-29 16:19:34,,11,917,"<p>I noticed that the docs do not have that function. Thus, it's unclear where one should be calling that. Does one have to:</p>
<ol>
<li>call it at the end of each worker code (i.e. inside of mp.spawn)</li>
<li>or call it outside of mp.spawn i.e. by the main process</li>
</ol>
<p>Note there is a gitissue requesting to put this function on the docs: <a href=""https://github.com/pytorch/pytorch/issues/48203"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/48203</a></p>
<p>this is an example of what 2 means:</p>
<pre><code>def test_setup():
    print('test_setup')
    if torch.cuda.is_available():
        world_size = torch.cuda.device_count()
    else:
        world_size = 4
    master_port = find_free_port()
    mp.spawn(setup_process, args=(world_size, master_port), nprocs=world_size)
    dist.destroy_process_group()
    print('successful test_setup!')
</code></pre>
",1601580.0,,168986.0,,2022-10-23 07:50:29,2023-04-27 13:04:58,Where is one supposed to call torch.distributed.destroy_process_group() in Pytorch?,<machine-learning><neural-network><pytorch><conv-neural-network><distributed-computing>,1,0,,,,CC BY-SA 4.0
62724824,1,62727799.0,,2020-07-04 03:00:40,,11,4123,"<p>Regarding <a href=""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"" rel=""noreferrer"">Pytorch Optimizer's Schedulers</a>, what actually means the <code>last_epoch</code> argument?</p>
<p>It says</p>
<blockquote>
<p><code>last_epoch (int)</code> – The index of last epoch. Default: <code>-1</code>.</p>
</blockquote>
<p>But it doesn't really explains much for those, like me, are just learning about these schedules.</p>
<p>I read most of that documentation, if not all, and I could understand what it does and why.</p>
",2313889.0,,,,,2020-07-04 10:14:32,What is the param `last_epoch` on Pytorch Optimizer's Schedulers is for?,<pytorch>,1,0,,,,CC BY-SA 4.0
67096544,1,67100802.0,,2021-04-14 17:37:29,,11,8088,"<p>What is the working of Output_padding in Conv2dTranspose? Please Help me to understand this?</p>
<pre><code>Conv2dTranspose(1024, 512, kernel_size=3, stride=2, padding=1, output_padding=1)
</code></pre>
",13183261.0,,,,,2021-04-15 00:49:54,What output_padding does in nn.ConvTranspose2d?,<pytorch><conv-neural-network><convolution><deconvolution>,1,0,,,,CC BY-SA 4.0
69968477,1,,,2021-11-15 01:02:07,,10,32919,"<p>I am working on this model:</p>
<pre><code>class Model(torch.nn.Module):
    def __init__(self, sizes, config):
        super(Model, self).__init__()

        self.lstm = []
        for i in range(len(sizes) - 2):
            self.lstm.append(LSTM(sizes[i], sizes[i+1], num_layers=8))
        self.lstm.append(torch.nn.Linear(sizes[-2], sizes[-1]).cuda())
        self.lstm = torch.nn.ModuleList(self.lstm)

        self.config_mel = config.mel_features

    def forward(self, x):
        # convert to log-domain
        x = x.clip(min=1e-6).log10()

        for layer in self.lstm[:-1]:
            x, _ = layer(x)
            x = torch.relu(x)

        #x = torch_unpack_seq(x)[0]

        x = self.lstm[-1](x)
        mask = torch.sigmoid(x)

        return mask
</code></pre>
<p>and then:</p>
<pre><code>model = Model(model_width, config)
model.cuda()
</code></pre>
<p>But I am getting this error:</p>
<pre><code>File &quot;main.py&quot;, line 29, in &lt;module&gt;
    Model.train(args)
  File &quot;.../src/model.py&quot;, line 57, in train
    model.cuda()
  File &quot;.../.local/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 637, in cuda
    return self._apply(lambda t: t.cuda(device))
  File &quot;.../.local/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 530, in _apply
    module._apply(fn)
  File &quot;/.../.local/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 530, in _apply
    module._apply(fn)
  File &quot;.../.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py&quot;, line 189, in _apply
    self.flatten_parameters()
  File &quot;.../.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py&quot;, line 175, in flatten_parameters
    torch._cudnn_rnn_flatten_weight(
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>I have no idea why it is happening. I am trying to push model and the inputs in cuda, and I understand if the error was due to some models in CPU and some in GPU. But that is not the case here. I found some pip install solution here: <a href=""https://stackoverflow.com/questions/65739700/pytorch-cuda-error-no-kernel-image-is-available-for-execution-on-the-device-on"">Pytorch CUDA error: no kernel image is available for execution on the device on RTX 3090 with cuda 11.1</a></p>
<p>but I cannot use it as I am trying to do the work in a remote repo where I don't have access to pip install.</p>
<p>Is there a way I can solve this?</p>
",14608622.0,,681865.0,,2021-11-15 01:39:58,2022-11-14 07:05:59,RuntimeError: CUDA error: no kernel image is available for execution on the device after model.cuda(),<python><pytorch>,3,1,,,,CC BY-SA 4.0
63930621,1,63931190.0,,2020-09-17 03:00:51,,10,21260,"<p>I have a question that How to get the total number of batch iteration from pytorch dataloader?</p>
<p>The following is a common code for training</p>
<pre><code>for i, batch in enumerate(dataloader):
</code></pre>
<p>Then, is there any method to get the total number of iteration for the &quot;for loop&quot;?</p>
<p>In my NLP problem, the total number of iteration is different from int(n_train_samples/batch_size)...</p>
<p>For example, if I truncate train data only 10,000 samples and set the batch size as 1024, then 363 iteration occurs in my NLP problem.</p>
<p>I wonder how to get the number of total iteration in &quot;the for-loop&quot;.</p>
<p>Thank you.</p>
",8064813.0,,,,,2020-09-18 06:49:07,How to get the total number of batch iteration from pytorch dataloader?,<for-loop><pytorch><dataloader>,2,0,,,,CC BY-SA 4.0
64206070,1,64206420.0,,2020-10-05 09:37:50,,10,27242,"<p><strong>Problem</strong></p>
<p>I'm trying to load a file using PyTorch, but the error states <code>archive/data.pkl</code> does not exist.</p>
<p><strong>Code</strong></p>
<pre><code>import torch
cachefile = 'cacheddata.pth'
torch.load(cachefile)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-4-8edf1f27a4bd&gt; in &lt;module&gt;
      1 import torch
      2 cachefile = 'cacheddata.pth'
----&gt; 3 torch.load(cachefile)

~/opt/anaconda3/envs/matching/lib/python3.8/site-packages/torch/serialization.py in load(f, map_location, pickle_module, **pickle_load_args)
    582                     opened_file.seek(orig_position)
    583                     return torch.jit.load(opened_file)
--&gt; 584                 return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
    585         return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)
    586 

~/opt/anaconda3/envs/matching/lib/python3.8/site-packages/torch/serialization.py in _load(zip_file, map_location, pickle_module, **pickle_load_args)
    837 
    838     # Load the data (which may in turn use `persistent_load` to load tensors)
--&gt; 839     data_file = io.BytesIO(zip_file.get_record('data.pkl'))
    840     unpickler = pickle_module.Unpickler(data_file, **pickle_load_args)
    841     unpickler.persistent_load = persistent_load

RuntimeError: [enforce fail at inline_container.cc:209] . file not found: archive/data.pkl
</code></pre>
<p><strong>Hypothesis</strong></p>
<p>I'm guessing this has something to do with pickle, from the <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html"" rel=""noreferrer"">docs</a>:</p>
<blockquote>
<p>This save/load process uses the most intuitive syntax and involves the
least amount of code. Saving a model in this way will save the entire
module using Python’s pickle module. The disadvantage of this approach
is that the serialized data is bound to the specific classes and the
exact directory structure used when the model is saved. The reason for
this is because pickle does not save the model class itself. Rather,
it saves a path to the file containing the class, which is used during
load time. Because of this, your code can break in various ways when
used in other projects or after refactors.</p>
</blockquote>
<p><strong>Versions</strong></p>
<ul>
<li>PyTorch version: 1.6.0</li>
<li>Python version: 3.8.0</li>
</ul>
",2205969.0,,2205969.0,,2020-10-05 09:59:46,2023-06-15 01:28:40,PyTorch - RuntimeError: [enforce fail at inline_container.cc:209] . file not found: archive/data.pkl,<python><pytorch><pickle>,6,0,0.0,,,CC BY-SA 4.0
70452465,1,70453596.0,,2021-12-22 16:50:13,,10,9952,"<p><strong>Goal:</strong> I am trying to import a graph <strong>FROM</strong> networkx into PyTorch geometric and <strong>set labels and node features</strong>.</p>
<p>(This is in Python)</p>
<p><strong>Question(s)</strong>:</p>
<ol>
<li>How do I do this [the conversion from networkx to PyTorch geometric]? (presumably by using the <code>from_networkx</code> function)</li>
<li><strong>How do I transfer over node features and labels?</strong> (more important question)</li>
</ol>
<p>I have seen some other/previous posts with this question but they weren't answered (correct me if I am wrong).</p>
<p><strong>Attempt</strong>: (I have just used an unrealistic example below, as I cannot post anything real on here)</p>
<p>Let us imagine we are trying to do a graph learning task (e.g. node classification) on a group of cars (not very realistic as I said). That is, we have a group of cars, an adjacency matrix, and some features  (e.g. price at the end of the year). We want to predict the node label (i.e. brand of the car).</p>
<p>I will be using the following adjacency matrix: (apologies, cannot use latex to format this)</p>
<p>A = [(0, 1, 0, 1, 1), (1, 0, 1, 1, 0), (0, 1, 0, 0, 1), (1, 1, 0, 0, 0), (1, 0, 1, 0, 0)]</p>
<p>Here is the code (for Google Colab environment):</p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from torch_geometric.utils.convert import to_networkx, from_networkx
import torch

!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.10.0+cpu.html

# Make the networkx graph
G = nx.Graph()

# Add some cars (just do 4 for now)
G.add_nodes_from([
      (1, {'Brand': 'Ford'}),
      (2, {'Brand': 'Audi'}),
      (3, {'Brand': 'BMW'}),
      (4, {'Brand': 'Peugot'}),
      (5, {'Brand': 'Lexus'}),
])

# Add some edges
G.add_edges_from([
                  (1, 2), (1, 4), (1, 5),
                  (2, 3), (2, 4),
                  (3, 2), (3, 5), 
                  (4, 1), (4, 2),
                  (5, 1), (5, 3)
])

# Convert the graph into PyTorch geometric
pyg_graph = from_networkx(G)
</code></pre>
<p>So this correctly converts the networkx graph to PyTorch Geometric. However, I still don't know how to properly set the labels.</p>
<p>The brand values for each node have been converted and are stored within:</p>
<pre><code>pyg_graph.Brand
</code></pre>
<p>Below, I have just made some random numpy arrays of length 5 for each node (just pretend that these are realistic).</p>
<pre><code>ford_prices = np.random.randint(100, size = 5)
lexus_prices = np.random.randint(100, size = 5)
audi_prices = np.random.randint(100, size = 5)
bmw_prices = np.random.randint(100, size = 5)
peugot_prices = np.random.randint(100, size = 5)
</code></pre>
<p>This brings me to the main question:</p>
<ul>
<li>How do I set the prices to be the node features of this graph?</li>
<li>How do I set the labels of the nodes? (and will I need to remove the labels from <code>pyg_graph.Brand</code> when training the network?)</li>
</ul>
<p>Thanks in advance and happy holidays.</p>
",15177497.0,,11339311.0,,2022-01-02 14:31:58,2022-01-02 14:31:58,How to load in graph from networkx into PyTorch geometric and set node features and labels?,<python><pytorch><networkx><pytorch-geometric>,1,0,0.0,,,CC BY-SA 4.0
62932368,1,,,2020-07-16 10:11:11,,10,10964,"<p>I would like to store thousands to millions of tensors with different shapes to disk. The goal is to use them as a time series dataset. The dataset will probably not fit into memory and I will have to load samples or ranges of samples from disk.</p>
<p>What is the best way to accomplish this while keeping storage and access time low?</p>
",3666302.0,,3666302.0,,2020-07-16 11:18:06,2021-05-03 12:02:43,Best way to save many tensors of different shapes?,<io><pytorch>,2,0,0.0,2023-05-01 19:46:54,,CC BY-SA 4.0
66646604,1,,,2021-03-15 22:33:42,,10,8600,"<p>I'm trying to run code that I wrote a week ago on Google Colab (and it worked), but I'm getting this error now for some reason.</p>
<pre><code>#libraries
import torch
import torchvision
from torchvision import datasets, transforms
import torch.nn as nn

train_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST(root='./data', train=True, download=True,
                             transform=torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize(
                                 (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size_train, shuffle=True)

test_loader = torch.utils.data.DataLoader(
  torchvision.datasets.MNIST(root='./data', train=False, download=True,
                             transform=torchvision.transforms.Compose([
                               torchvision.transforms.ToTensor(),
                               torchvision.transforms.Normalize(
                                 (0.1307,), (0.3081,))
                             ])),
  batch_size=batch_size_test, shuffle=True)
</code></pre>
<p>Output:</p>
<pre><code>Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz
---------------------------------------------------------------------------
HTTPError                                 Traceback (most recent call last)
&lt;ipython-input-3-03d990013546&gt; in &lt;module&gt;()
      6                                torchvision.transforms.ToTensor(),
      7                                torchvision.transforms.Normalize(
----&gt; 8                                  (0.1307,), (0.3081,))
      9                              ])),
     10   batch_size=batch_size_train, shuffle=True)

11 frames
/usr/lib/python3.7/urllib/request.py in http_error_default(self, req, fp, code, msg, hdrs)
    647 class HTTPDefaultErrorHandler(BaseHandler):
    648     def http_error_default(self, req, fp, code, msg, hdrs):
--&gt; 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)
    650 
    651 class HTTPRedirectHandler(BaseHandler):

HTTPError: HTTP Error 503: Service Unavailable
</code></pre>
<p>I'm not entirely sure what's wrong. Could they have moved the dataset?</p>
<p><strong>Update:</strong>
Looks like you can't access the mnist dataset <a href=""http://yann.lecun.com/exdb/"" rel=""noreferrer"">here</a> anymore</p>
",14735451.0,,14735451.0,,2021-04-18 21:57:02,2021-04-18 21:57:02,HTTP Error 503: Service Unavailable when trying to download MNIST data,<python><pytorch><google-colaboratory><mnist>,0,0,,2021-03-17 02:20:51,,CC BY-SA 4.0
67302634,1,67308015.0,,2021-04-28 14:55:10,,10,28526,"<p>I need to avoid downloading the model from the web (due to restrictions on the machine installed).</p>
<p>This works, but it downloads the model from the Internet</p>
<pre><code>model = torch.hub.load('pytorch/vision:v0.9.0', 'deeplabv3_resnet101', pretrained=True)
</code></pre>
<p>I have placed the <code>.pth</code> file and the <code>hubconf.py</code> file in the /tmp/ folder and changed my code to</p>
<pre><code>model = torch.hub.load('/tmp/', 'deeplabv3_resnet101', pretrained=True, source='local')
</code></pre>
<p>but to my surprise, it still downloads the model from the Internet. What am I doing wrong? How can I load the model locally?</p>
<p>Just to give you a bit more details, I'm doing all this in a Docker container that has a read-only volume at runtime, so that's why the download of new files fails.</p>
",2587784.0,,63550.0,,2022-03-25 22:59:00,2023-04-22 21:24:03,How do I load a local model with torch.hub.load?,<python><machine-learning><pytorch><torch><torchvision>,3,3,0.0,,,CC BY-SA 4.0
66788555,1,,,2021-03-24 20:06:13,,10,7197,"<p>How can I convert my own dataset to be usable by pytorch geometric for a graph neural network?</p>
<p>All the tutorials use existing dataset already converted to be usable by pytorch. For example if I have my own pointcloud dataset how can i use it to train for classification with graph neural network? What about my own image dataset for classification?</p>
",8738685.0,,11339311.0,,2022-01-03 16:08:19,2022-01-03 16:08:19,How to create a graph neural network dataset? (pytorch geometric),<python><pytorch><graph-databases><pytorch-geometric>,3,0,,,,CC BY-SA 4.0
64997817,1,,,2020-11-25 02:44:40,,10,7122,"<p>I know there is quite a bit of content out there about &quot;computing the Hessian&quot; in pytorch, but as far as I've seen I haven't found anything working for me. So to try to be most precise, the Hessian that I want is the Jacobian of the gradient of the loss with respect to the network parameters. Also called the matrix of second-order derivatives with respect to the parameters.</p>
<p>I found some code that works in an intuitive way, although shouldn't be fast. It clearly just computes the gradient of the gradient of the loss w.r.t. the params w.r.t the params, and it does it one element (of the gradient) at a time. I think the logic is definitely right but I am getting an error, having to do with <code>requires_grad</code>. I'm a pytorch beginner so maybe its a simple thing, but the error seems to be saying that it can't take the gradient of the <code>env_grads</code> variable, which is the output from the previous grad function call.</p>
<p>Any help with this would be greatly appreciated. Here is the code followed by the error message. I also printed out the <code>env_grads[0]</code> variable so we can see that it is in fact a tensor, which is the correct output from the previous <code>grad</code> call.</p>
<pre><code>env_loss = loss_fn(env_outputs, env_targets)
total_loss += env_loss
env_grads = torch.autograd.grad(env_loss, params,retain_graph=True)

print( env_grads[0] )
hess_params = torch.zeros_like(env_grads[0])
for i in range(env_grads[0].size(0)):
    for j in range(env_grads[0].size(1)):
        hess_params[i, j] = torch.autograd.grad(env_grads[0][i][j], params, retain_graph=True)[0][i, j] #  &lt;--- error here
print( hess_params )
exit()
</code></pre>
<p>Output:</p>
<pre><code>tensor([[-6.4064e-03, -3.1738e-03,  1.7128e-02,  8.0391e-03],
        [ 7.1698e-03, -2.4640e-03, -2.2769e-03, -1.0687e-03],
        [-3.0390e-04, -2.4273e-03, -4.0799e-02, -1.9149e-02],
        ...,
        [ 1.1258e-02, -2.5911e-05, -9.8133e-02, -4.6059e-02],
        [ 8.1502e-04, -2.5814e-03,  4.1772e-02,  1.9606e-02],
        [-1.0075e-02,  6.6072e-03,  8.3118e-04,  3.9011e-04]], device='cuda:0')
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/jefferythewind/anaconda3/envs/rapids3/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/home/jefferythewind/anaconda3/envs/rapids3/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/home/jefferythewind/Projects/Irina/learning-explanations-hard-to-vary/and_mask/run_synthetic.py&quot;, line 258, in &lt;module&gt;
    main(args)
  File &quot;/home/jefferythewind/Projects/Irina/learning-explanations-hard-to-vary/and_mask/run_synthetic.py&quot;, line 245, in main
    deep_mask=args.deep_mask
  File &quot;/home/jefferythewind/Projects/Irina/learning-explanations-hard-to-vary/and_mask/run_synthetic.py&quot;, line 103, in train
    scale_grad_inverse_sparsity=scale_grad_inverse_sparsity
  File &quot;/home/jefferythewind/Projects/Irina/learning-explanations-hard-to-vary/and_mask/and_mask_utils.py&quot;, line 154, in get_grads_deep
    hess_params[i, j] = torch.autograd.grad(env_grads[0][i][j], params, retain_graph=True)[0][i, j]
  File &quot;/home/jefferythewind/anaconda3/envs/rapids3/lib/python3.7/site-packages/torch/autograd/__init__.py&quot;, line 157, in grad
    inputs, allow_unused)
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
",959306.0,,,,,2022-03-17 11:41:39,How to compute Hessian of the loss w.r.t. the parameters in PyTorch using autograd.grad,<python><pytorch><hessian>,2,1,0.0,,,CC BY-SA 4.0
66028743,1,66030725.0,,2021-02-03 13:41:53,,10,1837,"<p>I'm implementing a U-Net based architecture in PyTorch. At train time, I've patches of size <code>256x256</code> which doesn't cause any problem. However at test time, I've full HD images (<code>1920x1080</code>). This is causing a problem during skip connections.</p>
<p>Downsampling <code>1920x1080</code> 3 times gives <code>240x135</code>. If I downsample one more time, the resolution becomes <code>120x68</code> which when upsampled gives <code>240x136</code>. Now, I cannot concatenate these two feature maps. How can I solve this?</p>
<p>PS: I thought this is a fairly common problem, but I didn't get any solution or even mentioning of this problem anywhere on the web. Am I missing something?</p>
",3337089.0,,2956066.0,,2021-02-03 20:14:32,2021-02-03 20:14:32,How to handle odd resolutions in Unet architecture PyTorch,<python><image-processing><deep-learning><pytorch><hourglass>,1,6,0.0,,,CC BY-SA 4.0
63746182,1,,,2020-09-04 18:13:18,,10,22912,"<p>I've looked everywhere but couldn't quite find what I want. Basically the MNIST dataset has images with pixel values in the range <code>[0, 255]</code>. People say that in general, it is good to do the following:</p>
<ul>
<li>Scale the data to the <code>[0,1]</code> range.</li>
<li>Normalize the data to have zero mean and unit standard deviation <code>(data - mean) / std</code>.</li>
</ul>
<p>Unfortunately, no one ever shows how to do both of these things. They all subtract a mean of <code>0.1307</code> and divide by a standard deviation of <code>0.3081</code>. These values are basically the mean and the standard deviation of the dataset divided by 255:</p>
<pre><code>from torchvision.datasets import MNIST        
import torchvision.transforms as transforms 

trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True)
print('Min Pixel Value: {} \nMax Pixel Value: {}'.format(trainset.data.min(), trainset.data.max()))
print('Mean Pixel Value {} \nPixel Values Std: {}'.format(trainset.data.float().mean(), trainset.data.float().std()))
print('Scaled Mean Pixel Value {} \nScaled Pixel Values Std: {}'.format(trainset.data.float().mean() / 255, trainset.data.float().std() / 255))
</code></pre>
<p>This outputs the following</p>
<pre><code>Min Pixel Value: 0 
Max Pixel Value: 255
Mean Pixel Value 33.31002426147461 
Pixel Values Std: 78.56748962402344
Scaled Mean: 0.13062754273414612 
Scaled Std: 0.30810779333114624
</code></pre>
<p>However clearly this does none of the above! The resulting data 1) will not be between <code>[0, 1]</code> and will not have mean <code>0</code> or std <code>1</code>. In fact this is what we are doing:</p>
<p><code>[data - (mean / 255)] / (std / 255)</code></p>
<p>which is <strong>very</strong> different from this</p>
<p><code>[(scaled_data) - (mean/255)] / (std/255)</code></p>
<p>where <code>scaled_data</code> is just <code>data / 255</code>.</p>
",6435921.0,,4685471.0,,2020-09-05 09:39:10,2023-06-27 17:25:46,Correct way of normalizing and scaling the MNIST dataset,<python><machine-learning><pytorch><mnist>,5,0,0.0,,,CC BY-SA 4.0
63169760,1,66602158.0,,2020-07-30 09:01:38,,10,10823,"<p>I am new to Numba and I need to use Numba to speed up some Pytorch functions. But I find even a very simple function does not work :(</p>
<pre><code>import torch
import numba

@numba.njit()
def vec_add_odd_pos(a, b):
    res = 0.
    for pos in range(len(a)):
        if pos % 2 == 0:
            res += a[pos] + b[pos]
    return res

x = torch.tensor([3, 4, 5.])
y = torch.tensor([-2, 0, 1.])
z = vec_add_odd_pos(x, y)
</code></pre>
<p>But the following error appears
def vec_add_odd_pos(a, b):
res = 0.
^</p>
<p>This error may have been caused by the following argument(s):</p>
<ul>
<li>argument 0: cannot determine Numba type of &lt;class 'torch.Tensor'&gt;</li>
<li>argument 1: cannot determine Numba type of &lt;class 'torch.Tensor'&gt;</li>
</ul>
<p>Can anyone help me? A link with more examples would be also appreciated. Thanks.</p>
",2669433.0,,2669433.0,,2020-07-30 10:00:11,2023-02-27 13:03:07,How can I use Numba for Pytorch tensors?,<pytorch><tensor><numba>,4,1,0.0,,,CC BY-SA 4.0
67522026,1,,,2021-05-13 15:35:16,,10,14105,"<p>I am running a pytorch NLP model in python and I keep encountering the following strange error:</p>
<pre><code>RuntimeError: [enforce fail at ..\c10\core\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough 
memory: you tried to allocate 364742208 bytes. Buy new RAM!
</code></pre>
<p>This is strange considering I have 16GB of RAM and the allocation is only 0.3GB. So I fail to understand the memory error.</p>
<p>Any help on the above will be appreciated.</p>
",15183282.0,,,,,2023-01-21 06:01:05,DefaultCPUAllocator: not enough memory: you tried to allocate 364742208 bytes. Buy new RAM,<python><memory><pytorch>,1,7,0.0,,,CC BY-SA 4.0
65327247,1,69076302.0,,2020-12-16 16:26:49,,10,25236,"<p>Is there a way to load a pytorch DataLoader (<code>torch.utils.data.Dataloader</code>) entirely into my GPU?</p>
<p>Now, I load every batch separately into my GPU.</p>
<pre class=""lang-py prettyprint-override""><code>CTX = torch.device('cuda')

train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=0,
)

net = Net().to(CTX)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)

for epoch in range(EPOCHS):
    for inputs, labels in test_loader:
        inputs = inputs.to(CTX)        # this is where the data is loaded into GPU
        labels = labels.to(CTX)        

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f'training accuracy: {net.validate(train_loader, device=CTX)}/{len(train_dataset)}')
    print(f'validation accuracy: {net.validate(test_loader, device=CTX)}/{len(test_dataset)}')
</code></pre>
<p>where the <code>Net.validate()</code> function is given by</p>
<pre class=""lang-py prettyprint-override""><code>def validate(self, val_loader, device=torch.device('cpu')):
    correct = 0
    for inputs, labels in val_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = torch.argmax(self(inputs), dim=1)
        correct += int(torch.sum(outputs==labels))
    return correct
</code></pre>
<br>
<p>I would like to improve the speed by loading the entire dataset <code>trainloader</code> into my GPU, instead of loading every batch separately. So, I would like to do something like</p>
<pre class=""lang-py prettyprint-override""><code>train_loader.to(CTX)
</code></pre>
<p>Is there an equivalent function for this? Because <code>torch.utils.data.DataLoader</code> does not have this attribute <code>.to()</code>.</p>
<p>I work with an NVIDIA GeForce RTX 2060 with CUDA Toolkit 10.2 installed.</p>
",9397607.0,,681865.0,,2020-12-16 17:50:13,2021-09-07 01:06:39,load pytorch dataloader into GPU,<python><pytorch><gpu><dataloader>,2,2,,,,CC BY-SA 4.0
65993494,1,65994247.0,,2021-02-01 13:23:08,,10,14467,"<p>What are the differences between <code>torch.flatten()</code> and <code>torch.nn.Flatten()</code>?</p>
",13439875.0,,2956066.0,,2021-02-01 20:36:47,2021-02-03 07:15:14,Difference between torch.flatten() and nn.Flatten(),<python><neural-network><pytorch><tensor><difference>,2,0,0.0,,,CC BY-SA 4.0
66261729,1,66463916.0,,2021-02-18 14:13:13,,10,2109,"<p>When I launch my main script on the cluster with ddp mode (2 GPU's), Pytorch Lightning duplicates whatever is executed in the main script, e.g. prints or other logic. I need some extended training logic, which I would like to handle myself. E.g. do something (once!) after <code>Trainer.fit()</code>. But with the duplication of the main script, this doesn't work as I intend. I also tried to wrap it in <code>if __name__ == &quot;__main__&quot;</code>, but it doesn't change behavior. How could one solve this problem? Or, how can I use some logic around my Trainer object, without the duplicates?</p>
",10318790.0,,,,,2023-01-26 05:27:09,Pytorch Lightning duplicates main script in ddp mode,<pytorch><multi-gpu><ddp><pytorch-lightning>,5,2,0.0,,,CC BY-SA 4.0
70148547,1,,,2021-11-29 00:09:35,,10,5993,"<p>I have been struggling for day to make torch work on WSL2 using an RTX 3080.</p>
<p>I Installed the CUDA-toolkit version 11.3</p>
<p>Running <code>nvcc -V</code> returns this :</p>
<pre class=""lang-sh prettyprint-override""><code>nvcc -V
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0
</code></pre>
<p><code>nvidia-smi</code> returns this</p>
<pre class=""lang-sh prettyprint-override""><code>Mon Nov 29 00:38:26 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.00       Driver Version: 510.06       CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |
| N/A   52C    P5    17W /  N/A |   1082MiB / 16384MiB |     N/A      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<p>I verified the installation of the toolkit with blackscholes</p>
<pre class=""lang-sh prettyprint-override""><code>./BlackScholes
[./BlackScholes] - Starting...
GPU Device 0: &quot;Ampere&quot; with compute capability 8.6

Initializing data...
...allocating CPU memory for options.
...allocating GPU memory for options.
...generating input data in CPU mem.
...copying input data to GPU mem.
Data init done.

Executing Black-Scholes GPU kernel (512 iterations)...
Options count             : 8000000
BlackScholesGPU() time    : 0.242822 msec
Effective memory bandwidth: 329.459087 GB/s
Gigaoptions per second    : 32.945909

BlackScholes, Throughput = 32.9459 GOptions/s, Time = 0.00024 s, Size = 8000000 options, NumDevsUsed = 1, Workgroup = 128

Reading back GPU results...
Checking the results...
...running CPU calculations.

Comparing the results...
L1 norm: 1.741792E-07
Max absolute error: 1.192093E-05

Shutting down...
...releasing GPU memory.
...releasing CPU memory.
Shutdown done.

[BlackScholes] - Test Summary

NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.

Test passed
</code></pre>
<p>And when I try to use torch, it doesn't find any GPU. Btw, I had to install torch==1.10.0+cu113 if I wanted to use torch with my RTX 3080 as the sm_ with the simple 1.10.0 version are not compatible with the rtx3080.</p>
<p>Running torch returns this :</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.version
&lt;module 'torch.version' from '/home/snihar/miniconda3/envs/tscbasset/lib/python3.7/site-packages/torch/version.py'&gt;
&gt;&gt;&gt; torch.version.cuda
'11.3'
&gt;&gt;&gt; torch.cuda.get_arch_list()
[]
&gt;&gt;&gt; torch.cuda.device_count()
0
&gt;&gt;&gt;  torch.cuda.current_device()
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/snihar/miniconda3/envs/tscbasset/lib/python3.7/site-packages/torch/cuda/__init__.py&quot;, line 479, in current_device
    _lazy_init()
  File &quot;/home/snihar/miniconda3/envs/tscbasset/lib/python3.7/site-packages/torch/cuda/__init__.py&quot;, line 214, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
</code></pre>
<p>At last, interestingly, I am completely able to run tensorflow-gpu on the same machine.</p>
<p>Installed pytorch like this : <code>conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch</code></p>
<p>Also, I managed to run pytorch in a docker container started from my WSL2 machine with this command :</p>
<pre><code>sudo docker run --gpus all -it --rm -v /home/...:/home/... nvcr.io/nvidia/pytorch:21.11-py3.  
</code></pre>
<p>When running pytorch on the windows machine I am running the WSL from, it works too. Both return  ['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'compute_37'] which says that the library is compatible with rtx 3080.</p>
",17535414.0,,681865.0,,2021-11-30 02:28:44,2022-06-14 18:40:53,WSL2 Pytorch - RuntimeError: No CUDA GPUs are available with RTX3080,<pytorch>,5,2,0.0,,,CC BY-SA 4.0
64751157,1,,,2020-11-09 11:53:49,,10,10458,"<p>I am working on Multiclass Classification (4 classes) for Language Task and I am using the BERT model for classification task. I am following <a href=""https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/"" rel=""noreferrer"">this blog as reference</a>. <strong>My BERT Fine Tuned model returns <code>nn.LogSoftmax(dim=1)</code>.</strong></p>
<p>My data is pretty imbalanced so I used <code>sklearn.utils.class_weight.compute_class_weight</code> to compute weights of the classes and used the weights inside the Loss.</p>
<pre><code>class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)
weights= torch.tensor(class_weights,dtype=torch.float)
cross_entropy  = nn.NLLLoss(weight=weights) 

</code></pre>
<p>My results were not so good so I thought of Experementing with  <code>Focal Loss</code> and have a code for Focal Loss.</p>
<pre><code>class FocalLoss(nn.Module):
  def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):
    super(FocalLoss, self).__init__()
    self.alpha = alpha
    self.gamma = gamma
    self.logits = logits
    self.reduce = reduce

  def forward(self, inputs, targets):
    BCE_loss = nn.CrossEntropyLoss()(inputs, targets)

    pt = torch.exp(-BCE_loss)
    F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss

    if self.reduce:
      return torch.mean(F_loss)
    else:
      return F_loss
</code></pre>
<p>I have 3 questions now. First and the Most important is</p>
<ol>
<li><strong>Should I use Class Weight with Focal Loss?</strong></li>
<li><strong>If I have to Implement weights inside this <code>Focal Loss</code>, can I use <code>weights</code> parameters inside <code> nn.CrossEntropyLoss()</code></strong></li>
<li>If this implement is incorrect, what should be the proper code for this one including the weights (if possible)</li>
</ol>
",11725056.0,,,,,2021-12-09 16:11:50,How to Use Class Weights with Focal Loss in PyTorch for Imbalanced dataset for MultiClass Classification,<python><machine-learning><deep-learning><neural-network><pytorch>,3,2,0.0,,,CC BY-SA 4.0
63979544,1,,,2020-09-20 13:33:59,,10,7299,"<p>When using a pre-trained BERT embeddings from pytorch (which are then fine-tuned), should the text data fed into the model be pre-processed like in any standard NLP task?</p>
<p>For instance, should  stemming, removing low frequency words, de-captilisation, be performed or should the raw text simply be passed to `transformers.BertTokenizer'?</p>
",4255748.0,,,,,2022-05-29 09:17:56,Using trained BERT Model and Data Preprocessing,<nlp><pytorch><bert-language-model>,3,0,0.0,,,CC BY-SA 4.0
69797614,1,69797906.0,,2021-11-01 13:27:42,,10,8360,"<p>I've seen this syntax to index a tensor in PyTorch, not sure what it means:</p>
<pre><code>v = torch.div(t, n[:, None])
</code></pre>
<p>where <code>v</code>, <code>t</code>, and <code>n</code> are tensors.</p>
<p>What is the role of &quot;<code>None</code>&quot; here? I can't seem to find it in the documentation.</p>
",5023456.0,,6331369.0,,2021-11-01 19:45:06,2021-11-01 19:45:06,Indexing a tensor with None in PyTorch,<indexing><syntax><pytorch><tensor>,1,0,,,,CC BY-SA 4.0
67730325,1,,,2021-05-27 21:42:03,,10,17943,"<p>I am training a PyTorch model to perform binary classification. My minority class makes up about 10% of the data, so I want to use a weighted loss function. The docs for <code>BCELoss</code> and <code>CrossEntropyLoss</code> say that I can use a <code>'weight'</code> for each sample.</p>
<p>However, when I declare <code>CE_loss = nn.BCELoss()</code> or <code>nn.CrossEntropyLoss()</code> and then do <code>CE_Loss(output, target, weight=batch_weights)</code>, where <code>output</code>, <code>target</code>, and <code>batch_weights</code> are <code>Tensor</code>s of <code>batch_size</code>, I get the following error message:</p>
<pre class=""lang-py prettyprint-override""><code>forward() got an unexpected keyword argument 'weight'
</code></pre>
",15859984.0,,10749432.0,,2021-05-27 22:02:36,2022-08-05 13:22:47,Using weights in CrossEntropyLoss and BCELoss (PyTorch),<pytorch><loss-function>,5,0,0.0,,,CC BY-SA 4.0
65467621,1,,,2020-12-27 15:57:14,,10,11013,"<p>I am <a href=""https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457/4"" rel=""noreferrer"">following</a> <a href=""https://github.com/pytorch/examples/blob/master/mnist/main.py"" rel=""noreferrer"">some</a> <a href=""https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627"" rel=""noreferrer"">tutorials</a> and I keep seeing different numbers that seem quite arbitrary to me in the <code>transforms</code> section</p>
<p>namely,</p>
<pre><code>transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
</code></pre>
<p>or</p>
<pre><code>transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
</code></pre>
<p>or</p>
<pre><code>transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
</code></pre>
<p>or others.</p>
<p>I wonder where these numbers arise, and how to know to select the correct ones?</p>
<p>I am about to use MNIST for sanity, but very soon to use my own unique dataset and will probably need my own normaliztion.</p>
",913098.0,,,,,2023-04-11 13:28:22,What are the numbers in torch.transforms.normalize and how to select them?,<python><machine-learning><deep-learning><pytorch><mnist>,2,0,0.0,,,CC BY-SA 4.0
63302534,1,63302819.0,,2020-08-07 13:09:08,,10,27652,"<p>I'm a beginner to Pytorch and wanted to type this statement as a whole if else statement:-</p>
<pre class=""lang-py prettyprint-override""><code>torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<p>Can somebody help me?</p>
",13993580.0,,681865.0,,2020-08-07 13:49:37,2020-08-07 13:49:37,How to write torch.device('cuda' if torch.cuda.is_available() else 'cpu') as a full if else statement?,<python><if-statement><pytorch><gpu>,1,0,,,,CC BY-SA 4.0
67710297,1,,,2021-05-26 17:55:48,,10,531,"<p>I'm trying to unpickle a pytorch tensor, but pickling it back yields different results across runs:</p>
<pre><code>&gt;&gt;&gt; import pickle

&gt;&gt;&gt; tensor1 = pickle.load(f) # I cannot reproduce the issue with some minimal manually-created tensor, only with this specific file
&gt;&gt;&gt; tensor2 = pickle.load(f)
&gt;&gt;&gt; pickled_tensor1 = pickle.dumps(tensor1) 
&gt;&gt;&gt; pickled_tensor2 = pickle.dumps(tensor2)
&gt;&gt;&gt; pickled_tensor1 == pickled_tensor2
False
</code></pre>
<p>Below are the values of <code>pickled_tensor1</code> and <code>pickled_tensor2</code> respectively:</p>
<pre><code>b'\x80\x04\x95\x98\x01\x00\x00\x00\x00\x00\x00\x8c\x0ctorch._utils\x94\x8c\x12_rebuild_tensor_v2\x94\x93\x94(\x8c\rtorch.storage\x94\x8c\x10_load_from_bytes\x94\x93\x94B\r\x01\x00\x00\x80\x02\x8a\nl\xfc\x9cF\xf9 j\xa8P\x19.\x80\x02M\xe9\x03.\x80\x02}q\x00(X\x10\x00\x00\x00protocol_versionq\x01M\xe9\x03X\r\x00\x00\x00little_endianq\x02\x88X\n\x00\x00\x00type_sizesq\x03}q\x04(X\x05\x00\x00\x00shortq\x05K\x02X\x03\x00\x00\x00intq\x06K\x04X\x04\x00\x00\x00longq\x07K\x04uu.\x80\x02(X\x07\x00\x00\x00storageq\x00ctorch\nFloatStorage\nq\x01X\x0f\x00\x00\x00140382183041680q\x02X\x03\x00\x00\x00cpuq\x03K\x04Ntq\x04Q.\x80\x02]q\x00X\x0f\x00\x00\x00140382183041680q\x01a.\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00?\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00?\x94\x85\x94R\x94K\x00K\x02K\x02\x86\x94K\x02K\x01\x86\x94\x89\x8c\x0bcollections\x94\x8c\x0bOrderedDict\x94\x93\x94)R\x94t\x94R\x94.'
b'\x80\x04\x95\x98\x01\x00\x00\x00\x00\x00\x00\x8c\x0ctorch._utils\x94\x8c\x12_rebuild_tensor_v2\x94\x93\x94(\x8c\rtorch.storage\x94\x8c\x10_load_from_bytes\x94\x93\x94B\r\x01\x00\x00\x80\x02\x8a\nl\xfc\x9cF\xf9 j\xa8P\x19.\x80\x02M\xe9\x03.\x80\x02}q\x00(X\x10\x00\x00\x00protocol_versionq\x01M\xe9\x03X\r\x00\x00\x00little_endianq\x02\x88X\n\x00\x00\x00type_sizesq\x03}q\x04(X\x05\x00\x00\x00shortq\x05K\x02X\x03\x00\x00\x00intq\x06K\x04X\x04\x00\x00\x00longq\x07K\x04uu.\x80\x02(X\x07\x00\x00\x00storageq\x00ctorch\nFloatStorage\nq\x01X\x0f\x00\x00\x00140382172016592q\x02X\x03\x00\x00\x00cpuq\x03K\x04Ntq\x04Q.\x80\x02]q\x00X\x0f\x00\x00\x00140382172016592q\x01a.\x04\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00?\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00?\x94\x85\x94R\x94K\x00K\x02K\x02\x86\x94K\x02K\x01\x86\x94\x89\x8c\x0bcollections\x94\x8c\x0bOrderedDict\x94\x93\x94)R\x94t\x94R\x94.'
</code></pre>
<p>My question is why is it happening and how can I prevent this?</p>
<p>I am using Python 3.8; pytorch 1.7.0</p>
<p>Cheers, Hlib.</p>
",1888471.0,,,,,2023-06-09 05:49:30,Pytorch tensor pickling inconsistent between runs,<python><pytorch><pickle>,1,7,0.0,,,CC BY-SA 4.0
67383458,1,67420239.0,,2021-05-04 10:59:52,,10,1396,"<p>I'm learning DRL with the book <strong>Deep Reinforcement Learning in Action</strong>. In chapter 3, they present the simple game Gridworld (<a href=""https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff#4283"" rel=""noreferrer"">instructions here</a>, in the rules section) with the corresponding code in <strong>PyTorch</strong>.</p>
<p>I've experimented with the code and it takes less than 3 minutes to train the network with 89% of wins (won 89 of 100 games after training).</p>
<p><a href=""https://i.stack.imgur.com/s5lAj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/s5lAj.png"" alt=""Training loss with pytorch"" /></a></p>
<p>As an exercise, I have migrated the code to <strong>tensorflow</strong>. All the code is <a href=""https://github.com/navi2000/drl_test"" rel=""noreferrer"">here</a>.</p>
<p>The problem is that with my tensorflow port it takes near 2 hours to train the network with a win rate of 84%. Both versions are using the only CPU to train (I don't have GPU)</p>
<p><a href=""https://i.stack.imgur.com/EtGIZ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/EtGIZ.png"" alt=""Training loss with tensorflow"" /></a></p>
<p>Training loss figures seem correct and also the rate of a win (we have to take into consideration that the game is random and can have impossible states). The problem is the performance of the overall process.</p>
<p>I'm doing something terribly wrong, but what?</p>
<p>The main differences are in the training loop, in torch is this:</p>
<pre><code>        loss_fn = torch.nn.MSELoss()
        learning_rate = 1e-3
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        ....
        Q1 = model(state1_batch) 
        with torch.no_grad():
            Q2 = model2(state2_batch) #B
        
        Y = reward_batch + gamma * ((1-done_batch) * torch.max(Q2,dim=1)[0])
        X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()
        loss = loss_fn(X, Y.detach())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
</code></pre>
<p>and in the tensorflow version:</p>
<pre><code>        loss_fn = tf.keras.losses.MSE
        learning_rate = 1e-3
        optimizer = tf.keras.optimizers.Adam(learning_rate)
        ...
        Q2 = model2(state2_batch) #B
        with tf.GradientTape() as tape:
            Q1 = model(state1_batch)
            Y = reward_batch + gamma * ((1-done_batch) * tf.math.reduce_max(Q2, axis=1))
            X = [Q1[i][action_batch[i]] for i in range(len(action_batch))]
            loss = loss_fn(X, Y)
        grads = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
</code></pre>
<p>Why is the training taking so long?</p>
",299897.0,,9215780.0,,2021-05-06 14:09:50,2021-05-13 12:42:44,Why is this tensorflow training taking so long?,<python><performance><tensorflow><deep-learning><pytorch>,1,9,0.0,,,CC BY-SA 4.0
64789217,1,66636234.0,,2020-11-11 15:20:25,,10,807,"<p>I may be mistaken, but it seems that PyTorch Transformers are autoregressive, which is what masking is for. However, I've seen some implementations where people use just the Encoder and output that directly to a <code>Linear</code> layer.</p>
<p>In my case, I'm trying to convert a spectrogram (rows are frequencies and columns are timesteps) to another spectrogram of the same dimensions. I'm having an impossible time trying to figure out how to do this.</p>
<p>For my model, I have:</p>
<pre><code>class TransformerReconstruct(nn.Module):
    def __init__(self, feature_size=250, num_layers=1, dropout=0.1, nhead=10, output_dim=1):
        super(TransformerReconstruct, self).__init__()
        self.model_type = 'Transformer'

        self.src_mask = None
        self.pos_encoder = PositionalEncoding(feature_size)
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=nhead, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
        self.decoder = nn.Linear(feature_size, output_dim)
        self.init_weights()

    def init_weights(self):
        initrange = 0.1
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, src):
        if self.src_mask is None or self.src_mask.size(0) != len(src):
            device = src.device
            mask = self._generate_square_subsequent_mask(len(src)).to(device)
            self.src_mask = mask

        src = self.pos_encoder(src)
        output = self.transformer_encoder(src, self.src_mask)
        output = self.decoder(output)
        return output

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask
</code></pre>
<p>And when training, I have:</p>
<pre><code>model = TransformerReconstruct(feature_size=128, nhead=8, output_dim=128, num_layers=6).to(device)
</code></pre>
<p>This returns the right shape, but doesn't seem to learn.</p>
<p>My basic training loop looks like:</p>
<pre><code>for i in range(0, len(data_source) - 1, input_window):
  data, target = get_batch(data_source, i, 1)
  output = recreate_model(data)
</code></pre>
<p>and I'm using an <code>MSELoss</code> and I'm trying to learn a very simple identity. Where the input and output are the same, however this is not learning. What could I be doing wrong? Thanks in advance.</p>
",239879.0,,,,,2021-03-30 09:06:22,How can I do a seq2seq task with PyTorch Transformers if I am not trying to be autoregressive?,<python><pytorch><transformer-model>,2,2,0.0,,,CC BY-SA 4.0
68106457,1,,,2021-06-23 20:07:05,,10,27822,"<p>Relatively new to using CUDA. I keep getting the following error after a seemingly random period of time:
RuntimeError: CUDA error: an illegal memory access was encountered</p>
<p>I have seen people suggest things such as using <code>cuda.set_device()</code> rather than <code>cuda.device()</code>, setting <code>torch.backends.cudnn.benchmark = False</code></p>
<p>but I can't seem to get the error to go away. Here are some pieces of my code:
<code>torch.cuda.set_device(torch.device('cuda:0'))</code>
<code>torch.backends.cudnn.benchmark = False</code></p>
<pre><code>class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)

        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()

        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))

        out = self.fc(out[:, -1, :]) 
        
        return out

    def pred(self, x):
        return self(x) &gt; 0

</code></pre>
<pre><code>def train(model, loss_fn, optimizer, num_epochs, x_train, y_train, x_val, y_val, loss_stop=60):
    cur_best_loss = 999
    loss_recur_count = 0
    best_model = None
    for t in range(num_epochs):
        model.train()

        y_train_pred = model(x_train)

        train_loss = loss_fn(y_train_pred, y_train)

        tr_l = train_loss.item()
        
        optimizer.zero_grad()

        train_loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():  
            y_val_pred = model(x_val)

            val_loss = loss_fn(y_val_pred, y_val)

            va_l = val_loss.item()
            
            if va_l &lt; cur_best_loss:
                cur_best_loss = va_l
                best_model = model
                loss_recur_count = 0
            else:
                loss_recur_count += 1

        if loss_recur_count == loss_stop:
            break
    if best_model is None:
        print(&quot;model is None.&quot;)
    return best_model
</code></pre>
<pre><code>def lstm_test(cols, df, test_percent, test_bal, initial_shares_test, max_price, last_sell_day):
    wdw = 20
    x_train, y_train, x_test, y_test, x_val, y_val = load_data(df, wdw, test_percent, cols)

    x_train = torch.from_numpy(x_train).type(torch.Tensor).cuda()
    x_test = torch.from_numpy(x_test).type(torch.Tensor).cuda()
    x_val = torch.from_numpy(x_val).type(torch.Tensor).cuda()
    y_train = torch.from_numpy(y_train).type(torch.Tensor).cuda()
    y_test = torch.from_numpy(y_test).type(torch.Tensor).cuda()
    y_val = torch.from_numpy(y_val).type(torch.Tensor).cuda()

    input_dim = x_train.shape[-1]
    hidden_dim = 32
    num_layers = 2
    output_dim = 1
    y_preds_dict = {}
    for i in range(11):
        model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers).cuda()

        r = (y_train.cpu().shape[0] - np.count_nonzero(y_train.cpu()))/np.count_nonzero(y_train.cpu())/2
        pos_w = torch.tensor([r]).cuda()

        loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_w).cuda()

        optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)

        best_model = train(model, loss_fn, optimizer, 300, x_train, y_train, x_val, y_val)
        
        y_test_pred = get_predictions(best_model, x_test)
        y_preds_dict[i] = y_test_pred.cpu().detach().numpy().flatten()
</code></pre>
<p>and here is the error msg:</p>
<pre><code>&lt;ipython-input-5-c52edc2c0508&gt; in train(model, loss_fn, optimizer, num_epochs, x_train, y_train, x_val, y_val, loss_stop)
     19         model.eval()
     20         with torch.no_grad():
---&gt; 21             y_val_pred = model(x_val)
     22 
     23             val_loss = loss_fn(y_val_pred, y_val)

~\anaconda3\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
   1049         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1050                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1051             return forward_call(*input, **kwargs)
   1052         # Do not call functions when jit is used
   1053         full_backward_hooks, non_full_backward_hooks = [], []

&lt;ipython-input-4-9da8c811c037&gt; in forward(self, x)
     10 
     11     def forward(self, x):
---&gt; 12         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()
     13         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_().cuda()
     14 

RuntimeError: CUDA error: an illegal memory access was encountered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
",13442734.0,,365102.0,,2022-03-29 05:13:18,2022-06-29 04:34:15,PyTorch CUDA error: an illegal memory access was encountered,<python><pytorch>,3,0,0.0,,,CC BY-SA 4.0
62786264,1,63304842.0,,2020-07-08 01:37:00,,10,19180,"<p>I'm trying to optimize some weighs (<code>weigts</code>) in Pytorch but I keep getting this error:</p>
<blockquote>
<p>RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can't allocate memory: you tried to allocate 8000000000000 bytes. Error code 12 (Cannot allocate memory).</p>
</blockquote>
<p>Namely, things blow up when I run <code>(weights * col).sum() / weights.sum()</code>. Weights is a tensor of size (1000000,1) and col is also a tensor of size (1000000, 1). Both tensors are decently sized, but it seems odd that I'm using up all the memory in my computer (8GB) for these operations.</p>
",11274877.0,,2745495.0,,2020-08-13 00:06:46,2020-08-13 00:08:06,PyTorch running out of memory: DefaultCPUAllocator can't allocate memory,<pytorch><out-of-memory>,1,0,,,,CC BY-SA 4.0
64218678,1,64218982.0,,2020-10-06 02:50:36,,9,1864,"<p>I am confused with these two structures. In theory, the output of them are all connected to their input. what magic make 'self-attention mechanism' is more powerful than the full-connection layer?</p>
",859147.0,,,,,2022-05-03 06:08:20,"what's the difference between ""self-attention mechanism"" and ""full-connection"" layer?",<pytorch><bert-language-model><transformer-model>,1,0,0.0,,,CC BY-SA 4.0
66906884,1,,,2021-04-01 14:58:44,,9,17599,"<p>According to <a href=""https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html"" rel=""noreferrer"">the PyTorch documentation</a>, the advantage of the class <code>BCEWithLogitsLoss()</code> is that one can use the</p>
<blockquote>
<p>log-sum-exp trick for numerical stability.</p>
</blockquote>
<p>If we use the class <code>BCEWithLogitsLoss()</code> with the parameter <code>reduction</code> set to <code>None</code>, they have a formula for that:</p>
<p><a href=""https://i.stack.imgur.com/5pE4O.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5pE4O.png"" alt=""Loss"" /></a></p>
<p>I now simplified the terms, and obtain after some lines of calculation:</p>
<p><a href=""https://i.stack.imgur.com/8wpss.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8wpss.png"" alt=""enter image description here"" /></a></p>
<p>I was curious to see whether this is the way how the <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html#BCEWithLogitsLoss"" rel=""noreferrer"">Source code</a> does it, but I couldn't find it.. The only code they have is this:</p>
<p><a href=""https://i.stack.imgur.com/xa0pd.png"" rel=""noreferrer"">Code for BCEWithLogitsLoss</a></p>
",15528750.0,,4177926.0,,2021-04-02 15:32:23,2022-11-20 10:37:24,How is PyTorch's Class BCEWithLogitsLoss exactly implemented?,<python><deep-learning><pytorch><implementation><loss>,3,0,0.0,,,CC BY-SA 4.0
69782823,1,69785985.0,,2021-10-30 23:13:33,,9,8197,"<p>I am trying to understand an example snippet that makes use of the PyTorch transposed convolution function, with documentation <a href=""https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html"" rel=""nofollow noreferrer"">here</a>, where in the docs the author writes:</p>
<blockquote>
<p>&quot;The padding argument effectively adds dilation * (kernel_size - 1) -
padding amount of zero padding to both sizes of the input.&quot;</p>
</blockquote>
<p>Consider the snippet below where a sample image of shape <code>[1, 1, 4, 4]</code> containing all ones is input to a <code>ConvTranspose2D</code> operation with arguments <code>stride=2</code> and <code>padding=1</code> with a weight matrix of shape <code>(1, 1, 4, 4)</code> that has entries from a range between <code>1</code> and <code>16</code> (in this case <code>dilation=1</code> and <code>added_padding = 1*(4-1)-1 = 2</code>)</p>
<pre><code>sample_im = torch.ones(1, 1, 4, 4).cuda()
sample_deconv = nn.ConvTranspose2d(1, 1, 4, 2, 1, bias=False).cuda()
sample_deconv.weight = torch.nn.Parameter(
    torch.tensor([[[[ 1.,  2.,  3.,  4.], 
                    [ 5.,  6.,  7.,  8.], 
                    [ 9., 10., 11., 12.], 
                    [13., 14., 15., 16.]]]]).cuda())
</code></pre>
<p>Which yields:</p>
<pre><code>&gt;&gt;&gt; sample_deconv(sample_im)
tensor([[[[ 6., 12., 14., 12., 14., 12., 14.,  7.],
          [12., 24., 28., 24., 28., 24., 28., 14.],
          [20., 40., 44., 40., 44., 40., 44., 22.],
          [12., 24., 28., 24., 28., 24., 28., 14.],
          [20., 40., 44., 40., 44., 40., 44., 22.],
          [12., 24., 28., 24., 28., 24., 28., 14.],
          [20., 40., 44., 40., 44., 40., 44., 22.],
          [10., 20., 22., 20., 22., 20., 22., 11.]]]], device='cuda:0',
       grad_fn=&lt;CudnnConvolutionTransposeBackward&gt;)
</code></pre>
<p>Now I have seen simple examples of transposed convolution without stride and padding. For instance, if the input is a <code>2x2</code> image  <code>[[2, 4], [0, 1]]</code>, and the convolutional filter with one output channel is <code>[[3, 1], [1, 5]]</code>, then the resulting tensor of shape <code>(1, 1, 3, 3)</code> can be seen as the sum of the four colored matrices in the image below:</p>
<p><a href=""https://i.stack.imgur.com/vSZgS.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vSZgS.png"" alt=""enter image description here"" /></a></p>
<p>The problem is I can't seem to find examples that use strides and/or padding in the same visualization. As per my snippet, I am having a very difficult time understanding how the padding is applied to the sample image, or how the stride works to get this output. Any insights appreciated, even just understanding how the <code>6</code> in the <code>(0,0)</code> entry or the <code>12</code> in the <code>(0,1)</code> entry of the resulting matrix are computed would be very helpful.</p>
",6159217.0,,6159217.0,,2022-11-27 04:00:45,2022-11-27 04:00:45,Understanding the PyTorch implementation of Conv2DTranspose,<pytorch><conv-neural-network>,2,0,0.0,,,CC BY-SA 4.0
70051984,1,70052988.0,,2021-11-21 05:11:50,,9,8919,"<p>I'm trying to free up GPU memory after finishing using the model.</p>
<ul>
<li>I checked the <code>nvidia-smi</code> before creating and trainning the model: <code>402MiB /  7973MiB</code></li>
<li>After creating and training the model, I checked again the GPU memory status with  <code>nvidia-smi</code>: <code>7801MiB /  7973MiB</code></li>
<li>Now I tried to free up GPU memory with:</li>
</ul>
<pre><code>del model
torch.cuda.empty_cache() 
gc.collect()
</code></pre>
<p>and checked again the GPU memory:  <code>2361MiB /  7973MiB</code></p>
<ul>
<li>As you can see not all the GPU memory was released (<strong>I expected to get</strong> 400~MiB / 7973MiB).</li>
<li>I can only relase the GPU memory via terminal (<code>sudo fuser -v /dev/nvidia*</code>  and <code>kill</code> pid)</li>
</ul>
<p>Is there a way to free up the GPU memory after I done using the model ?</p>
",3668129.0,,,,,2021-11-21 08:36:25,How to clear GPU memory after using model?,<pytorch>,1,0,0.0,,,CC BY-SA 4.0
67738319,1,,,2021-05-28 11:53:22,,9,378,"<p>How do I create a joint model that shares the parameters of a <strong>Knowledge Graph Embedding</strong> (KGE) model, TuckER (given below), and GloVe (assume a co-occurrence matrix along with the dimensions is already available) in <a href=""/questions/tagged/pytorch"" class=""post-tag"" title=""show questions tagged &#39;pytorch&#39;"" rel=""tag"">pytorch</a>?</p>
<p>In other words, the joint model must obey the criterion of the CMTF (<strong>C</strong>oupled <strong>M</strong>atrix and <strong>T</strong>ensor <strong>F</strong>actorizations) Framework and the weights from the two embeddings must be tied during training. The problem here is that the KGE expects a triple (subject, relation, object) whereas the GloVe expects a co-occurrence matrix. Additionally, their loss functions are also computed differently.</p>
<pre><code>class TuckER(torch.nn.Module):
    def __init__(self, d, d1, d2, **kwargs):
        super(TuckER, self).__init__()

        self.E = torch.nn.Embedding(len(d.entities), d1)
        self.R = torch.nn.Embedding(len(d.relations), d2)
        self.W = torch.nn.Parameter(torch.tensor(np.random.uniform(-1, 1, (d2, d1, d1)), 
                                    dtype=torch.float, device=&quot;cuda&quot;, requires_grad=True))

        self.input_dropout = torch.nn.Dropout(kwargs[&quot;input_dropout&quot;])
        self.hidden_dropout1 = torch.nn.Dropout(kwargs[&quot;hidden_dropout1&quot;])
        self.hidden_dropout2 = torch.nn.Dropout(kwargs[&quot;hidden_dropout2&quot;])
        self.loss = torch.nn.BCELoss()

        self.bn0 = torch.nn.BatchNorm1d(d1)
        self.bn1 = torch.nn.BatchNorm1d(d1)
        
    def init(self):
        xavier_normal_(self.E.weight.data)
        xavier_normal_(self.R.weight.data)

    def forward(self, e1_idx, r_idx):
        e1 = self.E(e1_idx)
        x = self.bn0(e1)
        x = self.input_dropout(x)
        x = x.view(-1, 1, e1.size(1))

        r = self.R(r_idx)
        W_mat = torch.mm(r, self.W.view(r.size(1), -1))
        W_mat = W_mat.view(-1, e1.size(1), e1.size(1))
        W_mat = self.hidden_dropout1(W_mat)

        x = torch.bmm(x, W_mat) 
        x = x.view(-1, e1.size(1))      
        x = self.bn1(x)
        x = self.hidden_dropout2(x)
        x = torch.mm(x, self.E.weight.transpose(1,0))
        pred = torch.sigmoid(x)
        return pred
</code></pre>
<p>I know how to jointly train two pre-trained models by loading the state dicts, taking an instance, running them on the two models, and then applying a feedforward layer on top. But I seem to be not able to figure this scenario out. Can you please suggest how I can achieve this?</p>
<hr />
<p><strong>Important Resources:</strong></p>
<ol>
<li>Code for TuckER - <a href=""https://github.com/ibalazevic/TuckER"" rel=""noreferrer"">https://github.com/ibalazevic/TuckER</a></li>
</ol>
",6207849.0,,9215780.0,,2021-07-04 09:57:24,2021-07-04 09:57:24,Joint training of two embedding models (KGE + GloVe),<python><deep-learning><pytorch><torch><knowledge-graph>,0,12,0.0,,,CC BY-SA 4.0
63187161,1,63236882.0,,2020-07-31 06:59:44,,9,24298,"<p>I just newly install python 3.8 via anaconda installer and install pytorch using command</p>
<pre><code>conda install pytorch torchvision cpuonly -c pytorch
</code></pre>
<p>when i try to import torch, I got this error message.</p>
<pre><code>OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\chunc\anaconda3\lib\site-packages\torch\lib\asmjit.dll&quot; or one of its dependencies.
</code></pre>
<p>I can see <a href=""https://i.stack.imgur.com/s92PY.png"" rel=""noreferrer"">dll files</a> are still in the directory.</p>
<p>I ran Dependency Walker and it gave me <a href=""https://i.stack.imgur.com/nsnqp.png"" rel=""noreferrer"">this result</a>.</p>
<p>I am with this problem for a day.</p>
<p>What should i do if i want to use PyTorch module?</p>
",10539725.0,,,,,2022-11-17 04:41:47,error while import pytorch module. (The specified module could not be found.),<python><dll><pytorch>,3,0,0.0,,,CC BY-SA 4.0
64876788,1,66519480.0,,2020-11-17 14:03:01,,9,1348,"<p>I'm trying to go <code>seq2seq</code> with a Transformer model. My input and output are the same shape (<code>torch.Size([499, 128])</code> where 499 is the sequence length and 128 is the number of features.</p>
<p>My input looks like:
<a href=""https://i.stack.imgur.com/90IRq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/90IRq.png"" alt=""enter image description here"" /></a></p>
<p>My output looks like:
<a href=""https://i.stack.imgur.com/C0qAY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/C0qAY.png"" alt=""enter image description here"" /></a></p>
<p>My training loop is:</p>
<pre><code>    for batch in tqdm(dataset):
        optimizer.zero_grad()
        x, y = batch

        x = x.to(DEVICE)
        y = y.to(DEVICE)

        pred = model(x, torch.zeros(x.size()).to(DEVICE))

        loss = loss_fn(pred, y)
        loss.backward()
        optimizer.step()
</code></pre>
<p>My model is:</p>
<pre><code>import math
from typing import final
import torch
import torch.nn as nn

class Reconstructor(nn.Module):
    def __init__(self, input_dim, output_dim, dim_embedding, num_layers=4, nhead=8, dim_feedforward=2048, dropout=0.5):
        super(Reconstructor, self).__init__()

        self.model_type = 'Transformer'
        self.src_mask = None
        self.pos_encoder = PositionalEncoding(d_model=dim_embedding, dropout=dropout)
        self.transformer = nn.Transformer(d_model=dim_embedding, nhead=nhead, dim_feedforward=dim_feedforward, num_encoder_layers=num_layers, num_decoder_layers=num_layers)
        self.decoder = nn.Linear(dim_embedding, output_dim)
        self.decoder_act_fn = nn.PReLU()

        self.init_weights()

    def init_weights(self):
        initrange = 0.1
        nn.init.zeros_(self.decoder.weight)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, src, tgt):

        pe_src = self.pos_encoder(src.permute(1, 0, 2))  # (seq, batch, features)
        transformer_output = self.transformer_encoder(pe_src)
        decoder_output = self.decoder(transformer_output.permute(1, 0, 2)).squeeze(2)
        decoder_output = self.decoder_act_fn(decoder_output)
        return decoder_output
</code></pre>
<p>My output has a shape of <code>torch.Size([32, 499, 128])</code> where <code>32</code> is batch, <code>499</code> is my sequence length and <code>128</code> is the number of features. But the output has the same values:</p>
<pre><code>tensor([[[0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         ...,
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017]]],
       grad_fn=&lt;PreluBackward&gt;)
</code></pre>
<p>What am I doing wrong? Thank you so much for any help.</p>
",239879.0,,,,,2021-03-07 17:49:59,How to use the PyTorch Transformer with multi-dimensional sequence-to-seqence?,<python><machine-learning><pytorch><transformer-model><sequence-to-sequence>,1,1,0.0,,,CC BY-SA 4.0
66594136,1,66594890.0,,2021-03-12 04:20:23,,9,4004,"<p>A lot of the PyTorch tutorials I've been viewing do something like this.</p>
<p>Define model:</p>
<pre><code>class Network(nn.Module):
    def __init__():
        super().__init__()
        self.conv1 = ..
        ... 
    
    def forward(x)
        ...
    ...
</code></pre>
<p>Once the Network has been instantiated (<code>net = Network()</code>), the people in the tutorials write <code>net(input_data)</code> instead of <code>net.forward(input_data)</code>.
I tried <code>net.forward()</code> and it gives the same results as <code>net()</code>.</p>
<p>Why is this a common practice, and also why does this work?</p>
",10643571.0,,,,,2021-03-12 05:54:38,Calling the forward method in PyTorch vs. calling the model instance,<python><class><oop><pytorch>,1,0,0.0,,,CC BY-SA 4.0
68806265,1,,,2021-08-16 16:27:34,,9,7822,"<p>I'm following this tutorial to train some models:</p>
<p><a href=""https://huggingface.co/transformers/training.html"" rel=""noreferrer"">https://huggingface.co/transformers/training.html</a></p>
<p>I'd like to track not only the evaluation loss and accuracy but also the train loss and accuracy, to monitor overfitting. While running the code in Jupyter, I do see all of htis:</p>
<pre><code>Epoch   Training Loss   Validation Loss Accuracy    Glue
1   0.096500    0.928782    {'accuracy': 0.625} {'accuracy': 0.625, 'f1': 0.0}
2   0.096500    1.203832    {'accuracy': 0.625} {'accuracy': 0.625, 'f1': 0.0}
3   0.096500    1.643788    {'accuracy': 0.625} {'accuracy': 0.625, 'f1': 0.0}
</code></pre>
<p>but when I go into trainer.state.log_history, that stuff is not there. This really doesn't make sense to me.</p>
<pre><code>for obj in trainer.state.log_history:
    print(obj)

{'loss': 0.0965, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.25, 'step': 1}
{'eval_loss': 0.9287818074226379, 'eval_accuracy': {'accuracy': 0.625}, 'eval_glue': {'accuracy': 0.625, 'f1': 0.0}, 'eval_runtime': 1.3266, 'eval_samples_per_second': 6.03, 'eval_steps_per_second': 0.754, 'epoch': 1.0, 'step': 4}
{'eval_loss': 1.2038320302963257, 'eval_accuracy': {'accuracy': 0.625}, 'eval_glue': {'accuracy': 0.625, 'f1': 0.0}, 'eval_runtime': 1.3187, 'eval_samples_per_second': 6.067, 'eval_steps_per_second': 0.758, 'epoch': 2.0, 'step': 8}
{'eval_loss': 1.6437877416610718, 'eval_accuracy': {'accuracy': 0.625}, 'eval_glue': {'accuracy': 0.625, 'f1': 0.0}, 'eval_runtime': 1.3931, 'eval_samples_per_second': 5.742, 'eval_steps_per_second': 0.718, 'epoch': 3.0, 'step': 12}
{'train_runtime': 20.9407, 'train_samples_per_second': 1.146, 'train_steps_per_second': 0.573, 'total_flos': 6314665328640.0, 'train_loss': 0.07855576276779175, 'epoch': 3.0, 'step': 12}
</code></pre>
<p>How do I get these back in an object, and not a printout?</p>
<p>Thanks</p>
<p>Edit: Reproducable code below:</p>
<pre><code>import numpy as np
from datasets import load_metric, load_dataset
from transformers import TrainingArguments, AutoModelForSequenceClassification, Trainer, AutoTokenizer
from datasets import list_metrics

raw_datasets = load_dataset(&quot;imdb&quot;)

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(8))
small_eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(8))
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=2)
    
training_args = TrainingArguments(&quot;IntroToBERT&quot;, evaluation_strategy=&quot;epoch&quot;)
training_args.logging_strategy = 'step'
training_args.logging_first_step = True
training_args.logging_steps = 1
training_args.num_train_epochs = 3
training_args.per_device_train_batch_size = 2
training_args.eval_steps = 1

metrics = {}
for metric in ['accuracy','glue']:
    metrics[metric] = load_metric(metric,'mrpc')


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    out = {}
    for metric in metrics.keys():
        out[metric] = metrics[metric].compute(predictions=predictions, references=labels)
    return out

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train() 

# here the printout is as shown

for obj in trainer.state.log_history:
    print(obj)

# here the logging data is displayed
</code></pre>
",4493798.0,,4493798.0,,2021-08-16 17:41:19,2021-10-30 08:05:22,HuggingFace Trainer logging train data,<pytorch><huggingface-transformers>,1,3,0.0,,,CC BY-SA 4.0
71689095,1,71750812.0,,2022-03-31 08:11:36,,9,23987,"<p>I am running a simple CNN using Pytorch for some audio classification on my Raspberry Pi 4 on Python 3.9.2 (64-bit). For the audio manipulation needed I am using librosa. librosa depends on the numba package which is only compatible with numpy version &lt;= 1.20.</p>
<p>When running my code, the line</p>
<pre><code>spect_tensor = torch.from_numpy(spect).double()
</code></pre>
<p>throws the RuntimeError:</p>
<pre><code>RuntimeError: Numpy is not available
</code></pre>
<p>Searching the internet for solutions I found upgrading Numpy to the latest version to resolve that specific error, but throwing another error, because Numba only works with Numpy &lt;= 1.20.</p>
<p>Is there a solution to this problem which does not include searching for an alternative to using librosa?</p>
",18640828.0,,,,,2023-06-02 02:40:44,How to solve the pytorch RuntimeError: Numpy is not available without upgrading numpy to the latest version because of other dependencies,<python><numpy><pytorch><librosa>,3,2,,,,CC BY-SA 4.0
63855608,1,63856493.0,,2020-09-11 23:59:13,,9,3838,"<p>I am using Pytorch to training some neural networks. The part I am confused about is:</p>
<pre><code>prediction = myNetwork(img_batch)
max_act = prediction.max(1)[0].sum()
loss = softcrossentropy_loss - alpha * max_act
</code></pre>
<p>In the above codes, &quot;prediction&quot; is the output tensor of &quot;myNetwork&quot;.
I hope to maximize the larget output of &quot;prediction&quot; over a batch.</p>
<p>For example:
[[-1.2, 2.0, <strong>5.0</strong>, 0.1, -1.5] [<strong>9.6</strong>, -1.1, 0.7, 4,3, 3.3]]
For the first prediction vector, the 3rd element is the larget, while for the second vector, the 1st element is the largets. And I want to maximize &quot;5.0+9.6&quot;, although we cannot know what index is the larget output for a new input data.</p>
<p>In fact, my training seems to be successful, because the &quot;max_act&quot; part was really increased, which is the desired behavior to me. However, I heard some discussion about whether max() operation is differentiable or not:</p>
<pre><code>Some says, mathmatically, max() is not differentiable.
Some says, max() is just an identity function to select the largest element, and this largest element is differentiable.
</code></pre>
<p>So I got confused now, and I am worried if my idea of maximizing &quot;max_act&quot; is wrong from the beginning.
Could someone provide some guidance if max() operation is differentiable in Pytorch?</p>
",10881963.0,,,,,2020-09-12 03:17:31,Is max operation differentiable in Pytorch?,<optimization><deep-learning><neural-network><pytorch><gradient>,1,0,0.0,,,CC BY-SA 4.0
63030692,1,66593509.0,,2020-07-22 09:07:00,,9,6430,"<p>I want to use BertForMaskedLM or BertModel to calculate perplexity of a sentence, so I write code like this:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForMaskedLM
# Load pre-trained model (weights)
with torch.no_grad():
    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')
    model.eval()
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')
    sentence = &quot;我不会忘记和你一起奋斗的时光。&quot;
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    sen_len = len(tokenize_input)
    sentence_loss = 0.

    for i, word in enumerate(tokenize_input):
        # add mask to i-th character of the sentence
        tokenize_input[i] = '[MASK]'
        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])

        output = model(mask_input)

        prediction_scores = output[0]
        softmax = nn.Softmax(dim=0)
        ps = softmax(prediction_scores[0, i]).log()
        word_loss = ps[tensor_input[0, i]]
        sentence_loss += word_loss.item()

        tokenize_input[i] = word
    ppl = np.exp(-sentence_loss/sen_len)
    print(ppl)
</code></pre>
<p>I think this code is right, but I also notice BertForMaskedLM's paramaters <code>masked_lm_labels</code>, so could I use this paramaters to calculate PPL of a sentence easiler?
I know the input_ids argument is the masked input, the masked_lm_labels argument is the desired output. But I couldn't understand the actual meaning of its output loss, its code like this:</p>
<pre><code>if masked_lm_labels is not None:
    loss_fct = CrossEntropyLoss()  # -100 index = padding token
    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), 
    masked_lm_labels.view(-1))
    outputs = (masked_lm_loss,) + outputs
</code></pre>
",10814749.0,,6664872.0,,2020-07-22 10:08:35,2021-03-15 11:14:55,How do I use BertForMaskedLM or BertModel to calculate perplexity of a sentence?,<nlp><pytorch><transformer-model><huggingface-transformers><bert-language-model>,1,0,0.0,,,CC BY-SA 4.0
65815668,1,,,2021-01-20 18:42:21,,9,11726,"<p>The task seems to be simple, but I cannot figure out how to do it.</p>
<p>So what I have are two tensors:</p>
<ul>
<li>an indices tensor <code>indices</code> with shape <code>(2, 5, 2)</code>, where the last dimensions corresponds to indices in x and y dimension</li>
<li>a &quot;value tensor&quot; <code>value</code> with shape <code>(2, 5, 2, 16, 16)</code>, where I want the last two dimensions to be selected with x and y indices</li>
</ul>
<p>To be more concrete, the indices are between 0 and 15 and I want to get an output:</p>
<pre><code>out = value[:, :, :, x_indices, y_indices]
</code></pre>
<p>The shape of the output should therefore be of <code>(2, 5, 2)</code>. Can anybody help me here? Thanks a lot!</p>
<p>Edit:</p>
<p>I tried the suggestion with gather, but unfortunately it does not seem to work (I changed the dimensions, but it doesn't matter):</p>
<p>First I generate a coordinate grid:</p>
<pre><code>y_t = torch.linspace(-1., 1., 16, device='cpu').reshape(16, 1).repeat(1, 16).unsqueeze(-1)
x_t = torch.linspace(-1., 1., 16, device='cpu').reshape(1, 16).repeat(16, 1).unsqueeze(-1)
grid = torch.cat((y_t, x_t), dim=-1).permute(2, 0, 1).unsqueeze(0)
grid = grid.unsqueeze(1).repeat(1, 3, 1, 1, 1)
</code></pre>
<p>In the next step, I am creating some indices. In this case, I always take index 1:</p>
<pre><code>indices = torch.ones([1, 3, 2], dtype=torch.int64)
</code></pre>
<p>Next, I am using your method:</p>
<pre><code>indices = indices.unsqueeze(-1).unsqueeze(-1)
new_coords = torch.gather(grid, -1, indices).squeeze(-1).squeeze(-1)
</code></pre>
<p>Finally, I manually select index 1 for x and y coordinate:</p>
<pre><code>new_coords_manual = grid[:, :, :, 1, 1]
</code></pre>
<p>This outputs the following new coordinates:</p>
<pre><code>new_coords
tensor([[[-1.0000, -0.8667],
         [-1.0000, -0.8667],
         [-1.0000, -0.8667]]])

new_coords_manual
tensor([[[-0.8667, -0.8667],
         [-0.8667, -0.8667],
         [-0.8667, -0.8667]]])
</code></pre>
<p>As you can see, it only works for one dimension. Do you have an idea how to fix that?</p>
",12486121.0,,12486121.0,,2021-01-20 20:27:37,2021-01-20 21:59:26,How to select indices according to another tensor in pytorch,<python><pytorch><indices>,2,2,0.0,,,CC BY-SA 4.0
75580832,1,,,2023-02-27 13:02:12,,9,582,"<p>I have been experimenting with various options to speed up some for-loop-heavy-logic in PyTorch. The two obvious options to do so are either using <a href=""https://stackoverflow.com/a/75580380/1804173"">numba</a> or <a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html"" rel=""nofollow noreferrer"">writing a custom C++ extension</a>.</p>
<p>As an example I have picked a &quot;variable length delay line&quot; from digital signal processing. This can be written trivially but inefficiently using a plain Python for loop:</p>
<pre class=""lang-py prettyprint-override""><code>def delay_line(samples, delays):
    &quot;&quot;&quot;
    :param samples: Float tensor of shape (N,)
    :param delays: Int tensor of shape (N,)
    
    The goal is basically to mix each `samples[i]` with the delayed sample
    specified by a per-sample `delays[i]`.
    &quot;&quot;&quot;
    for i in range(len(samples)):
        delay = int(delays[i].item())
        index_delayed = i - delay
        if index_delayed &lt; 0:
            index_delayed = 0

        samples[i] = 0.5 * (samples[i] + samples[index_delayed])
</code></pre>
<p>Knowing how bad for loops perform in Python, my hope was that I can get significantly better performance by implementing the same in C++. Following <a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html"" rel=""nofollow noreferrer"">the tutorial</a>, I came up with this literal translation from Python to C++:</p>
<pre class=""lang-cpp prettyprint-override""><code>void delay_line(torch::Tensor samples, torch::Tensor delays) {

  int64_t input_size = samples.size(-1);

  for (int64_t i = 0; i &lt; input_size; ++i) {
    int64_t delay = delays[i].item&lt;int64_t&gt;();
    int64_t index_delayed = i - delay;
    if (index_delayed &lt; 0) {
      index_delayed = 0;
    }

    samples[i] = 0.5 * (samples[i] + samples[index_delayed]);
  }
}
</code></pre>
<p>I have also taken the Python function and wrapped it into various jit decorators to get numba and torchscript versions of that function (see my other <a href=""https://stackoverflow.com/a/75580380/1804173"">answer</a> for details on the numba wrapping). I then executed my benchmark for all the versions, also depending on whether tensors reside on the CPU or GPU. The results are rather surprising:</p>
<pre><code>╭──────────────┬──────────┬────────────────────╮
│ Method       │ Device   │   Median time [ms] │
├──────────────┼──────────┼────────────────────┤
│ plain_python │ CPU      │             13.481 │
│ torchscript  │ CPU      │              6.318 │
│ numba        │ CPU      │              0.016 │
│ cpp          │ CPU      │              9.056 │
│ plain_python │ GPU      │             45.412 │
│ torchscript  │ GPU      │             47.809 │
│ numba        │ GPU      │              0.236 │
│ cpp          │ GPU      │             31.145 │
╰──────────────┴──────────┴────────────────────╯
</code></pre>
<p><sub>Notes: sample buffer size was fixed to 1024; results are medians of 100 executions to ignore artifacts from the initial jit overhead; input data creation and moving it to the device is excluded from the measurements; full benchmark script <a href=""https://gist.github.com/bluenote10/3370da06204b94995614ed014410f6c2"" rel=""nofollow noreferrer"">gist</a></sub></p>
<p>The most notable result: The C++ variant seems to be surprisingly slow. The fact that numba is two orders of magnitude faster indicates that the problem indeed can be solved faster. And the fact that the C++ variant is still pretty close to the know-to-be-slow Python for loop may indicate that something isn't quite right.</p>
<p>I'm wondering what could explain the poor performance of the C++ extension. The first thing that comes to mind is missing optimization. However, I've made sure that the compilation uses optimization. Switching from <code>-O2</code> to <code>-O3</code> also didn't make a difference.</p>
<p>To isolate the overhead of the pybind11 function call, I have replaced the C++ function with an empty body, i.e., just doing nothing. This reduced the time to 2-3 <strong>μs</strong>, which means that the time really is spend in that particular function body.</p>
<p>Any ideas why I'm observing such a poor performance, and is there anything I could do on the C++ side to match the performance of the numba implementation?</p>
<p>Bonus question: Is it expected that the GPU versions are much slower than the CPU versions?</p>
",1804173.0,,1804173.0,,2023-02-27 16:25:40,2023-02-27 16:25:40,Why is PyTorch C++ extension much slower than its equivalent numba version?,<c++><pytorch><numba><torch>,0,5,,,,CC BY-SA 4.0
69325760,1,69326348.0,,2021-09-25 11:36:03,,9,4549,"<p>PyTorch's negative log-likelihood loss, <a href=""https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html"" rel=""noreferrer""><code>nn.NLLLoss</code></a> is defined as:</p>
<p><a href=""https://i.stack.imgur.com/S9taM.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/S9taM.png"" alt=""enter image description here"" /></a></p>
<p>So, if the loss is calculated with the standard weight of one in a single batch the formula for the loss is always:</p>
<blockquote>
<p>-1 * (prediction of model for correct class)</p>
</blockquote>
<p><strong>Example:</strong></p>
<p><a href=""https://i.stack.imgur.com/XTtEB.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/XTtEB.png"" alt=""enter image description here"" /></a></p>
<p><em>Correct Class = 0</em></p>
<p><em>prediction of model for correct class = 0.5</em></p>
<p><em>loss = -1 * 0.5</em></p>
<p>So, why is it called the &quot;negative log-likelihood loss&quot;, if there isn't a log function involved in calculating the loss?
​</p>
",11945707.0,,6331369.0,,2021-10-04 15:12:07,2021-10-04 15:12:07,Understanding of Pytorch NLLLOSS,<pytorch><classification><loss-function>,1,0,0.0,,,CC BY-SA 4.0
65023526,1,65052927.0,,2020-11-26 14:01:21,,9,18632,"<p>I'm trying to build a model for document classification. I'm using <code>BERT</code> with <code>PyTorch</code>.</p>
<p>I got the bert model with below code.</p>
<pre><code>bert = AutoModel.from_pretrained('bert-base-uncased')
</code></pre>
<p>This is the code for training.</p>
<pre><code>for epoch in range(epochs):
 
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))

    #train model
    train_loss, _ = modhelper.train(proc.train_dataloader)

    #evaluate model
    valid_loss, _ = modhelper.evaluate()

    #save the best model
    if valid_loss &lt; best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(modhelper.model.state_dict(), 'saved_weights.pt')

    # append training and validation loss
    train_losses.append(train_loss)
    valid_losses.append(valid_loss)

    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')
</code></pre>
<p>this is my train method, accessible with the object <code>modhelper</code>.</p>
<pre><code>def train(self, train_dataloader):
    self.model.train()
    total_loss, total_accuracy = 0, 0
    
    # empty list to save model predictions
    total_preds=[]
    
        # iterate over batches
    for step, batch in enumerate(train_dataloader):
        
        # progress update after every 50 batches.
        if step % 50 == 0 and not step == 0:
            print('  Batch {:&gt;5,}  of  {:&gt;5,}.'.format(step, len(train_dataloader)))
        
        # push the batch to gpu
        #batch = [r.to(device) for r in batch]
        
        sent_id, mask, labels = batch
        
        # clear previously calculated gradients 
        self.model.zero_grad()        

        print(sent_id.size(), mask.size())
        # get model predictions for the current batch
        preds = self.model(sent_id, mask) #This line throws the error
        
        # compute the loss between actual and predicted values
        self.loss = self.cross_entropy(preds, labels)
        
        # add on to the total loss
        total_loss = total_loss + self.loss.item()
        
        # backward pass to calculate the gradients
        self.loss.backward()
        
        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        
        # update parameters
        self.optimizer.step()
        
        # model predictions are stored on GPU. So, push it to CPU
        #preds=preds.detach().cpu().numpy()
        
        # append the model predictions
        total_preds.append(preds)
      
    # compute the training loss of the epoch
    avg_loss = total_loss / len(train_dataloader)
    
    # predictions are in the form of (no. of batches, size of batch, no. of classes).
    # reshape the predictions in form of (number of samples, no. of classes)
    total_preds  = np.concatenate(total_preds, axis=0)
      
    #returns the loss and predictions
    return avg_loss, total_preds
</code></pre>
<p><code>preds = self.model(sent_id, mask)</code> this line throws the following error(including full traceback).</p>
<pre><code> Epoch 1 / 1
torch.Size([32, 4000]) torch.Size([32, 4000])
Traceback (most recent call last):

File &quot;&lt;ipython-input-39-17211d5a107c&gt;&quot;, line 8, in &lt;module&gt;
train_loss, _ = modhelper.train(proc.train_dataloader)

File &quot;E:\BertTorch\model.py&quot;, line 71, in train
preds = self.model(sent_id, mask)

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\model.py&quot;, line 181, in forward
#pass the inputs to the model

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 837, in forward
embedding_output = self.embeddings(

File &quot;E:\BertTorch\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 727, in _call_impl
result = self.forward(*input, **kwargs)

File &quot;E:\BertTorch\venv\lib\site-packages\transformers\modeling_bert.py&quot;, line 201, in forward
embeddings = inputs_embeds + position_embeddings + token_type_embeddings

RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>If you observe I've printed the torch size in the code.
<code>print(sent_id.size(), mask.size())</code></p>
<p>The output of that line of code is <code>torch.Size([32, 4000]) torch.Size([32, 4000])</code>.</p>
<p>as we can see that size is the same but it throws the error. Please put your thoughts. Really appreciate it.</p>
<p>please comment if you need further information. I'll be quick to add whatever is required.</p>
",8340027.0,,6664872.0,,2020-11-27 22:47:26,2020-11-28 18:08:29,RuntimeError: The size of tensor a (4000) must match the size of tensor b (512) at non-singleton dimension 1,<python><deep-learning><pytorch><bert-language-model><huggingface-transformers>,1,5,0.0,,,CC BY-SA 4.0
63058355,1,,,2020-07-23 15:56:59,,9,4876,"<p>Why does <code>super(LR, self).__init__()</code> need to be called in the code below? I get the error &quot;AttributeError: cannot assign module before Module.<strong>init</strong>() call&quot; otherwise. That error is caused by <code>self.linear = nn.Linear(input_size, output_size)</code>.</p>
<p>I don't understand what the connection is between calling <code>super(LR, self).__init__()</code> and being able to assign the nn.Linear object to self.linear. nn.Linear is a separate object which can be assigned to a variable outside of any class, so why does <code>super(LR, self).__init__()</code> need to be called to assign a Linear object to self.linear within the class?</p>
<pre><code>class LR(nn.Module):
    
    # Constructor
    def __init__(self, input_size, output_size):
        
        # Inherit from parent
        super(LR, self).__init__()
        self.test = 1
        self.linear = nn.Linear(input_size, output_size)
        
    
    # Prediction function
    def forward(self, x):
        out = self.linear(x)
        return out
</code></pre>
",13983710.0,,2790047.0,,2020-07-23 17:04:56,2020-07-23 17:04:56,Why is the super constructor necessary in PyTorch custom modules?,<python><inheritance><pytorch><super>,2,0,,,,CC BY-SA 4.0
73325131,1,73326058.0,,2022-08-11 18:08:34,,9,1925,"<p>I've two PyTorch tensors</p>
<pre><code>mask = torch.ones(1024, 64, dtype=torch.float32)
indices = torch.randint(0, 64, (1024, ))
</code></pre>
<p>For every <code>i</code>th row in <code>mask</code>, I want to set all the elements after the index specified by <code>i</code>th element of <code>indices</code> to zero. For example, if the first element of <code>indices</code> is <code>50</code>, then I want to set <code>mask[0, 50:]=0</code>. Is it possible to achieve this without using for loop?</p>
<p>Solution with for loop:</p>
<pre><code>for i in range(mask.shape[0]):
    mask[i, indices[i]:] = 0
</code></pre>
",3337089.0,,,,,2022-08-11 19:35:13,"How to set all elements of pytorch tensor to zero after a certain index in the given axis, where the index is given by another pytorch tensor?",<python><pytorch><vectorization>,1,0,,,,CC BY-SA 4.0
63128641,1,63129074.0,,2020-07-28 07:00:35,,9,10172,"<p>I have two Pytorch tensors of the form [y11, y12] and [y21, y22]. How do I get the weighted mean of the two tensors?</p>
",13153990.0,,,,,2020-07-28 12:01:25,Weighted Average of PyTorch Tensors,<python><numpy><pytorch><tensor><weighted-average>,1,0,0.0,,,CC BY-SA 4.0
64320883,1,,,2020-10-12 15:34:18,,9,18321,"<p>I am trying to do text classification using pretrained BERT model. I trained the model on my dataset, and in the phase of testing; I know that BERT can only take to 512 tokens, so I wrote if condition to check the length of the test senetence in my dataframe. If it is longer than 512 I split the sentence into sequences each sequence has 512 token. And then do tokenizer encode. The length of the seqience is 512, however, after doing tokenize encode the length becomes 707 and I get this error.</p>
<pre><code>The size of tensor a (707) must match the size of tensor b (512) at non-singleton dimension 1
</code></pre>
<p>Here is the code I used to do the preivous steps:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)
import math

pred=[]
if (len(test_sentence_in_df.split())&gt;512):
  
  n=math.ceil(len(test_sentence_in_df.split())/512)
  for i in range(n):
    if (i==(n-1)):
      print(i)
      test_sentence=' '.join(test_sentence_in_df.split()[i*512::])
    else:
      print(&quot;i in else&quot;,str(i))
      test_sentence=' '.join(test_sentence_in_df.split()[i*512:(i+1)*512])
      
      #print(len(test_sentence.split()))  ##here's the length is 512
    tokenized_sentence = tokenizer.encode(test_sentence)
    input_ids = torch.tensor([tokenized_sentence]).cuda()
    print(len(tokenized_sentence)) #### here's the length is 707
    with torch.no_grad():
      output = model(input_ids)
      label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)
    pred.append(label_indices)

print(pred)
</code></pre>
",10291435.0,,,,,2022-07-13 18:38:03,The size of tensor a (707) must match the size of tensor b (512) at non-singleton dimension 1,<python><tensorflow><pytorch><tokenize><bert-language-model>,2,1,0.0,,,CC BY-SA 4.0
70998767,1,,,2022-02-05 14:06:45,,9,11246,"<p>When I use the environment of pytorch=1.10.0, torchvision=0.11.1 to run the code, I run to the statement from torchvision.models.utils import load_state_dict_from_url. The following error will appear when:</p>
<pre><code>&gt;&gt;&gt; from torchvision.models.utils import load_state_dict_from_url
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'torchvision.models.utils'
</code></pre>
",5480274.0,,,,,2023-04-07 08:50:59,No module named ‘torchvision.models.utils‘,<python><pytorch><torchvision>,1,0,0.0,,,CC BY-SA 4.0
63486381,1,63527089.0,,2020-08-19 11:51:12,,9,5117,"<p>I am trying to run GCP TPU with Pytorch/XLA, I am using a VM with debian-9-torch-xla-v20200818 image, I initiate the TPU and check it is running using ctpu status which shows that both the CPU and TPU are running, I then activate the torch-xla-nightly environment, but when I try to invoke this simple code:</p>
<pre><code>import torch
import torch_xla
import torch_xla.core.xla_model as xm

dev = xm.xla_device()
t1 = torch.ones(3, 3, device = dev)
print(t1)
</code></pre>
<p>this error comes up:</p>
<pre><code>Traceback (most recent call last):
File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
File &quot;/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/core/xla_model.py&quot;, line 231, in xla_device
devkind=devkind if devkind is not None else None)
File &quot;/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/core/xla_model.py&quot;, line 136, in get_xla_supported_devices
 xla_devices = _DEVICES.value
File &quot;/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/utils/utils.py&quot;, line 32, in value
self._value = self._gen_fn()
File &quot;/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/core/xla_model.py&quot;, line 18, in &lt;lambda&gt;
_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())
RuntimeError: tensorflow/compiler/xla/xla_client/computation_client.cc:274 : Missing XLA configuration
</code></pre>
<p>I tried everything but nothing seem to work.</p>
",8402112.0,,,,,2023-03-08 09:53:42,Missing XLA configuration when running pytorch/xla,<google-cloud-platform><pytorch><tpu>,2,2,0.0,,,CC BY-SA 4.0
71630563,1,71630606.0,,2022-03-26 18:00:51,,9,579,"<p>I understand that in python user-defined objects can be made callable by defining a <code>__call__()</code> method in the class definition. For example,</p>
<pre><code>class MyClass:
  def __init__(self):
    pass

  def __call__(self, input1):
    self.my_function(input1)

  def my_function(self, input1):
    print(f&quot;MyClass - print {input1}&quot;)

my_obj = MyClass()
# same as calling my_obj.my_function(&quot;haha&quot;)
my_obj(&quot;haha&quot;) # prints &quot;MyClass - print haha&quot;
</code></pre>
<p>I was looking at how <code>pytorch</code> makes the <code>forward()</code> method of a <code>nn.Module</code> object be called implicitly when the object is called and saw some syntax I didn't understand.</p>
<p>In <a href=""https://github.com/pytorch/pytorch/blob/1c5a8125798392f8d7c57e88735f43a14ae0beca/torch/nn/modules/module.py#L1156"" rel=""noreferrer"">the line</a> that supposedly defines the <code>__call__</code> method the syntax used is,</p>
<pre><code>__call__ : Callable[..., Any] = _call_impl
</code></pre>
<p>This seemed like a combination of an annotation (keyword <code>Callable[</code> following <code>:</code> ignored by python) and a value of <code>_call_impl</code> which we want to be called when <code>__call__</code> is invoked, and my guess is that this is a shorthand for,</p>
<pre><code>def __call__(self, *args, **kwargs):
    return self._call_impl(*args, **kwargs)
</code></pre>
<p>but wanted to understand clearly how this method of defining functions worked.</p>
<p>My question is: When would we want to use such a definition of callable attributes of a class instead of the usual <code>def myfunc(self, *args, **kwargs)</code></p>
",3642162.0,,3642162.0,,2022-03-26 18:07:31,2022-03-26 18:08:27,Syntax for making objects callable in python,<python><pytorch>,1,0,0.0,,,CC BY-SA 4.0
65575871,1,,,2021-01-05 09:03:50,,9,20273,"<p>I am trying to run <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""noreferrer"">this tutorial</a> in colab.</p>
<p>However, when I try to import a bunch of modules:</p>
<pre><code>import io
import torch
from torchtext.utils import download_from_url
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
</code></pre>
<p>It gives me the errors for <code>extract_archive</code> and <code>build_vocab_from_iterator</code>:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-5-a24e72502dbc&gt; in &lt;module&gt;()
      1 import io
      2 import torch
----&gt; 3 from torchtext.utils import download_from_url, extract_archive
      4 from torchtext.data.utils import get_tokenizer
      5 from torchtext.vocab import build_vocab_from_iterator

ImportError: cannot import name 'extract_archive'


ImportError                               Traceback (most recent call last)
&lt;ipython-input-4-02a401fd241b&gt; in &lt;module&gt;()
      3 from torchtext.utils import download_from_url
      4 from torchtext.data.utils import get_tokenizer
----&gt; 5 from torchtext.vocab import build_vocab_from_iterator
      6 
      7 url = 'https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip'

ImportError: cannot import name 'build_vocab_from_iterator'
</code></pre>
<p>Please help me with this one.</p>
",10163246.0,,,,,2023-05-21 18:49:07,torchtext ImportError in colab,<python><pytorch><google-colaboratory><importerror><torchtext>,5,2,0.0,,,CC BY-SA 4.0
66337562,1,66425941.0,,2021-02-23 16:55:48,,9,22670,"<p>I was trying to run a python file named <code>api.py</code>. In this file, I'm loading the pickle file of the Deep Learning model that was built and trained using PyTorch.</p>
<p><a href=""https://github.com/miguelgfierro/sciblog_support/blob/master/Intro_to_Recommendation_Systems/api.py"" rel=""noreferrer"">api.py</a>
In <code>api.py</code> the below-given functions are the most important ones.</p>
<pre><code>def load_model_weights(model_architecture, weights_path):
  if os.path.isfile(weights_path):
      cherrypy.log(&quot;CHERRYPYLOG Loading model from: {}&quot;.format(weights_path))
      model_architecture.load_state_dict(torch.load(weights_path))
  else:
      raise ValueError(&quot;Path not found {}&quot;.format(weights_path))

        
def load_recommender(vector_dim, hidden, activation, dropout, weights_path):

    rencoder_api = model.AutoEncoder(layer_sizes=[vector_dim] + [int(l) for l in hidden.split(',')],
                               nl_type=activation,
                               is_constrained=False,
                               dp_drop_prob=dropout,
                               last_layer_activations=False)
    load_model_weights(rencoder_api, weights_path) 
    rencoder_api.eval()
    rencoder_api = rencoder_api.cuda()
    return rencoder_api
</code></pre>
<p>The directory structure</p>
<pre><code>📦MP1
 ┣ 📂.ipynb_checkpoints
 ┃ ┗ 📜RS_netflix3months_100epochs_64,128,128-checkpoint.ipynb
 ┣ 📂data
 ┃ ┣ 📜AutoEncoder.png
 ┃ ┣ 📜collaborative_filtering.gif
 ┃ ┣ 📜movie_titles.txt
 ┃ ┗ 📜shut_up.gif
 ┣ 📂DeepRecommender
 ┃ ┣ 📂data_utils
 ┃ ┃ ┣ 📜movielens_data_convert.py
 ┃ ┃ ┗ 📜netflix_data_convert.py
 ┃ ┣ 📂reco_encoder
 ┃ ┃ ┣ 📂data
 ┃ ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┃ ┣ 📜input_layer.cpython-37.pyc
 ┃ ┃ ┃ ┃ ┣ 📜input_layer_api.cpython-37.pyc
 ┃ ┃ ┃ ┃ ┗ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜input_layer.py
 ┃ ┃ ┃ ┣ 📜input_layer_api.py
 ┃ ┃ ┃ ┗ 📜__init__.py
 ┃ ┃ ┣ 📂model
 ┃ ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┃ ┣ 📜model.cpython-37.pyc
 ┃ ┃ ┃ ┃ ┗ 📜__init__.cpython-37.pyc
 ┃ ┃ ┃ ┣ 📜model.py
 ┃ ┃ ┃ ┗ 📜__init__.py
 ┃ ┃ ┣ 📂__pycache__
 ┃ ┃ ┃ ┗ 📜__init__.cpython-37.pyc
 ┃ ┃ ┗ 📜__init__.py
 ┃ ┣ 📂__pycache__
 ┃ ┃ ┗ 📜__init__.cpython-37.pyc
 ┃ ┣ 📜compute_RMSE.py
 ┃ ┣ 📜infer.py
 ┃ ┣ 📜run.py
 ┃ ┗ 📜__init__.py
 ┣ 📂model_save
 ┃ ┣ 📂model.epoch_99
 ┃ ┃ ┗ 📂archive
 ┃ ┃ ┃ ┣ 📂data
 ┃ ┃ ┃ ┃ ┣ 📜92901648
 ┃ ┃ ┃ ┃ ┣ 📜92901728
 ┃ ┃ ┃ ┃ ┣ 📜92901808
 ┃ ┃ ┃ ┃ ┣ 📜92901888
 ┃ ┃ ┃ ┃ ┣ 📜92901968
 ┃ ┃ ┃ ┃ ┣ 📜92902048
 ┃ ┃ ┃ ┃ ┣ 📜92902128
 ┃ ┃ ┃ ┃ ┣ 📜92902208
 ┃ ┃ ┃ ┃ ┣ 📜92902288
 ┃ ┃ ┃ ┃ ┣ 📜92902368
 ┃ ┃ ┃ ┃ ┣ 📜92902448
 ┃ ┃ ┃ ┃ ┗ 📜92902608
 ┃ ┃ ┃ ┣ 📜data.pkl
 ┃ ┃ ┃ ┗ 📜version
 ┃ ┣ 📜model.epoch_99.zip
 ┃ ┗ 📜model.onnx
 ┣ 📂Netflix
 ┃ ┣ 📂N1Y_TEST
 ┃ ┃ ┗ 📜n1y.test.txt
 ┃ ┣ 📂N1Y_TRAIN
 ┃ ┃ ┗ 📜n1y.train.txt
 ┃ ┣ 📂N1Y_VALID
 ┃ ┃ ┗ 📜n1y.valid.txt
 ┃ ┣ 📂N3M_TEST
 ┃ ┃ ┗ 📜n3m.test.txt
 ┃ ┣ 📂N3M_TRAIN
 ┃ ┃ ┗ 📜n3m.train.txt
 ┃ ┣ 📂N3M_VALID
 ┃ ┃ ┗ 📜n3m.valid.txt
 ┃ ┣ 📂N6M_TEST
 ┃ ┃ ┗ 📜n6m.test.txt
 ┃ ┣ 📂N6M_TRAIN
 ┃ ┃ ┗ 📜n6m.train.txt
 ┃ ┣ 📂N6M_VALID
 ┃ ┃ ┗ 📜n6m.valid.txt
 ┃ ┣ 📂NF_TEST
 ┃ ┃ ┗ 📜nf.test.txt
 ┃ ┣ 📂NF_TRAIN
 ┃ ┃ ┗ 📜nf.train.txt
 ┃ ┗ 📂NF_VALID
 ┃ ┃ ┗ 📜nf.valid.txt
 ┣ 📂test
 ┃ ┣ 📂testData_iRec
 ┃ ┃ ┣ 📜.part-00199-f683aa3b-8840-4835-b8bc-a8d1eaa11c78.txt.crc
 ┃ ┃ ┣ 📜part-00000-f683aa3b-8840-4835-b8bc-a8d1eaa11c78.txt
 ┃ ┃ ┣ 📜part-00003-f683aa3b-8840-4835-b8bc-a8d1eaa11c78.txt
 ┃ ┃ ┗ 📜_SUCCESS
 ┃ ┣ 📂testData_uRec
 ┃ ┃ ┣ 📜.part-00000-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt.crc
 ┃ ┃ ┣ 📜._SUCCESS.crc
 ┃ ┃ ┣ 📜part-00161-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt
 ┃ ┃ ┣ 📜part-00196-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt
 ┃ ┃ ┗ 📜part-00199-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt
 ┃ ┣ 📜data_layer_tests.py
 ┃ ┣ 📜test_model.py
 ┃ ┗ 📜__init__.py
 ┣ 📂__pycache__
 ┃ ┣ 📜api.cpython-37.pyc
 ┃ ┣ 📜load_test.cpython-37.pyc
 ┃ ┣ 📜parameters.cpython-37.pyc
 ┃ ┗ 📜utils.cpython-37.pyc
 ┣ 📜api.py
 ┣ 📜compute_RMSE.py
 ┣ 📜load_test.py
 ┣ 📜logger.py
 ┣ 📜netflix_1y_test.csv
 ┣ 📜netflix_1y_train.csv
 ┣ 📜netflix_1y_valid.csv
 ┣ 📜netflix_3m_test.csv
 ┣ 📜netflix_3m_train.csv
 ┣ 📜netflix_3m_valid.csv
 ┣ 📜netflix_6m_test.csv
 ┣ 📜netflix_6m_train.csv
 ┣ 📜netflix_6m_valid.csv
 ┣ 📜netflix_full_test.csv
 ┣ 📜netflix_full_train.csv
 ┣ 📜netflix_full_valid.csv
 ┣ 📜parameters.py
 ┣ 📜preds.txt
 ┣ 📜RS_netflix3months_100epochs_64,128,128.ipynb
 ┗ 📜utils.py
</code></pre>
<p>I am getting such an error (<a href=""https://github.com/pytorch/pytorch/blob/master/torch/serialization.py"" rel=""noreferrer"">serialization.py</a>). Can someone help me with this error?</p>
<pre><code>D:\Anaconda\envs\practise\lib\site-packages\torch\serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)
    762             &quot;functionality.&quot;)
    763 
--&gt; 764     magic_number = pickle_module.load(f, **pickle_load_args)
    765     if magic_number != MAGIC_NUMBER:
    766         raise RuntimeError(&quot;Invalid magic number; corrupt file?&quot;)

UnpicklingError: A load persistent id instruction was encountered,
but no persistent_load function was specified.
</code></pre>
",10730632.0,,,,,2021-03-01 16:53:21,"UnpicklingError: A load persistent id instruction was encountered, but no persistent_load function was specified",<python><serialization><deep-learning><pytorch><pickle>,1,0,0.0,,,CC BY-SA 4.0
70102323,1,70106583.0,,2021-11-24 20:07:56,,9,17732,"<p>I trained a model for sequence classification using transformers <strong>(BertForSequenceClassification)</strong> and I get the error:</p>
<p><em>Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)</em></p>
<p>I don't really get where is the problem, if it's on my model, on how I tokenize the data, or what.</p>
<p>Here is my code:</p>
<p><strong>LOADING THE PRETRAINED MODEL</strong></p>
<pre><code>model_state_dict = torch.load(&quot;../MODELOS/TRANSFORMERS/TransformersNormal&quot;,  map_location='cpu') #Doesnt work with map_location='cuda:0' neither
model = BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path=&quot;bert-base-uncased&quot;, state_dict=model_state_dict, cache_dir='./data')
</code></pre>
<p><strong>CREATING DATALOAD</strong></p>
<pre><code>def crearDataLoad(dfv,tokenizer): 

  dft=dfv  # usamos el del validacion para que nos salga los resultados y no tener que cambiar mucho codigo

  #validation=dfv['text']  
  validation=dfv['text'].str.lower()  # para modelos uncased  # el fichero que hemos llamado test es usado en la red neuronal
  validation_labels=dfv['label']
  
  validation_inputs = crearinputs (validation,tokenizer)
  validation_masks= crearmask (validation_inputs)
  
  validation_inputs = torch.tensor(validation_inputs)
  
  validation_labels = torch.tensor(validation_labels.values)
  
 
  validation_masks = torch.tensor(validation_masks)
  
  from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler# The DataLoader needs to know our batch size for training, so we specify it 

  #Colab
  batch_size = 32
  #local
  #batch_size = 15
  
  validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
  validation_sampler = SequentialSampler(validation_data)
  validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

  return validation_dataloader
</code></pre>
<p><strong>SHOWING RESULTS</strong></p>
<pre><code>def resultados(validation_dataloader, model, tokenizer):
    
  model.eval()

  # Tracking variables 
  predictions , true_labels = [], []
  pred = []
  t_label =[]
  # Predict 
  for batch in validation_dataloader:    
    # Add batch to GPU , como no tengo lo dejo aquí
    batch = tuple(t.to(device) for t in batch)
  
    # Unpack the inputs from our dataloader
    b_input_ids, b_input_mask, b_labels = batch
  
    # Telling the model not to compute or store gradients, saving memory and 
    # speeding up prediction
    with torch.no_grad():
      # Forward pass, calculate logit predictions
      outputs = model(b_input_ids, #toktype_ids=None, #
                      attention_mask=b_input_mask) #I GET THE ERROR HERE
     
    logits = outputs[0]
 
  
    # Move logits and labels to CPU
    logits = logits.detach().cpu().numpy()
    label_ids = b_labels.to('cpu').numpy()
  
    # Store predictions and true labels
    # Store predictions and true labels
    predictions.append(logits)
    true_labels.append(label_ids)
 
    for l in logits:
      # para cada tupla del logits, se selecciona 0 o 1 dependiendo del valor
      # que sea el mayor (argmax)
      pred_labels_i = np.argmax(l).item()
      pred.append(pred_labels_i)
  
  #Si no me equivoco, en pred guardamos las predicciones hechas por el modelo
  pred=np.asarray(pred).tolist()
  t_label = [val for sublist in true_labels for val in sublist] # para aplanar la lista de etiquetas
  #print('predicciones',pred)
  #print('t_labels',t_label)
  #print('validation_labels',validation_labels )
  print(&quot;RESULTADOS KFOLD validacion cruzada&quot;)
  from sklearn.metrics import confusion_matrix
  from sklearn.metrics import classification_report
  print(classification_report(t_label, pred))
  print (&quot;Distribution test {}&quot;.format(Counter(t_label)))
  from sklearn.metrics import confusion_matrix
  print(confusion_matrix(t_label, pred))
  from sklearn.metrics import roc_auc_score
  print('AUC ROC:')
  print(roc_auc_score(t_label, pred))
  from sklearn.metrics import f1_score
  result=f1_score(t_label, pred, average='binary',labels=[0,1],pos_label=1,zero_division=0)
  print('f1-score macro:')
  print(result)
  print(&quot;****************************************************************&quot;)
  return result
</code></pre>
<p>I get the error at this line in function <em>resultados:</em></p>
<pre><code>with torch.no_grad():
     # Forward pass, calculate logit predictions
     outputs = model(b_input_ids, #toktype_ids=None, #
                     attention_mask=b_input_mask) #Esto falla
</code></pre>
<p><strong>MAIN PROGRAM</strong></p>
<pre><code>trial_data = pd.DataFrame(trial_dataset)

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print('no hay gpu')
print('Found GPU at: {}'.format(device_name))

#import torch# If there's a GPU available...
if torch.cuda.is_available():  # Tell PyTorch to use the GPU. 
 device = torch.device(&quot;cuda&quot;) 
 print('There are %d GPU(s) available.' % torch.cuda.device_count()) 
 print('We will use the GPU:', torch.cuda.get_device_name(0)) # If not...
else:
 print('No GPU available, using the CPU instead.')
 device = torch.device(&quot;cpu&quot;)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

validation_dataloader = crearDataLoad(trial_data,tokenizer)
# obteniendo metricas del modelo generado en el paso anterior
model.eval() 
result= resultados(validation_dataloader, model,tokenizer)
</code></pre>
",17152483.0,,681865.0,,2021-11-24 21:01:17,2021-11-25 06:19:09,"RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! When predicting with my model",<python><pytorch><huggingface-transformers>,1,0,,,,CC BY-SA 4.0
69040271,1,,,2021-09-03 06:15:04,,9,5329,"<p>During training <a href=""https://github.com/WongKinYiu/yolor/blob/main/train.py"" rel=""noreferrer"">this code</a> with <a href=""https://docs.ray.io/en/latest/tune/index.html"" rel=""noreferrer"">ray tune</a>(1 gpu for 1 trial), after few hours
of training (about 20 trials) <code>CUDA out of memory</code> error occurred from GPU:0,1. And even after terminated the training process, the GPUS still give <code>out of memory</code> error.</p>
<p><a href=""https://i.stack.imgur.com/gGL8v.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/gGL8v.png"" alt=""nvidia-smi result"" /></a></p>
<p>As above, currently, all of my GPU devices are empty. And there is no other python process running except <a href=""https://stackoverflow.com/questions/55108619/what-is-the-process-python3-unattended-upgrade-shutdown"">these two</a>.</p>
<pre><code>import torch
torch.rand(1, 2).to('cuda:0') # cuda out of memory error
torch.rand(1, 2).to('cuda:1') # cuda out of memory error
torch.rand(1, 2).to('cuda:2') # working
torch.rand(1, 2).to('cuda:3') # working

torch.cuda.device_count() # 4
torch.cuda.memory_reserved() # 0
torch.cuda.is_available() # True
</code></pre>
<pre><code># error message of GPU 0, 1
RuntimeError: CUDA error: out of memory
</code></pre>
<p>However, GPU:0,1 give <code>out of memory</code> errors. If I reboot the computer(ubuntu 18.04.3), it returns to normal, but if I train the code again, the same problem occurs.</p>
<p>How can I debug this problem, or resolve it without rebooting?</p>
<ul>
<li>ubuntu 18.04.3</li>
<li>RTX 2080ti</li>
<li>CUDA version 10.2</li>
<li>nvidia driver version: 460.27.04</li>
<li>cudnn 7.6.4.38</li>
<li>Python 3.8.4</li>
<li>pytorch 1.7.0, 1.9.0, 1.9.0+cu111</li>
<li>cpu: (AMD Ryzen Threadripper 2950X 16-Core Processor)x32</li>
<li>memory: 125G</li>
<li>power: 2000W</li>
<li>dmesg result. (No <code>GPU has fallen off the bus</code> error)</li>
</ul>
<pre><code>dmesg | grep -i -e nvidia -e nvrm
[    5.946174] nvidia: loading out-of-tree module taints kernel.
[    5.946181] nvidia: module license 'NVIDIA' taints kernel.
[    5.956595] nvidia: module verification failed: signature and/or required key missing - tainting kernel
[    5.968280] nvidia-nvlink: Nvlink Core is being initialized, major device number 235
[    5.970485] nvidia 0000:09:00.0: enabling device (0000 -&gt; 0003)
[    5.970571] nvidia 0000:09:00.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=none
[    6.015145] nvidia 0000:0a:00.0: enabling device (0000 -&gt; 0003)
[    6.015394] nvidia 0000:0a:00.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=none
[    6.064993] nvidia 0000:42:00.0: enabling device (0000 -&gt; 0003)
[    6.065072] nvidia 0000:42:00.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=none
[    6.115778] nvidia 0000:43:00.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=io+mem
[    6.164680] NVRM: loading NVIDIA UNIX x86_64 Kernel Module  460.27.04  Fri Dec 11 23:35:05 UTC 2020
[    6.174137] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms  460.27.04  Fri Dec 11 23:24:19 UTC 2020
[    6.176472] [drm] [nvidia-drm] [GPU ID 0x00000900] Loading driver
[    6.176567] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:09:00.0 on minor 0
[    6.176635] [drm] [nvidia-drm] [GPU ID 0x00000a00] Loading driver
[    6.176636] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:0a:00.0 on minor 1
[    6.176709] [drm] [nvidia-drm] [GPU ID 0x00004200] Loading driver
[    6.176710] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:42:00.0 on minor 2
[    6.176760] [drm] [nvidia-drm] [GPU ID 0x00004300] Loading driver
[    6.176761] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:43:00.0 on minor 3
[    6.189768] nvidia-uvm: Loaded the UVM driver, major device number 511.
[    6.744582] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:40/0000:40:03.1/0000:43:00.1/sound/card4/input12
[    6.744664] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:40/0000:40:03.1/0000:43:00.1/sound/card4/input15
[    6.744755] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:40/0000:40:03.1/0000:43:00.1/sound/card4/input17
[    6.744852] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:40/0000:40:03.1/0000:43:00.1/sound/card4/input19
[    6.744952] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:40/0000:40:01.3/0000:42:00.1/sound/card3/input11
[    6.745301] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:40/0000:40:01.3/0000:42:00.1/sound/card3/input16
[    6.745739] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:40/0000:40:01.3/0000:42:00.1/sound/card3/input18
[    6.746280] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:40/0000:40:01.3/0000:42:00.1/sound/card3/input20
[    7.117377] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:00/0000:00:01.3/0000:09:00.1/sound/card0/input9
[    7.117453] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:00/0000:00:01.3/0000:09:00.1/sound/card0/input10
[    7.117505] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:00/0000:00:01.3/0000:09:00.1/sound/card0/input13
[    7.117559] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:00/0000:00:01.3/0000:09:00.1/sound/card0/input14
[    7.117591] input: HDA NVidia HDMI/DP,pcm=3 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card1/input21
[    7.117650] input: HDA NVidia HDMI/DP,pcm=7 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card1/input22
[    7.117683] input: HDA NVidia HDMI/DP,pcm=8 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card1/input23
[    7.117720] input: HDA NVidia HDMI/DP,pcm=9 as /devices/pci0000:00/0000:00:03.1/0000:0a:00.1/sound/card1/input24
[    9.462521] caller os_map_kernel_space.part.8+0x74/0x90 [nvidia] mapping multiple BARs
</code></pre>
<ul>
<li>numba and tensorflow have same problem, so it seems like it is not because of pytorch.</li>
</ul>
<pre><code>&gt;&gt;&gt; from numba import cuda
&gt;&gt;&gt; device = cuda.get_current_device()
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/api.py&quot;, line 460, in get_current_device
    return current_context().device
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/cudadrv/devices.py&quot;, line 212, in get_context
    return _runtime.get_or_create_context(devnum)
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/cudadrv/devices.py&quot;, line 138, in get_or_create_context
    return self._get_or_create_context_uncached(devnum)
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/cudadrv/devices.py&quot;, line 153, in _get_or_create_context_uncached
    return self._activate_context_for(0)
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/cudadrv/devices.py&quot;, line 169, in _activate_context_for
    newctx = gpu.get_primary_context()
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/cudadrv/driver.py&quot;, line 542, in get_primary_context
    driver.cuDevicePrimaryCtxRetain(byref(hctx), self.id)
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/cudadrv/driver.py&quot;, line 302, in safe_cuda_api_call
    self._check_error(fname, retcode)
  File &quot;/home/user_name/.pyenv/versions/tensorflow/lib/python3.8/site-packages/numba/cuda/cudadrv/driver.py&quot;, line 342, in _check_error
    raise CudaAPIError(retcode, msg)
numba.cuda.cudadrv.driver.CudaAPIError: [2] Call to cuDevicePrimaryCtxRetain results in CUDA_ERROR_OUT_OF_MEMORY
</code></pre>
<h3>update</h3>
<p>It seems like it is not happening after rebooting and upgrading pytorch version to 1.9.1+cu111.</p>
<h3>update 2</h3>
<p>It is occurring again, but I cannot do the suggested commands in the comment section because I don't have root access.</p>
",6259476.0,,6259476.0,,2022-05-02 10:40:32,2022-05-02 10:40:32,"GPU memory is empty, but CUDA out of memory error occurs",<pytorch><cuda><gpu><nvidia><ray>,0,9,0.0,,,CC BY-SA 4.0
64229717,1,64230770.0,,2020-10-06 16:08:24,,9,12674,"<p>So, I've read about half the original ResNet paper, and am trying to figure out how to make my version for tabular data.</p>
<p>I've read a few blog posts on how it works in PyTorch, and I see heavy use of <code>nn.Identity()</code>. Now, the paper also frequently uses the term <em>identity mapping</em>. However, it just refers to adding the input for a stack of layers the output of that same stack in an element-wise fashion. If the in and out dimensions are different, then the paper talks about padding the input with zeros or using a matrix <code>W_s</code> to project the input to a different dimension.</p>
<p>Here is an abstraction of a residual block I found in a blog post:</p>
<pre><code>
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, activation='relu'):
        super().__init__()
        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation
        self.blocks = nn.Identity()
        self.shortcut = nn.Identity()   
    
    def forward(self, x):
        residual = x
        if self.should_apply_shortcut: residual = self.shortcut(x)
        x = self.blocks(x)
        x += residual
        return x
    
    @property
    def should_apply_shortcut(self):
        return self.in_channels != self.out_channels
    
block1 = ResidualBlock(4, 4)
</code></pre>
<p>And my own application to a dummy tensor:</p>
<pre><code>x = tensor([1, 1, 2, 2])
block1 = ResidualBlock(4, 4)
block2 = ResidualBlock(4, 6)
x = block1(x)
print(x)
x = block2(x)
print(x)

&gt;&gt;&gt; tensor([2, 2, 4, 4])
&gt;&gt;&gt; tensor([4, 4, 8, 8])
</code></pre>
<p>So at the end of it, <code>x = nn.Identity(x)</code> and I'm not sure the point of its use except to mimic math lingo found in the original paper. I'm sure that's not the case though, and that it has some hidden use that I'm just not seeing yet. What could it be?</p>
<p><strong>EDIT</strong> Here is another example of implementing residual learning, this time in Keras. It does just what I suggested above and just keeps a copy of the input for adding to the output:</p>
<pre><code>def residual_block(x: Tensor, downsample: bool, filters: int,                                        kernel_size: int = 3) -&gt; Tensor:
    y = Conv2D(kernel_size=kernel_size,
               strides= (1 if not downsample else 2),
               filters=filters,
               padding=&quot;same&quot;)(x)
    y = relu_bn(y)
    y = Conv2D(kernel_size=kernel_size,
               strides=1,
               filters=filters,
               padding=&quot;same&quot;)(y)

    if downsample:
        x = Conv2D(kernel_size=1,
                   strides=2,
                   filters=filters,
                   padding=&quot;same&quot;)(x)
    out = Add()([x, y])
    out = relu_bn(out)
    return out
</code></pre>
",3696204.0,,10375049.0,,2020-10-06 21:46:26,2020-10-06 21:46:26,What is the idea behind using nn.Identity for residual learning?,<python><neural-network><pytorch><deep-residual-networks>,1,1,0.0,,,CC BY-SA 4.0
65447992,1,65448680.0,,2020-12-25 12:18:32,,9,13090,"<p>I am writing a simple transformation for a dataset which contains many pairs of images. As a data augmentation, I want to apply some random transformation for each pair but the images in that pair should be transformed in the same way.
For example, given a pair of two images <code>A</code> and <code>B</code>, if <code>A</code> is flipped horizontally, <code>B</code> must be flipped horizontally as <code>A</code>. Then the next pair <code>C</code> and <code>D</code> should be differently transformed from <code>A</code> and <code>B</code> but <code>C</code> and <code>D</code> are transformed in the same way. I am trying that in the way below</p>
<pre class=""lang-py prettyprint-override""><code>import random
import numpy as np
import torchvision.transforms as transforms
from PIL import Image

img_a = Image.open(&quot;sample_ajpg&quot;) # note that two images have the same size
img_b = Image.open(&quot;sample_b.png&quot;)
img_c, img_d = Image.open(&quot;sample_c.jpg&quot;), Image.open(&quot;sample_d.png&quot;)

transform = transforms.RandomChoice(
    [transforms.RandomHorizontalFlip(), 
     transforms.RandomVerticalFlip()]
)
random.seed(0)
display(transform(img_a))
display(transform(img_b))

random.seed(1)
display(transform(img_c))
display(transform(img_d))
</code></pre>
<p>Yet、 the above code does not choose the same transformation and as I tested, it is dependent on the number of times <code>transform</code> is called.</p>
<p>Is there any way to force <code>transforms.RandomChoice</code> to use the same transform when specified?</p>
",12757690.0,,,,,2022-07-24 19:04:00,PyTorch : How to apply the same random transformation to multiple image?,<python><pytorch><torchvision>,5,0,0.0,,,CC BY-SA 4.0
65424771,1,65424772.0,,2020-12-23 13:03:55,,9,10281,"<p>How to transform vectors of labels to one-hot encoding and back in Pytorch?</p>
<p>The solution to the question was copied to here after having to go through the entire forum discussion, instead of just finding an easy one from googling.</p>
",913098.0,,913098.0,,2020-12-24 08:32:39,2023-03-19 09:57:25,How to convert one-hot vector to label index and back in Pytorch?,<python><pytorch><one-hot-encoding><multiclass-classification>,2,3,,,,CC BY-SA 4.0
67799246,1,,,2021-06-02 05:10:19,,9,10065,"<h2>Problem</h2>
<p>I am training a deep learning model in PyTorch for binary classification, and I have a dataset containing unbalanced class proportions. My minority class makes up about <code>10%</code> of the given observations. To avoid the model learning to just predict the majority class, I want to use the <code>WeightedRandomSampler</code> from <code>torch.utils.data</code> in my <code>DataLoader</code>.</p>
<p>Let's say I have <code>1000</code> observations (<code>900</code> in class <code>0</code>, <code>100</code> in class <code>1</code>), and a batch size of <code>100</code> for my dataloader.</p>
<p>Without weighted random sampling, I would expect each training epoch to consist of 10 batches.</p>
<h2>Questions</h2>
<ul>
<li>Will only 10 batches be sampled per epoch when using this sampler - and consequently, would the model 'miss' a large portion of the majority class during each epoch, since the minority class is now overrepresented in the training batches?</li>
<li>Will using the sampler result in more than 10 batches being sampled per epoch (meaning the same minority class observations may appear many times, and also that training would slow down)?</li>
</ul>
",15859984.0,,10886420.0,,2021-06-02 09:27:27,2022-08-09 12:52:17,Weighted random sampler - oversample or undersample?,<pytorch><oversampling><pytorch-dataloader>,2,2,0.0,,,CC BY-SA 4.0
68140977,1,,,2021-06-26 09:27:26,,9,3625,"<p>huggingface-hub 0.0.12 requires packaging&gt;=20.9, but you'll have packaging 20.4 which is incompatible</p>
<p><img src=""https://i.stack.imgur.com/j7F5g.png"" alt=""enter image description here"" /></p>
",14910704.0,,3528321.0,,2023-01-09 06:08:53,2023-01-09 06:08:53,"huggingface-hub 0.0.12 requires packaging>=20.9, but you'll have packaging 20.4 which is incompatible",<pytorch>,1,0,,,,CC BY-SA 4.0
73629154,1,73642630.0,,2022-09-07 01:21:14,,9,19246,"<p>I installed the GUI version of Stable Diffusion <a href=""https://grisk.itch.io/stable-diffusion-gui"" rel=""noreferrer"">here</a>. With it I was able to make 512 by 512 pixel images using my GeForce RTX 3070 GPU with 8 GB of memory:</p>
<p><a href=""https://i.stack.imgur.com/vN4MW.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vN4MW.png"" alt=""GUI screenshot"" /></a></p>
<p>However when I try to do the same thing with the command line interface, I run out of memory:</p>
<p>Input:<br>
<code>&gt;&gt; C:\SD\stable-diffusion-main&gt;python scripts/txt2img.py --prompt &quot;a close-up portrait of a cat by pablo picasso, vivid, abstract art, colorful, vibrant&quot; --plms --n_iter 3 --n_samples 1 --H 512 --W 512</code></p>
<p>Error:</p>
<p><code>RuntimeError: CUDA out of memory. Tried to allocate 1024.00 MiB (GPU 0; 8.00 GiB total capacity; 6.13 GiB already allocated; 0 bytes free; 6.73 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF</code></p>
<p>If I reduce the size of the image to 256 X 256, it gives a result, but obviously much lower quality.</p>
<p>So part 1 of my question is why do I run out of memory at 6.13 GiB when I have 8 GiB on the card, and part 2 is what does the GUI do differently to allow 512 by 512 output? Is there a setting I can change to reduce the load on the GPU?</p>
<p>Thanks a lot,
Alex</p>
",728286.0,,,,,2022-10-26 18:46:37,Command Line stable diffusion runs out of GPU memory but GUI version doesn't,<python><pytorch><generative-art>,2,0,0.0,,,CC BY-SA 4.0
64216189,1,,,2020-10-05 21:05:39,,8,4258,"<p>I'm trying to run the following code:</p>
<pre><code>import matplotlib.pyplot as plt
%matplotlib inline
import torch

x = y = torch.tensor([1,2,3]).numpy()
plt.plot(x,y);
</code></pre>
<p>I keep getting the message: <code>The kernel appears to have died. It will restart automatically.</code> and a restart and a red &quot;Dead kernel&quot; tag on the toolbar.</p>
<p>But the weird thing is, if I import <code>matplotlib.pyplot</code> and plot some random graph first, the above code plots just fine. In other words, the following code works fine.</p>
<pre><code>import matplotlib.pyplot as plt
%matplotlib inline

plt.subplots(figsize=(0.01,0.01))
plt.gca().set_visible(False);

import torch
x = torch.tensor([1,2,3]).numpy()

plt.plot(x,x);
</code></pre>
<p>What is going on here? I'm using numpy 1.18.5, pytorch 1.6.0, matplotlib 3.2.2 on Python 3.7.7, if it matters. Thank you.</p>
",,user7864386,,,,2023-04-13 09:19:45,Kernel keeps dying when plotting a graph after importing the torch library,<python><matplotlib><jupyter-notebook><pytorch>,4,2,0.0,,,CC BY-SA 4.0
65037368,1,,,2020-11-27 12:27:17,,8,9595,"<p>I would like to remove tensorflow and hugging face models from my laptop.
I did find one link <a href=""https://github.com/huggingface/transformers/issues/861"" rel=""noreferrer"">https://github.com/huggingface/transformers/issues/861</a>
but is there not command that can remove them because as mentioned in the link manually deleting can cause problems because we don't know which other files are linked to those models or are expecting some model to be present in that location or simply it may cause some error.</p>
",5687866.0,,5687866.0,,2020-11-28 07:02:47,2023-02-17 22:18:58,Remove downloaded tensorflow and pytorch(Hugging face) models,<tensorflow><pytorch><huggingface-transformers>,3,2,,,,CC BY-SA 4.0
63880081,1,63880190.0,,2020-09-14 07:45:12,,8,7340,"<p>I'm trying to serialize a torch tensor using protobuf and it seems using <code>BytesIO</code> along with <code>torch.save()</code> doesn't work. I have tried:</p>
<pre class=""lang-py prettyprint-override""><code>import torch 
import io
x = torch.randn(size=(1,20))
buff = io.BytesIO()
torch.save(x, buff)
print(f'buffer: {buff.read()}')
</code></pre>
<p>to no avail as it results in <code>b''</code> in the output! How should I be going about this?</p>
",2736559.0,,472495.0,,2020-09-16 21:26:42,2023-02-01 16:32:46,How to convert a torch tensor into a byte string?,<python><pytorch><torch>,3,0,0.0,,,CC BY-SA 4.0
65031764,1,,,2020-11-27 03:33:30,,8,293,"<p>I'm trying to warp a frame from view1 to view2 using ground truth depth map, pose information, and camera matrix. I've been able to remove most of the for-loops and vectorize it, except one for-loop. When warping, multiple pixels in view1 may get mapped to a single location in view2, due to occlusions. In this case, I need to pick the pixel with the lowest depth value (foreground object). I'm not able to vectorize this part of the code. Any help to vectorize this for loop is appreciated.</p>
<h3>Context:</h3>I'm trying to warp an image into a new view, given ground truth pose, depth, and camera matrix. After computing warped locations, I'm rounding them off. Any suggestions to implement inverse bilinear interpolation are also welcome. My images are of full HD resolution. Hence it is taking a lot of time to warp the frames to the new view. If I can vectorize, I'm planning to convert the code to TensorFlow or PyTorch and run it on a GPU. Any other suggestions to speed up warping, or existing implementations are also welcome.
<h3> Code: </h3>
<pre><code>def warp_frame_04(frame1: numpy.ndarray, depth: numpy.ndarray, intrinsic: numpy.ndarray, transformation1: numpy.ndarray,
                  transformation2: numpy.ndarray, convert_to_uint: bool = True, verbose_log: bool = True):
    &quot;&quot;&quot;
    Vectorized Forward warping. Nearest Neighbor.
    Offset requirement of warp_frame_03() overcome.
    mask: 1 if pixel found, 0 if no pixel found
    Drawback: Nearest neighbor, collision resolving not vectorized
    &quot;&quot;&quot;
    height, width, _ = frame1.shape
    assert depth.shape == (height, width)
    transformation = numpy.matmul(transformation2, numpy.linalg.inv(transformation1))

    y1d = numpy.array(range(height))
    x1d = numpy.array(range(width))
    x2d, y2d = numpy.meshgrid(x1d, y1d)
    ones_2d = numpy.ones(shape=(height, width))
    ones_4d = ones_2d[:, :, None, None]
    pos_vectors_homo = numpy.stack([x2d, y2d, ones_2d], axis=2)[:, :, :, None]

    intrinsic_inv = numpy.linalg.inv(intrinsic)
    intrinsic_4d = intrinsic[None, None]
    intrinsic_inv_4d = intrinsic_inv[None, None]
    depth_4d = depth[:, :, None, None]
    trans_4d = transformation[None, None]

    unnormalized_pos = numpy.matmul(intrinsic_inv_4d, pos_vectors_homo)
    world_points = depth_4d * unnormalized_pos
    world_points_homo = numpy.concatenate([world_points, ones_4d], axis=2)
    trans_world_homo = numpy.matmul(trans_4d, world_points_homo)
    trans_world = trans_world_homo[:, :, :3]
    trans_norm_points = numpy.matmul(intrinsic_4d, trans_world)
    trans_pos = trans_norm_points[:, :, :2, 0] / trans_norm_points[:, :, 2:3, 0]
    trans_pos_int = numpy.round(trans_pos).astype('int')

    # Solve occlusions
    a = trans_pos_int.reshape(-1, 2)
    d = depth.ravel()
    b = numpy.unique(a, axis=0, return_index=True, return_counts=True)
    collision_indices = b[1][b[2] &gt;= 2]  # Unique indices which are involved in collision
    for c1 in tqdm(collision_indices, disable=not verbose_log):
        cl = a[c1].copy()  # Collision Location
        ci = numpy.where((a[:, 0] == cl[0]) &amp; (a[:, 1] == cl[1]))[0]  # Colliding Indices: Indices colliding for cl
        cci = ci[numpy.argmin(d[ci])]  # Closest Collision Index: Index of the nearest point among ci
        a[ci] = [-1, -1]
        a[cci] = cl
    trans_pos_solved = a.reshape(height, width, 2)

    # Offset both axes by 1 and set any out of frame motion to edge. Then crop 1-pixel thick edge
    trans_pos_offset = trans_pos_solved + 1
    trans_pos_offset[:, :, 0] = numpy.clip(trans_pos_offset[:, :, 0], a_min=0, a_max=width + 1)
    trans_pos_offset[:, :, 1] = numpy.clip(trans_pos_offset[:, :, 1], a_min=0, a_max=height + 1)

    warped_image = numpy.ones(shape=(height + 2, width + 2, 3)) * numpy.nan
    warped_image[trans_pos_offset[:, :, 1], trans_pos_offset[:, :, 0]] = frame1
    cropped_warped_image = warped_image[1:-1, 1:-1]
    mask = numpy.isfinite(cropped_warped_image)
    cropped_warped_image[~mask] = 0
    if convert_to_uint:
        final_warped_image = cropped_warped_image.astype('uint8')
    else:
        final_warped_image = cropped_warped_image
    mask = mask[:, :, 0]
    return final_warped_image, mask
    
    
</code></pre>
<h3> Code Explanation </h3>
<ul>
<li>I'm using the equations[1,2] to get pixel locations in view2</li>
<li> Once I have the pixel locations, I need to figure out if there are any occlusions and if so, I have to pick the foreground pixels.</li>  
<li> `b = numpy.unique(a, axis=0, return_index=True, return_counts=True)` gives me unique locations.</li>  
<li> If multiple pixels from view1 map to a single pixel in view2 (collision), `return_counts` will give a value greater than 1.</li>  
<li> `collision_indices = b[1][b[2] >= 2]` gives indices which are involved in collision. Note that this gives only one index per collision.</li>  
<li> For each of such collision points, `ci = numpy.where((a[:, 0] == cl[0]) & (a[:, 1] == cl[1]))[0]` provides indices of all pixels from view1 which map to the same point in view2.</li>  
<li> `cci = ci[numpy.argmin(d[ci])]` gives the pixel index with lowest depth value.</li>  
<li> `a[ci] = [-1, -1]` and `a[cci] = cl` maps all other background pixels to location (-1,-1) which is out of frame and hence will be ignored.</li>
</ul>
<p>[1] <a href=""https://i.stack.imgur.com/s1D9t.png"" rel=""noreferrer"">https://i.stack.imgur.com/s1D9t.png</a><br />
[2] <a href=""https://dsp.stackexchange.com/q/69890/32876"">https://dsp.stackexchange.com/q/69890/32876</a></p>
",3337089.0,,,,,2020-11-27 03:33:30,PoseWarping: How to vectorize this for loop (z-buffer),<python><numpy><tensorflow><computer-vision><pytorch>,0,0,,,,CC BY-SA 4.0
63057468,1,63064444.0,,2020-07-23 15:12:23,,8,14470,"<p>My saved state_dict does not contain all the layers that are in my model.
How can I ignore the Missing key(s) in state_dict error and initialize the remaining weights?</p>
",4439753.0,,1174966.0,,2020-07-23 23:39:05,2021-11-25 19:20:53,How to ignore and initialize Missing key(s) in state_dict,<python><pytorch>,2,0,0.0,,,CC BY-SA 4.0
66994662,1,,,2021-04-07 21:59:31,,8,4940,"<p>After a recent upgrade, when running my PyTorch loop, I now get the warning</p>
<blockquote>
<p>Using a non-full backward hook when the forward contains multiple autograd Nodes`&quot;.</p>
</blockquote>
<p>The training still runs and completes, but I am unsure where I am supposed to place the <code>register_full_backward_hook</code> function.</p>
<p>I have tried adding it to each of the layers in my neural network but this gives further errors about using different hooks.</p>
<p>Can anyone please advise?</p>
",10468535.0,,6331369.0,,2022-02-07 22:38:48,2022-02-07 22:38:48,PyTorch warning about using a non-full backward hook when the forward contains multiple autograd Nodes,<python><pytorch><hook><autograd>,1,0,0.0,,,CC BY-SA 4.0
67472361,1,67561093.0,,2021-05-10 14:32:04,,8,5791,"<p>In my previous <a href=""https://stackoverflow.com/questions/67320792/how-to-use-pytorchs-autograd-efficiently-with-tensors/67334809#67334809"">question</a> I found how to use PyTorch's autograd with tensors:</p>
<pre><code>import torch
from torch.autograd import grad
import torch.nn as nn
import torch.optim as optim

class net_x(nn.Module): 
        def __init__(self):
            super(net_x, self).__init__()
            self.fc1=nn.Linear(1, 20) 
            self.fc2=nn.Linear(20, 20)
            self.out=nn.Linear(20, 4) #a,b,c,d

        def forward(self, x):
            x=torch.tanh(self.fc1(x))
            x=torch.tanh(self.fc2(x))
            x=self.out(x)
            return x

nx = net_x()

#input
t = torch.tensor([1.0, 2.0, 3.2], requires_grad = True) #input vector
t = torch.reshape(t, (3,1)) #reshape for batch

#method 
dx = torch.autograd.functional.jacobian(lambda t_: nx(t_), t)
dx = torch.diagonal(torch.diagonal(dx, 0, -1), 0)[0] #first vector
#dx = torch.diagonal(torch.diagonal(dx, 1, -1), 0)[0] #2nd vector
#dx = torch.diagonal(torch.diagonal(dx, 2, -1), 0)[0] #3rd vector
#dx = torch.diagonal(torch.diagonal(dx, 3, -1), 0)[0] #4th vector
dx 
&gt;&gt;&gt; 
tensor([-0.0142, -0.0517, -0.0634])
</code></pre>
<p>The issue is that <code>grad</code> only knows how to propagate gradients from a scalar tensor (which my network's output is not), which is why I had to calculate the Jacobian.</p>
<p>However, this is not very efficient and a bit slow as my matrix is large and calculating the entire Jacobian takes a while (and I'm also not using the entire Jacobian matrix).</p>
<p>Is there a way to calculate only the diagonals of the Jacobian (to get the 4 vectors in this example)?</p>
<p>There appears to be an <a href=""https://github.com/pytorch/pytorch/issues/41530"" rel=""nofollow noreferrer"">open feature request</a> but it doesn't appear to have gotten much attention.</p>
<p><strong>Update 1:</strong><br />
I tried what @iacob said about setting <code>torch.autograd.functional.jacobian(vectorize=True)</code>.<br />
However, this seems to be slower. To test this I changed my network output from <code>4</code> to <code>400</code>, and my input <code>t</code> to be:</p>
<pre><code>val = 100
t = torch.rand(val, requires_grad = True) #input vector
t = torch.reshape(t, (val,1)) #reshape for batch
</code></pre>
<p>Without <code>vectorized = True</code>:</p>
<pre><code>Wall time: 10.4 s
</code></pre>
<p>With:</p>
<pre><code>Wall time: 14.6 s
</code></pre>
",14735451.0,,14735451.0,,2021-05-15 20:42:35,2021-05-17 01:30:17,Using PyTorch's autograd efficiently with tensors by calculating the Jacobian,<python><pytorch>,2,0,0.0,,,CC BY-SA 4.0
65083581,1,65084300.0,,2020-12-01 01:38:17,,8,3824,"<p>I'm using the HuggingFace Transformers BERT model, and I want to compute a summary vector (a.k.a. embedding) over the tokens in a sentence, using either the <code>mean</code> or <code>max</code> function. The complication is that some tokens are <code>[PAD]</code>, so I want to ignore the vectors for those tokens when computing the average or max.</p>
<p>Here's an example. I initially instantiate a <code>BertTokenizer</code> and a <code>BertModel</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import transformers
from transformers import AutoTokenizer, AutoModel

transformer_name = 'bert-base-uncased'

tokenizer = AutoTokenizer.from_pretrained(transformer_name, use_fast=True)

model = AutoModel.from_pretrained(transformer_name)
</code></pre>
<p>I then input some sentences into the tokenizer and get out <code>input_ids</code> and <code>attention_mask</code>. Notably, an <code>attention_mask</code> value of 0 means that the token was a <code>[PAD]</code> that I can ignore.</p>
<pre class=""lang-py prettyprint-override""><code>sentences = ['Deep learning is difficult yet very rewarding.',
             'Deep learning is not easy.',
             'But is rewarding if done right.']
tokenizer_result = tokenizer(sentences, max_length=32, padding=True, return_attention_mask=True, return_tensors='pt')

input_ids = tokenizer_result.input_ids
attention_mask = tokenizer_result.attention_mask

print(input_ids.shape) # torch.Size([3, 11])

print(input_ids)
# tensor([[  101,  2784,  4083,  2003,  3697,  2664,  2200, 10377,  2075,  1012,  102],
#         [  101,  2784,  4083,  2003,  2025,  3733,  1012,   102,     0,     0,    0],
#         [  101,  2021,  2003, 10377,  2075,  2065,  2589,  2157,  1012,   102,   0]])

print(attention_mask.shape) # torch.Size([3, 11])

print(attention_mask)
# tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#         [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],
#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])
</code></pre>
<p>Now, I call the BERT model to get the 768-D token embeddings (the top-layer hidden states).</p>
<pre class=""lang-py prettyprint-override""><code>model_result = model(input_ids, attention_mask=attention_mask, return_dict=True)

token_embeddings = model_result.last_hidden_state
print(token_embeddings.shape) # torch.Size([3, 11, 768])
</code></pre>
<p>So at this point, I have:</p>
<ol>
<li>token embeddings in a [3, 11, 768] matrix: 3 sentences, 11 tokens, 768-D vector for each token.</li>
<li>attention mask in a [3, 11] matrix: 3 sentences, 11 tokens. A 1 value indicates non-<code>[PAD]</code>.</li>
</ol>
<p>How do I compute the <code>mean</code> / <code>max</code> over the vectors for the valid, non-<code>[PAD]</code> tokens?</p>
<p>I tried using the attention mask as a mask and then called <code>torch.max()</code>, but I don't get the right dimensions:</p>
<pre class=""lang-py prettyprint-override""><code>masked_token_embeddings = token_embeddings[attention_mask==1]
print(masked_token_embeddings.shape) # torch.Size([29, 768] &lt;-- WRONG. SHOULD BE [3, 11, 768]

pooled = torch.max(masked_token_embeddings, 1)
print(pooled.values.shape) # torch.Size([29]) &lt;-- WRONG. SHOULD BE [3, 768]
</code></pre>
<p>What I really want is a tensor of shape [3, 768]. That is, a 768-D vector for each of the 3 sentences.</p>
",4561314.0,,4561314.0,,2020-12-01 01:49:57,2022-09-07 17:39:44,How to compute mean/max of HuggingFace Transformers BERT token embeddings with attention mask?,<machine-learning><pytorch><bert-language-model><huggingface-transformers>,3,0,0.0,,,CC BY-SA 4.0
66370250,1,66372547.0,,2021-02-25 14:16:28,,8,6282,"<p>I'm creating a custom dataset for NLP-related tasks.</p>
<p>In the PyTorch <a href=""https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class"" rel=""noreferrer"">custom datast tutorial</a>, we see that the <code>__getitem__()</code> method leaves room for a transform before it returns a sample:</p>
<pre><code>def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img_name = os.path.join(self.root_dir,
                                self.landmarks_frame.iloc[idx, 0])
        image = io.imread(img_name)
       
        ### SOME DATA MANIPULATION HERE ###

        sample = {'image': image, 'landmarks': landmarks}
        if self.transform:
            sample = self.transform(sample)

        return sample
</code></pre>
<p>However, the code here:</p>
<pre><code>        if torch.is_tensor(idx):
            idx = idx.tolist()
</code></pre>
<p>implies that multiple items should be able to be retrieved at a time which leaves me wondering:</p>
<ol>
<li><p><strong>How does that transform work on multiple items?</strong> Take the custom transforms in the tutorial for example. They do not look like they could be applied to a batch of samples in a single call.</p>
</li>
<li><p><strong>Related, how does a DataLoader retrieve a batch of multiple samples in parallel and apply said transform if the transform can only be applied to a single sample?</strong></p>
</li>
</ol>
",3696204.0,,8306834.0,,2021-02-25 16:26:09,2021-02-25 16:31:44,How does PyTorch DataLoader interact with a PyTorch dataset to transform batches?,<python><pytorch><pytorch-dataloader>,2,0,,,,CC BY-SA 4.0
74594256,1,,,2022-11-27 22:24:45,,8,8193,"<p>I cannot use PyTorch and Shap getting this error</p>
<pre><code>PyTorch Error loading  &quot;\lib\site-packages\torch\lib\shm.dll&quot; or one of its dependencies.
</code></pre>
<p>I have tried</p>
<ul>
<li>to uninstall and re-install PyTorch, failed</li>
<li>create a new conda environment and reinstalled everything including PyTorch, failed</li>
<li>to install .NET C++ as suggested in other posts, but it was already installed</li>
</ul>
<p>I am not an expert on SO and dependencies, but i find it strange that there is not an easy way to fix it. Any idea?</p>
",1050187.0,,,,,2023-05-19 12:31:54,"PyTorch Error loading ""\lib\site-packages\torch\lib\shm.dll"" or one of its dependencies",<python><pytorch><conda><shap>,2,3,,,,CC BY-SA 4.0
65301875,1,65302248.0,,2020-12-15 07:48:16,,8,6499,"<p>From PyTorch <a href=""https://pytorch.org/docs/stable/autograd.html"" rel=""noreferrer"">documentation</a>:</p>
<pre><code>b = torch.rand(10, requires_grad=True).cuda()
b.is_leaf
False
# b was created by the operation that cast a cpu Tensor into a cuda Tensor

e = torch.rand(10).cuda().requires_grad_()
e.is_leaf
True
# e requires gradients and has no operations creating it

f = torch.rand(10, requires_grad=True, device=&quot;cuda&quot;)
f.is_leaf
True
# f requires grad, has no operation creating it
</code></pre>
<p>But why are <code>e</code> and <code>f</code> leaf Tensors, when they both were also cast from a CPU Tensor, into a Cuda Tensor (an operation)?</p>
<p>Is it because Tensor <code>e</code> was cast into Cuda <em>before</em> the in-place operation <code>requires_grad_()</code>?</p>
<p>And because <code>f</code> was cast by assignment <code>device=&quot;cuda&quot;</code> rather than by method <code>.cuda()</code>?</p>
",14492001.0,,7579547.0,,2021-03-17 09:14:06,2021-03-17 09:14:06,How to understand creating leaf tensors in PyTorch?,<python><pytorch><documentation>,1,0,0.0,,,CC BY-SA 4.0
67328098,1,67333163.0,,2021-04-30 04:14:43,,8,2221,"<p>I have a pytorch network, that have been trained and weights are updated (complete training).</p>
<pre><code>class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(1, H)
        self.fc2 = nn.Linear(1, H)
        self.fc3 = nn.Linear(H, 1)

    def forward(self, x, y):
        h1 = F.relu(self.fc1(x)+self.fc2(y))
        h2 = self.fc3(h1)
        return h2   
</code></pre>
<p>After training, I want to maximize the output of the network with respect to input. In other words, I want to optimize the input to maximize the neural network output, without changing weights. How can I achieve that.
My trial, but it doesn't make sense:</p>
<pre><code>in     = torch.autograd.Variable(x)
out    = Net(in)
grad   = torch.autograd.grad(out, input)

      
</code></pre>
",14881301.0,,,,,2021-04-30 11:34:10,How to find input that maximizes output of a neural network using pytorch,<optimization><neural-network><pytorch>,1,0,0.0,,,CC BY-SA 4.0
69180740,1,,,2021-09-14 15:38:18,,8,9510,"<p>I'm on Ubuntu 20.04 LTS with CUDA 11.1 installed (and working, with <code>PATH</code> and <code>LD_LIBRARY_PATH</code> configured correctly), and I'm trying to define a reusable conda environment (i.e., in an <code>environment.yml</code> file) that successfully installs PyTorch with CUDA support.</p>
<p>However, when I use the environment file, I get a message that Torch wasn't compiled with CUDA support:</p>
<pre><code>Python 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05)
[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; device = torch.device(&quot;cuda:0&quot;)
&gt;&gt;&gt; t = torch.tensor(device=device, data=[0,1,2,3])
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/jdr2160/anaconda3/envs/foo/lib/python3.8/site-packages/torch/cuda/__init__.py&quot;, line 166, in _lazy_init
    raise AssertionError(&quot;Torch not compiled with CUDA enabled&quot;)
AssertionError: Torch not compiled with CUDA enabled
</code></pre>
<p>My <code>environment.yml</code> is pretty bare-bones:</p>
<pre><code>name: foo
channels:
  - conda-forge
  - nvidia
  - pytorch
dependencies:
  - cudatoolkit=11.1
  - python=3.8
  - pytorch
</code></pre>
<p>When I create an 'empty' python 3.8 environment and install the Conda packages from the command line instead of from an environment file, everything works fine:</p>
<pre><code>$ conda env create --name bar python=3.8
...
$ conda activate bar
$ conda install pytorch cudatoolkit=11.1 -c pytorch -c nvidia
...
$ python
Python 3.8.10 | packaged by conda-forge | (default, May 11 2021, 07:01:05)
[GCC 9.3.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; device = torch.device(&quot;cuda:0&quot;)
&gt;&gt;&gt; t = torch.tensor(device=device, data=[0,1,2,3])
&gt;&gt;&gt;
</code></pre>
<p>Can anyone tell what's going on here? It seems that Conda doesn't see the <code>cudatoolkit=11.1</code> dependency while installing PyTorch from the environment file, but I have no idea how to fix it.</p>
",1730417.0,,681865.0,,2021-09-14 16:22:42,2021-12-22 11:47:33,Can't install GPU-enabled Pytorch in Conda environment from environment.yml,<python><pytorch><conda>,2,0,0.0,,,CC BY-SA 4.0
65498782,1,73388839.0,,2020-12-29 21:06:37,,8,6487,"<p><a href=""https://pytorch-lightning.readthedocs.io/en/0.8.3/metrics.html#tensormetric"" rel=""noreferrer"">The official doc</a> only states</p>
<pre><code>&gt;&gt;&gt; from pytorch_lightning.metrics import ConfusionMatrix
&gt;&gt;&gt; target = torch.tensor([1, 1, 0, 0])
&gt;&gt;&gt; preds = torch.tensor([0, 1, 0, 0])
&gt;&gt;&gt; confmat = ConfusionMatrix(num_classes=2)
&gt;&gt;&gt; confmat(preds, target)
</code></pre>
<p>This doesn't show how to use the metric with the framework.</p>
<p>My attempt (methods are not complete and only show relevant parts):</p>
<pre><code>def __init__(...):
    self.val_confusion = pl.metrics.classification.ConfusionMatrix(num_classes=self._config.n_clusters)

def validation_step(self, batch, batch_index):
    ...
    log_probs = self.forward(orig_batch)
    loss = self._criterion(log_probs, label_batch)
   
    self.val_confusion.update(log_probs, label_batch)
    self.log('validation_confusion_step', self.val_confusion, on_step=True, on_epoch=False)

def validation_step_end(self, outputs):
    return outputs

def validation_epoch_end(self, outs):
    self.log('validation_confusion_epoch', self.val_confusion.compute())

</code></pre>
<p>After the 0th epoch, this gives</p>
<pre><code>    Traceback (most recent call last):
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 521, in train
        self.train_loop.run_training_epoch()
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\training_loop.py&quot;, line 588, in run_training_epoch
        self.trainer.run_evaluation(test_mode=False)
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\trainer.py&quot;, line 613, in run_evaluation
        self.evaluation_loop.log_evaluation_step_metrics(output, batch_idx)
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\evaluation_loop.py&quot;, line 346, in log_evaluation_step_metrics
        self.__log_result_step_metrics(step_log_metrics, step_pbar_metrics, batch_idx)
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\evaluation_loop.py&quot;, line 350, in __log_result_step_metrics
        cached_batch_pbar_metrics, cached_batch_log_metrics = cached_results.update_logger_connector()
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 378, in update_logger_connector
        batch_log_metrics = self.get_latest_batch_log_metrics()
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 418, in get_latest_batch_log_metrics
        batch_log_metrics = self.run_batch_from_func_name(&quot;get_batch_log_metrics&quot;)
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 414, in run_batch_from_func_name
        results = [func(include_forked_originals=False) for func in results]
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 414, in &lt;listcomp&gt;
        results = [func(include_forked_originals=False) for func in results]
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 122, in get_batch_log_metrics
        return self.run_latest_batch_metrics_with_func_name(&quot;get_batch_log_metrics&quot;,
*args, **kwargs)
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 115, in run_latest_batch_metrics_with_func_name
        for dl_idx in range(self.num_dataloaders)
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 115, in &lt;listcomp&gt;
        for dl_idx in range(self.num_dataloaders)
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py&quot;, line 100, in get_latest_from_func_name
        results.update(func(*args, add_dataloader_idx=add_dataloader_idx, **kwargs))
      File &quot;C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\core\step_result.py&quot;, line 298, in get_batch_log_metrics
        result[dl_key] = self[k]._forward_cache.detach()
    AttributeError: 'NoneType' object has no attribute 'detach'

                                                      
</code></pre>
<p>It does pass the sanity validation check before training.</p>
<p>The failure happens on the return in <code>validation_step_end</code>. Makes little sense to me.</p>
<p>The exact same method of using mertics works fine with accuracy.</p>
<p><strong>How to get a correct confusion matrix?</strong></p>
",913098.0,,,,,2022-08-17 12:55:58,How to dump confusion matrix using TensorBoard logger in pytorch-lightning?,<python><deep-learning><pytorch><tensorboard><pytorch-lightning>,3,4,0.0,,,CC BY-SA 4.0
62726792,1,,,2020-07-04 08:06:55,,8,26355,"<p>I encounter this weird error when building a simple NN in Pytorch. I dont understand this error and why this consern Long and Float datatype in backward function. Anyone encounter this before? Thanks for any help.</p>
<pre><code>Traceback (most recent call last):
  File &quot;test.py&quot;, line 30, in &lt;module&gt;
    loss.backward()
  File &quot;/home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py&quot;, line 198, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File &quot;/home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py&quot;, line 100, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: expected dtype Float but got dtype Long (validate_dtype at /opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/ATen/native/TensorIterator.cpp:143)
frame #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x4e (0x7f5856661b5e in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torch/lib/libc10.so)
frame #1: at::TensorIterator::compute_types() + 0xce3 (0x7f587e3dc793 in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site
-packages/torch/lib/libtorch_cpu.so)
frame #2: at::TensorIterator::build() + 0x44 (0x7f587e3df174 in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages
/torch/lib/libtorch_cpu.so)
frame #3: at::native::smooth_l1_loss_backward_out(at::Tensor&amp;, at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, long)
 + 0x193 (0x7f587e22cf73 in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #4: &lt;unknown function&gt; + 0xe080b7 (0x7f58576960b7 in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torc
h/lib/libtorch_cuda.so)
frame #5: at::native::smooth_l1_loss_backward(at::Tensor const&amp;, at::Tensor const&amp;, at::Tensor const&amp;, long) + 0x16e (0x7f587
e23569e in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so)
frame #6: &lt;unknown function&gt; + 0xed98af (0x7f587e71c8af in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torc
h/lib/libtorch_cpu.so)
frame #7: &lt;unknown function&gt; + 0xe22286 (0x7f587e665286 in /home/liuyun/anaconda3/envs/torch/lib/python3.7/site-packages/torc
h/lib/libtorch_cpu.so)
</code></pre>
<p>Here is the source code:</p>
<pre><code>import torch
import torch.nn as nn
import numpy as np
import torchvision
from torchvision import models
from UTKLoss import MultiLoss
from ipdb import set_trace

# out features [13, 2, 5]
model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, 20)
model_ft.cuda()

criterion = MultiLoss()
optimizer = torch.optim.Adam(model_ft.parameters(), lr = 1e-3)

image = torch.randn((1, 3, 128, 128)).cuda()
age = torch.randint(110, (1,)).cuda()
gender = torch.randint(2, (1,)).cuda()
race = torch.randint(5, (1,)).cuda()
optimizer.zero_grad()
output = model_ft(image)
age_loss, gender_loss, race_loss = criterion(output, age, gender, race)
loss = age_loss + gender_loss + race_loss
loss.backward()
optimizer.step()
</code></pre>
<p>Here is what I define my loss function</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F


class MultiLoss(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, output, age, gender, race):
        age_pred = output[:, :13]
        age_pred = torch.sum(age_pred, 1)
        gender_pred = output[:, 13: 15]
        race_pred = output[:, 15:]
        age_loss = F.smooth_l1_loss(age_pred.view(-1, 1), age.cuda())
        gender_loss = F.cross_entropy(gender_pred, torch.flatten(gender).cuda(), reduction='sum')
        race_loss = F.cross_entropy(race_pred, torch.flatten(race).cuda(), reduction='sum')
        return age_loss, gender_loss, race_loss
</code></pre>
",10120782.0,,10120782.0,,2020-07-04 09:22:53,2022-02-17 20:09:17,Pytorch: RuntimeError: expected dtype Float but got dtype Long,<python><deep-learning><pytorch><backpropagation>,2,0,0.0,,,CC BY-SA 4.0
63383347,1,63383749.0,,2020-08-12 19:14:37,,8,29541,"<p>I'm running into an issue while calculating the loss for my Neural Net. I'm not sure why the program expects a long object because all my Tensors are in float form. I looked at threads with similar errors and the solution was to cast Tensors as floats instead of longs, but that wouldn't work in my case because all my data is already in float form when passed to the network.</p>
<p>Here's my code:</p>
<pre><code># Dataloader
from torch.utils.data import Dataset, DataLoader

class LoadInfo(Dataset):    
    def __init__(self, prediction, indicator):  
        self.prediction = prediction
        self.indicator = indicator
    def __len__(self):
        return len(self.prediction)
    def __getitem__(self, idx):
        data = torch.tensor(self.indicator.iloc[idx, :],dtype=torch.float)
        data = torch.unsqueeze(data, 0)
        label = torch.tensor(self.prediction.iloc[idx, :],dtype=torch.float)
        sample = {'data': data, 'label': label} 
        return sample

# Trainloader
test_train = LoadInfo(train_label, train_indicators)
trainloader = DataLoader(test_train, batch_size=64,shuffle=True, num_workers=1,pin_memory=True) 

# The Network
class NetDense2(nn.Module):

    def __init__(self):
        super(NetDense2, self).__init__()
        self.rnn1 = nn.RNN(11, 100, 3)  
        self.rnn2 = nn.RNN(100, 500, 3)  
        self.fc1 = nn.Linear(500, 100)  
        self.fc2 = nn.Linear(100, 20)
        self.fc3 = nn.Linear(20, 3)

    def forward(self, x):
        x1, h1 = self.rnn1(x)
        x2, h2 = self.rnn2(x1)
        x = F.relu(self.fc1(x2))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Allocate / Transfer to GPU      
dense2 = NetDense2()
dense2.cuda()

# Optimizer
import torch.optim as optim
criterion = nn.CrossEntropyLoss()                                 # specify the loss function
optimizer = optim.SGD(dense2.parameters(), lr=0.001, momentum=0.9,weight_decay=0.001)

# Training
dense2.train()
loss_memory = []
for epoch in range(50):  # loop over the dataset multiple times
    running_loss = 0.0
    for i, samp in enumerate(trainloader):
        # get the inputs
        ins = samp['data']
        targets = samp['label']
        tmp = []
        tmp = torch.squeeze(targets.float())
        ins, targets = ins.cuda(),  tmp.cuda()
        # zero the parameter gradients
        optimizer.zero_grad()
        # forward + backward + optimize
        outputs = dense2(ins)
        loss = criterion(outputs, targets)     # The loss
        loss.backward()
        optimizer.step()
        # keep track of loss
        running_loss += loss.data.item()

</code></pre>
<p>I get the error from above in the line &quot; loss = criterion(outputs, targets) &quot;</p>
",14055259.0,,13073020.0,,2020-08-13 07:26:29,2022-04-07 18:55:16,RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 'target',<python><floating-point><pytorch><recurrent-neural-network><torch>,3,2,0.0,,,CC BY-SA 4.0
68153551,1,,,2021-06-27 17:07:28,,8,2005,"<p>I am training a GCN (Graph Convolutional Network) on Cora dataset.</p>
<p>The Cora dataset has the following attributes:</p>
<pre><code>Number of graphs: 1
Number of features: 1433
Number of classes: 7
Number of nodes: 2708
Number of edges: 10556
Number of training nodes: 140
Training node label rate: 0.05
Is undirected: True

Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])
</code></pre>
<p>Since my code is very long, I only put the relevent parts of my code here. Firstly, I split the Cora dataset as follows:</p>
<pre><code>def to_mask(index, size):
    mask = torch.zeros(size, dtype=torch.bool)
    mask[index] = 1
    return mask

def cora_splits(data, num_classes):
    indices = []

    for i in range(num_classes):
        # returns all indices of the elements = i from data.y tensor
        index = (data.y == i).nonzero().view(-1)

        # returns a random permutation of integers from 0 to index.size(0).
        index = index[torch.randperm(index.size(0))]

        # indices is a list of tensors and it has a length of 7
        indices.append(index)

    # select 20 nodes from each class for training
    train_index = torch.cat([i[:20] for i in indices], dim=0)

    rest_index = torch.cat([i[20:] for i in indices], dim=0)
    rest_index = rest_index[torch.randperm(len(rest_index))]

    data.train_mask = to_mask(train_index, size=data.num_nodes)
    data.val_mask = to_mask(rest_index[:500], size=data.num_nodes)
    data.test_mask = to_mask(rest_index[500:], size=data.num_nodes)

    return data
</code></pre>
<p>The <code>train</code> is as follows (taken from <a href=""https://github.com/tkipf/pygcn/blob/master/pygcn/train.py"" rel=""nofollow noreferrer"">here</a> with few modifications):</p>
<pre><code>
def train(model, optimizer, data, epoch):
    t = time.time()
    model.train()
    optimizer.zero_grad()
    output = model(data)
    loss_train = F.nll_loss(output[data.train_mask], data.y[data.train_mask])
    acc_train = accuracy(output[data.train_mask], data.y[data.train_mask])
    loss_train.backward()
    optimizer.step()

    loss_val = F.nll_loss(output[data.val_mask], data.y[data.val_mask])
    acc_val = accuracy(output[data.val_mask], data.y[data.val_mask])

def accuracy(output, labels):
    preds = output.max(1)[1].type_as(labels)
    correct = preds.eq(labels).double()
    correct = correct.sum()
    return correct / len(labels)
</code></pre>
<p>When I ran my code with 200 epochs in 10 runs I gained:</p>
<pre><code>tensor([0.7690, 0.8030, 0.8530, 0.8760, 0.8600, 0.8550, 0.8850, 0.8580, 0.8940, 0.8830])

Val Loss: 0.5974, Test Accuracy: 0.854 ± 0.039
</code></pre>
<p>where each value in the tensor belongs to the model accurracy of each run and the mean accuracy of all those 10 runs is 0.854 with std ± 0.039.</p>
<p>As it can be observed, the accuracy from the first run to the 10th one is increasing substantially. Therefore, I think the model is overfitting. One reason of overfitting is that in the code, the test data has been seen by the model in the training time since in the <code>train</code> function, there is a line <code>output = model(data)</code> so the model is trained over the whole data. What I intend to do is to train my model only on a part of the data (something similar to <code>data[data.train_mask]</code>) but the problem is I cannot pass <code>data[data.train_mask]</code>, due to the <code>forward</code> function of the <code>GCN</code> model (from <a href=""https://github.com/rusty1s/pytorch_geometric/blob/master/benchmark/kernel/gcn.py"" rel=""nofollow noreferrer"">this repository</a>):</p>
<pre><code>def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.relu(self.conv1(x, edge_index))
        for conv in self.convs:
            x = F.relu(conv(x, edge_index))
        x = F.relu(self.lin1(x))
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.lin2(x)
        return F.log_softmax(x, dim=-1)
</code></pre>
<p>If I pass <code>data[data.train_mask]</code> to the GCN model, then in the above <code>forward</code> function in line <code>x, edge_index = data.x, data.edge_index</code>, <code>x</code> and <code>edge_index</code> cannot be retrieved from <code>data[data.train_mask]</code>. Therefore, I need to find a way to split the Cora dataset in a way that I can pass a specefic part of it with the nodes, edge-index and other attributes to the model. My question is how to do it?</p>
<p>Also, any suggestion about k-fold cross validation is much appreciated.</p>
",12469912.0,,12469912.0,,2021-06-28 19:39:18,2021-10-17 21:30:03,How to split the Cora dataset to train a GCN model only on training part?,<python><python-3.x><validation><neural-network><pytorch>,1,3,0.0,,,CC BY-SA 4.0
74229178,1,74409869.0,,2022-10-27 23:13:21,,8,15139,"<p>I am trying to implement SAC with a custom environment in Stable Baselines3 and I keep getting the error in the title. The error occurs with any off policy algorithm not just SAC.</p>
<p>Traceback:</p>
<pre><code>File &quot;&lt;MY PROJECT PATH&gt;\src\main.py&quot;, line 70, in &lt;module&gt;
  main()
File &quot;&lt;MY PROJECT PATH&gt;\src\main.py&quot;, line 66, in main
  model.learn(total_timesteps=timesteps, reset_num_timesteps=False, tb_log_name=f&quot;sac_{num_cars}_cars&quot;)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\stable_baselines3\sac\sac.py&quot;, line 309, in learn
  return super().learn(
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\stable_baselines3\common\off_policy_algorithm.py&quot;, line 375, in learn
  self.train(batch_size=self.batch_size, gradient_steps=gradient_steps)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\stable_baselines3\sac\sac.py&quot;, line 256, in train
  current_q_values = self.critic(replay_data.observations, replay_data.actions)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1190, in _call_impl
  return forward_call(*input, **kwargs)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\stable_baselines3\common\policies.py&quot;, line 885, in forward
  return tuple(q_net(qvalue_input) for q_net in self.q_networks)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\stable_baselines3\common\policies.py&quot;, line 885, in &lt;genexpr&gt;
  return tuple(q_net(qvalue_input) for q_net in self.q_networks)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1190, in _call_impl
  return forward_call(*input, **kwargs)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\torch\nn\modules\container.py&quot;, line 204, in forward
  input = module(input)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\torch\nn\modules\module.py&quot;, line 1190, in _call_impl
  return forward_call(*input, **kwargs)
File &quot;&lt;MY PROJECT PATH&gt;\venv\lib\site-packages\torch\nn\modules\linear.py&quot;, line 114, in forward
  return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 must have the same dtype
</code></pre>
<p>Action and observation spaces:</p>
<pre><code>self.action_space = Box(low=-1., high=1., shape=(2,), dtype=np.float)
self.observation_space = Box(
    np.array(
        [-np.inf] * (9 * 40) + [-np.inf] * 3 + [-np.inf] * 3 + [-np.inf] * 3
        + [0.] + [0.] + [0.] + [-1.] + [0.] * 4 + [0.] * 4 + [0.] * 4,
        dtype=np.float
    ),
    np.array(
        [np.inf] * (9 * 40) + [np.inf] * 3 + [np.inf] * 3 + [np.inf] * 3
        + [np.inf] + [1.] + [1.] + [1.] + [1.] * 4 + [np.inf] * 4 + [np.inf] * 4,
        dtype=np.float
    ),
    dtype=np.float
)
</code></pre>
<p>Observations are returned in the step and reset methods as a numpy array of floats.</p>
<p>Is there something I'm missing which is causing this error? If I use one of the environments that come with gym such as pendulum it works fine which is why I think I have a problem with my custom environment.</p>
<p>Thanks in advance for any help and please let me know if more information is required.</p>
",9241425.0,,,,,2023-06-20 21:08:06,Stable Baselines3 RuntimeError: mat1 and mat2 must have the same dtype,<python><pytorch><openai-gym><stable-baselines>,1,1,,,,CC BY-SA 4.0
64523498,1,64616186.0,,2020-10-25 12:08:23,,8,7500,"<p>I've noticed that <code>torch.device</code> can accept a range of arguments, precisely <code>cpu</code>, <code>cuda</code>, <code>mkldnn</code>, <code>opengl</code>, <code>opencl</code>, <code>ideep</code>, <code>hip</code>, <code>msnpu</code>.</p>
<p>However, when training deep learning models, I've only ever seen <code>cuda</code> or <code>cpu</code> being used. Very often the code looks something like this</p>
<pre><code>if torch.cuda.is_available():
    device = torch.device(&quot;cuda&quot;)
else:
    device = torch.device(&quot;cpu&quot;)
</code></pre>
<p>I've never seen any of the others being used, and was wondering if they can be used and how. The latest MacBooks with an AMD graphic card I believe should be able to use <code>&quot;hip&quot;</code>, but is that true? And will the training speed be similar to that of using one CUDA GPU? If not, what is the point in <code>torch.device</code> accepting so many options if they cannot actually be used?</p>
",13066590.0,,6210807.0,,2020-10-26 04:41:41,2021-10-28 19:16:00,"Can you accelerate torch DL training on anything other than ""cuda"" like ""hip"" or ""OpenCL""?",<deep-learning><pytorch><gpu><opencl>,1,0,0.0,,,CC BY-SA 4.0
65807601,1,68086349.0,,2021-01-20 10:24:19,,8,20723,"<p>This is potentially a very easy question. I just started with PyTorch lightning and can't figure out how to receive the output of my model after training.</p>
<p>I am interested in both predictions of y_train and y_test as an array of some sort (PyTorch tensor or NumPy array in a later step) to plot next to the labels using different scripts.</p>
<pre><code>dataset = Dataset(train_tensor)
val_dataset = Dataset(val_tensor)
training_generator = torch.utils.data.DataLoader(dataset, **train_params)
val_generator = torch.utils.data.DataLoader(val_dataset, **val_params)
mynet = Net(feature_len)
trainer = pl.Trainer(gpus=0,max_epochs=max_epochs, logger=logger, progress_bar_refresh_rate=20, callbacks=[early_stop_callback], num_sanity_val_steps=0)
trainer.fit(mynet)
</code></pre>
<p>In my lightning module I have the functions:</p>
<pre><code>def __init__(self, random_inputs):

def forward(self, x):

def train_dataloader(self):
    
def val_dataloader(self):

def training_step(self, batch, batch_nb):

def training_epoch_end(self, outputs):

def validation_step(self, batch, batch_nb):

def validation_epoch_end(self, outputs):

def configure_optimizers(self):
</code></pre>
<p>Do I need a specific predict function or is there any already implemented way I don't see?</p>
",13320076.0,,13320076.0,,2021-01-20 10:28:01,2021-12-03 13:02:45,output prediction of pytorch lightning model,<python><pytorch><pytorch-lightning>,4,0,0.0,,,CC BY-SA 4.0
65181294,1,,,2020-12-07 11:54:31,,8,1254,"<h2>Update</h2>
<p>This is now officially supported by <a href=""https://github.com/keras-team/keras-cv"" rel=""nofollow noreferrer"">keras-cv</a>.</p>
<hr />
<p>To create a class label in <code>CutMix</code> or <code>MixUp</code> type augmentation, we can use <code>beta</code> such as <code>np.random.beta</code> or <code>scipy.stats.beta</code> and do as follows for two labels:</p>
<pre><code>label = label_one*beta + (1-beta)*label_two
</code></pre>
<p>But what if we've <strong>more than two</strong> images? In <a href=""https://arxiv.org/abs/2004.10934"" rel=""nofollow noreferrer"">YoLo4</a>, they've tried an interesting augmentation called <strong>Mosaic Augmentation</strong> for object detection problems. Unlike <code>CutMix</code> or <code>MixUp</code>, this augmentation creates augmented samples with <strong>4</strong> images. In object detection cases, we can compute the shift of each instance co-ords and thus possible to get the proper ground truth, <a href=""https://stackoverflow.com/questions/64335735/how-can-i-get-class-label-from-mosaic-augmentation-in-object-detection-dataloade"">here</a>. But for only image classification cases, how can we do that?</p>
<p>Here is a <strong>starter</strong>.</p>
<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt 
import random

(train_images, train_labels), (test_images, test_labels) = \
tf.keras.datasets.cifar10.load_data()
train_images = train_images[:10,:,:]
train_labels = train_labels[:10]
train_images.shape, train_labels.shape

((10, 32, 32, 3), (10, 1))
</code></pre>
<p>Here is a function we've written for this augmentation; ( too ugly with an `inner-outer loop! Please suggest if we can do it efficiently.)</p>
<pre class=""lang-py prettyprint-override""><code>def mosaicmix(image, label, DIM, minfrac=0.25, maxfrac=0.75):
    '''image, label: batches of samples 
    '''
    xc, yc  = np.random.randint(DIM * minfrac, DIM * maxfrac, (2,))
    indices = np.random.permutation(int(image.shape[0]))
    mosaic_image = np.zeros((DIM, DIM, 3), dtype=np.float32)
    final_imgs, final_lbs = [], []

    # Iterate over the full indices 
    for j in range(len(indices)): 
        # Take 4 sample for to create a mosaic sample randomly 
        rand4indices = [j] + random.sample(list(indices), 3) 
        
        # Make mosaic with 4 samples 
        for i in range(len(rand4indices)):
            if i == 0:    # top left
                x1a, y1a, x2a, y2a =  0,  0, xc, yc
                x1b, y1b, x2b, y2b = DIM - xc, DIM - yc, DIM, DIM # from bottom right        
            elif i == 1:  # top right
                x1a, y1a, x2a, y2a = xc, 0, DIM , yc
                x1b, y1b, x2b, y2b = 0, DIM - yc, DIM - xc, DIM # from bottom left
            elif i == 2:  # bottom left
                x1a, y1a, x2a, y2a = 0, yc, xc, DIM
                x1b, y1b, x2b, y2b = DIM - xc, 0, DIM, DIM-yc   # from top right
            elif i == 3:  # bottom right
                x1a, y1a, x2a, y2a = xc, yc,  DIM, DIM
                x1b, y1b, x2b, y2b = 0, 0, DIM-xc, DIM-yc    # from top left
                
            # Copy-Paste
            mosaic_image[y1a:y2a, x1a:x2a] = image[i,][y1b:y2b, x1b:x2b]

        # Append the Mosiac samples
        final_imgs.append(mosaic_image)
        
    return final_imgs, label
</code></pre>
<p>The augmented samples, currently with the wrong labels.</p>
<pre><code>data, label = mosaicmix(train_images, train_labels, 32)
plt.imshow(data[5]/255)
</code></pre>
<p><a href=""https://i.stack.imgur.com/JYouf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/JYouf.png"" alt=""enter image description here"" /></a></p>
<hr />
<p>However, here are some more examples to motivate you. Data is from the <a href=""https://www.kaggle.com/c/cassava-leaf-disease-classification/overview"" rel=""nofollow noreferrer"">Cassava Leaf</a> competition.</p>
<p><a href=""https://i.stack.imgur.com/f3lSC.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/f3lSC.png"" alt="""" /></a><br />
<sub>(source: <a href=""https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1984321%2Fb0530886377eb3d7f0c29401ba069ffa%2Fdownload%20(1).png?generation=1607625768667914&amp;alt=media"" rel=""nofollow noreferrer"">googleapis.com</a>)</sub></p>
<p><a href=""https://i.stack.imgur.com/bGJes.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/bGJes.png"" alt="""" /></a><br />
<sub>(source: <a href=""https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F1984321%2Fdfce6cf87b2125c98affa7574ed38b18%2Fdownload%20(3).png?generation=1607625855803714&amp;alt=media"" rel=""nofollow noreferrer"">googleapis.com</a>)</sub></p>
",9215780.0,,9215780.0,,2023-05-06 21:15:56,2023-05-06 21:15:56,How to Create Class Label for Mosaic Augmentation in Image Classification?,<python><tensorflow><keras><pytorch><data-augmentation>,2,0,0.0,,,CC BY-SA 4.0
68321832,1,,,2021-07-09 19:22:38,,8,275,"<p>While going through PyTorch documentation, I came across the term <code>hermetic packages</code>:</p>
<blockquote>
<p>torch.package adds support for creating <strong>hermetic packages</strong> containing arbitrary PyTorch code. These packages can be saved, shared, used to load and execute models at a later date or on a different machine, and can even be deployed to production using torch::deploy.</p>
</blockquote>
<p>I don't understand what hermetic packages mean in this context.</p>
<ul>
<li>Can someone explain what makes packages hermetic?</li>
<li>What would non-hermetic packages look like?</li>
</ul>
<p>With some search over Stack Overflow [1][2], it seems this terminology is a generic term used in software world. Any examples - even outside of PyTorch/Python world -  would help in solidifing my understanding.</p>
<p>Thank you!</p>
<hr />
<p>[1] <a href=""https://stackoverflow.com/questions/12966202/creating-hermetic-maven-builds"">Creating Hermetic Maven Builds</a>
[2] <a href=""https://stackoverflow.com/questions/43327657/bazel-hermetic-use-of-jar-command"">Bazel: hermetic use of jar command?</a></p>
",3243104.0,,3243104.0,,2021-07-09 19:36:57,2021-07-09 23:01:05,Hermetic / Non Hermetic Packages in Python,<python><pytorch><python-packaging>,1,0,,,,CC BY-SA 4.0
65132527,1,,,2020-12-03 18:50:45,,8,651,"<p>I have some questions about using the <code>torch.multiprocessing</code> module. Let’s say I have a <code>torch.nn.Module</code> called <code>model</code> and I call <code>model.share_memory()</code> on it.</p>
<p>What happens if two threads call the <code>forward()</code>, i.e. <code>model(input)</code> at the same time? Is it safe? Or should I use Lock mechanisms to be sure that <code>model</code> is not accessed at the same time by multiple threads?
Similarly, what happens if two or more threads have an optimizer working on <code>model.parameters()</code> and they call <code>optimizer.step()</code> at the same time?</p>
<p>I ask these questions because I often see the <code>optimizer.step()</code> being called on shared models without lock mechanisms (i.e. in RL implementations of A3C or ACER) and I wonder if it is a safe thing to do.</p>
",8737016.0,Federico Taschin,8737016.0,,2020-12-04 11:13:26,2022-04-04 02:21:51,PyTorch mutiprocessing: Do I need to use Lock() when accessing a shared model?,<pytorch><python-multiprocessing><reinforcement-learning>,1,3,0.0,,,CC BY-SA 4.0
66542007,1,,,2021-03-09 06:42:56,,8,4346,"<p><strong>2021-03-09</strong></p>
<p>I trained my transformer models in pytrorch. In the first few batches, the loss calculation and gradient updates were all performing well. However, the output of the model turned out to be nan values after several iterations. I am confident that there are no flawed data in the dataset. Besides, it's not a classification problem, the labels are float numbers.</p>
<p><a href=""https://i.stack.imgur.com/ggqLa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ggqLa.png"" alt=""loss changing process"" /></a></p>
<p><strong>2021-03-10</strong></p>
<p><em>Follow-up:</em>
What an interesting story! When I ran this transformer model with a larger architecture (like 6 encoder layers, 8 heads, etc.). The NAN values disappeared. It seems that the gradient explosion only existed in tiny models.</p>
<p><em>Solutions:</em>
I searched the Pytorch forum and Stackoverflow and found out the accurate reason for this NAN instance. First, since the NAN loss didn't appear at the very beginning. We can conclude that the model might be well defined. The cause might be the data or the training process. I ran <code>torch.autograd.set_detect_anomaly(True)</code> as told in <a href=""https://discuss.pytorch.org/t/gradient-value-is-nan/91663/2"" rel=""noreferrer"">https://discuss.pytorch.org/t/gradient-value-is-nan/91663/2</a>. It returned that the <code>RuntimeError: Function ‘StdBackward1’ returned nan values in its 0th output</code>.</p>
<p>According to the similar question in <a href=""https://discuss.pytorch.org/t/gradient-of-standard-deviation-is-nan/14713"" rel=""noreferrer"">https://discuss.pytorch.org/t/gradient-of-standard-deviation-is-nan/14713</a>, I double-checked the output in each layer inside the transformer. Strangely, after dozens of iterations, the positional embedding layer outputted a vector full of zeros. As a result, the LayerNorm that does the normalization job cannot backward the loss well, since it calculated the standard deviations and the standard deviation has no gradient at zero (or you can say it's infinite)! The possible solution is to add <code>x.std(unbiased=False)</code> if you are using pytorch.</p>
<p>This's my encounter with the NAN loss and mse. I hope my experience can give some insights to you when you meet this circumstance!</p>
<p>Relative Questions: <a href=""https://stackoverflow.com/questions/40050397/deep-learning-nan-loss-reasons"">Deep-Learning Nan loss reasons</a></p>
",12304367.0,,12304367.0,,2021-03-10 04:54:11,2023-05-08 17:03:13,Transformer Model Output Nan Values in Pytorch,<deep-learning><pytorch><gradient><nan>,1,2,0.0,,,CC BY-SA 4.0
73924697,1,,,2022-10-02 09:21:17,,8,9424,"<p>After reading the pytorch documentation, I still require help in understanding the difference between <code>torch.mm</code>, <code>torch.matmul</code> and <code>torch.mul</code>. As I do not fully understand them, I cannot concisely explain this.</p>
<pre><code>B = torch.tensor([[ 1.1207],
        [-0.3137],
        [ 0.0700],
        [ 0.8378]])

C = torch.tensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])

print(torch.mul(B,C))

print(torch.matmul(B,C))

print(torch.mm(B,C))
</code></pre>
<p>All three produce the following output (i.e. they perform matrix multiplication):</p>
<pre><code>tensor([[ 0.5767,  0.1363, -0.5877,  2.5084],
        [-0.1614, -0.0381,  0.1645, -0.7021],
        [ 0.0360,  0.0085, -0.0367,  0.1567],
        [ 0.4311,  0.1019, -0.4393,  1.8752]])
</code></pre>
<pre><code>A = torch.tensor([[1.8351,2.1536], [-0.8320,-1.4578]])
B = torch.tensor([[2.9355, 0.3450], [0.5708, 1.9957]])
print(torch.mul(A,B))
print(torch.matmul(A,B))
print(torch.mm(A,B))
</code></pre>
<p><strong>Different</strong> outputs are produced. torch.mm no longer performs matrix multiplication (broadcasts and performs element-wise multiplication instead, whilst the other two still perform matrix multiplication.</p>
<pre><code>tensor([[ 5.3869,  0.7430],
        [-0.4749, -2.9093]])
tensor([[ 6.6162,  4.9310],
        [-3.2744, -3.1964]])
tensor([[ 6.6162,  4.9310],
        [-3.2744, -3.1964]])
</code></pre>
<p>Inputs</p>
<pre><code>tensor1 = torch.randn(10, 3, 4)
tensor2 = torch.randn(4)

</code></pre>
<pre><code>tensor1 = 
tensor([[[-0.2267,  0.6311, -0.5689,  1.2712],
         [-0.0241, -0.5362,  0.5481, -0.4534],
         [-0.9773, -0.6842,  0.6927,  0.3363]],

        [[-2.6759,  0.7817,  2.6821,  0.7037],
         [ 0.1804,  0.3938, -1.2235,  0.8729],
         [-1.9873, -0.5030,  0.0945,  0.2688]],

        [[ 0.4244,  1.7350,  0.0558, -0.1861],
         [-0.9063, -0.4737, -0.4284, -0.3883],
         [ 0.4827, -0.2628,  1.0084,  0.2769]],

        [[ 0.2939,  0.4604,  0.8014, -1.8760],
         [ 1.8807,  0.1623,  0.2344, -0.6221],
         [ 1.3964,  3.1637,  0.7889,  0.1195]],

        [[-0.7202,  1.4250,  2.4302,  1.4811],
         [-0.2301,  0.6280,  0.5379,  0.5178],
         [-2.1073, -1.4399, -0.9451,  0.8534]],

        [[ 2.8178, -0.4451, -0.7871, -0.5198],
         [ 0.2825,  1.0692,  0.1559,  1.2945],
         [-0.5828, -1.6287, -2.0661, -0.4107]],

        [[ 0.5077, -0.6349, -0.0160, -0.4477],
         [-0.8070,  0.3746,  1.1852,  0.0351],
         [-0.6454,  1.5877,  0.8561,  1.1021]],

        [[ 0.1191,  1.0116,  0.5807,  1.2105],
         [-0.5403,  1.2404,  1.1532,  0.6537],
         [ 1.4757, -1.3648, -1.7158, -1.0289]],

        [[-0.1326,  0.3715,  0.2429, -0.0794],
         [ 0.3224, -0.3064,  0.1963,  0.7276],
         [ 0.9098,  1.5984, -1.4953,  0.0420]],

        [[ 0.1511,  0.9691, -0.5204,  0.3858],
         [ 0.4566,  1.5482, -0.3401,  0.5960],
         [-0.9998,  0.7198,  0.9286,  0.4498]]])

tensor2 =
tensor([-1.6350,  1.0335, -0.9023,  0.0696])
</code></pre>
<pre><code>print(torch.mul(tensor1,tensor2))
print(torch.matmul(tensor1,tensor2))
print(torch.mm(tensor1,tensor2))
</code></pre>
<p>Outputs are <strong>all different</strong>. I think <code>torch.mul</code> broadcasts and multiplies every 4 elements of the matrix by the vector, tensor2, i.e. <code>[-0.2267,  0.6311, -0.5689,  1.2712] x tensor 2 </code> element-wise, <code>[-0.0241, -0.5362,  0.5481, -0.4534] x tensor 2</code> element-wise and so on. I do not understand what <code> torch.matmul</code> is doing. I think it is to do with the 5th bullet-point of the documentation (If both arugments...), but I am unable to make sense of this. <a href=""https://pytorch.org/docs/stable/generated/torch.matmul.html"" rel=""noreferrer"">https://pytorch.org/docs/stable/generated/torch.matmul.html</a></p>
<p>I think the reason <code>torch.mm</code> is unable to produce an output is the fact that it cannot broadcast (please correct me if I'm wrong).</p>
<pre><code>tensor([[[ 3.7071e-01,  6.5221e-01,  5.1335e-01,  8.8437e-02],
         [ 3.9400e-02, -5.5417e-01, -4.9460e-01, -3.1539e-02],
         [ 1.5979e+00, -7.0715e-01, -6.2499e-01,  2.3398e-02]],

        [[ 4.3752e+00,  8.0790e-01, -2.4201e+00,  4.8957e-02],
         [-2.9503e-01,  4.0699e-01,  1.1040e+00,  6.0723e-02],
         [ 3.2494e+00, -5.1981e-01, -8.5253e-02,  1.8701e-02]],

        [[-6.9397e-01,  1.7931e+00, -5.0379e-02, -1.2945e-02],
         [ 1.4818e+00, -4.8954e-01,  3.8657e-01, -2.7010e-02],
         [-7.8920e-01, -2.7163e-01, -9.0992e-01,  1.9265e-02]],

        [[-4.8055e-01,  4.7582e-01, -7.2309e-01, -1.3051e-01],
         [-3.0750e+00,  1.6770e-01, -2.1146e-01, -4.3281e-02],
         [-2.2832e+00,  3.2697e+00, -7.1183e-01,  8.3139e-03]],

        [[ 1.1775e+00,  1.4727e+00, -2.1928e+00,  1.0304e-01],
         [ 3.7617e-01,  6.4900e-01, -4.8534e-01,  3.6025e-02],
         [ 3.4455e+00, -1.4882e+00,  8.5277e-01,  5.9369e-02]],

        [[-4.6072e+00, -4.6005e-01,  7.1024e-01, -3.6160e-02],
         [-4.6191e-01,  1.1051e+00, -1.4067e-01,  9.0053e-02],
         [ 9.5283e-01, -1.6833e+00,  1.8643e+00, -2.8571e-02]],

        [[-8.3005e-01, -6.5622e-01,  1.4461e-02, -3.1148e-02],
         [ 1.3195e+00,  3.8716e-01, -1.0694e+00,  2.4421e-03],
         [ 1.0553e+00,  1.6409e+00, -7.7250e-01,  7.6669e-02]],

        [[-1.9477e-01,  1.0455e+00, -5.2398e-01,  8.4209e-02],
         [ 8.8343e-01,  1.2820e+00, -1.0405e+00,  4.5478e-02],
         [-2.4128e+00, -1.4106e+00,  1.5482e+00, -7.1578e-02]],

        [[ 2.1675e-01,  3.8391e-01, -2.1914e-01, -5.5219e-03],
         [-5.2707e-01, -3.1668e-01, -1.7711e-01,  5.0619e-02],
         [-1.4876e+00,  1.6520e+00,  1.3493e+00,  2.9198e-03]],

        [[-2.4706e-01,  1.0015e+00,  4.6955e-01,  2.6842e-02],
         [-7.4663e-01,  1.6001e+00,  3.0685e-01,  4.1462e-02],
         [ 1.6347e+00,  7.4395e-01, -8.3792e-01,  3.1291e-02]]])
tensor([[ 1.6247, -1.0409,  0.2891],
        [ 2.8120,  1.2767,  2.6630],
        [ 1.0358,  1.3518, -1.9515],
        [-0.8583, -3.1620,  0.2830],
        [ 0.5605,  0.5759,  2.8694],
        [-4.3932,  0.5925,  1.1053],
        [-1.5030,  0.6397,  2.0004],
        [ 0.4109,  1.1704, -2.3467],
        [ 0.3760, -0.9702,  1.5165],
        [ 1.2509,  1.2018,  1.5720]])
</code></pre>
",13985175.0,,,,,2022-10-03 21:43:47,"What's the difference between torch.mm, torch.matmul and torch.mul?",<python-3.x><pytorch>,1,0,,,,CC BY-SA 4.0
71166789,1,71232059.0,,2022-02-17 23:45:30,,8,10784,"<p>I am trying to fine-tune the BERT language model on my own data. I've gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here's my code:</p>
<pre><code>from datasets import load_dataset
from transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer
import glob
import os


base_path = '../data/'
model_name = 'bert-base-uncased'
max_length = 512
checkpoints_dir = 'checkpoints'

tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)


def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)


dataset = load_dataset('text',
        data_files={
            'train': f'{base_path}train.txt',
            'test': f'{base_path}test.txt',
            'validation': f'{base_path}valid.txt'
        }
)

print('Tokenizing data. This may take a while...')
tokenized_dataset = dataset.map(tokenize_function, batched=True)
train_dataset = tokenized_dataset['train']
eval_dataset = tokenized_dataset['test']

model = AutoModel.from_pretrained(model_name)

training_args = TrainingArguments(checkpoints_dir)

print('Training the model...')
trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
trainer.train()
</code></pre>
<p>I get the following error:</p>
<pre><code>  File &quot;train_lm_hf.py&quot;, line 44, in &lt;module&gt;
    trainer.train()
...
  File &quot;/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py&quot;, line 130, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 165 at dim 1 (got 128)
</code></pre>
<p>What am I doing wrong?</p>
",2713263.0,,,,,2022-02-23 06:08:18,HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128),<python><deep-learning><pytorch><huggingface-transformers>,1,3,,,,CC BY-SA 4.0
64024312,1,64028123.0,,2020-09-23 08:52:09,,8,2716,"<p>Suppose vector <code>\theta</code> is all the parameters in a neural network, I wonder how to compute hessian matrix for <code>\theta</code> in pytorch.</p>
<p>Suppose the network is as follows:</p>
<pre><code>class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x
</code></pre>
<p>I know the second derivative can be calculated by calling <code>torch.autograd.grad()</code> twice, but the parameters in pytorch is organized by <code>net.parameters()</code>, and I don't know how to compute the hessian for all parameters.</p>
<p>I have tried to use <code>torch.autograd.functional.hessian()</code> in pytorch 1.5 as follows:</p>
<pre><code>import torch
import numpy as np
from torch.nn import Module
import torch.nn.functional as F


class Net(Module):
    def __init__(self, h, w):
        super(Net, self).__init__()
        self.c1 = torch.nn.Conv2d(1, 32, 3, 1, 1)
        self.f2 = torch.nn.Linear(32 * h * w, 5)

    def forward(self, x):
        x = self.c1(x)
        x = x.view(x.size(0), -1)
        x = self.f2(x)
        return x


def func_(a, b c, d):
    p = [a, b, c, d]
    x = torch.randn(size=[8, 1, 12, 12], dtype=torch.float32)
    y = torch.randint(0, 5, [8])
    x = F.conv2d(x, p[0], p[1], 1, 1)
    x = x.view(x.size(0), -1)
    x = F.linear(x, p[2], p[3])
    loss = F.cross_entropy(x, y)
    return loss


if __name__ == '__main__':
    net = Net(12, 12)

    h = torch.autograd.functional.hessian(func_, tuple([_ for _ in net.parameters()]))
    print(type(h), len(h))

</code></pre>
<p><code>h</code> is a tuple, and the results are in strange shape. For example, the shape of <code>\frac{\delta Loss^2}{\delta c1.weight^2}</code> is <code>[32,1,3,3,32,1,3,3]</code>. It seems like I can combine them into a complete <code>H</code>, but I don't know which part it is in the whole Hessian Matrix and the corresponding order.</p>
",6187009.0,,6187009.0,,2020-09-23 11:38:53,2020-09-23 12:44:01,How to compute hessian matrix for all parameters in a network in pytorch?,<machine-learning><pytorch><hessian>,1,3,,,,CC BY-SA 4.0
72416726,1,72416727.0,,2022-05-28 15:01:31,,8,16412,"<p>On 18th May 2022, PyTorch <a href=""https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/"" rel=""noreferrer"">announced support</a> for GPU-accelerated PyTorch training on Mac.</p>
<p>I followed the following process to set up PyTorch on my Macbook Air M1 (using miniconda).</p>
<pre><code>conda create -n torch-nightly python=3.8 

$ conda activate torch-nightly

$ pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu
</code></pre>
<p>I am trying to execute a script from Udacity's Deep Learning Course available <a href=""https://github.com/udacity/deep-learning-v2-pytorch/blob/master/dcgan-svhn/DCGAN_Solution.ipynb"" rel=""noreferrer"">here</a>.</p>
<p>The script moves the models to GPU using the following code:</p>
<pre class=""lang-py prettyprint-override""><code>G.cuda()
D.cuda()
</code></pre>
<p>However, this will not work on M1 chips, since there is no CUDA.</p>
<p>If we want to move models to M1 GPU and our tensors to M1 GPU, and train entirely on M1 GPU, what should we be doing?</p>
<hr />
<p>If Relevant: <code>G</code> and <code>D</code> are Discriminator and Generators for GAN's.</p>
<pre class=""lang-py prettyprint-override""><code>class Discriminator(nn.Module):

    def __init__(self, conv_dim=32):
        super(Discriminator, self).__init__()
        self.conv_dim = conv_dim
        # complete init function
        self.cv1 = conv(in_channels=3, out_channels=conv_dim, kernel_size=4, stride=2, padding=1, batch_norm=False)            # 32*32*3  -&gt; 16*16*32
        self.cv2 = conv(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=4, stride=2, padding=1, batch_norm=True)    # 16*16*32 -&gt; 8*8*64
        self.cv3 = conv(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=4, stride=2, padding=1, batch_norm=True)  # 8*8*64   -&gt; 4*4*128
        self.fc1 = nn.Linear(in_features = 4*4*conv_dim*4, out_features = 1, bias=True)
        

    def forward(self, x):
        # complete forward function
        out = F.leaky_relu(self.cv1(x), 0.2)
        out = F.leaky_relu(self.cv2(x), 0.2)
        out = F.leaky_relu(self.cv3(x), 0.2)
        out = out.view(-1, 4*4*conv_dim*4)
        out = self.fc1(out)
        return out    

D = Discriminator(conv_dim)

class Generator(nn.Module):    
    def __init__(self, z_size, conv_dim=32):
        super(Generator, self).__init__()
        self.conv_dim = conv_dim
        self.z_size = z_size
        # complete init function
        self.fc1 = nn.Linear(in_features = z_size, out_features = 4*4*conv_dim*4)
        self.dc1 = deconv(in_channels = conv_dim*4, out_channels = conv_dim*2, kernel_size=4, stride=2, padding=1, batch_norm=True)
        self.dc2 = deconv(in_channels = conv_dim*2, out_channels = conv_dim, kernel_size=4, stride=2, padding=1, batch_norm=True)
        self.dc3 = deconv(in_channels = conv_dim, out_channels = 3, kernel_size=4, stride=2, padding=1, batch_norm=False)

    def forward(self, x):
        # complete forward function
        x = self.fc1(x)
        x = x.view(-1, conv_dim*4, 4, 4)
        x = F.relu(self.dc1(x))
        x = F.relu(self.dc2(x))
        x = F.tanh(self.dc3(x))
        return x

G = Generator(z_size=z_size, conv_dim=conv_dim)
</code></pre>
",14384792.0,,14384792.0,,2022-05-28 16:00:01,2022-09-23 08:22:48,How to move PyTorch model to GPU on Apple M1 chips?,<pytorch><metal><apple-m1>,2,0,0.0,,,CC BY-SA 4.0
68904476,1,,,2021-08-24 08:49:51,,8,904,"<p>I'm trying to create GAN model.
This is my discriminator.py</p>
<pre><code>import torch.nn as nn
class D(nn.Module):
    feature_maps = 64
    kernel_size = 4
    stride = 2
    padding = 1
    bias = False
    inplace = True

    def __init__(self):
        super(D, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(4, self.feature_maps, self.kernel_size, self.stride, self.padding, bias=self.bias),
            nn.LeakyReLU(0.2, inplace=self.inplace),
            nn.Conv2d(self.feature_maps, self.feature_maps * 2, self.kernel_size, self.stride, self.padding,
                      bias=self.bias),
            nn.BatchNorm2d(self.feature_maps * 2), nn.LeakyReLU(0.2, inplace=self.inplace),
            nn.Conv2d(self.feature_maps * 2, self.feature_maps * (2 * 2), self.kernel_size, self.stride, self.padding,
                      bias=self.bias),
            nn.BatchNorm2d(self.feature_maps * (2 * 2)), nn.LeakyReLU(0.2, inplace=self.inplace),
            nn.Conv2d(self.feature_maps * (2 * 2), self.feature_maps * (2 * 2 * 2), self.kernel_size, self.stride,
                      self.padding, bias=self.bias),
            nn.BatchNorm2d(self.feature_maps * (2 * 2 * 2)), nn.LeakyReLU(0.2, inplace=self.inplace),
            nn.Conv2d(self.feature_maps * (2 * 2 * 2), 1, self.kernel_size, 1, 0, bias=self.bias),
            nn.Sigmoid()
        )

    def forward(self, input):
        output = self.main(input)
        return output.view(-1)
</code></pre>
<p>this is my generator.py</p>
<pre><code>import torch.nn as nn
class G(nn.Module):
    feature_maps = 512
    kernel_size = 4
    stride = 2
    padding = 1
    bias = False

    def __init__(self, input_vector):
        super(G, self).__init__()
        self.main = nn.Sequential(
            nn.ConvTranspose2d(input_vector, self.feature_maps, self.kernel_size, 1, 0, bias=self.bias),
            nn.BatchNorm2d(self.feature_maps), nn.ReLU(True),
            nn.ConvTranspose2d(self.feature_maps, int(self.feature_maps // 2), self.kernel_size, self.stride, self.padding,
                               bias=self.bias),
            nn.BatchNorm2d(int(self.feature_maps // 2)), nn.ReLU(True),
            nn.ConvTranspose2d(int(self.feature_maps // 2), int((self.feature_maps // 2) // 2), self.kernel_size, self.stride,
                               self.padding,
                               bias=self.bias),
            nn.BatchNorm2d(int((self.feature_maps // 2) // 2)), nn.ReLU(True),
            nn.ConvTranspose2d((int((self.feature_maps // 2) // 2)), int(((self.feature_maps // 2) // 2) // 2), self.kernel_size,
                               self.stride, self.padding,
                               bias=self.bias),
            nn.BatchNorm2d(int((self.feature_maps // 2) // 2) // 2), nn.ReLU(True),
            nn.ConvTranspose2d(int(((self.feature_maps // 2) // 2) // 2), 4, self.kernel_size, self.stride, self.padding,
                               bias=self.bias),
            nn.Tanh()
        )

    def forward(self, input):
        output = self.main(input)
        return output
</code></pre>
<p>This is my gans.py</p>
<pre><code># Importing the libraries
from __future__ import print_function
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
import torchvision.datasets as dset
import torchvision.transforms as transforms
import torchvision.utils as vutils
from torch.autograd import Variable
from generator import G
from discriminator import D
import os
from PIL import Image

batchSize = 64  # We set the size of the batch.
imageSize = 64  # We set the size of the generated images (64x64).
input_vector = 100
nb_epochs = 500
# Creating the transformations
transform = transforms.Compose([transforms.Resize((imageSize, imageSize)), transforms.ToTensor(),
                                transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5,
                                                                       0.5)), ])  # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.


def pil_loader_rgba(path: str) -&gt; Image.Image:
    with open(path, 'rb') as f:
        img = Image.open(f)
        return img.convert('RGBA')


# Loading the dataset
dataset = dset.ImageFolder(root='./data', transform=transform, loader=pil_loader_rgba)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=True,
                                         num_workers=2)  # We use dataLoader to get the images of the training set batch by batch.


# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        m.weight.data.normal_(1.0, 0.02)
        m.bias.data.fill_(0)


def is_cuda_available():
    return torch.cuda.is_available()


def is_gpu_available():
    if is_cuda_available():
        if int(torch.cuda.device_count()) &gt; 0:
            return True
        return False
    return False


# Create results directory
def create_dir(name):
    if not os.path.exists(name):
        os.makedirs(name)


# Creating the generator
netG = G(input_vector)
netG.apply(weights_init)

# Creating the discriminator
netD = D()
netD.apply(weights_init)

if is_gpu_available():
    netG.cuda()
    netD.cuda()

# Training the DCGANs

criterion = nn.BCELoss()
optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))

generator_model = 'generator_model'
discriminator_model = 'discriminator_model'


def save_model(epoch, model, optimizer, error, filepath, noise=None):
    if os.path.exists(filepath):
        os.remove(filepath)
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': error,
        'noise': noise
    }, filepath)


def load_checkpoint(filepath):
    if os.path.exists(filepath):
        return torch.load(filepath)
    return None


def main():
    print(&quot;Device name : &quot; + torch.cuda.get_device_name(0))
    for epoch in range(nb_epochs):

        for i, data in enumerate(dataloader, 0):
            checkpointG = load_checkpoint(generator_model)
            checkpointD = load_checkpoint(discriminator_model)
            if checkpointG:
                netG.load_state_dict(checkpointG['model_state_dict'])
                optimizerG.load_state_dict(checkpointG['optimizer_state_dict'])
            if checkpointD:
                netD.load_state_dict(checkpointD['model_state_dict'])
                optimizerD.load_state_dict(checkpointD['optimizer_state_dict'])

            # 1st Step: Updating the weights of the neural network of the discriminator

            netD.zero_grad()

            # Training the discriminator with a real image of the dataset
            real, _ = data
            if is_gpu_available():
                input = Variable(real.cuda()).cuda()
                target = Variable(torch.ones(input.size()[0]).cuda()).cuda()
            else:
                input = Variable(real)
                target = Variable(torch.ones(input.size()[0]))
            output = netD(input)
            errD_real = criterion(output, target)

            # Training the discriminator with a fake image generated by the generator
            if is_gpu_available():
                noise = Variable(torch.randn(input.size()[0], input_vector, 1, 1)).cuda()
                target = Variable(torch.zeros(input.size()[0])).cuda()
            else:
                noise = Variable(torch.randn(input.size()[0], input_vector, 1, 1))
                target = Variable(torch.zeros(input.size()[0]))
            fake = netG(noise)
            output = netD(fake.detach())
            errD_fake = criterion(output, target)

            # Backpropagating the total error
            errD = errD_real + errD_fake
            errD.backward()
            optimizerD.step()

            # 2nd Step: Updating the weights of the neural network of the generator
            netG.zero_grad()
            if is_gpu_available():
                target = Variable(torch.ones(input.size()[0])).cuda()
            else:
                target = Variable(torch.ones(input.size()[0]))
            output = netD(fake)
            errG = criterion(output, target)
            errG.backward()
            optimizerG.step()

            # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps

            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (
            epoch, nb_epochs, i, len(dataloader), errD.data, errG.data))
            save_model(epoch, netG, optimizerG, errG, generator_model, noise)
            save_model(epoch, netD, optimizerD, errD, discriminator_model, noise)

            if i % 100 == 0:
                create_dir('results')
                vutils.save_image(real, '%s/real_samples.png' % &quot;./results&quot;, normalize=True)
                fake = netG(noise)
                vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (&quot;./results&quot;, epoch), normalize=True)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>So AFTER few hours I decided to look at my results folder. I saw weird thing AFTER 39th epoch.
Generator started generating worst images. Until 39th epoch generator IMPROVED.
Pls look at below Screenshot.
<a href=""https://i.stack.imgur.com/V16hS.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/V16hS.jpg"" alt=""enter image description here"" /></a></p>
<p>Why generator suddenly became worst ?
I'm trying to run 500 epochs. I thought more epochs more success</p>
<p>So I had a look at logs and I'm seeing below</p>
<pre><code>[40/500][0/157] Loss_D: 0.0141 Loss_G: 5.7559
[40/500][1/157] Loss_D: 0.0438 Loss_G: 5.5805
[40/500][2/157] Loss_D: 0.0161 Loss_G: 6.4947
[40/500][3/157] Loss_D: 0.0138 Loss_G: 7.1711
[40/500][4/157] Loss_D: 0.0547 Loss_G: 4.6262
[40/500][5/157] Loss_D: 0.0295 Loss_G: 4.7831
[40/500][6/157] Loss_D: 0.0103 Loss_G: 6.3700
[40/500][7/157] Loss_D: 0.0276 Loss_G: 5.9162
[40/500][8/157] Loss_D: 0.0205 Loss_G: 6.3571
[40/500][9/157] Loss_D: 0.0139 Loss_G: 6.4961
[40/500][10/157] Loss_D: 0.0117 Loss_G: 6.4371
[40/500][11/157] Loss_D: 0.0057 Loss_G: 6.6858
[40/500][12/157] Loss_D: 0.0203 Loss_G: 5.4308
[40/500][13/157] Loss_D: 0.0078 Loss_G: 6.5749
[40/500][14/157] Loss_D: 0.0115 Loss_G: 6.3202
[40/500][15/157] Loss_D: 0.0187 Loss_G: 6.2258
[40/500][16/157] Loss_D: 0.0052 Loss_G: 6.5253
[40/500][17/157] Loss_D: 0.0158 Loss_G: 5.5672
[40/500][18/157] Loss_D: 0.0156 Loss_G: 5.5416
[40/500][19/157] Loss_D: 0.0306 Loss_G: 5.4550
[40/500][20/157] Loss_D: 0.0077 Loss_G: 6.1985
[40/500][21/157] Loss_D: 0.0158 Loss_G: 5.3092
[40/500][22/157] Loss_D: 0.0167 Loss_G: 5.8395
[40/500][23/157] Loss_D: 0.0119 Loss_G: 6.0849
[40/500][24/157] Loss_D: 0.0104 Loss_G: 6.5493
[40/500][25/157] Loss_D: 0.0182 Loss_G: 5.6758
[40/500][26/157] Loss_D: 0.0145 Loss_G: 5.8336
[40/500][27/157] Loss_D: 0.0050 Loss_G: 6.8472
[40/500][28/157] Loss_D: 0.0080 Loss_G: 6.4894
[40/500][29/157] Loss_D: 0.0186 Loss_G: 5.5563
[40/500][30/157] Loss_D: 0.0143 Loss_G: 6.4144
[40/500][31/157] Loss_D: 0.0377 Loss_G: 5.4557
[40/500][32/157] Loss_D: 0.0540 Loss_G: 4.6034
[40/500][33/157] Loss_D: 0.0200 Loss_G: 5.6417
[40/500][34/157] Loss_D: 0.0189 Loss_G: 5.7760
[40/500][35/157] Loss_D: 0.0197 Loss_G: 6.1732
[40/500][36/157] Loss_D: 0.0093 Loss_G: 6.4046
[40/500][37/157] Loss_D: 0.0281 Loss_G: 5.5217
[40/500][38/157] Loss_D: 0.0410 Loss_G: 5.9157
[40/500][39/157] Loss_D: 0.0667 Loss_G: 5.2522
[40/500][40/157] Loss_D: 0.0530 Loss_G: 5.6412
[40/500][41/157] Loss_D: 0.0315 Loss_G: 5.9325
[40/500][42/157] Loss_D: 0.0097 Loss_G: 6.7819
[40/500][43/157] Loss_D: 0.0157 Loss_G: 5.8630
[40/500][44/157] Loss_D: 0.0382 Loss_G: 5.1942
[40/500][45/157] Loss_D: 0.0331 Loss_G: 5.1490
[40/500][46/157] Loss_D: 0.0362 Loss_G: 5.7026
[40/500][47/157] Loss_D: 0.0237 Loss_G: 5.7493
[40/500][48/157] Loss_D: 0.0227 Loss_G: 5.7636
[40/500][49/157] Loss_D: 0.0230 Loss_G: 5.6500
[40/500][50/157] Loss_D: 0.0329 Loss_G: 5.4542
[40/500][51/157] Loss_D: 0.0306 Loss_G: 5.6473
[40/500][52/157] Loss_D: 0.0254 Loss_G: 5.8464
[40/500][53/157] Loss_D: 0.0402 Loss_G: 5.8609
[40/500][54/157] Loss_D: 0.0242 Loss_G: 5.9952
[40/500][55/157] Loss_D: 0.0400 Loss_G: 5.8378
[40/500][56/157] Loss_D: 0.0302 Loss_G: 5.8990
[40/500][57/157] Loss_D: 0.0239 Loss_G: 5.8134
[40/500][58/157] Loss_D: 0.0348 Loss_G: 5.8109
[40/500][59/157] Loss_D: 0.0361 Loss_G: 5.9011
[40/500][60/157] Loss_D: 0.0418 Loss_G: 5.8825
[40/500][61/157] Loss_D: 0.0501 Loss_G: 6.2302
[40/500][62/157] Loss_D: 0.0184 Loss_G: 6.2755
[40/500][63/157] Loss_D: 0.0273 Loss_G: 5.9655
[40/500][64/157] Loss_D: 0.0250 Loss_G: 5.7513
[40/500][65/157] Loss_D: 0.0298 Loss_G: 6.0434
[40/500][66/157] Loss_D: 0.0299 Loss_G: 6.4280
[40/500][67/157] Loss_D: 0.0205 Loss_G: 6.3743
[40/500][68/157] Loss_D: 0.0173 Loss_G: 6.2749
[40/500][69/157] Loss_D: 0.0199 Loss_G: 6.0541
[40/500][70/157] Loss_D: 0.0309 Loss_G: 6.5044
[40/500][71/157] Loss_D: 0.0177 Loss_G: 6.6093
[40/500][72/157] Loss_D: 0.0363 Loss_G: 7.2993
[40/500][73/157] Loss_D: 0.0093 Loss_G: 7.6995
[40/500][74/157] Loss_D: 0.0087 Loss_G: 7.3493
[40/500][75/157] Loss_D: 0.0540 Loss_G: 8.2688
[40/500][76/157] Loss_D: 0.0172 Loss_G: 8.3312
[40/500][77/157] Loss_D: 0.0086 Loss_G: 7.6863
[40/500][78/157] Loss_D: 0.0232 Loss_G: 7.4930
[40/500][79/157] Loss_D: 0.0175 Loss_G: 7.8834
[40/500][80/157] Loss_D: 0.0109 Loss_G: 9.5329
[40/500][81/157] Loss_D: 0.0093 Loss_G: 7.3253
[40/500][82/157] Loss_D: 0.0674 Loss_G: 10.6709
[40/500][83/157] Loss_D: 0.0010 Loss_G: 10.8321
[40/500][84/157] Loss_D: 0.0083 Loss_G: 8.5728
[40/500][85/157] Loss_D: 0.0124 Loss_G: 6.9085
[40/500][86/157] Loss_D: 0.0181 Loss_G: 7.0867
[40/500][87/157] Loss_D: 0.0130 Loss_G: 7.3527
[40/500][88/157] Loss_D: 0.0189 Loss_G: 7.2494
[40/500][89/157] Loss_D: 0.0302 Loss_G: 8.7555
[40/500][90/157] Loss_D: 0.0147 Loss_G: 7.7668
[40/500][91/157] Loss_D: 0.0325 Loss_G: 7.7779
[40/500][92/157] Loss_D: 0.0257 Loss_G: 8.3955
[40/500][93/157] Loss_D: 0.0113 Loss_G: 8.3687
[40/500][94/157] Loss_D: 0.0124 Loss_G: 7.6081
[40/500][95/157] Loss_D: 0.0088 Loss_G: 7.6012
[40/500][96/157] Loss_D: 0.0241 Loss_G: 7.6573
[40/500][97/157] Loss_D: 0.0522 Loss_G: 10.8114
[40/500][98/157] Loss_D: 0.0071 Loss_G: 11.0529
[40/500][99/157] Loss_D: 0.0043 Loss_G: 8.0707
[40/500][100/157] Loss_D: 0.0141 Loss_G: 7.2864
[40/500][101/157] Loss_D: 0.0234 Loss_G: 7.3585
[40/500][102/157] Loss_D: 0.0148 Loss_G: 7.4577
[40/500][103/157] Loss_D: 0.0190 Loss_G: 8.1904
[40/500][104/157] Loss_D: 0.0201 Loss_G: 8.1518
[40/500][105/157] Loss_D: 0.0220 Loss_G: 9.1069
[40/500][106/157] Loss_D: 0.0108 Loss_G: 9.0069
[40/500][107/157] Loss_D: 0.0044 Loss_G: 8.0970
[40/500][108/157] Loss_D: 0.0076 Loss_G: 7.2699
[40/500][109/157] Loss_D: 0.0052 Loss_G: 7.4036
[40/500][110/157] Loss_D: 0.0167 Loss_G: 7.2742
[40/500][111/157] Loss_D: 0.0032 Loss_G: 7.9825
[40/500][112/157] Loss_D: 0.3462 Loss_G: 32.6314
[40/500][113/157] Loss_D: 0.1704 Loss_G: 40.6010
[40/500][114/157] Loss_D: 0.0065 Loss_G: 44.4607
[40/500][115/157] Loss_D: 0.0142 Loss_G: 43.9761
[40/500][116/157] Loss_D: 0.0160 Loss_G: 45.0376
[40/500][117/157] Loss_D: 0.0042 Loss_G: 45.9534
[40/500][118/157] Loss_D: 0.0061 Loss_G: 45.2998
[40/500][119/157] Loss_D: 0.0023 Loss_G: 45.4654
[40/500][120/157] Loss_D: 0.0033 Loss_G: 44.6643
[40/500][121/157] Loss_D: 0.0042 Loss_G: 44.6020
[40/500][122/157] Loss_D: 0.0002 Loss_G: 44.4807
[40/500][123/157] Loss_D: 0.0004 Loss_G: 44.0402
[40/500][124/157] Loss_D: 0.0055 Loss_G: 43.9188
[40/500][125/157] Loss_D: 0.0021 Loss_G: 43.1988
[40/500][126/157] Loss_D: 0.0008 Loss_G: 41.6770
[40/500][127/157] Loss_D: 0.0001 Loss_G: 40.8719
[40/500][128/157] Loss_D: 0.0009 Loss_G: 40.3803
[40/500][129/157] Loss_D: 0.0023 Loss_G: 39.0143
[40/500][130/157] Loss_D: 0.0254 Loss_G: 39.0317
[40/500][131/157] Loss_D: 0.0008 Loss_G: 37.9451
[40/500][132/157] Loss_D: 0.0253 Loss_G: 37.1046
[40/500][133/157] Loss_D: 0.0046 Loss_G: 36.2807
[40/500][134/157] Loss_D: 0.0025 Loss_G: 35.5878
[40/500][135/157] Loss_D: 0.0011 Loss_G: 33.6500
[40/500][136/157] Loss_D: 0.0061 Loss_G: 33.5011
[40/500][137/157] Loss_D: 0.0015 Loss_G: 30.0363
[40/500][138/157] Loss_D: 0.0019 Loss_G: 31.0197
[40/500][139/157] Loss_D: 0.0027 Loss_G: 28.4693
[40/500][140/157] Loss_D: 0.0189 Loss_G: 27.3072
[40/500][141/157] Loss_D: 0.0051 Loss_G: 26.6637
[40/500][142/157] Loss_D: 0.0077 Loss_G: 24.8390
[40/500][143/157] Loss_D: 0.0123 Loss_G: 23.8334
[40/500][144/157] Loss_D: 0.0014 Loss_G: 23.3755
[40/500][145/157] Loss_D: 0.0036 Loss_G: 19.6341
[40/500][146/157] Loss_D: 0.0025 Loss_G: 18.1076
[40/500][147/157] Loss_D: 0.0029 Loss_G: 16.9415
[40/500][148/157] Loss_D: 0.0028 Loss_G: 16.4647
[40/500][149/157] Loss_D: 0.0048 Loss_G: 14.6184
[40/500][150/157] Loss_D: 0.0074 Loss_G: 13.2544
[40/500][151/157] Loss_D: 0.0053 Loss_G: 13.0052
[40/500][152/157] Loss_D: 0.0070 Loss_G: 11.8815
[40/500][153/157] Loss_D: 0.0078 Loss_G: 12.1657
[40/500][154/157] Loss_D: 0.0094 Loss_G: 10.4259
[40/500][155/157] Loss_D: 0.0073 Loss_G: 9.9345
[40/500][156/157] Loss_D: 0.0082 Loss_G: 9.7609
[41/500][0/157] Loss_D: 0.0079 Loss_G: 9.2920
[41/500][1/157] Loss_D: 0.0134 Loss_G: 8.5241
[41/500][2/157] Loss_D: 0.0156 Loss_G: 8.6983
[41/500][3/157] Loss_D: 0.0250 Loss_G: 8.1148
[41/500][4/157] Loss_D: 0.0160 Loss_G: 8.3324
[41/500][5/157] Loss_D: 0.0187 Loss_G: 7.6281
[41/500][6/157] Loss_D: 0.0191 Loss_G: 7.4707
[41/500][7/157] Loss_D: 0.0092 Loss_G: 8.3976
[41/500][8/157] Loss_D: 0.0118 Loss_G: 7.9800
[41/500][9/157] Loss_D: 0.0126 Loss_G: 7.3999
[41/500][10/157] Loss_D: 0.0165 Loss_G: 7.0854
[41/500][11/157] Loss_D: 0.0095 Loss_G: 7.6392
[41/500][12/157] Loss_D: 0.0079 Loss_G: 7.3862
[41/500][13/157] Loss_D: 0.0181 Loss_G: 7.3812
[41/500][14/157] Loss_D: 0.0168 Loss_G: 6.9518
[41/500][15/157] Loss_D: 0.0094 Loss_G: 7.8525
[41/500][16/157] Loss_D: 0.0165 Loss_G: 7.3024
[41/500][17/157] Loss_D: 0.0029 Loss_G: 8.4487
[41/500][18/157] Loss_D: 0.0169 Loss_G: 7.0449
[41/500][19/157] Loss_D: 0.0167 Loss_G: 7.1307
[41/500][20/157] Loss_D: 0.0255 Loss_G: 6.7970
[41/500][21/157] Loss_D: 0.0154 Loss_G: 6.9745
[41/500][22/157] Loss_D: 0.0110 Loss_G: 6.9925
</code></pre>
<p>As you can see there is a HUGE change happened to Generator loss(Loss_G).</p>
<p>Any idea why that happened ?</p>
<p>Any idea how to overcome such a problem ?</p>
",8089694.0,,,,,2021-09-01 14:17:53,After some number of epochs fake image creation become worst in GAN,<python><pytorch><conv-neural-network><artificial-intelligence><generative-adversarial-network>,2,3,0.0,,,CC BY-SA 4.0
70555815,1,,,2022-01-02 12:06:38,,8,2399,"<p>When training a PyTorch Lightning model in a Jupyter Notebook, the console log output is awkward:</p>
<pre><code>Epoch 0: 100%|█████████▉| 2315/2318 [02:05&lt;00:00, 18.41it/s, loss=1.69, v_num=26, acc=0.562]
Validating: 0it [00:00, ?it/s]
Validating:   0%|          | 0/1 [00:00&lt;?, ?it/s]
Epoch 0: 100%|██████████| 2318/2318 [02:09&lt;00:00, 17.84it/s, loss=1.72, v_num=26, acc=0.500, val_loss=1.570, val_acc=0.564]
Epoch 1: 100%|█████████▉| 2315/2318 [02:04&lt;00:00, 18.63it/s, loss=1.56, v_num=26, acc=0.594, val_loss=1.570, val_acc=0.564]
Validating: 0it [00:00, ?it/s]
Validating:   0%|          | 0/1 [00:00&lt;?, ?it/s]
Epoch 1: 100%|██████████| 2318/2318 [02:08&lt;00:00, 18.07it/s, loss=1.59, v_num=26, acc=0.528, val_loss=1.490, val_acc=0.583]
Epoch 2: 100%|█████████▉| 2315/2318 [02:01&lt;00:00, 19.02it/s, loss=1.53, v_num=26, acc=0.617, val_loss=1.490, val_acc=0.583]
Validating: 0it [00:00, ?it/s]
Validating:   0%|          | 0/1 [00:00&lt;?, ?it/s]
Epoch 2: 100%|██████████| 2318/2318 [02:05&lt;00:00, 18.42it/s, loss=1.57, v_num=26, acc=0.500, val_loss=1.460, val_acc=0.589]
</code></pre>
<p>Expectingly, the &quot;correct&quot; output from the same training should be:</p>
<pre><code>Epoch 0: 100%|██████████| 2318/2318 [02:09&lt;00:00, 17.84it/s, loss=1.72, v_num=26, acc=0.500, val_loss=1.570, val_acc=0.564]
Epoch 1: 100%|██████████| 2318/2318 [02:08&lt;00:00, 18.07it/s, loss=1.59, v_num=26, acc=0.528, val_loss=1.490, val_acc=0.583]
Epoch 2: 100%|██████████| 2318/2318 [02:05&lt;00:00, 18.42it/s, loss=1.57, v_num=26, acc=0.500, val_loss=1.460, val_acc=0.589]
</code></pre>
<p>How comes epoch lines are uselessly repeated and split in this manner? Also I'm not sure what use the <code>Validating</code> lines are, since they don't seem to provide any information.</p>
<p>Training and validation steps from the model are as follow:</p>
<pre class=""lang-py prettyprint-override""><code>    def training_step(self, train_batch, batch_idx):
        x, y = train_batch
        y_hat = self.forward(x)
        loss = torch.nn.NLLLoss()(torch.log(y_hat), y.argmax(dim=1)) 
        acc = tm.functional.accuracy(y_hat.argmax(dim=1), y.argmax(dim=1))
        self.log(&quot;acc&quot;, acc, prog_bar=True)
        return loss

    def validation_step(self, valid_batch, batch_idx):
        x, y = valid_batch
        y_hat = self.forward(x)
        loss = torch.nn.NLLLoss()(torch.log(y_hat), y.argmax(dim=1)) 
        acc = tm.functional.accuracy(y_hat.argmax(dim=1), y.argmax(dim=1))
        self.log(&quot;val_loss&quot;, loss, prog_bar=True)
        self.log(&quot;val_acc&quot;, acc, prog_bar=True)
</code></pre>
",2091169.0,,2091169.0,,2022-01-02 12:22:14,2022-05-17 03:00:41,PyTorch Lightning training console output is weird,<python><logging><jupyter-notebook><pytorch><pytorch-lightning>,2,2,0.0,,,CC BY-SA 4.0
66475902,1,,,2021-03-04 13:20:52,,8,4005,"<p>I'm getting following error when I try to use one of the huggingface models for sentimental analysis:</p>
<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 3.00 GiB total capacity; 1.84 GiB already allocated; 5.45 MiB free; 2.04 GiB reserved in total by PyTorch)
</code></pre>
<p>Although I'm not using the CUDA memory it is still staying on the same level. <a href=""https://i.stack.imgur.com/vTJJ1.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/vTJJ1.png"" alt=""enter image description here"" /></a></p>
<p>I tried to use <code>torch.cuda.empty_cache()</code> however it didn't affect the problem. When I closed the jupyter notebook it decreases to 0. So I am well sure that it is something with pytorch and python.</p>
<p>Here is my code:</p>
<pre><code>import joblib
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification,pipeline
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm

tokenizer = AutoTokenizer.from_pretrained(&quot;savasy/bert-base-turkish-sentiment-cased&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;savasy/bert-base-turkish-sentiment-cased&quot;)

sa= pipeline(&quot;sentiment-analysis&quot;, tokenizer=tokenizer, model=model,device=0)
batcher = DataLoader(dataset=comments,
                      batch_size=100,
                      shuffle=True,
                      pin_memory=True)
predictions= []
for batch in tqdm(batcher):
     p = sa(batch)
     predictions.append(p)
</code></pre>
<p>I have a GTX 1060, python 3.8 and torch==1.7.1 and my os is Windows 10.
And the count of comments are 187K. I would like to know if there is any work around for this memory issue. Maybe holding tensors somehow on CPU and only use batch on GPU. After using and getting this error the memory usage still continues. When I close my jupyter notebook it goes away. Is there any way that I can clear this memory ? Is there any way I can utilize Shared GPU memory ?</p>
",12216259.0,,681865.0,,2021-03-04 15:08:11,2021-03-09 15:42:30,How can I decrease Dedicated GPU memory usage and use Shared GPU memory for CUDA and Pytorch,<python><memory><pytorch><gpu>,0,2,0.0,,,CC BY-SA 4.0
66534762,1,66614722.0,,2021-03-08 17:53:37,,8,4490,"<p>The <code>model.eval()</code> method modifies certain modules (layers) which are required to behave differently during training and inference. Some examples are listed in <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval"" rel=""noreferrer"">the docs</a>:</p>
<blockquote>
<p>This has [an] effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <code>Dropout</code>, <code>BatchNorm</code>, etc.</p>
</blockquote>
<p>Is there an exhaustive list of which modules are affected?</p>
",9067615.0,,9067615.0,,2021-04-19 14:33:19,2021-06-08 21:20:46,Which PyTorch modules are affected by model.eval() and model.train()?,<python><machine-learning><pytorch><batch-normalization><dropout>,2,1,0.0,,,CC BY-SA 4.0
74327447,1,74327448.0,,2022-11-05 11:50:11,,8,4882,"<p>I tried to use <code>torch.utils.data.random_split</code> as follows:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.utils.data import DataLoader, random_split

list_dataset = [1,2,3,4,5,6,7,8,9,10]
dataset = DataLoader(list_dataset, batch_size=1, shuffle=False)

random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(123))
</code></pre>
<p>However, when I tried this, I got the error <code>raise ValueError(&quot;Sum of input lengths does not equal the length of the input dataset!&quot;)</code></p>
<p>I looked at <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split"" rel=""noreferrer"">the docs</a> and it seems like I should be able to pass in decimals that sum to 1, but clearly it's not working.</p>
<p>I also Googled this error and the closest thing that comes up is <a href=""https://github.com/pytorch/pytorch/issues/67740"" rel=""noreferrer"">this issue</a>.</p>
<p>What am I doing wrong?</p>
",5049813.0,,,,,2022-12-13 17:02:12,How to use random_split with percentage split (sum of input lengths does not equal the length of the input dataset),<pytorch><dataset>,2,0,,,,CC BY-SA 4.0
64483856,1,64591119.0,,2020-10-22 13:58:08,,8,257,"<p>After struggling with this amazing <a href=""https://github.com/facebookresearch/PyTorch-BigGraph"" rel=""noreferrer"">facebookresearch
/
PyTorch-BigGraph</a> project, and its impossible API, I managed to get a grip on how to run it (thanks to <a href=""https://github.com/nadbordrozd/blog_stuff/blob/master/PBG/hello_graph.py"" rel=""noreferrer"">stand alone simple example</a>)</p>
<p>My system restrictions do not allow me to train the dense (embedding) representation of all edges, and I need from time to time to upload past embeddings and train the model using both new edges and existing nodes, notice that nodes in past and new edge list do not necessarily overlap.</p>
<p>I tried to understand from here: <a href=""https://torchbiggraph.readthedocs.io/en/latest/input_output.html"" rel=""noreferrer"">see the context section</a> how to do it, so far with no success.</p>
<p>Following is a stand-alone PGD code, that turned <code>batch_edges</code> into an embedding node list, however, I need it to use pre-trained nodes list <code>past_trained_nodes</code>.</p>
<pre><code>import os
import shutil
from pathlib import Path

from torchbiggraph.config import parse_config
from torchbiggraph.converters.importers import TSVEdgelistReader, convert_input_data
from torchbiggraph.train import train
from torchbiggraph.util import SubprocessInitializer, setup_logging

DIMENSION = 4
DATA_DIR = 'data'
GRAPH_PATH = DATA_DIR + '/output1.tsv'
MODEL_DIR = 'model'


raw_config = dict(
        entity_path=DATA_DIR,
        edge_paths=[DATA_DIR + '/edges_partitioned', ],
        checkpoint_path=MODEL_DIR,
        entities={&quot;n&quot;: {&quot;num_partitions&quot;: 1}},
        relations=[{&quot;name&quot;: &quot;doesnt_matter&quot;, &quot;lhs&quot;: &quot;n&quot;, &quot;rhs&quot;: &quot;n&quot;, &quot;operator&quot;: &quot;complex_diagonal&quot;, }],
        dynamic_relations=False, dimension=DIMENSION, global_emb=False, comparator=&quot;dot&quot;,
        num_epochs=7, num_uniform_negs=1000, loss_fn=&quot;softmax&quot;, lr=0.1, eval_fraction=0.,)

batch_edges = [[&quot;A&quot;, &quot;B&quot;], [&quot;B&quot;, &quot;C&quot;],  [&quot;C&quot;, &quot;D&quot;], [&quot;D&quot;, &quot;B&quot;], [&quot;B&quot;, &quot;D&quot;]]


# I want the model to use these pretrained nodes, Notice that Node A exist, And F Does not
#I dont have all past nodes, as some are gained from data
past_trained_nodes = {'A': [0.5, 0.3, 1.5, 8.1], 'F': [3, 0.6, 1.2, 4.3]}


try:
    shutil.rmtree('data')
except:
    pass
try:
    shutil.rmtree(MODEL_DIR)
except:
    pass

os.makedirs(DATA_DIR, exist_ok=True)
with open(GRAPH_PATH, 'w') as f:
    for edge in batch_edges:
        f.write('\t'.join(edge) + '\n')

setup_logging()
config = parse_config(raw_config)
subprocess_init = SubprocessInitializer()
input_edge_paths = [Path(GRAPH_PATH)]

convert_input_data(config.entities, config.relations, config.entity_path, config.edge_paths,
                   input_edge_paths, TSVEdgelistReader(lhs_col=0, rel_col=None, rhs_col=1),
                   dynamic_relations=config.dynamic_relations, )

train(config, subprocess_init=subprocess_init)
</code></pre>
<p>How can I use my pre-trained nodes in the current model?</p>
<p>Thanks in advance!</p>
",3386991.0,,,,,2020-10-29 12:28:10,Use pre trained Nodes from past runs - Pytorch Biggraph,<python><graph><pytorch>,1,0,0.0,,,CC BY-SA 4.0
65404049,1,,,2020-12-22 06:18:19,,8,1259,"<p>I am trying to install <code>Pytorch Library</code> on My <code>Windows 10</code>, having <code>Python Version 3.6.9</code> and using the following command taken from this website :<a href=""https://pytorch.org/get-started/locally/#windows-package-manager"" rel=""noreferrer"">https://pytorch.org/get-started/locally/#windows-package-manager</a></p>
<blockquote>
<p>conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c
pytorch</p>
</blockquote>
<p>The installation ended with the following message:</p>
<blockquote>
<p>Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.</p>
</blockquote>
<p>PackagesNotFoundError: The following packages are not available from current channels:</p>
<ul>
<li>cuda80</li>
<li>torchvision==0.2.1</li>
<li>pytorch==1.0.0</li>
</ul>
<p>Current channels:</p>
<ul>
<li><a href=""https://conda.anaconda.org/pytorch/win-64"" rel=""noreferrer"">https://conda.anaconda.org/pytorch/win-64</a></li>
<li><a href=""https://conda.anaconda.org/pytorch/noarch"" rel=""noreferrer"">https://conda.anaconda.org/pytorch/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/main/win-64"" rel=""noreferrer"">https://repo.anaconda.com/pkgs/main/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/main/noarch"" rel=""noreferrer"">https://repo.anaconda.com/pkgs/main/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/win-64"" rel=""noreferrer"">https://repo.anaconda.com/pkgs/r/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/r/noarch"" rel=""noreferrer"">https://repo.anaconda.com/pkgs/r/noarch</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/msys2/win-64"" rel=""noreferrer"">https://repo.anaconda.com/pkgs/msys2/win-64</a></li>
<li><a href=""https://repo.anaconda.com/pkgs/msys2/noarch"" rel=""noreferrer"">https://repo.anaconda.com/pkgs/msys2/noarch</a></li>
</ul>
<p>To search for alternate channels that may provide the <code>conda</code> package you're
looking for, navigate to</p>
<pre><code>https://anaconda.org
</code></pre>
<p>and use the search bar at the top of the page.</p>
<p>Please require solution of the  above problem. Thank you in advance</p>
",7887965.0,,12256384.0,,2020-12-22 07:18:52,2022-03-12 03:26:12,PackagesNotFoundError: The following packages are not available from current channels: pytorch,<python><anaconda><pytorch>,1,0,,,,CC BY-SA 4.0
63974588,1,,,2020-09-20 00:44:57,,8,6079,"<p>I’m currently using Pipenv to maintain the Python packages used in a specific project. Most of the downloads I’ve tried so far have worked as intended; that is, I enter <code>pipenv install [package]</code> and it installs the package into the virtual environment, then records the package information into both the Pipfile and Pipfile.lock.</p>
<p>However, I’m running into some problems installing PyTorch.</p>
<p>I’ve tried running <code>pipenv install torch</code>, but every time the locking step fails. Instead, I’ve tried forcing a download directly from the PyTorch website using</p>
<pre><code>pipenv run pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>And it actually installs! If I run <code>pipenv graph</code> it displays both torch and torchvision with their dependencies. But one problem remains: neither torch nor torchvision are being saved into Pipfile and Pipfile.lock.</p>
<p>Any idea on how I can make this happen?</p>
",14307698.0,,2745495.0,,2020-12-27 07:13:03,2022-02-22 08:17:25,How to install PyTorch with pipenv and save it to Pipfile and Pipfile.lock?,<python><windows><pytorch><pipenv><torchvision>,4,0,0.0,,,CC BY-SA 4.0
68479235,1,,,2021-07-22 04:34:30,,8,10758,"<p>I want to run some experiments on my GPU device, but I get this error:</p>
<blockquote>
<p>RuntimeError: CUDA out of memory. Tried to allocate 3.63 GiB (GPU 0;
15.90 GiB total capacity; 13.65 GiB already allocated; 1.57 GiB free; 13.68 GiB reserved in total by PyTorch)</p>
</blockquote>
<p>I read about possible solutions <a href=""https://github.com/pytorch/pytorch/issues/16417"" rel=""noreferrer"">here</a>, and the common solution is this:</p>
<blockquote>
<p>It is because of mini-batch of data does not fit onto GPU memory.
Just decrease the batch size. When I set batch size = 256 for cifar10
dataset I got the same error; Then I set the batch size = 128, it is
solved.</p>
</blockquote>
<p>But in my case, it is a research project, and I want to have specific hyper-parameters and I can not reduce anything such as batch size.</p>
<p>Does anyone have a solution for this?</p>
",10575373.0,,681865.0,,2021-07-22 05:15:57,2021-07-22 05:55:40,"CUDA out of memory error, cannot reduce batch size",<python><pytorch>,2,5,0.0,,,CC BY-SA 4.0
66854148,1,,,2021-03-29 12:22:35,,8,3785,"<p>I was wondering what is the proper way of logging metrics when using DDP. I noticed that if I want to print something inside <code>validation_epoch_end</code> it will be printed twice when using 2 GPUs. I was expecting <code>validation_epoch_end</code> to be called only on rank 0 and to receive the outputs from all GPUs, but I am not sure this is correct anymore. Therefore I have several questions:</p>
<ol>
<li><code>validation_epoch_end(self, outputs)</code> - When using DDP does every subprocess receive the data processed from the current GPU or data processed from all GPUs, i.e. does the input parameter <code>outputs</code> contains the outputs of the entire validation set, from all GPUs?</li>
<li>If <code>outputs</code> is GPU/process specific what is the proper way to calculate any metric on the entire validation set in <code>validation_epoch_end</code> when using DDP?</li>
</ol>
<p>I understand that I can solve the printing by checking <code>self.global_rank == 0</code> and printing/logging only in that case, however I am trying to get a deeper understanding of what I am printing/logging in this case.</p>
<p>Here is a code snippet from my use case. I would like to be able to report f1, precision and recall on the entire validation dataset and I am wondering what is the correct way of doing it when using DDP.</p>
<pre><code>    def _process_epoch_outputs(self,
                               outputs: List[Dict[str, Any]]
                               ) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &quot;&quot;&quot;Creates and returns tensors containing all labels and predictions

        Goes over the outputs accumulated from every batch, detaches the
        necessary tensors and stacks them together.

        Args:
            outputs (List[Dict])
        &quot;&quot;&quot;
        all_labels = []
        all_predictions = []

        for output in outputs:
            for labels in output['labels'].detach():
                all_labels.append(labels)

            for predictions in output['predictions'].detach():
                all_predictions.append(predictions)

        all_labels = torch.stack(all_labels).long().cpu()
        all_predictions = torch.stack(all_predictions).cpu()

        return all_predictions, all_labels

    def validation_epoch_end(self, outputs: List[Dict[str, Any]]) -&gt; None:
        &quot;&quot;&quot;Logs f1, precision and recall on the validation set.&quot;&quot;&quot;

        if self.global_rank == 0:
            print(f'Validation Epoch: {self.current_epoch}')

        predictions, labels = self._process_epoch_outputs(outputs)
        for i, name in enumerate(self.label_columns):

            f1, prec, recall, t = metrics.get_f1_prec_recall(predictions[:, i],
                                                             labels[:, i],
                                                             threshold=None)
            self.logger.experiment.add_scalar(f'{name}_f1/Val',
                                              f1,
                                              self.current_epoch)
            self.logger.experiment.add_scalar(f'{name}_Precision/Val',
                                              prec,
                                              self.current_epoch)
            self.logger.experiment.add_scalar(f'{name}_Recall/Val',
                                              recall,
                                              self.current_epoch)

            if self.global_rank == 0:
                print((f'F1: {f1}, Precision: {prec}, '
                       f'Recall: {recall}, Threshold {t}'))
</code></pre>
",3687776.0,,,,,2021-03-29 15:56:48,Proper way to log things when using Pytorch Lightning DDP,<python-3.x><pytorch><pytorch-lightning>,1,0,,,,CC BY-SA 4.0
73698041,1,73701220.0,,2022-09-13 05:49:08,,8,2450,"<p>in a simple test in pytorch, I want to see grad in a non-leaf tensor, so I use retain_grad():</p>
<pre><code>import torch
a = torch.tensor([1.], requires_grad=True)
y = torch.zeros((10))
gt = torch.zeros((10))

y[0] = a
y[1] = y[0] * 2
y.retain_grad()

loss = torch.sum((y-gt) ** 2)
loss.backward()
print(y.grad)
</code></pre>
<p>it gives me a normal output:</p>
<pre><code>tensor([2., 4., 0., 0., 0., 0., 0., 0., 0., 0.])
</code></pre>
<p>but when I use retain grad() before y[1] and after y[0] is assigned:</p>
<pre><code>import torch
a = torch.tensor([1.], requires_grad=True)
y = torch.zeros((10))
gt = torch.zeros((10))

y[0] = a
y.retain_grad()
y[1] = y[0] * 2

loss = torch.sum((y-gt) ** 2)
loss.backward()
print(y.grad)
</code></pre>
<p>now the output changes to:</p>
<pre><code>tensor([10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])
</code></pre>
<p>I can't understand the result at all.</p>
",19982618.0,,14277722.0,,2022-09-13 07:50:03,2022-09-13 10:14:51,How retain_grad() in pytorch works? I found its position changes the grad result,<python><pytorch>,1,0,0.0,,,CC BY-SA 4.0
64523788,1,64526124.0,,2020-10-25 12:41:21,,8,3465,"<p>I have an multi-task encoder/decoder model in PyTorch with a (trainable) <code>torch.nn.Embedding</code> embedding layer at the input.</p>
<p>In one particular task, I'd like to pre-train the model self-supervised (to re-construct masked input data) and use it for inference (to fill in gaps in data).</p>
<p>I guess for training time I can just measure loss as the distance between the input embedding and the output embedding... But for inference, how do I invert an <code>Embedding</code> to reconstruct the proper category/token the output corresponds to? I can't see e.g. a &quot;nearest&quot; function on the Embedding class...</p>
",13352657.0,,,,,2021-08-02 02:48:11,How to invert a PyTorch Embedding?,<deep-learning><pytorch><autoencoder>,1,1,0.0,,,CC BY-SA 4.0
67959327,1,67960934.0,,2021-06-13 14:30:03,,8,16531,"<p>I have a pyTorch-code to train a model that should be able to detect placeholder-images among product-images. I didn't write the code by myself as I am very unexperienced with CNNs and Machine Learning.</p>
<p>My boss told me to calculate the <strong>f1-score</strong> for that model and i found out that the formula for that is <code>((precision * recall)/(precision + recall))</code> but I don't know how I get precision and recall. Is someone able to tell me how I can get those two parameters from that following code?
(Sorry for the long piece of code, but I didn't really know what is necessary and what isn't)</p>
<pre><code>from __future__ import print_function 
from __future__ import division
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy
print(&quot;PyTorch Version: &quot;,torch.__version__)
print(&quot;Torchvision Version: &quot;,torchvision.__version__)

data_dir = &quot;data&quot;

# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]
model_name = &quot;resnet&quot;

# Number of classes in the dataset [we have four classes A-Balik-Duz-Princess]
num_classes = 2

# Batch size for training (change depending on how much memory you have)
batch_size = 25

# Number of epochs to train for (This will need to be calculated in order to address under and over fitting issue)
num_epochs = 20

# Flag for feature extracting. When False, we fine tune the whole model, 
#   when True we only update the reshaped layer params
feature_extract = True

def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):
    since = time.time()
    print(&quot;model is : &quot;,model)

    val_acc_history = []
    val_loss_history = []
    train_acc_history = []
    train_loss_history = []
    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)


        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients (This can be changed to the Adam and other optimizers)
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    # Get model outputs and calculate loss
                    # Special case for inception because in training it has an auxiliary output. In train
                    #   mode we calculate the loss by summing the final output and the auxiliary output
                    #   but in testing we only consider the final output.
                    if is_inception and phase == 'train':
                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958
                        outputs, aux_outputs = model(inputs)
                        loss1 = criterion(outputs, labels)
                        loss2 = criterion(aux_outputs, labels)
                        loss = loss1 + 0.4*loss2
                    else:
                        outputs = model(inputs)
                        loss = criterion(outputs, labels)

                    _, preds = torch.max(outputs, 1)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))


            # deep copy the model
            if phase == 'val' and epoch_acc &gt; best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())
            if phase == 'val':
                val_acc_history.append(epoch_acc)
                val_loss_history.append(epoch_loss)
            if phase == 'train':
                train_acc_history.append(epoch_acc)
                train_loss_history.append(epoch_loss)

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model, val_acc_history, train_acc_history,val_loss_history,train_loss_history

def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False

###############################################
###   Initialize and Reshape the Networks
###############################################

def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):
    # Initialize these variables which will be set in this if statement. Each of these
    #   variables is model specific.
    model_ft = None
    input_size = 0

    if model_name == &quot;resnet&quot;:
        &quot;&quot;&quot; Resnet18
        &quot;&quot;&quot;
        model_ft = models.resnet152(pretrained=use_pretrained)
        #we can select any possible variation of ResNet such as Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = nn.Linear(num_ftrs, num_classes)
        input_size = 224

    elif model_name == &quot;alexnet&quot;:
        &quot;&quot;&quot; Alexnet
        &quot;&quot;&quot;
        model_ft = models.alexnet(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier[6].in_features
        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)
        input_size = 224

    elif model_name == &quot;vgg&quot;:
        &quot;&quot;&quot; VGG11_bn
        &quot;&quot;&quot;
        model_ft = models.vgg11_bn(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier[6].in_features
        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)
        input_size = 224

    elif model_name == &quot;squeezenet&quot;:
        &quot;&quot;&quot; Squeezenet
        &quot;&quot;&quot;
        model_ft = models.squeezenet1_0(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))
        model_ft.num_classes = num_classes
        input_size = 224

    elif model_name == &quot;densenet&quot;:
        &quot;&quot;&quot; Densenet
        &quot;&quot;&quot;
        model_ft = models.densenet121(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        num_ftrs = model_ft.classifier.in_features
        model_ft.classifier = nn.Linear(num_ftrs, num_classes) 
        input_size = 224

    elif model_name == &quot;inception&quot;:
        &quot;&quot;&quot; Inception v3 
        Be careful, expects (299,299) sized images and has auxiliary output
        &quot;&quot;&quot;
        model_ft = models.inception_v3(pretrained=use_pretrained)
        set_parameter_requires_grad(model_ft, feature_extract)
        # Handle the auxilary net
        num_ftrs = model_ft.AuxLogits.fc.in_features
        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)
        # Handle the primary net
        num_ftrs = model_ft.fc.in_features
        model_ft.fc = nn.Linear(num_ftrs,num_classes)
        input_size = 299

    else:
        print(&quot;Invalid model name, exiting...&quot;)
        exit()

    return model_ft, input_size

# Initialize the model for this run
model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)

# Print the model we just instantiated
#print(model_ft)

########################
###   LOAD DATA
########################

# Data augmentation and normalization for training
# there are multiple approaches for data augmentation  which can be added in the future
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(input_size),
        #transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(input_size),
        transforms.CenterCrop(input_size),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

print(&quot;Initializing Datasets and Dataloaders...&quot;)

# Create training and validation datasets
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}
# Create training and validation dataloaders
dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}

# Detect if we have a GPU available
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


#############################
###   Create the Optimizer
#############################


# Send the model to GPU
model_ft = model_ft.to(device)

# Gather the parameters to be optimized/updated in this run. If we are
#  fine tuning we will be updating all parameters. However, if we are 
#  doing feature extract method, we will only update the parameters
#  that we have just initialized, i.e. the parameters with requires_grad
#  is True.
params_to_update = model_ft.parameters()
print(&quot;Params to learn:&quot;)
if feature_extract:
    params_to_update = []
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            params_to_update.append(param)
            print(&quot;\t&quot;,name)
else:
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            print(&quot;\t&quot;,name)

# Observe that all parameters are being optimized we can add leaky ReLU and much more
optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)


###########################
###   Run Training and Validation Step
###########################

%time
# Setup the loss fxn
criterion = nn.CrossEntropyLoss()

# Train and evaluate
model_ft, hist, loss_t,vloss_acc, tloss_acc = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==&quot;inception&quot;))
</code></pre>
",8028034.0,,8028034.0,,2021-06-13 14:43:35,2023-06-15 13:41:57,How to calculate the f1-score?,<pytorch><conv-neural-network><resnet>,4,0,,,,CC BY-SA 4.0
64092369,1,64100913.0,,2020-09-27 19:15:45,,8,10245,"<p>I want to load MNIST dataset in PyTorch and Torchvision, dividing it into train, validation and test parts. So far I have:</p>
<pre><code>def load_dataset():
    train_loader = torch.utils.data.DataLoader(
        torchvision.datasets.MNIST(
            '/data/', train=True, download=True,
            transform=torchvision.transforms.Compose([
                torchvision.transforms.ToTensor()])),
        batch_size=batch_size_train, shuffle=True)

    test_loader = torch.utils.data.DataLoader(
        torchvision.datasets.MNIST(
            '/data/', train=False, download=True,
            transform=torchvision.transforms.Compose([
                torchvision.transforms.ToTensor()])),
        batch_size=batch_size_test, shuffle=True)
</code></pre>
<p>How can I divide the training dataset into training and validation if it's in the <code>DataLoader</code>? I want to use last 10000 examples from the training dataset as a validation dataset (I know that I should do CV for more accurate results, I just want a quick validation here).</p>
",9472066.0,,,,,2021-07-11 18:14:00,Validation dataset in PyTorch using DataLoaders,<neural-network><pytorch>,2,2,0.0,,,CC BY-SA 4.0
64499294,1,66990226.0,,2020-10-23 11:39:36,,7,3784,"<p>I'm training an image classification model with PyTorch Lightning and running on a machine with more than one GPU, so I use the recommended distributed backend for best performance <code>ddp</code> (DataDistributedParallel). This naturally splits up the dataset, so each GPU will only ever see one part of the data.</p>
<p>However, for validation, I would like to compute metrics like accuracy on the entire validation set and not just on a part. How would I do that? I found <a href=""https://pytorch-lightning.readthedocs.io/en/stable/multi_gpu.html"" rel=""noreferrer"">some hints in the official documentation</a>, but they do not work as expected or are confusing to me. What's happening is that <code>validation_epoch_end</code> is called <code>num_gpus</code> times with <code>1/num_gpus</code> of the validation data each. I would like to aggregate all results and only run the <code>validation_epoch_end</code> once.</p>
<p>In <a href=""https://pytorch-lightning.readthedocs.io/en/stable/multi_gpu.html#dp-ddp2-caveats"" rel=""noreferrer"">this section</a> they state that when using dp/ddp2 you can add an additional function called like this</p>
<pre><code>def validation_step(self, batch, batch_idx):
    loss, x, y, y_hat = self.step(batch)
    return {&quot;val_loss&quot;: loss, 'y': y, 'y_hat': y_hat}

def validation_step_end(self, self, *args, **kwargs):
    # do something here, I'm not sure what, 
    # as it gets called in ddp directly after validation_step with the exact same values
    return args[0]
</code></pre>
<p>However, the results are not being aggregated and <code>validation_epoch_end</code> is still called <code>num_gpu</code> times. Is this kind of behavior not available for <code>ddp</code>? Is there some other way how achieve this aggregation behavior?</p>
",448357.0,,,,,2022-03-29 09:44:20,Validate on entire validation set when using ddp backend with PyTorch Lightning,<python><parallel-processing><pytorch><distributed-computing><pytorch-lightning>,2,0,0.0,,,CC BY-SA 4.0
63822152,1,63824573.0,,2020-09-10 02:54:06,,7,3792,"<p>In machine translation, we always need to slice out the first timestep (the SOS token) in the annotation and prediction.</p>
<p>When using <code>batch_first=False</code>, slicing out the first timestep still keeps the tensor contiguous.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
batch_size = 128
seq_len = 12
embedding = 50

# Making a dummy output that is `batch_first=False`
batch_not_first = torch.randn((seq_len,batch_size,embedding))
batch_not_first = batch_first[1:].view(-1, embedding) # slicing out the first time step
</code></pre>
<p>However, if we use <code>batch_first=True</code>, after slicing, the tensor is no longer contiguous. We need to make it contiguous before we can do different operations such as <code>view</code>.</p>
<pre class=""lang-py prettyprint-override""><code>batch_first = torch.randn((batch_size,seq_len,embedding))
batch_first[:,1:].view(-1, embedding) # slicing out the first time step

output&gt;&gt;&gt;
&quot;&quot;&quot;
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-8-a9bd590a1679&gt; in &lt;module&gt;
----&gt; 1 batch_first[:,1:].view(-1, embedding) # slicing out the first time step

RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
&quot;&quot;&quot;
</code></pre>
<p>Does that mean <code>batch_first=False</code> is better, at least, in the context of machine translation? Since it saves us from doing the <code>contiguous()</code> step. Is there any cases that <code>batch_first=True</code> works better?</p>
",9793316.0,,,,,2023-04-19 21:51:14,PyTorch RNN is more efficient with `batch_first=False`?,<python><nlp><pytorch>,1,0,0.0,,,CC BY-SA 4.0
69628487,1,69683069.0,,2021-10-19 09:39:46,,7,2788,"<p>Given a Zero-Shot Classification Task via Huggingface as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)

example_text = &quot;This is an example text about snowflakes in the summer&quot;
labels = [&quot;weather&quot;, &quot;sports&quot;, &quot;computer industry&quot;]
        
output = classifier(example_text, labels, multi_label=True)
output 
{'sequence': 'This is an example text about snowflakes in the summer',
'labels': ['weather', 'sports'],
'scores': [0.9780895709991455, 0.021910419687628746]}
</code></pre>
<p>I am trying to extract the SHAP values to generate a text-based explanation for the prediction result like shown here: <a href=""https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/2021_04_22_shap_for_huggingface_transformers/explainable_transformers_using_shap.ipynb#scrollTo=Ocd9majYrupz"" rel=""nofollow noreferrer"">SHAP for Transformers</a></p>
<p>I already tried the following based on the above url:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline

model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')

pipe = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)

def score_and_visualize(text):
    prediction = pipe([text])
    print(prediction[0])

    explainer = shap.Explainer(pipe)
    shap_values = explainer([text])

    shap.plots.text(shap_values)

score_and_visualize(example_text)
</code></pre>
<p><strong>Any suggestions? Thanks for your help in advance!</strong></p>
<p>Alternatively to the above pipeline the following also works:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline

model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')

classifier = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)

example_text = &quot;This is an example text about snowflakes in the summer&quot;
labels = [&quot;weather&quot;, &quot;sports&quot;]

output = classifier(example_text, labels)
output 
{'sequence': 'This is an example text about snowflakes in the summer',
'labels': ['weather', 'sports'],
'scores': [0.9780895709991455, 0.021910419687628746]}
</code></pre>
",11181726.0,,11181726.0,,2021-10-25 07:56:31,2021-10-25 13:25:31,How to get SHAP values for Huggingface Transformer Model Prediction [Zero-Shot Classification]?,<pytorch><huggingface-transformers><transformer-model><shap>,2,0,0.0,,,CC BY-SA 4.0
69711410,1,,,2021-10-25 16:08:15,,7,7358,"<p>I'm trying to run a model for generating art images.
In Jupyter notebook ( on vs code )  the script run correctly but when I'm trying to run the same code on the .py file it gives this error :</p>
<p>[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.</p>
",13149592.0,,,,,2022-04-07 12:56:42,Could not initialize NNPACK,<pytorch>,1,0,,,,CC BY-SA 4.0
71328662,1,,,2022-03-02 20:19:07,,7,1069,"<p>I'm trying to profile my pytorch network to see what is the bottleneck. I noticed that there is an operation called <code>cudaLaunchKernel</code> which is taking up most of the time. <a href=""https://stackoverflow.com/a/69209389/3337089"">This answer</a> says that it is called for every operation done with cuda. If suppose I implement this network in C++ or any other language, would it be possible to reduce this time?</p>
<p>Basically, I'm asking if this overhead is because I've implemented my network in python or will this overhead be always there and impossible to optimize in any language?</p>
<p>Full profiler output:</p>
<pre><code>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                       cudaLaunchKernel        99.80%     933.739ms        99.80%     933.739ms      20.750ms       0.000us         0.00%       0.000us       0.000us            45  
                                        model_inference         0.05%     453.000us       100.00%     935.567ms     935.567ms       0.000us         0.00%     195.000us     195.000us             1  
                                aten::cudnn_convolution         0.04%     388.000us        99.84%     934.047ms     103.783ms     195.000us       100.00%     195.000us      21.667us             9  
                                     aten::_convolution         0.01%     138.000us        99.88%     934.419ms     103.824ms       0.000us         0.00%     195.000us      21.667us             9  
                                           aten::conv2d         0.01%     122.000us        99.89%     934.592ms     103.844ms       0.000us         0.00%     195.000us      21.667us             9  
                                             aten::add_         0.01%     112.000us         0.02%     155.000us      17.222us       0.000us         0.00%       0.000us       0.000us             9  
                               aten::upsample_nearest2d         0.01%      82.000us         0.01%     105.000us      26.250us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::empty         0.01%      79.000us         0.01%      79.000us       3.292us       0.000us         0.00%       0.000us       0.000us            24  
                                        aten::threshold         0.01%      74.000us         0.02%     149.000us      18.625us       0.000us         0.00%       0.000us       0.000us             8  
                                             aten::_cat         0.01%      71.000us         0.01%     119.000us      29.750us       0.000us         0.00%       0.000us       0.000us             4  
                                             aten::relu         0.01%      57.000us         0.02%     206.000us      25.750us       0.000us         0.00%       0.000us       0.000us             8  
                                      aten::convolution         0.01%      51.000us        99.88%     934.470ms     103.830ms       0.000us         0.00%     195.000us      21.667us             9  
                                             aten::view         0.01%      50.000us         0.01%      50.000us       5.556us       0.000us         0.00%       0.000us       0.000us             9  
                                              aten::cat         0.00%      32.000us         0.02%     151.000us      37.750us       0.000us         0.00%       0.000us       0.000us             4  
                                          aten::reshape         0.00%      29.000us         0.01%      79.000us       8.778us       0.000us         0.00%       0.000us       0.000us             9  
                                          aten::resize_         0.00%      25.000us         0.00%      25.000us       0.962us       0.000us         0.00%       0.000us       0.000us            26  
                                             aten::rsub         0.00%      21.000us         0.00%      33.000us      33.000us       0.000us         0.00%       0.000us       0.000us             1  
                                              aten::mul         0.00%      17.000us         0.00%      27.000us      27.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::zeros         0.00%      13.000us         0.00%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  
                                        cudaEventRecord         0.00%      12.000us         0.00%      12.000us       1.333us       0.000us         0.00%       0.000us       0.000us             9  
                                        cudaBindTexture         0.00%      11.000us         0.00%      11.000us       2.750us       0.000us         0.00%       0.000us       0.000us             4  
                                    aten::empty_strided         0.00%       6.000us         0.00%       6.000us       6.000us       0.000us         0.00%       0.000us       0.000us             1  
                                            aten::zero_         0.00%       1.000us         0.00%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  
cudnn::maxwell::gemm::computeOffsetsKernel(cudnn::ma...         0.00%       0.000us         0.00%       0.000us       0.000us     195.000us       100.00%     195.000us     195.000us             1  
                                      cudaUnbindTexture         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 935.583ms
Self CUDA time total: 195.000us
</code></pre>
<p>PS: Some configs<br />
Python version: 3.8.8<br />
PyTorch version: 1.8.1<br />
cudatoolkit version: 10.2.89<br />
cuda version (as given by nvidia-smi): 11.4</p>
<p>CPU specs: intel core i7 10700 @ 2.90GHz 16 cores
GPU specs: NVIDIA GM204GL [Quadro M4000]
RAM: 64GB
GPU RAM: 8GB
OS: 64-bit Ubuntu 20.04.3</p>
<p>PPS: I'm not looking for ways to speed up my code. I want to know <strong>if it is possible</strong> to speed it up by coding it in a different language like cpp or directly in cuda. (Like suppose if all my data is already on GPU, and I've written my code in cuda language itself, would it run in <code>195us</code>?)</p>
",3337089.0,,3337089.0,,2022-03-09 13:28:27,2022-03-09 13:28:27,What is cudaLaunchKernel in pytorch profiler output,<python><pytorch><profiling>,1,4,,,,CC BY-SA 4.0
65642832,1,65645010.0,,2021-01-09 12:51:36,,7,6935,"<p>I have <code>n</code>-vectors which need to be influenced by each other and output <code>n</code> vectors with same dimensionality <code>d</code>. I believe this is what <code>torch.nn.MultiheadAttention</code> does. But the forward function <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"" rel=""noreferrer"">expects</a> query, key and value as inputs. According to <a href=""https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a#8481"" rel=""noreferrer"">this</a> blog, I need to initialize a random weight matrix of shape <code>(d x d)</code> for each of <code>q</code>, <code>k</code> and <code>v</code> and multiply each of my vectors with these weight matrices and get 3 <code>(n x d)</code> matrices. Now are the <code>q</code>, <code>k</code> and <code>v</code> expected by <code>torch.nn.MultiheadAttention</code> just these three matrices or do I have it mistaken?</p>
",11971720.0,,,,,2021-01-09 16:34:28,Inputs to the nn.MultiheadAttention?,<python><deep-learning><pytorch><attention-model>,1,0,0.0,,,CC BY-SA 4.0
63449011,1,63449012.0,,2020-08-17 10:27:48,,7,12745,"<p>I am asking this question because I am successfully training a segmentation network on my GTX 2070 on laptop with 8GB VRAM and I use <strong>exactly the same code and exactly the same software libraries installed</strong> on my desktop PC with a GTX 1080TI and it still throws out of memory.</p>
<p>Why does this happen, considering that:</p>
<ol>
<li><p>The same Windows 10 + CUDA 10.1 + CUDNN 7.6.5.32 + Nvidia Driver 418.96 (comes along with CUDA 10.1) are both on laptop and on PC.</p>
</li>
<li><p>The fact that training with TensorFlow 2.3 runs smoothly on the GPU on my PC, yet it fails allocating memory for training only with PyTorch.</p>
</li>
<li><p>PyTorch recognises the GPU (prints GTX 1080 TI) via the command : <code>print(torch.cuda.get_device_name(0))</code></p>
</li>
<li><p>PyTorch allocates memory when running this command: <code>torch.rand(20000, 20000).cuda()</code> <code>#allocated 1.5GB of VRAM.</code></p>
</li>
</ol>
<p>What is the solution to this?</p>
",6117017.0,,6117017.0,,2020-11-17 16:06:05,2022-07-18 09:33:57,Why do I get CUDA out of memory when running PyTorch model [with enough GPU memory]?,<python><tensorflow><pytorch><out-of-memory>,1,7,0.0,,,CC BY-SA 4.0
68351091,1,68368157.0,,2021-07-12 16:46:16,,7,12344,"<p>I want to use an RNN with bilstm layers using pytorch on protein embeddings. It worked with Linear Layer but when i use Bilstm i have a Runtime error. Sorry if its not clear its my first publication and i will be grateful if someone can help me.</p>
<pre><code>from collections import Counter, OrderedDict
from typing import Optional
import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn.functional as F  # noqa
from deepchain import log
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from torch import Tensor, nn
num_layers=2
hidden_size=256
from torch.utils.data import DataLoader, TensorDataset

def classification_dataloader_from_numpy(
    x: np.ndarray, y: np.array, batch_size: int = 32
) -&gt; DataLoader:
    &quot;&quot;&quot;Build a dataloader from numpy for classification problem

    This dataloader is use only for classification. It detects automatically the class of
    the problem (binary or multiclass classification)
    Args:
        x (np.ndarray): [description]
        y (np.array): [description]
        batch_size (int, optional): [description]. Defaults to None.

    Returns:
        DataLoader: [description]
    &quot;&quot;&quot;
    n_class: int = len(np.unique(y))
    if n_class &gt; 2:
        log.info(&quot;This is a classification problem with %s classes&quot;, n_class)
    else:
        log.info(&quot;This is a binary classification problem&quot;)

    # y is float for binary classification, int for multiclass
    y_tensor = torch.tensor(y).long() if len(np.unique(y)) &gt; 2 else torch.tensor(y).float()
    tensor_set = TensorDataset(torch.tensor(x).float(), y_tensor)
    loader = DataLoader(tensor_set, batch_size=batch_size)
    return loader
</code></pre>
<pre><code>class RNN(pl.LightningModule):
     
    &quot;&quot;&quot;A `pytorch` based deep learning model&quot;&quot;&quot;
    def __init__(self, input_shape: int, n_class: int, num_layers,  n_neurons: int = 128, lr: float = 1e-3):
        super(RNN,self).__init__()
        self.lr = lr
        self.n_neurons=n_neurons
        self.num_layers=num_layers
        self.input_shape = input_shape
        self.output_shape = 1 if n_class &lt;= 2 else n_class
        self.activation = nn.Sigmoid() if n_class &lt;= 2 else nn.Softmax(dim=-1)
        self.lstm = nn.LSTM(self.input_shape, self.n_neurons, num_layers, batch_first=True, bidirectional=True)
        self.fc= nn.Linear(self.n_neurons, self.output_shape)
    def forward(self, x):
        h0=torch.zeros(self.num_layers, x_size(0), self.n_neurons).to(device)
        c0=torch.zeros(self.num_layers, x_size(0), self.n_neurons).to(device)
        out, _=self.lstm(x,(h0, c0))
        out=self.fc(out[:, -1, :])
        return self.fc(x)

    def training_step(self, batch, batch_idx):
        &quot;&quot;&quot;training_step defined the train loop. It is independent of forward&quot;&quot;&quot;
        x, y = batch
        y_hat = self.fc(x).squeeze()
        y = y.squeeze()
        if self.output_shape &gt; 1:
            y_hat = torch.log(y_hat)
        loss = self.loss(y_hat, y)
        self.log(&quot;train_loss&quot;, loss, on_epoch=True, on_step=False)
        return {&quot;loss&quot;: loss}
    def validation_step(self, batch, batch_idx):
        &quot;&quot;&quot;training_step defined the train loop. It is independent of forward&quot;&quot;&quot;
        x, y = batch
        y_hat = self.fc(x).squeeze()
        y = y.squeeze()
        if self.output_shape &gt; 1:
            y_hat = torch.log(y_hat)
        loss = self.loss(y_hat, y)
        self.log(&quot;val_loss&quot;, loss, on_epoch=True, on_step=False)
        return {&quot;val_loss&quot;: loss}
    def configure_optimizers(self):
        &quot;&quot;&quot;(Optional) Configure training optimizers.&quot;&quot;&quot;
        return torch.optim.Adam(self.parameters(),lr=self.lr)
    def compute_class_weight(self, y: np.array, n_class: int):
        &quot;&quot;&quot;Compute class weight for binary/multiple classification
        If n_class=2, only compute weights for the positve class.
        If n&gt;2, compute for all classes.
        Args:
            y ([np.array]):vector of int represented the class
            n_class (int) : number fo class to use
        &quot;&quot;&quot;
        if n_class == 2:
            class_count: typing.Counter = Counter(y)
            cond_binary = (0 in class_count) and (1 in class_count)
            assert cond_binary, &quot;Must have O and 1 class for binary classification&quot;
            weight = class_count[0] / class_count[1]
        else:
            weight = compute_class_weight(class_weight=&quot;balanced&quot;, classes=np.unique(y), y=y)
        return torch.tensor(weight).float()
    def fit(
        self,
        x: np.ndarray,
        y: np.array,
        epochs: int = 10,
        batch_size: int = 32,
        class_weight: Optional[str] = None,
        validation_data: bool = True, 
        **kwargs
    ):
        assert isinstance(x, np.ndarray), &quot;X should be a numpy array&quot;
        assert isinstance(y, np.ndarray), &quot;y should be a numpy array&quot;
        assert class_weight in (
            None,
            &quot;balanced&quot;,
        ), &quot;the only choice available for class_weight is 'balanced'&quot;
        n_class = len(np.unique(y))
        weight = None
        self.input_shape = x.shape[1]
        self.output_shape = 1 if n_class &lt;= 2 else n_class
        self.activation = nn.Sigmoid() if n_class &lt;= 2 else nn.Softmax(dim=-1)
        if class_weight == &quot;balanced&quot;:
            weight = self.compute_class_weight(y, n_class)
        self.loss = nn.NLLLoss(weight) if self.output_shape &gt; 1 else nn.BCELoss(weight)
        if validation_data:
            x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)
            train_loader = classification_dataloader_from_numpy(
                x_train, y_train, batch_size=batch_size
            )
            val_loader = classification_dataloader_from_numpy(x_val, y_val, batch_size=batch_size)
        else:
            train_loader = classification_dataloader_from_numpy(x, y, batch_size=batch_size)
            val_loader = None
        self.trainer = pl.Trainer(max_epochs=epochs, **kwargs)
        self.trainer.fit(self, train_loader, val_loader)
    def predict(self, x):
        &quot;&quot;&quot;Run inference on data.&quot;&quot;&quot;
        if self.output_shape is None:
            log.warning(&quot;Model is not fitted. Can't do predict&quot;)
            return
        return self.forward(x).detach().numpy()
    def save(self, path: str):
        &quot;&quot;&quot;Save the state dict model with torch&quot;&quot;&quot;
        torch.save(self.fc.state_dict(), path)
        log.info(&quot;Save state_dict parameters in model.pt&quot;)
    def load_state_dict(self, state_dict: &quot;OrderedDict[str, Tensor]&quot;, strict: bool = False):
        &quot;&quot;&quot;Load state_dict saved parameters
        Args:
            state_dict (OrderedDict[str, Tensor]): state_dict tensor
            strict (bool, optional): [description]. Defaults to False.
        &quot;&quot;&quot;
        self.fc.load_state_dict(state_dict, strict=strict)
        self.fc.eval()

    mlp = RNN(input_shape=1024, n_neurons=1024, num_layers=2, n_class=2)
mlp.fit(embeddings_train, np.array(y_train),validation_data=(embeddings_test, np.array(y_test)), epochs=30)
mlp.save(&quot;model.pt&quot;)
</code></pre>
<p>These are the errors that are occured. I really need help and i remain at your disposal for further informations.</p>
<p><strong>Error 1</strong></p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-154-e5fde11a675c&gt; in &lt;module&gt;
      1 # init MLP model, train it on the data, then save model
      2 mlp = RNN(input_shape=1024, n_neurons=1024, num_layers=2, n_class=2)
----&gt; 3 mlp.fit(embeddings_train, np.array(y_train),validation_data=(embeddings_test, np.array(y_test)), epochs=30)
      4 mlp.save(&quot;model.pt&quot;)

&lt;ipython-input-153-a8d51af53bb5&gt; in fit(self, x, y, epochs, batch_size, class_weight, validation_data, **kwargs)
    134             val_loader = None
    135         self.trainer = pl.Trainer(max_epochs=epochs, **kwargs)
--&gt; 136         self.trainer.fit(self, train_loader, val_loader)
    137     def predict(self, x):
    138         &quot;&quot;&quot;Run inference on data.&quot;&quot;&quot;

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in fit(self, model, train_dataloader, val_dataloaders, datamodule)
    456         )
    457 
--&gt; 458         self._run(model)
    459 
    460         assert self.state.stopped

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in _run(self, model)
    754 
    755         # dispatch `start_training` or `start_evaluating` or `start_predicting`
--&gt; 756         self.dispatch()
    757 
    758         # plugin will finalized fitting (e.g. ddp_spawn will load trained model)

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in dispatch(self)
    795             self.accelerator.start_predicting(self)
    796         else:
--&gt; 797             self.accelerator.start_training(self)
    798 
    799     def run_stage(self):

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in start_training(self, trainer)
     94 
     95     def start_training(self, trainer: 'pl.Trainer') -&gt; None:
---&gt; 96         self.training_type_plugin.start_training(trainer)
     97 
     98     def start_evaluating(self, trainer: 'pl.Trainer') -&gt; None:

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in start_training(self, trainer)
    142     def start_training(self, trainer: 'pl.Trainer') -&gt; None:
    143         # double dispatch to initiate the training loop
--&gt; 144         self._results = trainer.run_stage()
    145 
    146     def start_evaluating(self, trainer: 'pl.Trainer') -&gt; None:

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_stage(self)
    805         if self.predicting:
    806             return self.run_predict()
--&gt; 807         return self.run_train()
    808 
    809     def _pre_training_routine(self):

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_train(self)
    840             self.progress_bar_callback.disable()
    841 
--&gt; 842         self.run_sanity_check(self.lightning_module)
    843 
    844         self.checkpoint_connector.has_trained = False

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_sanity_check(self, ref_model)
   1105 
   1106             # run eval step
-&gt; 1107             self.run_evaluation()
   1108 
   1109             self.on_sanity_check_end()

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py in run_evaluation(self, on_epoch)
    960                 # lightning module methods
    961                 with self.profiler.profile(&quot;evaluation_step_and_end&quot;):
--&gt; 962                     output = self.evaluation_loop.evaluation_step(batch, batch_idx, dataloader_idx)
    963                     output = self.evaluation_loop.evaluation_step_end(output)
    964 

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop.py in evaluation_step(self, batch, batch_idx, dataloader_idx)
    172             model_ref._current_fx_name = &quot;validation_step&quot;
    173             with self.trainer.profiler.profile(&quot;validation_step&quot;):
--&gt; 174                 output = self.trainer.accelerator.validation_step(args)
    175 
    176         # capture any logged information

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/accelerators/accelerator.py in validation_step(self, args)
    224 
    225         with self.precision_plugin.val_step_context(), self.training_type_plugin.val_step_context():
--&gt; 226             return self.training_type_plugin.validation_step(*args)
    227 
    228     def test_step(self, args: List[Union[Any, int]]) -&gt; Optional[STEP_OUTPUT]:

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py in validation_step(self, *args, **kwargs)
    159 
    160     def validation_step(self, *args, **kwargs):
--&gt; 161         return self.lightning_module.validation_step(*args, **kwargs)
    162 
    163     def test_step(self, *args, **kwargs):

&lt;ipython-input-153-a8d51af53bb5&gt; in validation_step(self, batch, batch_idx)
     78         if self.output_shape &gt; 1:
     79             y_hat = torch.log(y_hat)
---&gt; 80         loss = self.loss(y_hat, y)
     81         self.log(&quot;val_loss&quot;, loss, on_epoch=True, on_step=False)
     82         return {&quot;val_loss&quot;: loss}

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/torch/nn/modules/loss.py in forward(self, input, target)
    611     def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
    612         assert self.weight is None or isinstance(self.weight, Tensor)
--&gt; 613         return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)
    614 
    615 

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/torch/nn/functional.py in binary_cross_entropy(input, target, weight, size_average, reduce, reduction)
   2760         weight = weight.expand(new_size)
   2761 
-&gt; 2762     return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
   2763 
   2764 

RuntimeError: all elements of input should be between 0 and 1
</code></pre>
<p><strong>Error 2</strong></p>
<pre><code>
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-139-b7e8b13763ef&gt; in &lt;module&gt;
      1 # Model evaluation
----&gt; 2 y_pred = mlp(embeddings_val).squeeze().detach().numpy()
      3 model_evaluation_accuracy(np.array(y_val), y_pred)

/opt/conda/envs/bio-transformers/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    887             result = self._slow_forward(*input, **kwargs)
    888         else:
--&gt; 889             result = self.forward(*input, **kwargs)
    890         for hook in itertools.chain(
    891                 _global_forward_hooks.values(),

&lt;ipython-input-136-e2fc535640ab&gt; in forward(self, x)
     55         self.fc= nn.Linear(self.hidden_size, self.output_shape)
     56     def forward(self, x):
---&gt; 57         h0=torch.zeros(self.num_layers, x_size(0), self.hidden_size).to(device)
     58         c0=torch.zeros(self.num_layers, x_size(0), self.hidden_size).to(device)
     59         out, _=self.lstm(x,(h0, c0))

NameError: name 'x_size' is not defined
</code></pre>
",16344180.0,,16344180.0,,2021-07-12 17:17:11,2023-01-19 10:45:28,RuntimeError: all elements of input should be between 0 and 1,<pytorch><pytorch-lightning>,3,4,,,,CC BY-SA 4.0
66498045,1,,,2021-03-05 18:49:49,,7,15014,"<p>I was to set up DDP (distributed data parallel) on a DGX A100 but it doesn't work. Whenever I try to run it simply hangs. My code is super simple just spawning 4 processes for 4 gpus (for the sake of debugging I simply destroy the group immediately but it doesn't even reach there):</p>
<pre><code>def find_free_port():
    &quot;&quot;&quot; https://stackoverflow.com/questions/1365265/on-localhost-how-do-i-pick-a-free-port-number &quot;&quot;&quot;
    import socket
    from contextlib import closing

    with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:
        s.bind(('', 0))
        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        return str(s.getsockname()[1])

def setup_process(rank, world_size, backend='gloo'):
    &quot;&quot;&quot;
    Initialize the distributed environment (for each process).

    gloo: is a collective communications library (https://github.com/facebookincubator/gloo). My understanding is that
    it's a library/API for process to communicate/coordinate with each other/master. It's a backend library.

    export NCCL_SOCKET_IFNAME=eth0
    export NCCL_IB_DISABLE=1

    https://stackoverflow.com/questions/61075390/about-pytorch-nccl-error-unhandled-system-error-nccl-version-2-4-8

    https://pytorch.org/docs/stable/distributed.html#common-environment-variables
    &quot;&quot;&quot;
    if rank != -1:  # -1 rank indicates serial code
        print(f'setting up rank={rank} (with world_size={world_size})')
        # MASTER_ADDR = 'localhost'
        MASTER_ADDR = '127.0.0.1'
        MASTER_PORT = find_free_port()
        # set up the master's ip address so this child process can coordinate
        os.environ['MASTER_ADDR'] = MASTER_ADDR
        print(f&quot;{MASTER_ADDR=}&quot;)
        os.environ['MASTER_PORT'] = MASTER_PORT
        print(f&quot;{MASTER_PORT}&quot;)

        # - use NCCL if you are using gpus: https://pytorch.org/tutorials/intermediate/dist_tuto.html#communication-backends
        if torch.cuda.is_available():
            # unsure if this is really needed
            # os.environ['NCCL_SOCKET_IFNAME'] = 'eth0'
            # os.environ['NCCL_IB_DISABLE'] = '1'
            backend = 'nccl'
        print(f'{backend=}')
        # Initializes the default distributed process group, and this will also initialize the distributed package.
        dist.init_process_group(backend, rank=rank, world_size=world_size)
        # dist.init_process_group(backend, rank=rank, world_size=world_size)
        # dist.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)
        print(f'--&gt; done setting up rank={rank}')
        dist.destroy_process_group()

mp.spawn(setup_process, args=(4,), world_size=4)
</code></pre>
<p>why is this hanging?</p>
<p>nvidia-smi output:</p>
<pre><code>$ nvidia-smi
Fri Mar  5 12:47:17 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.102.04   Driver Version: 450.102.04   CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  A100-SXM4-40GB      On   | 00000000:07:00.0 Off |                    0 |
| N/A   26C    P0    51W / 400W |      0MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  A100-SXM4-40GB      On   | 00000000:0F:00.0 Off |                    0 |
| N/A   25C    P0    52W / 400W |      3MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  A100-SXM4-40GB      On   | 00000000:47:00.0 Off |                    0 |
| N/A   25C    P0    51W / 400W |      3MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  A100-SXM4-40GB      On   | 00000000:4E:00.0 Off |                    0 |
| N/A   25C    P0    51W / 400W |      3MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   4  A100-SXM4-40GB      On   | 00000000:87:00.0 Off |                    0 |
| N/A   30C    P0    52W / 400W |      3MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   5  A100-SXM4-40GB      On   | 00000000:90:00.0 Off |                    0 |
| N/A   29C    P0    53W / 400W |      0MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   6  A100-SXM4-40GB      On   | 00000000:B7:00.0 Off |                    0 |
| N/A   29C    P0    52W / 400W |      0MiB / 40537MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   7  A100-SXM4-40GB      On   | 00000000:BD:00.0 Off |                    0 |
| N/A   48C    P0   231W / 400W |   7500MiB / 40537MiB |     99%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    7   N/A  N/A    147243      C   python                           7497MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>How do I set up ddp in this new machine?</p>
<hr />
<h1>Update</h1>
<p>btw I've successfully installed APEX because some other links say to do that but it still fails. For I did:</p>
<p>went to: <a href=""https://github.com/NVIDIA/apex"" rel=""noreferrer"">https://github.com/NVIDIA/apex</a> follwed their instructions</p>
<pre><code>git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext&quot; ./
</code></pre>
<p>but before the above <a href=""https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6"" rel=""noreferrer"">I had to update gcc</a>:</p>
<pre><code>conda install -c psi4 gcc-5
</code></pre>
<p>it did install it as I successfully imported it but it didn't help.</p>
<hr />
<p>Now it actually prints an error msg:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/miranda9/miniconda3/envs/metalearning/lib/python3.8/multiprocessing/process.py&quot;, line 315, in _bootstrap
    self.run()
  File &quot;/home/miranda9/miniconda3/envs/metalearning/lib/python3.8/multiprocessing/process.py&quot;, line 108, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/home/miranda9/miniconda3/envs/metalearning/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 19, in _wrap
    fn(i, *args)
KeyboardInterrupt
Process SpawnProcess-3:
Traceback (most recent call last):
  File &quot;/home/miranda9/miniconda3/envs/metalearning/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 19, in _wrap
    fn(i, *args)
  File &quot;/home/miranda9/ML4Coq/ml4coq-proj/embeddings_zoo/tree_nns/main_brando.py&quot;, line 252, in train
    setup_process(rank, world_size=opts.world_size)
  File &quot;/home/miranda9/ML4Coq/ml4coq-proj/embeddings_zoo/distributed.py&quot;, line 85, in setup_process
    dist.init_process_group(backend, rank=rank, world_size=world_size)
  File &quot;/home/miranda9/miniconda3/envs/metalearning/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py&quot;, line 436, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File &quot;/home/miranda9/miniconda3/envs/metalearning/lib/python3.8/site-packages/torch/distributed/rendezvous.py&quot;, line 179, in _env_rendezvous_handler
    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)
RuntimeError: connect() timed out.

During handling of the above exception, another exception occurred:
</code></pre>
<hr />
<p>related:</p>
<ul>
<li><a href=""https://github.com/pytorch/pytorch/issues/9696"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/9696</a></li>
<li><a href=""https://discuss.pytorch.org/t/dist-init-process-group-hangs-silently/55347/2"" rel=""noreferrer"">https://discuss.pytorch.org/t/dist-init-process-group-hangs-silently/55347/2</a></li>
<li><a href=""https://forums.developer.nvidia.com/t/imagenet-hang-on-dgx-1-when-using-multiple-gpus/61919"" rel=""noreferrer"">https://forums.developer.nvidia.com/t/imagenet-hang-on-dgx-1-when-using-multiple-gpus/61919</a></li>
<li>apex suggestion: <a href=""https://discourse.mozilla.org/t/hangs-on-dist-init-process-group-in-distribute-py/44686"" rel=""noreferrer"">https://discourse.mozilla.org/t/hangs-on-dist-init-process-group-in-distribute-py/44686</a></li>
<li><a href=""https://github.com/pytorch/pytorch/issues/15638"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/15638</a></li>
<li><a href=""https://github.com/pytorch/pytorch/issues/53395"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/53395</a></li>
</ul>
",1601580.0,,1601580.0,,2021-03-05 19:14:43,2021-10-05 00:41:40,How to solve dist.init_process_group from hanging (or deadlocks)?,<python><machine-learning><pytorch><gpu><multi-gpu>,1,3,0.0,,,CC BY-SA 4.0
68060944,1,,,2021-06-20 23:58:01,,7,1563,"<p>This issue happened when I restarted my cloud notebook server today.
Can be reproduced using the steps below:</p>
<ol>
<li><p>Create a Google Cloud Notebook server with Tensorflow or Pytorch and GPU</p>
</li>
<li><p>After start the server, open the python console:</p>
</li>
</ol>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.cuda.is_available()
True
</code></pre>
<p>CUDA device is available until now.</p>
<ol start=""3"">
<li>Restart the server, and open the notebook again.</li>
</ol>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.cuda.is_available()
/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378098133/work/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() &gt; 0
False
</code></pre>
<p><code>nvidia-smi</code> command works fine.</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |
| N/A   43C    P0    16W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
</code></pre>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Thu_Jun_11_22:26:38_PDT_2020
Cuda compilation tools, release 11.0, V11.0.194
Build cuda_11.0_bu.TC445_37.28540450_0
</code></pre>
<p>This issue can also be reproduced by using TensorFlow. How to fix this kind of case?</p>
",16274952.0,,681865.0,,2021-06-21 00:02:20,2021-06-29 13:13:51,Unable to detect GPU via Tensorflow/Pytorch after restart DLVM,<google-cloud-platform><pytorch><gcp-ai-platform-notebook><google-dl-platform>,1,4,0.0,,,CC BY-SA 4.0
66130547,1,66647424.0,,2021-02-10 03:32:10,,7,7193,"<p>My network includes 'torch.nn.MaxPool3d' which throw a RuntimeError when cudnn deterministic flag is on according to the PyTorch docs (version 1.7 - <a href=""https://pytorch.org/docs/stable/generated/torch.set_deterministic.html#torch.set_deterministic"" rel=""noreferrer"">https://pytorch.org/docs/stable/generated/torch.set_deterministic.html#torch.set_deterministic</a>), however, when I inserted the code 'torch.backends.cudnn.deterministic=True' at the beginning of my code, there was no RuntimeError. Why doesn't that code throw a RuntimeError?
I wonder whether that code guarantees the deterministic computation of my training process.</p>
",15180257.0,,15180257.0,,2021-02-10 07:10:09,2021-03-16 00:10:08,What does the difference between 'torch.backends.cudnn.deterministic=True' and 'torch.set_deterministic(True)'?,<pytorch><deterministic><reproducible-research>,1,0,,,,CC BY-SA 4.0
69127080,1,,,2021-09-10 03:56:15,,7,815,"<p>I am trying to profile my model with pytorch profiler. I used the below code to profile</p>
<pre><code>with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    with record_function(&quot;model_inference&quot;):
        output_batch = self.model(input_batch)
print(prof.key_averages().table(sort_by=&quot;cpu_time_total&quot;, row_limit=10))
</code></pre>
<p>The profiler output is as follows</p>
<pre><code>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                        model_inference         3.17%      83.011ms        63.97%        1.675s        1.675s       0.000us         0.00%     373.844ms     373.844ms             1  
                                            aten::copy_         0.24%       6.333ms        39.93%        1.046s       1.504ms      28.758ms         7.69%      29.035ms      41.777us           695  
                                          cudaHostAlloc        36.02%     943.053ms        36.02%     943.053ms      30.421ms       0.000us         0.00%       0.000us       0.000us            31  
                                       cudaLaunchKernel        35.93%     940.773ms        35.93%     940.773ms      86.619us       0.000us         0.00%       0.000us       0.000us         10861  
                                           aten::repeat         0.04%     979.000us        33.77%     884.170ms      30.489ms       0.000us         0.00%     204.000us       7.034us            29  
                                           aten::conv2d         0.06%       1.481ms         8.71%     228.183ms     695.680us       0.000us         0.00%     145.688ms     444.171us           328  
                                      aten::convolution         0.05%       1.391ms         8.66%     226.702ms     691.165us       0.000us         0.00%     145.688ms     444.171us           328  
                                     aten::_convolution         0.10%       2.742ms         8.61%     225.311ms     686.924us       0.000us         0.00%     145.688ms     444.171us           328  
                                aten::cudnn_convolution         0.53%      13.803ms         8.33%     218.051ms     664.790us     137.822ms        36.87%     137.822ms     420.189us           328  
                                               cudaFree         7.46%     195.373ms         7.46%     195.373ms      48.843ms       0.000us         0.00%       0.000us       0.000us             4  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.618s
Self CUDA time total: 373.844ms
</code></pre>
<p>I notice that a large part of the time (self CPU) is taken by <code>cudaHostAlloc</code> and <code>cudaLaunchKernel</code>. What are these <code>cudaHostAlloc</code> and <code>cudaLaunchKernel</code>? Is it possible to reduce this time? If yes how? Are there any standard operations that I'm missing which is leading to this high time consumption?</p>
<p>PS: I'm new to profiling as such. Kindly let me know if any other information is needed.</p>
",3337089.0,,,,,2021-09-16 13:30:49,How to optimize cudaHostAlloc and cudaLaunchKernel times in pytorch training,<deep-learning><pytorch><profiler>,1,0,0.0,,,CC BY-SA 4.0
75479462,1,,,2023-02-17 01:28:49,,7,370,"<p>EDIT: attaching some code to help generate similar results (appended at end)</p>
<p>I have a really small model with architecture <code>[2, 3, 6]</code> where the hidden layer uses ReLU and it's a softmax activation for multiclass classification. Trained offline and statically quantized later to qint8. What I would like to do now is extract the weights so I can use them on other hardware via matrix multiplication/addition. The problem I'm encountering is it doesn't seem to behave as expected. Take for instance this GraphModule output of state_dict():</p>
<pre class=""lang-py prettyprint-override""><code>OrderedDict([('input_layer_input_scale_0', tensor(0.0039)),
             ('input_layer_input_zero_point_0', tensor(0)),
             ('input_layer.scale', tensor(0.0297)),
             ('input_layer.zero_point', tensor(0)),
             ('input_layer._packed_params.dtype', torch.qint8),
             ('input_layer._packed_params._packed_params',
              (tensor([[-0.1180,  0.1180],
                       [-0.2949, -0.5308],
                       [-3.3029, -7.5496]], size=(3, 2), dtype=torch.qint8,
                      quantization_scheme=torch.per_tensor_affine, scale=0.05898105353116989,
                      zero_point=0),
               Parameter containing:
               tensor([-0.4747, -0.3563,  7.7603], requires_grad=True))),
             ('out.scale', tensor(1.5963)),
             ('out.zero_point', tensor(243)),
             ('out._packed_params.dtype', torch.qint8),
             ('out._packed_params._packed_params',
              (tensor([[  0.4365,   0.4365, -55.4356],
                       [  0.4365,   0.0000,   1.3095],
                       [  0.4365,   0.0000, -13.9680],
                       [  0.4365,  -0.4365,   4.3650],
                       [  0.4365,   0.4365,  -3.0555],
                       [  0.4365,   0.0000,  -1.3095],
                       [  0.4365,   0.0000,   3.0555]], size=(7, 3), dtype=torch.qint8,
                      quantization_scheme=torch.per_tensor_affine, scale=0.43650051951408386,
                      zero_point=0),
               Parameter containing:
               tensor([ 19.2761,  -1.0785,  14.2602, -22.3171,  10.1059,   7.2197, -11.7253],
                      requires_grad=True)))])
</code></pre>
<p>If I directly access the weights the way I think I should like so:</p>
<pre class=""lang-py prettyprint-override""><code>input_weights = np.array(
[[-0.1180,  0.1180],
 [-0.2949, -0.5308],
 [-3.3029, -7.5496]])
inputs_scale = 0.05898105353116989
inputs_zero_point = 0

W1=np.clip(np.round(input_weights/inputs_scale+ inputs_zero_scale), -127, 128)
b1=np.clip(np.round(np.array([-0.4747, -0.3563,  7.7603])/inputs_scale + inputs_zer_scale), -127, 128)

output_weights = np.array(
[[  0.4365,   0.4365, -55.4356],
 [  0.4365,   0.0000,   1.3095],
 [  0.4365,   0.0000, -13.9680],
 [  0.4365,  -0.4365,   4.3650],
 [  0.4365,   0.4365,  -3.0555],
 [  0.4365,   0.0000,  -1.3095],
 [  0.4365,   0.0000,   3.0555]])

outputs_scale=0.43650051951408386
outputs_zero_point=0
W1=np.clip(np.round(output_weights/outputs_scale+ outputs_zero_scale), -127, 128)
W2=np.clip(np.round(np.array([ 19.2761,  -1.0785,  14.2602, -22.3171,  10.1059,   7.2197, -11.7253])/outputs_scale + outputs_zero_scale), -127, 128)
</code></pre>
<p>And then I give it some data:</p>
<pre><code>inputs = np.array(
       [[1.  , 1.  ], # class 0 example
       [1.  , 0.  ], # class 1 example
       [0.  , 1.  ],
       [0.  , 0.  ],
       [0.  , 0.9 ],
       [0.  , 0.75],
       [0.  , 0.25]]) # class 6 example
</code></pre>
<p>Where each row is an example, then I would expect to be able to do matrix multiplication and argmax over the rows to get the result. However, doing that gives me this:</p>
<pre><code>&gt;&gt;&gt; (ReLU((inputs @ W1.T) + b1) @ W2.T + b2).argmax(axis=0)
array([0, 3, 0, 3, 0, 0, 3])
</code></pre>
<p>which is not right.
And when I test accuracy of the quantized model in pytorch it's high enough that it should get all examples correct here. So what am I misunderstanding in terms of accessing these weights/bias?</p>
<p>EDIT: adding code to help people mess around with quantization. Now technically it doesn't matter how this code is generated - an OrderedDict of the quantized model will remain similar. If you want to mess around with it, here is some code to generate a model and quantize it on the XOR problem. Note that I'm using a multiclass classification still to help stick to my original model. Anyway.... here you go...</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import random
import copy
import numpy as np
import tensorflow as tf
import torch.nn.functional as F
from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx
from torch.utils.data import DataLoader, TensorDataset
from pytorch_lightning.callbacks.progress import RichProgressBar
from pytorch_lightning.callbacks.early_stopping import EarlyStopping
import pytorch_lightning as pl

class XORModel(nn.Module):
    def __init__(self, h: int):
        super().__init__()
        self.input_layer = nn.Linear(2, h)
        self.out = nn.Linear(h, 2)
    
    def forward(self, x):
        out = self.input_layer(x)
        out = F.relu(out)
        out = self.out(out)
        return out

class LitModel(pl.LightningModule):
    def __init__(self, model: XORModel):
        super().__init__()
        self.model = model
    
    def forward(self, x):
        return self.model(x)
    
    def _generic_step(self, batch, batch_idx, calc_metric: bool = False):
        x, y = batch
        out = self.model(x)
        if calc_metric:
            with torch.no_grad():
                soft = F.softmax(out, dim=-1)
                metric = (soft.argmax(-1).ravel() == y.ravel()).float().mean()
                self.log('Accuracy', metric, prog_bar=True)
        
        loss = F.cross_entropy(out, y)
        return loss
    
    def training_step(self, batch, batch_idx):
        loss = self._generic_step(batch, batch_idx)
        self.log('train_loss', loss, prog_bar=True)
        return loss
    
    def validation_step(self, batch, batch_idx):
        loss = self._generic_step(batch, batch_idx, calc_metric=True)
        self.log('val_loss', loss, prog_bar=True)
        return loss
    
    def configure_optimizers(self):
        return torch.optim.Adam(self.model.parameters())

def get_accuracy(model: XORModel, seed: int):
    dataset = make_dataset(1000, 1000, False, seed)
    
    model.eval()
    ret = []
    with torch.no_grad():
        for X, y in dataset:
            out = F.softmax(model(X), dim=-1).argmax(-1)
            ret.append((out.cpu().numpy() == y.numpy()).mean())
    model.train()
    return np.array(ret).mean()

def make_dataset(samples: int, batch_size: int, shuffle: bool, seed: int):
    inputs, outputs = [], []
    rng = random.Random(seed)

    for _ in range(samples):
        x0 = rng.randint(0, 1)
        x1 = rng.randint(0, 1)
        y = x0 ^ x1
        inputs.append((x0, x1))
        outputs.append(y)
    
    dataset = TensorDataset(torch.tensor(inputs, dtype=torch.float), torch.tensor(outputs, dtype=torch.long))
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    return dataloader

def quantize_model(model: XORModel):
    model_to_quantize = copy.deepcopy(model)
    model_to_quantize.eval()
    def calibrate(m, data_loader):
        m.eval()
        with torch.no_grad():
            for x in data_loader:
                m(x)

    loader = make_dataset(1000, 1000, False, 0x42)
    sample_inputs = next(iter(loader))[0]
    qconfig_dict = {'': torch.quantization.get_default_qconfig('fbgemm')}
    prepared_model = prepare_fx(model, qconfig_dict)
    calibrate(prepared_model, sample_inputs)
    quantized_model = convert_fx(prepared_model)

    return quantized_model

if __name__ == '__main__':
    train_dataset = make_dataset(10_000, 256, True, 123456)
    val_dataset = make_dataset(500, 64, True, 0xabcd)
    test_dataset = make_dataset(1000, 1000, False, 0x1122)

    model = XORModel(3)
    lit_model = LitModel(model)
    trainer = pl.Trainer(accelerator='cpu', max_epochs=100,
                         callbacks=[
                            RichProgressBar(refresh_rate=50),
                            EarlyStopping(monitor='val_loss', mode='min', patience=3)
                         ])
    
    trainer.fit(lit_model, train_dataset, val_dataset)
    qmodel = quantize_model(lit_model.model)
    print('accuracy of model', get_accuracy(model, 0xbeef))  # prints 1
    print('accuray of qmodel', get_accuracy(qmodel, 0xbeef)) # prints 1
    
</code></pre>
<p>Now assuming you save off the qmodel for later, you can look at the parameters similar to how I do by calling <code>qmodel.state_dict()</code></p>
",2599709.0,,4286568.0,,2023-02-25 12:50:40,2023-02-26 16:34:56,How do I extract the weights of my quantized model for use on hardware?,<python><tensorflow><deep-learning><pytorch><quantization>,3,5,,,,CC BY-SA 4.0
69730835,1,,,2021-10-26 22:55:10,,7,712,"<p>I was trying to create a pytorch distributed data laoder with torchmeta but it failed with a deadlock:</p>
<pre class=""lang-py prettyprint-override""><code>python ~/ultimate-utils/tutorials_for_myself/my_torchmeta/torchmeta_ddp.py

test_basic_ddp_example

ABOUT TO SPAWN WORKERS (via mp.spawn)
-&gt; started ps with rank=0
-&gt; rank=0
-&gt; mp.current_process()=&lt;SpawnProcess name='SpawnProcess-1' parent=54167 started&gt;
-&gt; os.getpid()=54171
device=device(type='cpu')
----&gt; setting up rank=0 (with world_size=4)
---&gt; MASTER_ADDR='127.0.0.1'
---&gt; 57813
---&gt; backend='gloo'
-&gt; started ps with rank=2
-&gt; rank=2
-&gt; mp.current_process()=&lt;SpawnProcess name='SpawnProcess-3' parent=54167 started&gt;
-&gt; os.getpid()=54173
device=device(type='cpu')
----&gt; setting up rank=2 (with world_size=4)
---&gt; MASTER_ADDR='127.0.0.1'
---&gt; 57813
---&gt; backend='gloo'
-&gt; started ps with rank=1
-&gt; rank=1
-&gt; mp.current_process()=&lt;SpawnProcess name='SpawnProcess-2' parent=54167 started&gt;
-&gt; os.getpid()=54172
device=device(type='cpu')
----&gt; setting up rank=1 (with world_size=4)
---&gt; MASTER_ADDR='127.0.0.1'
---&gt; 57813
---&gt; backend='gloo'
-&gt; started ps with rank=3
-&gt; rank=3
-&gt; mp.current_process()=&lt;SpawnProcess name='SpawnProcess-4' parent=54167 started&gt;
-&gt; os.getpid()=54174
device=device(type='cpu')
----&gt; setting up rank=3 (with world_size=4)
---&gt; MASTER_ADDR='127.0.0.1'
---&gt; 57813
---&gt; backend='gloo'
[W ProcessGroupGloo.cpp:684] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[W ProcessGroupGloo.cpp:684] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[W ProcessGroupGloo.cpp:684] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
[W ProcessGroupGloo.cpp:684] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())
----&gt; done setting up rank=0
----&gt; done setting up rank=2
----&gt; done setting up rank=3
----&gt; done setting up rank=1
about to create model
about to create model
about to create model
about to create model
done creating ddp model
about to create torch meta data loader
about to get datasets
here
done creating ddp model
about to create torch meta data loader
about to get datasets
done creating ddp model
about to create torch meta data loader
done creating ddp model
about to create torch meta data loader
about to get datasets
about to get datasets
here
here
here

</code></pre>
<p>why does this happen?</p>
<p>whole code: <a href=""https://github.com/brando90/ultimate-utils/blob/master/tutorials_for_myself/my_torchmeta/torchmeta_ddp.py"" rel=""noreferrer"">https://github.com/brando90/ultimate-utils/blob/master/tutorials_for_myself/my_torchmeta/torchmeta_ddp.py</a>
ref: <a href=""https://github.com/tristandeleu/pytorch-meta/issues/116"" rel=""noreferrer"">https://github.com/tristandeleu/pytorch-meta/issues/116</a></p>
<hr />
<pre class=""lang-py prettyprint-override""><code>#%%
&quot;&quot;&quot;
test a basic DDP example
&quot;&quot;&quot;
from argparse import Namespace

import torch
from torch import nn

import torch.multiprocessing as mp
from torch.utils.data import DataLoader

# from meta_learning.base_models.learner_from_opt_as_few_shot_paper import get_learner_from_args
from uutils.torch_uu.models.learner_from_opt_as_few_shot_paper import get_learner_from_args
from uutils.torch_uu import process_meta_batch
from uutils.torch_uu.dataloaders import get_distributed_dataloader_miniimagenet_torchmeta, get_args_for_mini_imagenet
from uutils.torch_uu.distributed import print_process_info, print_gpu_info, setup_process, move_model_to_ddp, \
    cleanup, find_free_port


def get_dist_dataloader_torch_meta_mini_imagenet(args) -&gt; dict[str, DataLoader]:
    dataloaders: dict[str, DataLoader] = get_distributed_dataloader_miniimagenet_torchmeta(args)
    return dataloaders

def run_parallel_training_loop(rank: int, args: Namespace):
    &quot;&quot;&quot;
    Run torchmeta examples with a distributed dataloader.

    This should distribute the following loop:
    for batch_idx, batch in enumerate(dataloader['train']):
        print(f'{batch_idx=}')
        spt_x, spt_y, qry_x, qry_y = process_meta_batch(args, batch)
        print(f'Train inputs shape: {spt_x.size()}')  # (2, 25, 3, 28, 28)
        print(f'Train targets shape: {spt_y.size()}'.format(spt_y.shape))  # (2, 25)

        print(f'Test inputs shape: {qry_x.size()}')  # (2, 75, 3, 28, 28)
        print(f'Test targets shape: {qry_y.size()}')  # (2, 75)
        break

    Note:
        usual loop for ddp looks as follows:

    for i, batch in enumerate(train_loader):
        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)
        if rank == 0:
            print(f'{loss=}')

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()  # When the backward() returns, param.grad already contains the synchronized gradient tensor.
        optimizer.step()
    &quot;&quot;&quot;
    print(f'-&gt; started ps with {rank=}')
    args.rank = rank
    print_process_info(args.rank)
    print_gpu_info()
    args.gpu = rank
    setup_process(args, rank, master_port=args.master_port, world_size=args.world_size)

    # get ddp model
    print('about to create model')
    # args.Din, args.Dout = 10, 10
    # model = nn.Linear(args.Din, args.Dout)
    model = get_learner_from_args(args)
    model = move_model_to_ddp(rank, args, model)
    criterion = nn.CrossEntropyLoss().to(args.gpu)
    print('done creating ddp model')

    # can distributed dataloader
    print('about to create torch meta data loader')
    dataloaders: dict[str, DataLoader] = get_distributed_dataloader_miniimagenet_torchmeta(args)
    print('done created distributed data loaders')
    optimizer = torch.optim.SGD(model.parameters(), 1e-4)

    # do training
    print('about to train')
    for batch_idx, batch in enumerate(dataloaders['train']):
        print(f'{batch_idx=}')
        spt_x, spt_y, qry_x, qry_y = process_meta_batch(args, batch)
        outputs = model(spt_x)
        loss = criterion(outputs, spt_y)
        if rank == 0:
            print(f'{loss=}')

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()  # When the backward() returns, param.grad already contains the synchronized gradient tensor.
        optimizer.step()

    # Destroy a given process group, and deinitialize the distributed package
    cleanup(rank)

def hello(rank: int, args):
    print(f'hello {rank=}')

def ddp_example_torchmeta_dataloader_test():
    &quot;&quot;&quot;
    Useful links:
    - https://github.com/yangkky/distributed_tutorial/blob/master/src/mnist-distributed.py
    - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html

    &quot;&quot;&quot;
    print('test_basic_ddp_example')
    # args = Namespace(epochs=3, batch_size=8)
    args = get_args_for_mini_imagenet()
    if torch.cuda.is_available():
        args.world_size = torch.cuda.device_count()
    else:
        args.world_size = 4
    args.master_port = find_free_port()
    print('\nABOUT TO SPAWN WORKERS (via mp.spawn)')
    # mp.spawn(hello, args=(args,), nprocs=args.world_size)
    mp.spawn(run_parallel_training_loop, args=(args,), nprocs=args.world_size)
    print('mp.spawn finished\a')

if __name__ == '__main__':
    print('')
    ddp_example_torchmeta_dataloader_test()
    print('Done\a')
</code></pre>
",1601580.0,,4622046.0,,2021-11-08 17:50:15,2021-12-07 02:14:30,How does one create a distributed data loader with PyTorch's TorchMeta for meta-learning?,<machine-learning><deep-learning><pytorch><meta-learning>,0,0,0.0,,,CC BY-SA 4.0
64173515,1,,,2020-10-02 14:42:33,,7,837,"<p>My dataset depends on a 3GB tensor. This tensor could either be on the CPU or the GPU. The bottleneck of my code is the data loading preprocessing. But I can't add more than a few workers without killing my RAM.</p>
<p>This sounds silly for me: why could each worker receives a copy of the 3GB tensor, when this one is exactly the same across each worker?</p>
<p>Is there any solution for letting the workers access to a single version of this tensor?</p>
<p>Thanks,</p>
",4986615.0,,,,,2022-10-22 15:35:07,Can we share memory between workers in a Pytorch DataLoader?,<pytorch>,2,0,,,,CC BY-SA 4.0
65584330,1,65584479.0,,2021-01-05 18:12:12,,7,962,"<p>This is a follow up question to <a href=""https://stackoverflow.com/q/65038757/3337089"">this question</a>. I want to do the exactly same thing in pytorch. Is it possible to do this? If yes, how?</p>
<pre><code>import torch
image = torch.tensor([[246,  50, 101], [116,   1, 113], [187, 110,  64]])
iy = torch.tensor([[1, 0, 2], [1, 0, 2], [2, 2, 2]])
ix = torch.tensor([[0, 2, 1], [1, 2, 0], [0, 1, 2]])
warped_image = torch.zeros(size=image.shape)
</code></pre>
<p>I need something like <code>torch.add.at(warped_image, (iy, ix), image)</code> that gives the output as</p>
<pre><code>[[  0.   0.  51.]
 [246. 116.   0.]
 [300. 211.  64.]]
</code></pre>
<p>Note that the indices at <code>(0,1)</code> and <code>(1,1)</code> point to the same location <code>(0,2)</code>. So, I want <code>warped_image[0,2] = image[0,1] + image[1,1] = 51</code>.</p>
",3337089.0,,,,,2021-01-05 18:23:57,Add a index selected tensor to another tensor with overlapping indices in pytorch,<numpy><pytorch>,1,0,0.0,,,CC BY-SA 4.0
63629075,1,,,2020-08-28 07:09:06,,7,8973,"<p>I have torch 1.6 and python 3.8. When training OpenNMT, it throws the following error -</p>
<p>OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\Girish\AppData\Local\Programs\Python\Python38\lib\sitepackages\torch\lib\caffe2_detectron_ops.dll&quot; or one of its dependencies.</p>
<p>I checked the folder, the file is present there. I have tried uninstalling torch and reinstalling it, but no help.</p>
<p>Any help will be appreciated. thanks</p>
",14180831.0,,,,,2022-01-22 18:00:26,Error in training opennmt - caffe2_detectron_ops.dll not found,<pytorch><opennmt>,6,1,0.0,,,CC BY-SA 4.0
64987430,1,64989409.0,,2020-11-24 13:21:18,,7,20272,"<p>This example is taken verbatim from the <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"" rel=""nofollow noreferrer"">PyTorch Documentation</a>. Now I do have some background on Deep Learning in general and know that it should be obvious that the <code>forward</code> call represents a forward pass, passing through different layers and finally reaching the end, with 10 outputs in this case, then you take the output of the forward pass and compute the <code>loss</code> using the loss function one defined. Now, I forgot what exactly the output from the <code>forward()</code> pass yields me in this scenario.</p>
<p>I thought that the last layer in a Neural Network should be some sort of activation function like <code>sigmoid()</code> or <code>softmax()</code>, but I did not see these being defined anywhere, furthermore, when I was doing a project now, I found out that <code>softmax()</code> is called later on. So I just want to clarify what exactly is the <code>outputs = net(inputs)</code> giving me, from this <a href=""https://discuss.pytorch.org/t/two-output-nodes-for-binary-classification/58703/2"" rel=""nofollow noreferrer"">link</a>, it seems to me by default the output of a PyTorch model's forward pass is logits?</p>
<pre><code>transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


net = Net()

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        print(outputs)
        break
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
</code></pre>
",10570372.0,,472495.0,,2022-03-02 23:01:26,2022-03-02 23:01:26,What exactly does the forward function output in Pytorch?,<python><machine-learning><neural-network><pytorch>,1,2,0.0,,,CC BY-SA 4.0
72961448,1,,,2022-07-13 05:45:14,,7,1015,"<p>It seems like serialization and deserialization associated with python's multiprocessing limit the benefits of processing data in parallel.</p>
<p>In the following example, I create a custom iterable that returns a numpy array. As the size of the numpy array increases, the data fetching process becomes the bottleneck. This is expected. However, I would expect increasing <code>num_worker</code> and <code>prefetch_factor</code> would reduce this bottleneck by preparing batches in advance. But I do not see this behavior in the example below.</p>
<p>I test two cases where <code>MyIterable</code> returns</p>
<ol>
<li>small object <code>np.array((10, 150))</code></li>
<li>large object <code>np.array((1000, 150))</code></li>
</ol>
<p>The average time to process a batch in both scenarios is as follows:</p>
<pre><code># small np object
avg time per batch for num workers=0: 0.47068126868714444
avg time per batch for num workers=2: 0.20982365206225495
avg time per batch for num workers=4: 0.10560789656221914
avg time per batch for num workers=6: 0.07202646931250456
avg time per batch for num workers=8: 0.05311137337469063
</code></pre>
<pre><code># large np object
avg time per batch for num workers=0: 0.6090951558124971
avg time per batch for num workers=2: 0.4594530961876444
avg time per batch for num workers=4: 0.45023533212543043
avg time per batch for num workers=6: 0.3830978863124983
avg time per batch for num workers=8: 0.3811495694375253
</code></pre>
<p>For the small object, the time for each batch drops as expected when <code>num_workers</code> are increased. But for larger object, it does not change much. I attribute it to the fact the the worker process has to serialize the np object and the main process would then deserialize it. The larger the object, the more time it will take.</p>
<p>However, with large enough <code>num_worker</code> and <code>prefetch_factor</code>, shouldn't the queue in the dataloader be always filled such that data fetching is not the bottleneck?</p>
<p>Moreover, changing the <code>prefetch_factor</code> does not change anything. What is the point of <code>prefetch_factor</code>? The document says the main process pre-loads <code>num_worker * prefetch_factor</code> batches but as you can there is no effect in reducing the bottleneck.</p>
<p>I have added a more detailed step-by-step analysis in this <a href=""https://stackoverflow.com/questions/72959030/how-does-the-queue-in-pytorch-dataloader-work-with-num-workers-2"">question</a> for reference.</p>
<pre><code>import time
import torch
import numpy as np
from time import sleep
from torch.utils.data import DataLoader, IterableDataset


def collate_fn(records):
    # some custom collation function
    return records


class MyIterable(object):
    def __init__(self, n):
        self.n = n
        self.i = 0

    def __iter__(self):
        return self

    def __next__(self):
        if self.i &lt; self.n:
            sleep(0.003125)  # simulates data fetch time
            # return np.random.random((10, 150))  # small data item
            return np.random.random((1000, 150))  # large data item
        else:
            raise StopIteration


class MyIterableDataset(IterableDataset):
    def __init__(self, n):
        super(MyIterableDataset).__init__()
        self.n = n

    def __iter__(self):
        return MyIterable(self.n)


def get_performance_metrics(num_workers):
    ds = MyIterableDataset(n=10000)

    if num_workers == 0:
        dl = torch.utils.data.DataLoader(ds, num_workers=0, batch_size=128, collate_fn=collate_fn)
    else:
        dl = torch.utils.data.DataLoader(ds, num_workers=num_workers, prefetch_factor=4, persistent_workers=True,
                                         batch_size=128, collate_fn=collate_fn,
                                         multiprocessing_context='spawn')
    warmup = 5
    times = []
    t0 = time.perf_counter()
    for i, batch in enumerate(dl):
        sleep(0.05)  # simulates train step
        e = time.perf_counter()
        if i &gt;= warmup:
            times.append(e - t0)
        t0 = time.perf_counter()
        if i &gt;= 20:
            break
    print(f'avg time per batch for num workers={num_workers}: {sum(times) / len(times)}')


if __name__ == '__main__':
    num_worker_options = [0, 2, 4, 6, 8]
    for n in num_worker_options:
        get_performance_metrics(n)
</code></pre>
",4029467.0,,4029467.0,,2022-07-13 17:19:13,2022-07-13 17:19:13,num_worker and prefetch_factor in Pytorch DataLoader do not scale,<pytorch><python-multiprocessing><pytorch-dataloader>,0,0,,,,CC BY-SA 4.0
65011884,1,65176522.0,,2020-11-25 19:59:05,,7,9626,"<p>I wrote this snippet below to try and understand what's going on with these hooks.</p>
<pre class=""lang-py prettyprint-override""><code>class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(10,5)
        self.fc2 = nn.Linear(5,1)
        self.fc1.register_forward_hook(self._forward_hook)
        self.fc1.register_backward_hook(self._backward_hook)
    
    def forward(self, inp):
        return self.fc2(self.fc1(inp))

    def _forward_hook(self, module, input, output):
        print(type(input))
        print(len(input))
        print(type(output))
        print(input[0].shape)
        print(output.shape)
        print()

    def _backward_hook(self, module, grad_input, grad_output):
        print(type(grad_input))
        print(len(grad_input))
        print(type(grad_output))
        print(len(grad_output))
        print(grad_input[0].shape)
        print(grad_input[1].shape)
        print(grad_output[0].shape)
        print()

model = Model()
out = model(torch.tensor(np.arange(10).reshape(1,1,10), dtype=torch.float32))
out.backward()
</code></pre>
<p>Produces output</p>
<pre><code>&lt;class 'tuple'&gt;
1
&lt;class 'torch.Tensor'&gt;
torch.Size([1, 1, 10])
torch.Size([1, 1, 5])

&lt;class 'tuple'&gt;
2
&lt;class 'tuple'&gt;
1
torch.Size([1, 1, 5])
torch.Size([5])
torch.Size([1, 1, 5])
</code></pre>
<p>You can also follow the CNN example <a href=""https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks"" rel=""noreferrer"">here</a>. In fact, it's needed to understand the rest of my question.</p>
<p>I have a few questions:</p>
<ol>
<li><p>I would normally think that <code>grad_input</code> (backward hook) should be the  same shape as <code>output</code> (forward hook) because when we go backwards, the direction is reversed. But the CNN example seems to indicate otherwise. I'm still a bit confused. Which way around is it?</p>
</li>
<li><p>Why are <code>grad_input[0]</code> and <code>grad_output[0]</code> the same shape on my <code>Linear</code> layer here? Regardless of the answer to my question 1, at least one of them should be <code>torch.Size([1, 1, 10])</code> right?</p>
</li>
<li><p>What's with the second element of the tuple <code>grad_input</code>? In the CNN case I copy pasted the example and did <code>print(grad_input[1].size())</code> with output <code>torch.Size([20, 10, 5, 5])</code>. So I presume it's the gradients of the weights. I also ran <code>print(grad_input[2].size())</code> and got <code>torch.Size([20])</code>. So it seemed clear I was looking at the gradients of the biases. But then in my <code>Linear</code> example <code>grad_input</code> is length 2, so I can only access up to <code>grad_input[1]</code>, which seems to be giving me the gradients of the biases. So then where are the gradients of the weights?</p>
</li>
</ol>
<p>In summary, there are two apparent contradictions between the behaviour of the backwards hook in the cases of <code>Conv2d</code> and `Linear' modules. This has left me totally confused about what to expect with this hook.</p>
<p>Thanks for your help!</p>
",4391249.0,,4391249.0,,2020-11-26 09:28:02,2020-12-07 05:21:57,Understanding backward hooks,<pytorch>,1,0,0.0,,,CC BY-SA 4.0
70169219,1,70199786.0,,2021-11-30 12:15:15,,7,2908,"<p>I want to train a custom dataset on using faster_rcnn or mask_rcnn with the Pytorch and Detectron2 .Everything works well but I wanted to know I want to know what are the results I have.</p>
<pre><code>[11/29 20:16:31 d2.utils.events]:  eta: 0:24:04  iter: 19  total_loss: 9.6  loss_cls: 1.5  loss_box_reg: 0.001034  loss_mask: 0.6936  loss_rpn_cls: 6.773  loss_rpn_loc: 0.5983  time: 1.4664  data_time: 0.0702  lr: 4.9953e-06  max_mem: 2447M
</code></pre>
<p>I have this as result and I want to know what all of this means</p>
",17551894.0,,17551894.0,,2021-11-30 14:14:20,2021-12-02 12:49:39,"What is total_loss,loss_cls etc",<python><machine-learning><pytorch><conv-neural-network><detectron>,1,1,,,,CC BY-SA 4.0
72988735,1,72990619.0,,2022-07-15 03:23:52,,7,3931,"<p>I have question about replacing &lt;THC/THC.h&gt; method.
Recently, I'm working on installing different loss functions compiled with cpp and cuda.
However, what I faced was a fatal error of</p>
<pre><code>'THC/THC.h': No such file or directory
</code></pre>
<p>I found out that TH(C) methods were currently deprecated in recent version of pytorch, and was replaced by ATen API (<a href=""https://discuss.pytorch.org/t/question-about-thc-thc-h/147145/8"" rel=""noreferrer"">https://discuss.pytorch.org/t/question-about-thc-thc-h/147145/8</a>).</p>
<p>For sure, downgrading my pytorch version will solve the problem. However, due to my GPU compatibility issue, I have no choice but to modify the script by myself. Therefore, my question can be summarized into follows.</p>
<p><strong>First,</strong> how can I replace codes that have dependency of TH(C) method using ATen API?. Below are codes that I have to modify, replacing those three lines looked enough for my case.</p>
<pre><code>#include &lt;THC/THC.h&gt;
extern THCState *state;
cudaStream_t stream = THCState_getCurrentStream(state);
</code></pre>
<p><strong>Second</strong>, will single modification on cpp file be enough to clear the issue that I'm facing right now? (<em>This is just a minor question, answer on first question will suffice me</em>).</p>
<p>For reference, I attach the github link of the file I'm trying to build (<a href=""https://github.com/sshaoshuai/Pointnet2.PyTorch"" rel=""noreferrer"">https://github.com/sshaoshuai/Pointnet2.PyTorch</a>).</p>
",16342000.0,,681865.0,,2022-07-15 03:54:08,2023-07-01 08:53:02,Replacing THC/THC.h module to ATen/ATen.h module,<c++><pytorch>,1,0,0.0,,,CC BY-SA 4.0
64456843,1,,,2020-10-21 04:47:20,,7,6101,"<p>I am following a code from GitHub that uses Pytorch.
The model is saved using :</p>
<pre class=""lang-py prettyprint-override""><code>model.save(ARGS.working_dir + '/model_%d.ckpt' % (epoch+1)).  
</code></pre>
<p>What is the difference between using <code>.pth</code> and <code>.ckpt</code> in Pytorch?</p>
",14489938.0,,2736559.0,,2020-10-21 04:58:31,2020-10-21 04:58:31,What is the difference between a .ckpt and a .pth file in Pytorch?,<pytorch>,1,0,,,,CC BY-SA 4.0
69393214,1,69414268.0,,2021-09-30 13:36:25,,7,11051,"<h3>Problem Description</h3>
<p>I tried to load image data using a PyTorch custom dataset, however, I received the error message listed below. After its occurrence, I checked the data and found that my image set consists of 2 types of shape (512,512,3) and (1024,1024). My assumption is that the error is related to this.</p>
<p>Note: The code is able to read some of the images but throws the error message for others.</p>
<h3>Questions</h3>
<ol>
<li><p>How should one preprocess such image data for training?</p>
</li>
<li><p>Are there any other reasons for the error message?</p>
</li>
</ol>
<h3>Error message</h3>
<pre><code>KeyError                                  Traceback (most recent call last)
&lt;ipython-input-163-aa3385de8026&gt; in &lt;module&gt;
----&gt; 1 train_features, train_labels = next(iter(train_dataloader))
  2 print(f&quot;Feature batch shape: {train_features.size()}&quot;)
  3 print(f&quot;Labels batch shape: {train_labels.size()}&quot;)
  4 img = train_features[0].squeeze()
  5 label = train_labels[0]

 ~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils  /data/dataloader.py in __next__(self)
519             if self._sampler_iter is None:
520                 self._reset()
521             data = self._next_data()
522             self._num_yielded += 1
523             if self._dataset_kind == _DatasetKind.Iterable and \

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _next_data(self)
1201             else:
1202                 del self._task_info[idx]
1203                 return self._process_data(data)
1204 
1205     def _try_put_index(self):

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)
1227         self._try_put_index()
1228         if isinstance(data, ExceptionWrapper):
1229             data.reraise()
1230         return data
1231 

~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/_utils.py in reraise(self)
423             # have message field
424             raise self.exc_type(message=msg)
425         raise self.exc_type(msg)
426 
427 

KeyError: Caught KeyError in DataLoader worker process 0.
Original Traceback (most recent call last):
File &quot;/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas  /core/indexes/base.py&quot;, line 2898, in get_loc
return self._engine.get_loc(casted_key)
File &quot;pandas/_libs/index.pyx&quot;, line 70, in pandas._libs.index.IndexEngine.get_loc
File &quot;pandas/_libs/index.pyx&quot;, line 101, in pandas._libs.index.IndexEngine.get_loc
File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1032, in    pandas._libs.hashtable.Int64HashTable.get_item
File &quot;pandas/_libs/hashtable_class_helper.pxi&quot;, line 1039, in   pandas._libs.hashtable.Int64HashTable.get_item
KeyError: 16481

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
File &quot;/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py&quot;, line 287, in _worker_loop
data = fetcher.fetch(index)
File &quot;/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 44, in fetch
data = [self.dataset[idx] for idx in possibly_batched_index]
File &quot;/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 44, in &lt;listcomp&gt;
data = [self.dataset[idx] for idx in possibly_batched_index]
File &quot;&lt;ipython-input-161-f38b78d77dcb&gt;&quot;, line 19, in __getitem__
img_path =os.path.join(self.img_dir,self.image_ids[idx])
File &quot;/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/core/series.py&quot;, line 882, in __getitem__
return self._get_value(key)
File &quot;/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/core/series.py&quot;, line 990, in _get_value
loc = self.index.get_loc(label)
File &quot;/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/pandas/core/indexes/base.py&quot;, line 2900, in get_loc
raise KeyError(key) from err
KeyError: 16481
</code></pre>
<h3>Code</h3>
<pre class=""lang-py prettyprint-override""><code>from torchvision.io import read_image
import torch
from torchvision import transforms
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset

class CustomImageDataset(Dataset):
    def __init__(self, dataset, transforms=None, target_transforms=None):
        #self.train_data = pd.read_csv(&quot;Data/train_data.csv&quot;)
        self.image_ids = dataset.image_id
        self.image_labels = dataset.label
        self.img_dir = 'Data/images'
        self.transforms = transforms
        self.target_transforms = target_transforms

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self,idx):
        # image path
        img_path =os.path.join(self.img_dir,self.image_ids[idx])
        # image
        image = read_image(img_path)
        label = self.image_labels[idx]
        # transform image
        if self.transforms:
             image = self.transforms(image)
        # transform target
        if self.target_transforms:
             label = self.target_transforms(label)

    return image, label
</code></pre>
<p><code>train_data</code> is the pandas object of the csv file which has the image id and label information.</p>
<pre class=""lang-py prettyprint-override""><code>from sklearn.model_selection import train_test_split

X_train, X_test = train_test_split(train_data, test_size=0.1, random_state=42)
train_df = CustomImageDataset(X_train)
train_dataloader = torch.utils.data.DataLoader(
    train_df,
    batch_size=64,
    num_workers=1,
    shuffle=True)
</code></pre>
",4343984.0,,20087266.0,,2023-04-16 08:02:26,2023-06-15 13:59:49,"PyTorch: ""KeyError: Caught KeyError in DataLoader worker process 0.""",<machine-learning><image-processing><pytorch><dataset>,2,0,0.0,,,CC BY-SA 4.0
64022697,1,,,2020-09-23 07:03:35,,7,1314,"<p>Suppose I have a matrix <code>src</code> with shape <code>(5, 3)</code> and a boolean matrix <code>adj</code> with shape  <code>(5, 5)</code> as follow,</p>
<pre><code>src = tensor([[ 0,  1,  2],
              [ 3,  4,  5],
              [ 6,  7,  8],
              [ 9, 10, 11],
              [12, 13, 14]])
</code></pre>
<p>and</p>
<pre><code>adj = tensor([[1, 0, 1, 1, 0],
              [0, 1, 1, 1, 0],
              [1, 1, 0, 1, 1],
              [1, 1, 1, 0, 0],
              [0, 0, 1, 0, 1]])
</code></pre>
<p>We can take each row in <code>src</code> as one node embedding, and regard each row in <code>adj</code> as the indicator of which nodes are the neighborhood.</p>
<p>My goal is to operate a max-pooling among all neighborhood node embeddings for each node in <code>src</code>.
For example, as the neighborhood nodes (including itself) for the 0-th node is <code>0, 2, 3</code>, thus we compute a max-pooling on <code>[0, 1, 2]</code>, <code>[6, 7, 8]</code>, <code>[ 9, 10, 11]</code> and lead an updated embedding <code>[ 9, 10, 11]</code> to update 0-th node in <code>src_update</code>.</p>
<p>A simple solution I wrote is</p>
<pre><code>src_update = torch.zeros_like(src)
for index in range(adj.size(0)):
    list_of_non_zero = adj[index].nonzero().view(-1)
    mat_non_zero = torch.index_select(src, 0, list_of_non_zero)
    src_update[index] = torch.sum(mat_non_zero, dim=0)
</code></pre>
<p>And <code>src_update</code> is updated as:</p>
<pre><code>tensor([[ 9, 10, 11],
        [ 9, 10, 11],
        [12, 13, 14],
        [ 6,  7,  8],
        [12, 13, 14]])
</code></pre>
<p>Although it works, it runs very slowly and doesn't look elegant!
Any suggestions to improve it for <strong>better efficiency</strong>?</p>
<p>In addition, if both <code>src</code> and <code>adj</code> are appended with <strong>batches</strong> (<code>(batch, 5, 3)</code>, <code>(batch, 5, 5)</code>), how to make it works?</p>
",14325590.0,,6331369.0,,2021-08-09 15:26:27,2022-03-18 06:18:43,Max-pooling with complex masks in PyTorch,<pytorch>,2,1,,,,CC BY-SA 4.0
63224426,1,,,2020-08-03 06:00:15,,7,5231,"<p>I want to use cross-validation against the official Optuna and pytorch-based sample code (<a href=""https://github.com/optuna/optuna/blob/master/examples/pytorch_simple.py"" rel=""noreferrer"">https://github.com/optuna/optuna/blob/master/examples/pytorch_simple.py</a>).</p>
<p>I thought about splitting the data for cross-validation and trying parameter tuning for each fold, but it seems that the average accuracy of each parameter cannot be obtained because the parameters that can be checked in study.trials_dataframe() are different each time.</p>
",13772431.0,,13772431.0,,2020-08-03 06:05:49,2020-08-04 07:30:14,How can I cross-validate by Pytorch and Optuna,<pytorch><optuna>,1,1,0.0,,,CC BY-SA 4.0
63232732,1,63415069.0,,2020-08-03 15:51:48,,7,1603,"<p>I have:</p>
<pre><code>        context = torch.tensor(context, dtype=torch.long, device=self.device)
        context = context.unsqueeze(0)
        generated = context
        with torch.no_grad():
            past_outputs = None
            for i in trange(num_words):
                print(i, num_words)
                inputs = {&quot;input_ids&quot;: generated}

                outputs, past_outputs = self.model(
                    **inputs,
                    past=past_outputs
                )
                next_token_logits = outputs[
                    0, -1, :] / (temperature if temperature &gt; 0 else 1.0)

                # reptition penalty from CTRL
                # (https://arxiv.org/abs/1909.05858)
                for _ in set(generated.view(-1).tolist()):
                    next_token_logits[_] /= repetition_penalty

                filtered_logits = top_k_top_p_filtering(
                    next_token_logits, top_k=top_k, top_p=top_p)
                if temperature == 0:  # greedy sampling:
                    next_token = torch.argmax(filtered_logits).unsqueeze(0)
                else:
                    next_token = torch.multinomial(
                        F.softmax(filtered_logits, dim=-1), num_samples=1)

                generated = torch.cat(
                    (generated, next_token.unsqueeze(0)), dim=1)
</code></pre>
<p>This works for the first iteration, but then I get an error for the next iteration:</p>
<pre><code>  File &quot;/Users/shamoon/Sites/wordblot/packages/ml-server/generator.py&quot;, line 143, in sample_sequence
    past=past_outputs
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/transformers/modeling_gpt2.py&quot;, line 601, in forward
    output_hidden_states=output_hidden_states,
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/transformers/modeling_gpt2.py&quot;, line 470, in forward
    position_embeds = self.wpe(position_ids)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/module.py&quot;, line 550, in __call__
    result = self.forward(*input, **kwargs)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/modules/sparse.py&quot;, line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File &quot;/Users/shamoon/.local/share/virtualenvs/ml-server-EdimT5-E/lib/python3.7/site-packages/torch/nn/functional.py&quot;, line 1724, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>Is there something I'm doing wrong?</p>
",239879.0,,239879.0,,2020-08-03 20:30:05,2020-08-14 14:49:16,How to use the past with HuggingFace Transformers GPT-2?,<python><pytorch><huggingface-transformers>,2,8,,,,CC BY-SA 4.0
62872413,1,62876076.0,,2020-07-13 08:52:56,,7,9055,"<pre><code>from albumentations.pytorch.transforms import ToTensorV2
</code></pre>
<p>I used the above code, and it doesn't work.</p>
",13857925.0,,,,,2021-06-10 13:33:14,Can't import ToTensorV2 in Colab,<pytorch><google-colaboratory>,3,0,,,,CC BY-SA 4.0
65824714,1,65953473.0,,2021-01-21 09:41:00,,7,6704,"<h2>❔Question</h2>
<p>Hi, I have successfully trained a custom model based on YOLOv5s and converted the model to TFlite. I feel silly asking, but how do you use the output data?</p>
<p>I get as output:</p>
<ul>
<li>StatefulPartitionedCall: 0 = [1,25200,7]
from the converted YOLOv5 model
<a href=""https://user-images.githubusercontent.com/58934176/105021013-554a2600-5a48-11eb-90ef-6656a7fe63bb.png"" rel=""noreferrer"">Netron YOLOv5s.tflite model</a></li>
</ul>
<p>But I expect an output like:</p>
<ul>
<li>StatefulPartitionedCall:3 = [1, 10, 4] # boxes</li>
<li>StatefulPartitionedCall:2 = [1, 10] # classes</li>
<li>StatefulPartitionedCall:1 = [1, 10] #scores</li>
<li>StatefulPartitionedCall:0 = [1] #count
(this one is from a tensorflow lite mobilenet model (trained to give 10 output data, default for tflite))
<a href=""https://user-images.githubusercontent.com/58934176/105020926-3d72a200-5a48-11eb-8f48-406ae1524d24.png"" rel=""noreferrer"">Netron mobilenet.tflite model</a></li>
</ul>
<p>It may also be some other form of output, but I honestly have no idea how to get the boxes, classes, scores from a [1,25200,7] array.
(on 15-January-2021 I updated pytorch, tensorflow and yolov5 to the latest version)</p>
<p>The data contained in the [1, 25200, 7] array can be found in this file: <a href=""https://github.com/ultralytics/yolov5/files/5848088/outputdata.float.txt"" rel=""noreferrer"">outputdata.txt</a></p>
<pre><code>0.011428807862102985, 0.006756599526852369, 0.04274776205420494, 0.034441519528627396, 0.00012877583503723145, 0.33658933639526367, 0.4722323715686798
0.023071227595210075, 0.006947836373001337, 0.046426184475421906, 0.023744791746139526, 0.0002465546131134033, 0.29862138628959656, 0.4498370885848999
0.03636947274208069, 0.006819264497607946, 0.04913407564163208, 0.025004519149661064, 0.00013208389282226562, 0.3155967593193054, 0.4081345796585083
0.04930267855525017, 0.007249316666275263, 0.04969717934727669, 0.023645592853426933, 0.0001222355494974181, 0.3123127520084381, 0.40113094449043274
...
</code></pre>
<p>Should I add a Non Max Suppression or something else, can someone help me please? (<a href=""https://github.com/ultralytics/yolov5/issues/1981"" rel=""noreferrer"">github YOLOv5 #1981</a>)</p>
",15050934.0,,,,,2022-01-21 18:14:30,Process output data from YOLOv5 TFlite,<tensorflow><pytorch><tensorflow-lite><yolov5>,1,1,0.0,,,CC BY-SA 4.0
63092994,1,,,2020-07-25 20:01:00,,7,1864,"<p>PyTorch doesn't seem to have documentation for <code>tensor.stride()</code>.
Can someone confirm my understanding?</p>
<p>My questions are three-fold.</p>
<ol>
<li><p>Stride is for accessing an element in the storage. So stride size will be the same as the dimension of the tensor. Correct?</p>
</li>
<li><p>For each dimension, the corresponding element of stride tells how much it takes to move along the 1-dimensional storage. Correct?</p>
</li>
</ol>
<p>For example:</p>
<pre><code>In [15]: x = torch.arange(1,25)

In [16]: x
Out[16]:
tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18
, 19, 20, 21, 22, 23, 24])

In [17]: a = x.view(4,3,2)

In [18]: a
Out[18]:
tensor([[[ 1,  2],
         [ 3,  4],
         [ 5,  6]],

        [[ 7,  8],
         [ 9, 10],
         [11, 12]],

        [[13, 14],
         [15, 16],
         [17, 18]],

        [[19, 20],
         [21, 22],
         [23, 24]]])

In [20]: a.stride()
Out[20]: (6, 2, 1)
</code></pre>
<ol start=""3"">
<li>How does having this information help perform tensor operations efficiently? Basically this is showing the memory layout. So how does it help?</li>
</ol>
",3907250.0,,3907250.0,,2020-07-26 01:27:20,2020-07-26 01:27:20,pytorch tensor stride - how it works,<numpy><pytorch><tensor><stride>,0,3,,,,CC BY-SA 4.0
63970104,1,63970114.0,,2020-09-19 14:57:29,,7,8030,"<p>I would like to check if model is on CUDA. How to do that?</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torchvision
model = torchvision.models.resnet18()
model.to('cuda')
</code></pre>
<p>Seams that <code>model.is_cuda()</code> is not working.</p>
",5884955.0,,,,,2020-09-19 14:58:32,How to check if model in on CUDA?,<pytorch>,1,0,0.0,,,CC BY-SA 4.0
63968868,1,63969083.0,,2020-09-19 12:35:24,,7,13039,"<p>I tried searching for the documentation online but I can't find anything that gives me an answer. What does <code>.numpy()</code> function do? The example code given is:</p>
<pre><code>y_true = []
for X_batch, y_batch in mnist_test:
    y_true.append(y_batch.numpy()[0].tolist())
</code></pre>
",11303290.0,,4757715.0,,2020-09-19 13:11:41,2021-07-23 14:47:57,What does the .numpy() function do?,<numpy><tensorflow><pytorch>,3,0,,,,CC BY-SA 4.0
63533237,1,63533320.0,,2020-08-22 06:28:47,,7,12687,"<p>On my machine i can't &quot;pip install torch&quot; - i get infamous &quot;single source externally managed error&quot; - i could not fix it and used &quot;conda install torch&quot; from anaconda.</p>
<p>Still, checking version is easy - <code>torch.__version__</code></p>
<p>But how to see where is it installed -the home dir of torch?
Suppose if I had had both torches installed via pip and conda - how to know which one is used in a project?</p>
<pre><code>import torch
print(torch__version__)
</code></pre>
",898042.0,,,,,2022-05-30 14:17:08,how to see where exactly torch is installed pip vs conda torch installation,<python><pytorch><conda><torch>,3,1,0.0,,,CC BY-SA 4.0
64197754,1,64199479.0,,2020-10-04 17:29:49,,7,13723,"<p>I'd like to randomly rotate an image tensor (B, C, H, W) around it's center (2d rotation I think?). I would like to avoid using NumPy and Kornia, so that I basically only need to import from the torch module. I'm also not using <code>torchvision.transforms</code>, because I need it to be autograd compatible. Essentially I'm trying to create an autograd compatible version of <code>torchvision.transforms.RandomRotation()</code> for visualization techniques like DeepDream (so I need to avoid artifacts as much as possible).</p>
<pre><code>import torch
import math
import random
import torchvision.transforms as transforms
from PIL import Image


# Load image
def preprocess_simple(image_name, image_size):
    Loader = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor()])
    image = Image.open(image_name).convert('RGB')
    return Loader(image).unsqueeze(0)
    
# Save image   
def deprocess_simple(output_tensor, output_name):
    output_tensor.clamp_(0, 1)
    Image2PIL = transforms.ToPILImage()
    image = Image2PIL(output_tensor.squeeze(0))
    image.save(output_name)


# Somehow rotate tensor around it's center
def rotate_tensor(tensor, radians):
    ...
    return rotated_tensor

# Get a random angle within a specified range 
r_degrees = 5
angle_range = list(range(-r_degrees, r_degrees))
n = random.randint(angle_range[0], angle_range[len(angle_range)-1])

# Convert angle from degrees to radians
ang_rad = angle * math.pi / 180


# test_tensor = preprocess_simple('path/to/file', (512,512))
test_tensor = torch.randn(1,3,512,512)


# Rotate input tensor somehow
output_tensor = rotate_tensor(test_tensor, ang_rad)


# Optionally use this to check rotated image
# deprocess_simple(output_tensor, 'rotated_image.jpg')
</code></pre>
<p>Some example outputs of what I'm trying to accomplish:</p>
<p><a href=""https://i.stack.imgur.com/hiZv7s.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/hiZv7s.jpg"" alt=""First example of rotated image"" /></a> <a href=""https://i.stack.imgur.com/svOqhs.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/svOqhs.jpg"" alt=""Second example of rotated image"" /></a></p>
",6676139.0,,,,,2022-03-15 01:42:52,How do I rotate a PyTorch image tensor around it's center in a way that supports autograd?,<python><rotation><pytorch><image-rotation><rotational-matrices>,3,3,0.0,,,CC BY-SA 4.0
68229246,1,68474648.0,,2021-07-02 17:31:32,,7,18802,"<p>Tried to load training data with pytorch torch.datasets.ImageFolder in Colab.</p>
<pre><code>transform = transforms.Compose([transforms.Resize(400),
                                transforms.ToTensor()])
dataset_path = 'ss/'
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)
</code></pre>
<p>I encountered the following error :</p>
<pre><code>---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
&lt;ipython-input-27-7abcc1f434b1&gt; in &lt;module&gt;()
      2                                 transforms.ToTensor()])
      3 dataset_path = 'ss/'
----&gt; 4 dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
      5 dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

3 frames
/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in make_dataset(directory, class_to_idx, extensions, is_valid_file)
    100         if extensions is not None:
    101             msg += f&quot;Supported extensions are: {', '.join(extensions)}&quot;
--&gt; 102         raise FileNotFoundError(msg)
    103 
    104     return instances

FileNotFoundError: Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp
</code></pre>
<p>My Dataset folder contains a subfolder with many training images in png format, still the ImageFolder can't access them.</p>
",16367454.0,,16367454.0,,2021-07-03 12:03:43,2023-05-18 21:06:53,pytorch torchvision.datasets.ImageFolder FileNotFoundError: Found no valid file for the classes .ipynb_checkpoints,<machine-learning><computer-vision><pytorch><torchvision><pytorch-dataloader>,4,0,,,,CC BY-SA 4.0
67416496,1,,,2021-05-06 10:39:25,,7,4872,"<p>EDIT:  This is not about the general <code>__getitem__</code> method but the usage of <code>__getitem__</code> in the Pytorch Dataset-subclass, as @dataista correctly states.</p>
<p>I'm trying to implement the usage of Pytorchs Dataset-class.
The guide e.g <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"" rel=""nofollow noreferrer"">here</a> is really good, but I struggle to figure out Pytorch requirements for the return value of <code>__getitem__</code>. In the <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"" rel=""nofollow noreferrer"">Pytorch documentation</a> I cannot find anything about what it should return; is it any object which is iterable with size 2 e.g <code>[sample,target], (sample,target)</code>? In some guides they return a dict, but they do not specify if it <em>has</em> to be a dict which is returned.</p>
",6224975.0,,6224975.0,,2022-09-20 07:05:56,2022-09-20 07:05:56,Does pytorch Dataset.__getitem__ have to return a dict?,<python><pytorch>,1,6,,2021-05-06 21:02:06,,CC BY-SA 4.0
69826153,1,,,2021-11-03 13:56:43,,7,8220,"<p>I go to pytorch site and take this</p>
<pre><code>pip3 install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio===0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
</code></pre>
<p>I have windows 10 ,Python version is 3.10 ,CUDA version is 11.5</p>
<p>And I get this error</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement
torch==1.10.0+cu113 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch==1.10.0+cu113</p>
</blockquote>
<p>I really struggled and tried to solve it, Please help.</p>
",16737280.0,,1695960.0,,2021-11-03 14:00:22,2022-02-23 10:32:07,"Can't install Pytorch in Pycharm terminal, Python 3.10 .win 10",<pip><pytorch><pycharm>,2,0,0.0,,,CC BY-SA 4.0
62961627,1,65417726.0,,2020-07-17 20:52:48,,7,18050,"<p>When I load the BERT pretrained model online I get this error <code>OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory uncased_L-12_H-768_A-12 or 'from_tf' set to False</code> what should I do?</p>
",11777143.0,,3448527.0,,2020-07-18 14:59:48,2021-04-09 11:00:16,"OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index']",<python><tensorflow><pytorch><bert-language-model>,3,0,,,,CC BY-SA 4.0
65565461,1,65567587.0,,2021-01-04 15:42:54,,7,6386,"<p>Given a tensor:</p>
<pre><code>A = torch.tensor([2., 3., 4., 5., 6., 7.])
</code></pre>
<p>Then, give each element in <code>A</code> an id:</p>
<pre><code>id = torch.arange(A.shape[0], dtype = torch.int)   # tensor([0,1,2,3,4,5])
</code></pre>
<p>In other words, id of <code>2.</code> in <code>A</code> is 0 and  id of <code>3.</code> in <code>A</code> is 1:</p>
<pre><code>2. -&gt; 0
3. -&gt; 1
4. -&gt; 2
5. -&gt; 3
6. -&gt; 4
7. -&gt; 5
</code></pre>
<p>Then, I have a new tensor:</p>
<pre><code>B = torch.tensor([3., 6., 6., 5., 4., 4., 4.])
</code></pre>
<p>In pytorch, is there any way in Pytorch to map each element in B to id?
In other words, I want to obtain <code>tensor([1, 4, 4, 3, 2, 2, 2])</code>, in which each element is id of the element in <code>B</code>.</p>
",14876784.0,,,,,2022-11-15 09:49:17,How to map element in pytorch tensor to id?,<python-3.x><pytorch>,4,0,0.0,,,CC BY-SA 4.0
66909581,1,67410963.0,,2021-04-01 18:11:25,,7,7970,"<p>since Pytorch released the ROCm version, which enables me to use other gpus than nvidias, how can I select my radeon gpu as device in python?
Obviously, code like device = torch.cuda.is_available or device = torch.device(&quot;cuda&quot;) is not working. Thanks for any help. :)</p>
",13096099.0,,,,,2021-05-06 02:13:32,PyTorch ROCm is out - How to select Radeon GPU as device,<pytorch>,1,0,,,,CC BY-SA 4.0
64831567,1,64831844.0,,2020-11-14 06:42:06,,7,2399,"<p>Keras has an option to force the weights of the learned model to be positive:</p>
<pre><code>tf.keras.constraints.NonNeg()
</code></pre>
<p>But I couldn't find the equivalent of this in pytorch, does anyone know how can I force my linear model's weights to be all positives?</p>
<p>Tried asking this on other forums but the answers were not helpful.</p>
<p>Let's say I have a very simple linear model as shown below, how should I change it?</p>
<pre><code>class Classifier(nn.Module):

    def __init__(self,input , n_classes):
        super(Classifier, self).__init__()

        self.classify = nn.Linear( input  , n_classes)

     def forward(self, h ):

        final = self.classify(h)
        return final
</code></pre>
<p>I want to do exactly what the NonNeg() does but in pytorch, don't want to change what its doing.</p>
<p>This is the implementation of NonNeg in keras:</p>
<pre><code>class NonNeg(Constraint):
    &quot;&quot;&quot;Constrains the weights to be non-negative.
    &quot;&quot;&quot;

    def __call__(self, w):
        w *= K.cast(K.greater_equal(w, 0.), K.floatx())
        return w
</code></pre>
",9557861.0,,13302.0,,2020-11-14 07:23:13,2023-06-13 07:56:00,What is the equivalent of keras NonNeg weight constraint?,<keras><pytorch><torch>,3,0,,,,CC BY-SA 4.0
63674120,1,,,2020-08-31 15:54:57,,7,4588,"<p>PyTorch Dataloader hangs when num_workers &gt; 0. The code hangs with only about <code>500 M</code> GPU memory usage.</p>
<p>System info: <code>NVIDIA-SMI 418.56 Driver Version: 418.56  CUDA Version: 10.1</code>.
The same issue appears with pytorch1.5 or pytorch1.6, codes are run in anaconda envs.</p>
<p>Note that this error appears when I run my script in terminal as  <code>python main.py</code>, but when I
debug the same code on Pycharm or VScode, or when I run the same code (in terminal) on
other machines, everything goes fine.
Any idea about the reason for this?</p>
<p>Here is the trace when I <code>ctrl c</code> the code in terminal:</p>
<pre><code>  File &quot;train.py&quot;, line 226, in main
    train_domain_adaptation(model, source_loader, target_loader, val_loader,
  File &quot;/home/zhangyu/codes/person_seg/IntraDA/ADVENT/advent/domain_adaptation/train_UDA.py&quot;, line 326, in train_domain_adaptation
    train_advent(model, trainloader, targetloader, val_loader, cfg, group=group, fk_loader=fk_loader)
  File &quot;/home/zhangyu/codes/person_seg/IntraDA/ADVENT/advent/domain_adaptation/train_UDA.py&quot;, line 114, in train_advent
    _, (images_source, labels, src_names, voc_ids, _) = trainloader_iter.__next__()
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 363, in __next__
    data = self._next_data()
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 974, in _next_data
    idx, data = self._get_data()
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 931, in _get_data
    success, data = self._try_get_data()
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/dataloader.py&quot;, line 779, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/queue.py&quot;, line 179, in get
    self.not_empty.wait(remaining)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/threading.py&quot;, line 306, in wait
    gotit = waiter.acquire(True, timeout)
KeyboardInterrupt
Exception in thread Thread-2:
Traceback (most recent call last):
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/threading.py&quot;, line 932, in _bootstrap_inner
    self.run()
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/threading.py&quot;, line 870, in run
    self._target(*self._args, **self._kwargs)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py&quot;, line 25, in _pin_memory_loop
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/queues.py&quot;, line 116, in get
    return _ForkingPickler.loads(res)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/site-packages/torch/multiprocessing/reductions.py&quot;, line 282, in rebuild_storage_fd
    fd = df.detach()
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/resource_sharer.py&quot;, line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/resource_sharer.py&quot;, line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py&quot;, line 508, in Client
    answer_challenge(c, authkey)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py&quot;, line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py&quot;, line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py&quot;, line 414, in _recv_bytes
    buf = self._recv(4)
  File &quot;/home/zhangyu/anaconda3/envs/pt16/lib/python3.8/multiprocessing/connection.py&quot;, line 379, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer

</code></pre>
",6931072.0,,,,,2020-08-31 15:54:57,PyTorch Dataloader hangs when num_workers > 0,<multiprocessing><pytorch><dataloader>,0,7,0.0,,,CC BY-SA 4.0
70464428,1,70482924.0,,2021-12-23 15:50:06,,7,6424,"<p>I have several masked language models (mainly Bert, Roberta, Albert, Electra). I also have a dataset of sentences. How can I get the perplexity of each sentence?</p>
<p>From the huggingface documentation <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""noreferrer"">here</a> they mentioned that perplexity &quot;is not well defined for masked language models like BERT&quot;, though I still see people somehow calculate it.</p>
<p>For example in this <a href=""https://stackoverflow.com/questions/63030692/how-do-i-use-bertformaskedlm-or-bertmodel-to-calculate-perplexity-of-a-sentence"">SO</a> question they calculated it using the function</p>
<pre><code>def score(model, tokenizer, sentence,  mask_token_id=103):
  tensor_input = tokenizer.encode(sentence, return_tensors='pt')
  repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)
  mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]
  masked_input = repeat_input.masked_fill(mask == 1, 103)
  labels = repeat_input.masked_fill( masked_input != 103, -100)
  loss,_ = model(masked_input, masked_lm_labels=labels)
  result = np.exp(loss.item())
  return result

score(model, tokenizer, '我爱你') # returns 45.63794545581973
</code></pre>
<p>However, when I try to use the code I get <code>TypeError: forward() got an unexpected keyword argument 'masked_lm_labels'</code>.</p>
<p>I tried it with a couple of my models:</p>
<pre><code>from transformers import pipeline, BertForMaskedLM, BertForMaskedLM, AutoTokenizer, RobertaForMaskedLM, AlbertForMaskedLM, ElectraForMaskedLM
import torch

1)
tokenizer = AutoTokenizer.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)
model = BertForMaskedLM.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)
2)
tokenizer = AutoTokenizer.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)
model = ElectraForMaskedLM.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)
</code></pre>
<p><a href=""https://stackoverflow.com/questions/61470768/how-does-masked-lm-labels-argument-work-in-bertformaskedlm"">This</a> SO question also used the <code>masked_lm_labels</code> as an input and it seemed to work somehow.</p>
",14735451.0,,,,,2021-12-25 21:51:43,How to calculate perplexity of a sentence using huggingface masked language models?,<nlp><pytorch><huggingface-transformers><bert-language-model><transformer-model>,1,0,0.0,,,CC BY-SA 4.0
63403485,1,,,2020-08-13 21:31:18,,7,11858,"<p>I saw a sudoku solver CNN uses a sparse categorical cross-entropy as a loss function using the TensorFlow framework, I am wondering if there is a similar function for Pytorch? if not could how could I potentially calculate the loss of a 2d array using Pytorch?</p>
",14090053.0,,,,,2023-02-03 06:01:30,Is there a version of sparse categorical cross entropy in pytorch?,<pytorch><loss-function><conv-neural-network>,1,4,,,,CC BY-SA 4.0
68152634,1,70022558.0,,2021-06-27 15:26:56,,7,9340,"<p>I am learning this new ONNX framework that allows us to deploy the deep learning (and others) model into production.</p>
<p>However, there is one thing I am missing. I thought that the main reason for having such a framework is so that for inference purposes e.g. when we have a trained model and want to use it in a different venv (where for example we cannot have PyTorch) the model still can be used.</p>
<p>I have preped a &quot;from scratch&quot; example here:</p>
<pre><code># Modules
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import torchvision
import onnx
import onnxruntime
import matplotlib.pyplot as plt
import numpy as np


# %config Completer.use_jedi = False

# MNIST Example dataset
train_loader = torch.utils.data.DataLoader(
      torchvision.datasets.MNIST(
          'data', train=True, download=True,
          transform=torchvision.transforms.Compose([
              torchvision.transforms.ToTensor(),
          ])),
      batch_size=800)


# Take data and labels &quot;by hand&quot;
inputs_batch, labels_batch = next(iter(train_loader))


# Simple Model

class CNN(nn.Module):
    def __init__(self, in_channels, num_classes):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=in_channels, 
                               out_channels = 10, kernel_size = (3, 3), stride = (1, 1), padding=(1, 1))
        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride = (2, 2))
        self.conv2 = nn.Conv2d(in_channels = 10, out_channels=16, kernel_size = (3, 3), stride = (1, 1), padding=(1, 1))
        self.fc1 = nn.Linear(16*7*7, num_classes)
        
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = x.reshape(x.shape[0], -1)
        x = self.fc1(x)
        
        return x

# Training setting
device = 'cpu'
batch_size = 64
learning_rate = 0.001
n_epochs = 10

# Dataset prep
dataset = TensorDataset(inputs_batch, labels_batch)
TRAIN_DF = DataLoader(dataset = dataset, batch_size = batch_size, shuffle = True)

# Model Init
model = CNN(in_channels=1, num_classes=10)
optimizer = optim.Adam(model.parameters(), lr = learning_rate)


# Training Loop
for epoch in range(n_epochs):
    for data, labels in TRAIN_DF:
        model.train()
        # Send Data to GPU
        data = data.to(device)
        # Send Data to GPU
        labels = labels.to(device)
        
#       data = data.reshape(data.shape[0], -1)
        
        # Forward
        pred = model(data)
        loss = F.cross_entropy(pred, labels)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
# Check Accuracy
def check_accuracy(loader, model):
    num_correct = 0
    num_total = 0
    
    model.eval()
    
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device)
            y = y.to(device)

    #             x = x.reshape(x.shape[0], -1)

            scores = model(x)
            _, pred = scores.max(1)

            num_correct += (pred == y).sum()
            num_total += pred.size(0)

    print(F&quot;Got {num_correct} / {num_total} with accuracy {float(num_correct)/float(num_total)*100: .2f}&quot;)
        
check_accuracy(TRAIN_DF, model)

# Inference with ONNX


# Create Artifical data of the same size
img_size = 28
dummy_data = torch.randn(1, img_size, img_size)
dummy_input = torch.autograd.Variable(dummy_data).unsqueeze(0)

input_name = &quot;input&quot;
output_name = &quot;output&quot;

model_eval = model.eval()

torch.onnx.export(
    model_eval,
    dummy_input,
    &quot;model_CNN.onnx&quot;,
    input_names=[&quot;input&quot;],
    output_names=[&quot;output&quot;],
)

# Take Random Image from Training Data
X_pred = data[4].unsqueeze(0)

# Convert the Tensor image to PURE numpy and pretend we are working in venv where we only have numpy - NO PYTORCH
X_pred_np = X_pred.numpy()

X_pred_np = np.array(X_pred_np)

IMG_Rando = np.random.rand(1, 1, 28, 28)

np.shape(X_pred_np) == np.shape(IMG_Rando)

ort_session = onnxruntime.InferenceSession(
    &quot;model_CNN.onnx&quot;
)


def to_numpy(tensor):
    return (
        tensor.detach().gpu().numpy()
        if tensor.requires_grad
        else tensor.cpu().numpy()
    )


# compute ONNX Runtime output prediction

# WORKS
# ort_inputs = {ort_session.get_inputs()[0].name: X_pred_np}

# DOES NOT WORK
ort_inputs = {ort_session.get_inputs()[0].name: IMG_Rando}

# WORKS
# ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(X_pred)}
ort_outs = ort_session.run(None, ort_inputs)

ort_outs

</code></pre>
<p>Firstly, we create a simple model and train it on the MNIST dataset.</p>
<p>Then we export the trained model using the ONNX framework.
Now, when I want to classify an image using the <code>X_pred_np</code> It works even though it is a &quot;pure&quot; NumPy, which is what I want.</p>
<p>However, I suspect that this particular case works only because it has been derived from the PyTorch tensor object, and thus &quot;under the hood&quot; it still has PyTorch attributes.
While when I try to inference on the random &quot;pure&quot; NumPy object <code>IMG_Rando</code>, there seems to be a problem:</p>
<p><code>Unexpected input data type. Actual: (tensor(double)) , expected: (tensor(float))</code>.</p>
<p>Referring that PyTorch form is needed.
Is there a way how to be able to use only numpy Images for the ONNX predictions?. So the inference can be performed in separated venv where no pytorch is installed?</p>
<p>Secondly, is there a way that ONNX would remember the actual classes?</p>
<p>In this particular case, the index corresponds to the label of the image. However, in animal classification, ONNX would not provide us with the &quot;DOG&quot; and &quot;CAT&quot; and other labels but would only provide us the index of the predicted label. Which we would need to run throw our own &quot;prediction dictionary&quot; so we know that the fifth label is associated with &quot;cat&quot; and sixth label is associated with &quot;dog&quot; etc.</p>
",11238057.0,,7370153.0,,2021-06-28 12:14:46,2021-11-18 15:30:54,"Unexpected input data type. Actual: (tensor(double)) , expected: (tensor(float))",<numpy><pytorch><onnx>,2,0,,,,CC BY-SA 4.0
72861962,1,,,2022-07-04 20:57:19,,7,2454,"<p>i want to run the pipeline abstract for zero-shot-classification task on the mps device. Here is my code</p>
<pre><code>pipe = pipeline('zero-shot-classification', device = mps_device)
seq = &quot;i love watching the office show&quot;
labels = ['negative', 'positive']
pipe(seq, labels)
</code></pre>
<p>The error generated is</p>
<pre><code>RuntimeError: Placeholder storage has not been allocated on MPS device!
</code></pre>
<p>Which my guess is because seq is on my cpu and not mps. How can i fix this ?
Is there a way to send seq to the mps device so that i can pass it to the pipe for inference?</p>
<p>Thanks</p>
",15581306.0,,,,,2022-09-28 10:06:47,Using HuggingFace pipeline on pytorch mps device M1 pro,<nlp><pytorch><huggingface-transformers>,1,3,,,,CC BY-SA 4.0
69865825,1,69883649.0,,2021-11-06 16:25:08,,7,12197,"<p>I have a Nvidia RTX 3090 ti 24GB with this drivers</p>
<pre><code>CUDA Version: 11.4 

Driver Version: 470.74

18.04.1-Ubuntu SMP

Cuda compilation tools, release 9.1, V9.1.85
</code></pre>
<p>I've looked for this card architecture and it is Ampere so the version of library are compute_86 or sm_86(if I am not wrong). But while compiling with nvcc it gives me back</p>
<blockquote>
<p>nvcc fatal   : Unsupported gpu architecture 'compute_86'</p>
</blockquote>
<p>I've runned nvcc --help and I've found something strange, it returned me that for gpu-code and gpu-architecture</p>
<blockquote>
<p>Allowed values for this option:  'compute_30','compute_32','compute_35',
'compute_37','compute_50','compute_52','compute_53','compute_60','compute_61',
'compute_62','compute_70','compute_72','sm_30','sm_32','sm_35','sm_37','sm_50',
'sm_52','sm_53','sm_60','sm_61','sm_62','sm_70','sm_72'.</p>
</blockquote>
<p>So I'm missing any driver version or some library that has to be donwloaded or I can't compile with my GPU?</p>
",3858506.0,,4685471.0,,2022-10-05 10:51:18,2023-05-05 04:58:31,nvcc fatal : Unsupported gpu architecture 'compute_86',<ubuntu><compiler-errors><pytorch><nvidia><nvcc>,4,5,,,,CC BY-SA 4.0
66152766,1,66162559.0,,2021-02-11 10:20:00,,7,7335,"<p>Following <a href=""https://stackoverflow.com/questions/66137298/how-to-detect-source-of-under-fitting-and-vanishing-gradients-in-pytorch"">a previous question</a>, I want to plot weights, biases, activations and gradients to achieve a similar result to <a href=""https://stackoverflow.com/questions/42315202/understanding-tensorboard-weight-histograms"">this</a>.</p>
<p>Using</p>
<pre><code>for name, param in model.named_parameters():
    summary_writer.add_histogram(f'{name}.grad', param.grad, step_index)
</code></pre>
<p>as was suggested in <a href=""https://stackoverflow.com/questions/66137298/how-to-detect-source-of-under-fitting-and-vanishing-gradients-in-pytorch"">the previous question</a> gives sub-optimal results, since layer names come out similar to <code>'_decoder._decoder.4.weight'</code>, which is hard to follow, especially since the architecture is changing due to research. <code>4</code> in this run won't be the same in the next, and is really meaningless.</p>
<p>Thus, I wanted to give my own string names to each layer.</p>
<hr />
<p>I found <a href=""https://discuss.pytorch.org/t/how-to-give-pytorch-layer-a-name/5521"" rel=""noreferrer"">this</a> Pytorch forum discussion, but no single best practice was agreed upon.</p>
<p>What is the recommended way to assign names to Pytorch layers?</p>
<p>Namely, layers defined in various ways:</p>
<ol>
<li>Sequential:</li>
</ol>
<pre><code>self._seq = nn.Sequential(nn.Linear(1, 2), nn.Linear(3, 4),)
</code></pre>
<ol start=""2"">
<li>Dynamic:</li>
</ol>
<pre><code>self._dynamic = nn.ModuleList()
    for _ in range(self._n_features): 
        self._last_layer.append(nn.Conv1d(in_channels=5, out_channels=6, kernel_size=3, stride=1, padding=1,),)
</code></pre>
<ol start=""3"">
<li>Direct:</li>
</ol>
<pre><code>self._direct = nn.Linear(7, 8)
</code></pre>
<ol start=""4"">
<li>Other ways I didn't think about</li>
</ol>
<hr />
<p>I would like to be able to give a string name to each layer, defined in each of the above ways.</p>
",913098.0,,913098.0,,2021-02-11 10:28:58,2021-02-11 20:46:02,How to assign a name for a pytorch layer?,<python><machine-learning><deep-learning><neural-network><pytorch>,1,1,0.0,,,CC BY-SA 4.0
64636103,1,,,2020-11-01 19:30:54,,7,8002,"<p>I tried multiple times installing Pytorch on Pycharm. I used the code that the pytorch web site give you for a specific configuration. I use this one:</p>
<p><img src=""https://i.stack.imgur.com/oIIek.png"" alt=""enter image description here"" /></p>
<p>Then I copied this information on Pycharm Terminal and I get this message:</p>
<p>(venv) D:\Usuarios\AuCap\Documents\mnist&gt;pip install torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio===0.7.0 -f <a href=""https://download.pytorch.org/w"" rel=""nofollow noreferrer"">https://download.pytorch.org/w</a>
hl/torch_stable.html
Looking in links: <a href=""https://download.pytorch.org/whl/torch_stable.html"" rel=""nofollow noreferrer"">https://download.pytorch.org/whl/torch_stable.html</a>
ERROR: Could not find a version that satisfies the requirement torch==1.7.0+cpu (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch==1.7.0+cpu</p>
<p>I also tried installing it using Python interpreter in the Pycharm Settings and also didnd´t work.</p>
<p>Thanks for your help</p>
",14559975.0,,3528321.0,,2023-01-09 07:22:17,2023-01-09 07:22:17,Can´t install Pytorch on PyCharm: No matching distribution found for torch==1.7.0+cpu,<pycharm><pytorch>,3,1,,,,CC BY-SA 4.0
63017931,1,,,2020-07-21 15:30:09,,7,7748,"<p>To speed up performace I looked into pytorches <a href=""https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html"" rel=""noreferrer"">DistributedDataParallel</a> and tried to apply it to transformer <a href=""https://huggingface.co/transformers/main_classes/trainer.html"" rel=""noreferrer"">Trainer</a>.</p>
<p>The <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#comparison-between-dataparallel-and-distributeddataparallel"" rel=""noreferrer"">pytorch examples for DDP</a> states that this should <strong>at least</strong> be faster:</p>
<blockquote>
<p>DataParallel is single-process, multi-thread, and only works on a single machine, while DistributedDataParallel is multi-process and works for both single- and multi- machine training. DataParallel is usually slower than DistributedDataParallel even on a single machine due to GIL contention across threads, per-iteration replicated model, and additional overhead introduced by scattering inputs and gathering outputs.</p>
</blockquote>
<p>My DataParallel trainer looks like this:</p>
<pre><code>import os
from datetime import datetime
import sys
import torch
from transformers import Trainer, TrainingArguments, BertConfig

training_args = TrainingArguments(
        output_dir=os.path.join(path_storage, 'results', &quot;mlm&quot;),  # output directory
        num_train_epochs=1,  # total # of training epochs
        gradient_accumulation_steps=2,  # for accumulation over multiple steps
        per_device_train_batch_size=4,  # batch size per device during training
        per_device_eval_batch_size=4,  # batch size for evaluation
        logging_dir=os.path.join(path_storage, 'logs', &quot;mlm&quot;),  # directory for storing logs
        evaluate_during_training=False,
        max_steps=20,
    )

mlm_train_dataset = ProteinBertMaskedLMDataset(
        path_vocab, os.path.join(path_storage, &quot;data&quot;, &quot;uniparc&quot;, &quot;uniparc_train_sorted.h5&quot;),
)

mlm_config = BertConfig(
        vocab_size=mlm_train_dataset.tokenizer.vocab_size,
        max_position_embeddings=mlm_train_dataset.input_size
)
mlm_model = ProteinBertForMaskedLM(mlm_config)
trainer = Trainer(
   model=mlm_model,  # the instantiated 🤗 Transformers model to be trained
   args=training_args,  # training arguments, defined above
   train_dataset=mlm_train_dataset,  # training dataset
   data_collator=mlm_train_dataset.collate_fn,
)
print(&quot;build trainer with on device:&quot;, training_args.device, &quot;with n gpus:&quot;, training_args.n_gpu)
start = datetime.now()
trainer.train()
print(f&quot;finished in {datetime.now() - start} seconds&quot;)
</code></pre>
<p>The output:</p>
<pre><code>build trainer with on device: cuda:0 with n gpus: 4
finished in 0:02:47.537038 seconds
</code></pre>
<p>My DistributedDataParallel trainer is build like this:</p>
<pre><code>def create_transformer_trainer(rank, world_size, train_dataset, model):
    os.environ['MASTER_ADDR'] = 'localhost'
    os.environ['MASTER_PORT'] = '12355'
    os.environ[&quot;RANK&quot;] = str(rank)
    os.environ[&quot;WORLD_SIZE&quot;] = str(world_size)

    training_args = TrainingArguments(
        output_dir=os.path.join(path_storage, 'results', &quot;mlm&quot;),  # output directory
        num_train_epochs=1,  # total # of training epochs
        gradient_accumulation_steps=2,  # for accumulation over multiple steps
        per_device_train_batch_size=4,  # batch size per device during training
        per_device_eval_batch_size=4,  # batch size for evaluation
        logging_dir=os.path.join(path_storage, 'logs', &quot;mlm&quot;),  # directory for storing logs
        local_rank=rank,
        max_steps=20,
    )

    trainer = Trainer(
        model=model,  # the instantiated 🤗 Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=train_dataset,  # training dataset
        data_collator=train_dataset.collate_fn,
    )
    print(&quot;build trainer with on device:&quot;, training_args.device, &quot;with n gpus:&quot;, training_args.n_gpu)
    start = datetime.now()
    trainer.train()
    print(f&quot;finished in {datetime.now() - start} seconds&quot;)


mlm_train_dataset = ProteinBertMaskedLMDataset(
    path_vocab, os.path.join(path_storage, &quot;data&quot;, &quot;uniparc&quot;, &quot;uniparc_train_sorted.h5&quot;))

mlm_config = BertConfig(
    vocab_size=mlm_train_dataset.tokenizer.vocab_size,
    max_position_embeddings=mlm_train_dataset.input_size
)
mlm_model = ProteinBertForMaskedLM(mlm_config)
torch.multiprocessing.spawn(create_transformer_trainer,
     args=(4, mlm_train_dataset, mlm_model),
     nprocs=4,
     join=True)
</code></pre>
<p>The output:</p>
<pre><code>The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
The current process just got forked. Disabling parallelism to avoid deadlocks...
To disable this warning, please explicitly set TOKENIZERS_PARALLELISM=(true | false)
build trainer with on device: cuda:1 with n gpus: 1
build trainer with on device: cuda:2 with n gpus: 1
build trainer with on device: cuda:3 with n gpus: 1
build trainer with on device: cuda:0 with n gpus: 1
finished in 0:04:15.937331 seconds
finished in 0:04:16.899411 seconds
finished in 0:04:16.938141 seconds
finished in 0:04:17.391887 seconds
</code></pre>
<p>About the inital forking warning: What is exaclty forked and is this expected?</p>
<p>And about the resulting time: Is the trainer incorrectly used since it seemed to be a lot slower than the DataParallel approach?</p>
",13878065.0,,,,,2021-05-05 09:37:38,using huggingface Trainer with distributed data parallel,<python><pytorch><huggingface-transformers>,1,10,0.0,,,CC BY-SA 4.0
66979537,1,66980814.0,,2021-04-07 04:23:20,,7,3367,"<p>Here's my folder structure</p>
<pre><code>image-folders/
   ├── class_0/
   |   ├── 001.jpg
   |   ├── 002.jpg
   └── class_1/
   |   ├── 001.jpg
   |   └── 002.jpg
   └── class_2/
       ├── 001.jpg
       └── 002.jpg
</code></pre>
<p>By using <code>ImageFolder</code> from torchvision, I can create dataset with this syntax :<br />
<code>dataset = ImageFolder(&quot;image-folders&quot;,...)</code></p>
<p>But this will read the entire subfolder and create 3 target classes. I don't want to include the class_2 folder, I want my dataset to only contains class_0 and class_1 only, is there any way to achieve this besides delete/move the class_2 folder?</p>
",12082666.0,,,,,2021-04-07 06:42:58,filter class/subfolder with pytorch ImageFolder,<python><pytorch>,1,1,0.0,,,CC BY-SA 4.0
63627997,1,63628389.0,,2020-08-28 05:32:31,,7,19617,"<p>I have a neural network with the following structure:</p>
<pre><code>class myNetwork(nn.Module):
    def __init__(self):
        super(myNetwork, self).__init__()
        self.bigru = nn.GRU(input_size=2, hidden_size=100, batch_first=True, bidirectional=True)
        self.fc1 = nn.Linear(200, 32)
        torch.nn.init.xavier_uniform_(self.fc1.weight)
        self.fc2 = nn.Linear(32, 2)
        torch.nn.init.xavier_uniform_(self.fc2.weight)
</code></pre>
<p>I need to reinstate the model to an unlearned state by resetting the parameters of the neural network. I can do so for <code>nn.Linear</code> layers by using the method below:</p>
<pre><code>def reset_weights(self):
    torch.nn.init.xavier_uniform_(self.fc1.weight)
    torch.nn.init.xavier_uniform_(self.fc2.weight)
</code></pre>
<p>But, to reset the weight of the <code>nn.GRU</code> layer, I could not find any such snippet.</p>
<p>My question is how does one reset the <code>nn.GRU</code> layer? Any other way of resetting the network is also fine. Any help is appreciated.</p>
",6997665.0,,6997665.0,,2020-08-28 05:56:41,2021-11-09 22:05:34,Reset parameters of a neural network in pytorch,<python-3.x><neural-network><pytorch><gated-recurrent-unit>,3,1,,,,CC BY-SA 4.0
66217552,1,66217616.0,,2021-02-16 01:21:30,,7,9254,"<p>I want to assign NaN to a tensor element.</p>
<pre><code>import torch
x = torch.tensor([1, 2, 3])
x[x == 2] = None
</code></pre>
<p>I have the error <code>TypeError: can't assign a NoneType to a torch.LongTensor</code>.
I need it to make sure that some later sophisticated calculations are not made for certain values of x.</p>
",15217321.0,,,,,2021-02-16 01:33:39,How to assign NaN to tensor element?,<python><pytorch>,1,1,,,,CC BY-SA 4.0
69276961,1,69292406.0,,2021-09-22 00:44:35,,7,13909,"<p>I want to extract all data to make the plot, not with tensorboard. My understanding is all log with loss and accuracy is stored in a defined directory since tensorboard draw the line graph.</p>
<pre><code>%reload_ext tensorboard
%tensorboard --logdir lightning_logs/
</code></pre>
<p><a href=""https://i.stack.imgur.com/rt0jS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/rt0jS.png"" alt=""enter image description here"" /></a></p>
<p>However, I wonder how all log can be extracted from the logger in pytorch lightning. The next is the code example in training part.</p>
<pre><code>#model
ssl_classifier = SSLImageClassifier(lr=lr)

#train
logger = pl.loggers.TensorBoardLogger(name=f'ssl-{lr}-{num_epoch}', save_dir='lightning_logs')

trainer = pl.Trainer(progress_bar_refresh_rate=20,
                            gpus=1,
                            max_epochs = max_epoch,
                            logger = logger,
                            )

trainer.fit(ssl_classifier, train_loader, val_loader)
</code></pre>
<p>I had confirmed that <code>trainer.logger.log_dir</code> returned directory which seems to save logs and <code>trainer.logger.log_metrics</code> returned <code>&lt;bound method TensorBoardLogger.log_metrics of &lt;pytorch_lightning.loggers.tensorboard.TensorBoardLogger object at 0x7efcb89a3e50&gt;&gt;</code>.</p>
<p><code>trainer.logged_metrics</code> returned only the log in the final epoch, like</p>
<pre><code>{'epoch': 19,
 'train_acc': tensor(1.),
 'train_loss': tensor(0.1038),
 'val_acc': 0.6499999761581421,
 'val_loss': 1.2171183824539185}
</code></pre>
<p>Do you know how to solve the situation?</p>
",16532921.0,,16532921.0,,2021-09-22 23:19:17,2021-12-04 00:07:18,How to extract loss and accuracy from logger by each epoch in pytorch lightning?,<logging><pytorch><tensorboard><pytorch-lightning>,2,0,0.0,,,CC BY-SA 4.0
64594493,1,64594975.0,,2020-10-29 15:42:11,,7,10347,"<p>This question is very similar <a href=""https://stackoverflow.com/questions/61503138/filter-out-np-nan-values-from-pytorch-1d-tensor"">to filtering <code>np.nan</code> values from pytorch in a -Dimensional tensor</a>. The difference is that I want to apply the same concept to tensors of 2 or higher dimensions.</p>
<p>I have a tensor that looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

tensor = torch.Tensor(
[[1, 1, 1, 1, 1],
 [float('nan'), float('nan'), float('nan'), float('nan'), float('nan')],
 [2, 2, 2, 2, 2]]
)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; tensor.shape
&gt;&gt;&gt; [3, 5]
</code></pre>
<p>I would like to find the most pythonic / PyTorch way of to filter out (remove) the rows of the tensor which are <code>nan</code>. By filtering this <code>tensor</code> along the first (<code>0</code>th axis) I want to obtain a <code>filtered_tensor</code> which looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>&gt;&gt;&gt; print(filtered_tensor)
&gt;&gt;&gt; torch.Tensor(
[[1, 1, 1, 1, 1],
 [2, 2, 2, 2, 2]]
)
&gt;&gt;&gt; filtered_tensor.shape
&gt;&gt;&gt; [2, 5]
</code></pre>
",2850788.0,,,,,2020-10-29 16:30:40,Filter out NaN values from a PyTorch N-Dimensional tensor,<python><python-3.x><pytorch><filtering><nan>,1,0,,,,CC BY-SA 4.0
63108131,1,,,2020-07-27 03:04:16,,7,17115,"<p>I am trying to re-implement one paper, which suggests to adjust the learning rate as below:</p>
<blockquote>
<p>The learning rate is decreased by a <strong>factor</strong> of the regression value with patience epochs 10 on the <strong>change value</strong> of 0.0001.</p>
</blockquote>
<p>Should I use the <code>torch.optim.lr_scheduler.ReduceLROnPlateau()</code>?</p>
<p>I am not sure what value should I pass to each parameter.</p>
<ol>
<li><p>Is the <strong>change value</strong> in the statement denotes to the parameter <strong>threshold</strong>?</p>
</li>
<li><p>Is the <em>factor</em> in the statement denotes to the parameter <strong>factor</strong>?</p>
</li>
</ol>
",5480503.0,,9201239.0,,2020-08-09 01:46:30,2021-02-25 23:15:26,Pytorch schedule learning rate,<optimization><pytorch><learning-rate>,3,0,0.0,,,CC BY-SA 4.0
65947284,1,65949186.0,,2021-01-29 00:46:40,,7,7452,"<p>Before working on something more complex, where I knew I would have to implement my own <code>backward</code> pass, I wanted to try something nice and simple. So, I tried to do linear regression with mean squared error loss using PyTorch. This went wrong (see third implementation option below) when I defined my own <code>backward</code> method and I suspect it's because I'm not thinking very clearly about what I need to send PyTorch as gradients. So, I suspect what I need is some explanation/clarification/advice on what PyTorch expects me to provide in what form here.</p>
<p>I am using PyTorch 1.7.0, so a bunch of old examples no longer work (different way of working with user-defined autograd functions as described in the <a href=""https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"" rel=""nofollow noreferrer"">documentation</a>).</p>
<h1>First approach (standard PyTorch MSE loss function)</h1>
<p>Let's first do it the standard way without a custom loss function:</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

# Let's generate some fake data
torch.manual_seed(42)
resid = torch.rand(100)    
inputs = torch.tensor([ [ xx ] for xx in range(100)] , dtype=torch.float32)
labels = torch.tensor([ (2 + 0.5*yy + resid[yy]) for yy in range(100)], dtype=torch.float32)

# Now we define a linear regression model
class linearRegression(torch.nn.Module):
    def __init__(self, inputSize, outputSize):
        super(linearRegression, self).__init__()
        self.bn = torch.nn.BatchNorm1d(num_features=1)
        self.linear = torch.nn.Linear(inputSize, outputSize)

    def forward(self, inx):
        x = self.bn(inx) # Adding BN to standardize input helps us use a higher learning rate
        x = self.linear(x)
        return x
    
model = linearRegression(1, 1)     

# Using the standard mse_loss of PyTorch
epochs = 25    
mseloss = F.mse_loss
optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=1e-3)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)

for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = mseloss(outputs.view(-1), labels)
    loss.backward()
    optimizer.step()
    scheduler.step()        
    print(f'epoch {epoch}, loss {loss}')    
    
</code></pre>
<p>This train just fine and I get to a loss of about 0.0824 and a plot of the fit looks fine.</p>
<h1>Second approach (custom loss function, but relying on PyTorch's automatic gradient calculation)</h1>
<p>So, now I replace the loss function with my own implementation of the MSE loss, but I still rely on PyTorch autograd. The only things I change here are defining the custom loss function, correspondingly defining the loss based on that, and a minor detail for how I hand over the predictions and true labels to the loss function.</p>
<pre><code>#######################################################3
class MyMSELoss(nn.Module):
    
    def __init__(self):
        super(MyMSELoss, self).__init__()

    def forward(self, inputs, targets):        
        tmp = (inputs-targets)**2
        loss =  torch.mean(tmp)        
        return loss
#######################################################3

model = linearRegression(1, 1) 
    
mseloss = MyMSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=1e-3)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)

for epoch in range(epochs):
    model.train()    
    outputs = model(inputs)
    loss = mseloss(outputs.view(-1), labels)
    loss.backward()    
    optimizer.step()
    optimizer.zero_grad()
    scheduler.step()
    
    print(f'epoch {epoch}, loss {loss}')
</code></pre>
<p>This gives completely identical results as using the standard MSE loss function. Loss over epochs looks like this:</p>
<pre><code>epoch 0, loss 884.2006225585938
epoch 1, loss 821.930908203125
epoch 2, loss 718.7732543945312
epoch 3, loss 538.1835327148438
epoch 4, loss 274.50909423828125
epoch 5, loss 55.115299224853516
epoch 6, loss 2.405021905899048
epoch 7, loss 0.47621214389801025
epoch 8, loss 0.1584305614233017
epoch 9, loss 0.09725229442119598
epoch 10, loss 0.0853077694773674
epoch 11, loss 0.08297089487314224
epoch 12, loss 0.08251354098320007
epoch 13, loss 0.08242412656545639
epoch 14, loss 0.08240655809640884
epoch 15, loss 0.08240310847759247
epoch 16, loss 0.08240246027708054
epoch 17, loss 0.08240233361721039
epoch 18, loss 0.08240240067243576
epoch 19, loss 0.08240223675966263
epoch 20, loss 0.08240225911140442
epoch 21, loss 0.08240220695734024
epoch 22, loss 0.08240220695734024
epoch 23, loss 0.08240220695734024
epoch 24, loss 0.08240220695734024
</code></pre>
<h1>Third approach (custom loss function with my own backward method)</h1>
<p>Now, the final version, where I implement my own gradients for the MSE. For that I define my own <code>backward</code> method in the loss function class and apparently need to do <code>mseloss = MyMSELoss.apply</code>.</p>
<pre><code>from torch.autograd import Function

#######################################################
class MyMSELoss(Function):
    
    @staticmethod
    def forward(ctx, y_pred, y):    
        ctx.save_for_backward(y_pred, y)
        return ( (y - y_pred)**2 ).mean()
    
    @staticmethod
    def backward(ctx, grad_output):
        y_pred, y = ctx.saved_tensors
        grad_input = torch.mean( -2.0 * (y - y_pred)).repeat(y_pred.shape[0])        
        # This fails, as does grad_input = -2.0 * (y-y_pred)
        # I've also messed around with the sign and that's not the sole problem, either.
        return grad_input, None
    
#######################################################
    
model = linearRegression(1, 1) 
mseloss = MyMSELoss.apply
optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=1e-3)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)

for epoch in range(epochs):
    model.train()
    outputs = model(inputs)
    loss = mseloss(outputs.view(-1), labels)
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()
    scheduler.step()
    print(f'epoch {epoch}, loss {loss}')    
</code></pre>
<p>This is where things go wrong and instead of the training loss decreasing, I get increasing training loss. Now it looks like this:</p>
<pre><code>epoch 0, loss 884.2006225585938
epoch 1, loss 3471.384033203125
epoch 2, loss 47768555520.0
epoch 3, loss 1.7422577779621402e+33
epoch 4, loss inf
epoch 5, loss nan
epoch 6, loss nan
epoch 7, loss nan
epoch 8, loss nan
epoch 9, loss nan
epoch 10, loss nan
epoch 11, loss nan
epoch 12, loss nan
epoch 13, loss nan
epoch 14, loss nan
epoch 15, loss nan
epoch 16, loss nan
epoch 17, loss nan
epoch 18, loss nan
epoch 19, loss nan
epoch 20, loss nan
epoch 21, loss nan
epoch 22, loss nan
epoch 23, loss nan
epoch 24, loss nan
</code></pre>
",7744356.0,,2790047.0,,2021-01-29 12:04:42,2021-01-29 12:04:42,Loss with custom backward function in PyTorch - exploding loss in simple MSE example,<machine-learning><deep-learning><pytorch><loss-function><gradient-descent>,1,0,0.0,,,CC BY-SA 4.0
66567324,1,66656572.0,,2021-03-10 14:59:42,,6,10659,"<p>I have a very simple example</p>
<pre><code>import torch

if __name__ == &quot;__main__&quot;:
    DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    m = torch.nn.Linear(20, 30).to(DEVICE)
    input = torch.randn(128, 20).to(DEVICE)
    output = m(input)
    print('output', output.size())
    exit()
</code></pre>
<p>and I get:</p>
<pre><code>Traceback (most recent call last):
  File &quot;test.py&quot;, line 9, in &lt;module&gt;
    output = m(input)
  File &quot;/home/shamoon/.local/share/virtualenvs/speech-reconstruction-7HMT9fTW/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;/home/shamoon/.local/share/virtualenvs/speech-reconstruction-7HMT9fTW/lib/python3.8/site-packages/torch/nn/modules/linear.py&quot;, line 94, in forward
    return F.linear(input, self.weight, self.bias)
  File &quot;/home/shamoon/.local/share/virtualenvs/speech-reconstruction-7HMT9fTW/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 1753, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`
</code></pre>
<p>I'm using <code>PyTorch 1.7.1</code>. Any help would be greatly appreciated.</p>
<p>Thank you.</p>
<p>EDIT. The update of <code>python -m torch.utils.collect_env</code> is:</p>
<pre><code>Collecting environment information...
PyTorch version: 1.8.0
Is debug build: False
CUDA used to build PyTorch: 10.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: 11.1.0
CMake version: version 3.18.4

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: TITAN RTX
GPU 1: TITAN RTX
GPU 2: TITAN RTX
GPU 3: TITAN RTX
GPU 4: TITAN RTX
GPU 5: TITAN RTX
GPU 6: TITAN RTX
GPU 7: TITAN RTX

Nvidia driver version: 460.39
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.20.1
[pip3] torch==1.8.0
[pip3] torchaudio==0.8.0
[pip3] torchsummary==1.5.1
[conda] Could not collect
</code></pre>
",239879.0,,681865.0,,2021-03-10 22:37:23,2021-03-16 13:51:26,PyTorch error: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasCreate(handle)`,<python><pytorch>,1,2,,,,CC BY-SA 4.0
68002742,1,68067406.0,,2021-06-16 12:32:47,,6,6642,"<p>I have converted two models (vgg16 and resnet50) from Keras with TensorFlow backend (from as model.save file) into PyTorch using mmdnn. This was done with the following:</p>
<pre><code>mmconvert -sf keras -iw vgg.h5 -df pytorch -om keras_to_torch.pt

A = imp.load_source('MainModel','/weights/keras_to_torch.py')
model = torch.load('/weights/keras_to_torch.pt')
</code></pre>
<p>Predicting on the same data set gave me a different set of results so I investigated further.</p>
<p>I can see that the weights for all the convolutional layers are the same (after transposing), however the weights of the fully connected layers at the end are not.</p>
<p>Is there a reason this should be? As i understand they should be equivalent</p>
",4566639.0,,4566639.0,,2021-06-16 12:47:01,2021-06-21 11:47:54,Converted model from keras h5 to pytorch - fully connected layer mismatch,<keras><pytorch><conv-neural-network><data-conversion>,1,1,0.0,,,CC BY-SA 4.0
65450707,1,,,2020-12-25 18:29:45,,6,5715,"<p>I am new to Tensorboard.</p>
<p>I am using fairly simple code running an experiment, and this is the output:</p>
<p><a href=""https://i.stack.imgur.com/eBl90.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/eBl90.png"" alt=""enter image description here"" /></a></p>
<p>I don't remember asking for a <code>hp_metric</code> graph, yet here it is.</p>
<p><strong>What is it and how do I get rid of it?</strong></p>
<hr />
<p>Full code to reproduce, using Pytorch Lightning (not that I think anyone should have to reproduce this to answer):</p>
<p>Please notice the ONLY line dereferencing TensorBoard is</p>
<pre><code>self.logger.experiment.add_scalars(&quot;losses&quot;, {&quot;train_loss&quot;: loss}, global_step=self.current_epoch)
</code></pre>
<pre><code>import torch
from torch import nn
import torch.nn.functional as F
from typing import List, Optional
from pytorch_lightning.core.lightning import LightningModule
from Testing.Research.toy_datasets.ClustersDataset import ClustersDataset
from torch.utils.data import DataLoader
from Testing.Research.config.ConfigProvider import ConfigProvider
from pytorch_lightning import Trainer, seed_everything
from torch import optim
import os
from pytorch_lightning.loggers import TensorBoardLogger


class VAEFC(LightningModule):
    # see https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
    # for possible upgrades, see https://arxiv.org/pdf/1602.02282.pdf
    # https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder
    def __init__(self, encoder_layer_sizes: List, decoder_layer_sizes: List, config):
        super(VAEFC, self).__init__()
        self._config = config
        self.logger: Optional[TensorBoardLogger] = None

        assert len(encoder_layer_sizes) &gt;= 3, &quot;must have at least 3 layers (2 hidden)&quot;
        # encoder layers
        self._encoder_layers = nn.ModuleList()
        for i in range(1, len(encoder_layer_sizes) - 1):
            enc_layer = nn.Linear(encoder_layer_sizes[i - 1], encoder_layer_sizes[i])
            self._encoder_layers.append(enc_layer)

        # predict mean and covariance vectors
        self._mean_layer = nn.Linear(encoder_layer_sizes[
                                         len(encoder_layer_sizes) - 2],
                                     encoder_layer_sizes[len(encoder_layer_sizes) - 1])
        self._logvar_layer = nn.Linear(encoder_layer_sizes[
                                           len(encoder_layer_sizes) - 2],
                                       encoder_layer_sizes[len(encoder_layer_sizes) - 1])

        # decoder layers
        self._decoder_layers = nn.ModuleList()
        for i in range(1, len(decoder_layer_sizes)):
            dec_layer = nn.Linear(decoder_layer_sizes[i - 1], decoder_layer_sizes[i])
            self._decoder_layers.append(dec_layer)

        self._recon_function = nn.MSELoss(reduction='mean')

    def _encode(self, x):
        for i in range(len(self._encoder_layers)):
            layer = self._encoder_layers[i]
            x = F.relu(layer(x))

        mean_output = self._mean_layer(x)
        logvar_output = self._logvar_layer(x)
        return mean_output, logvar_output

    def _reparametrize(self, mu, logvar):
        if not self.training:
            return mu
        std = logvar.mul(0.5).exp_()
        if std.is_cuda:
            eps = torch.cuda.FloatTensor(std.size()).normal_()
        else:
            eps = torch.FloatTensor(std.size()).normal_()
        reparameterized = eps.mul(std).add_(mu)
        return reparameterized

    def _decode(self, z):
        for i in range(len(self._decoder_layers) - 1):
            layer = self._decoder_layers[i]
            z = F.relu((layer(z)))

        decoded = self._decoder_layers[len(self._decoder_layers) - 1](z)
        # decoded = F.sigmoid(self._decoder_layers[len(self._decoder_layers)-1](z))
        return decoded

    def _loss_function(self, recon_x, x, mu, logvar, reconstruction_function):
        &quot;&quot;&quot;
        recon_x: generating images
        x: origin images
        mu: latent mean
        logvar: latent log variance
        &quot;&quot;&quot;
        binary_cross_entropy = reconstruction_function(recon_x, x)  # mse loss TODO see if mse or cross entropy
        # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
        kld_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)
        kld = torch.sum(kld_element).mul_(-0.5)
        # KL divergence Kullback–Leibler divergence, regularization term for VAE
        # It is a measure of how different two probability distributions are different from each other.
        # We are trying to force the distributions closer while keeping the reconstruction loss low.
        # see https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73

        # read on weighting the regularization term here:
        # https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational
        # -auto-encoder
        return binary_cross_entropy + kld * self._config.regularization_factor

    def training_step(self, batch, batch_index):
        orig_batch, noisy_batch, _ = batch
        noisy_batch = noisy_batch.view(noisy_batch.size(0), -1)

        recon_batch, mu, logvar = self.forward(noisy_batch)

        loss = self._loss_function(
            recon_batch,
            orig_batch, mu, logvar,
            reconstruction_function=self._recon_function
        )
        # self.logger.experiment.add_scalars(&quot;losses&quot;, {&quot;train_loss&quot;: loss})
        self.logger.experiment.add_scalars(&quot;losses&quot;, {&quot;train_loss&quot;: loss}, global_step=self.current_epoch)
        # self.logger.experiment.add_scalar(&quot;train_loss&quot;, loss, self.current_epoch)
        self.logger.experiment.flush()
        return loss

    def train_dataloader(self):
        default_dataset, train_dataset, test_dataset = ClustersDataset.clusters_dataset_by_config()
        train_dataloader = DataLoader(train_dataset, batch_size=self._config.batch_size, shuffle=True)
        return train_dataloader

    def test_dataloader(self):
        default_dataset, train_dataset, test_dataset = ClustersDataset.clusters_dataset_by_config()
        test_dataloader = DataLoader(test_dataset, batch_size=self._config.batch_size, shuffle=True)
        return test_dataloader

    def configure_optimizers(self):
        optimizer = optim.Adam(model.parameters(), lr=self._config.learning_rate)
        return optimizer

    def forward(self, x):
        mu, logvar = self._encode(x)
        z = self._reparametrize(mu, logvar)
        decoded = self._decode(z)
        return decoded, mu, logvar


if __name__ == &quot;__main__&quot;:
    config = ConfigProvider.get_config()
    seed_everything(config.random_seed)
    latent_dim = config.latent_dim
    enc_layer_sizes = config.enc_layer_sizes + [latent_dim]
    dec_layer_sizes = [latent_dim] + config.dec_layer_sizes
    model = VAEFC(config=config, encoder_layer_sizes=enc_layer_sizes, decoder_layer_sizes=dec_layer_sizes)

    logger = TensorBoardLogger(save_dir='tb_logs', name='VAEFC')
    logger.hparams = config  # TODO only put here relevant stuff
    # trainer = Trainer(gpus=1)
    trainer = Trainer(deterministic=config.is_deterministic,
                      #auto_lr_find=config.auto_lr_find,
                      #log_gpu_memory='all',
                      # min_epochs=99999,
                      max_epochs=config.num_epochs,
                      default_root_dir=os.getcwd(),
                      logger=logger
                      )
    # trainer.tune(model)
    trainer.fit(model)
    print(&quot;done training vae with lightning&quot;)
</code></pre>
<p>ClustersDataset.py</p>
<pre><code>from torch.utils.data import Dataset
import matplotlib.pyplot as plt
import torch
import numpy as np
from Testing.Research.config.ConfigProvider import ConfigProvider


class ClustersDataset(Dataset):
    __default_dataset = None
    __default_dataset_train = None
    __default_dataset_test = None

    def __init__(self, cluster_size: int, noise_factor: float = 0, transform=None, n_clusters=2, centers_radius=4.0):
        super(ClustersDataset, self).__init__()
        self._cluster_size = cluster_size
        self._noise_factor = noise_factor
        self._n_clusters = n_clusters
        self._centers_radius = centers_radius
        # self._transform = transform
        self._size = self._cluster_size * self._n_clusters

        self._create_data_clusters()
        self._combine_clusters_to_array()
        self._normalize_data()
        self._add_noise()

        # self._plot()
        pass

    @staticmethod
    def clusters_dataset_by_config():
        if ClustersDataset.__default_dataset is not None:
            return \
                ClustersDataset.__default_dataset, \
                ClustersDataset.__default_dataset_train, \
                ClustersDataset.__default_dataset_test
        config = ConfigProvider.get_config()
        default_dataset = ClustersDataset(
            cluster_size=config.cluster_size,
            noise_factor=config.noise_factor,
            transform=None,
            n_clusters=config.n_clusters,
            centers_radius=config.centers_radius
        )
        
        train_size = int(config.train_size * len(default_dataset))
        test_size = len(default_dataset) - train_size
        train_dataset, test_dataset = torch.utils.data.random_split(default_dataset, [train_size, test_size])

        ClustersDataset.__default_dataset = default_dataset
        ClustersDataset.__default_dataset_train = train_dataset
        ClustersDataset.__default_dataset_test = test_dataset

        return default_dataset, train_dataset, test_dataset

    def _create_data_clusters(self):
        self._clusters = [torch.zeros((self._cluster_size, 2)) for _ in range(self._n_clusters)]
        centers_radius = self._centers_radius
        for i, c in enumerate(self._clusters):
            r, x, y = 3.0, centers_radius * np.cos(i * np.pi * 2 / self._n_clusters), centers_radius * np.sin(
                i * np.pi * 2 / self._n_clusters)
            cluster_length = 1.1
            cluster_start = i * 2 * np.pi / self._n_clusters
            cluster_end = cluster_length * (i + 1) * 2 * np.pi / self._n_clusters
            cluster_inds = torch.linspace(start=cluster_start, end=cluster_end, steps=self._cluster_size,
                                          dtype=torch.float)
            c[:, 0] = r * torch.sin(cluster_inds) + y
            c[:, 1] = r * torch.cos(cluster_inds) + x

    def _plot(self):
        plt.figure()
        plt.scatter(self._noisy_values[:, 0], self._noisy_values[:, 1], s=1, color='b', label=&quot;noisy_values&quot;)
        plt.scatter(self._values[:, 0], self._values[:, 1], s=1, color='r', label=&quot;values&quot;)
        plt.legend(loc=&quot;upper left&quot;)
        plt.show()

    def _combine_clusters_to_array(self):
        size = self._size
        self._values = torch.zeros(size, 2)
        self._labels = torch.zeros(size, dtype=torch.long)
        for i, c in enumerate(self._clusters):
            self._values[i * self._cluster_size: (i + 1) * self._cluster_size, :] = self._clusters[i]
            self._labels[i * self._cluster_size: (i + 1) * self._cluster_size] = i

    def _add_noise(self):
        size = self._size

        mean = torch.zeros(size, 2)
        std = torch.ones(size, 2)
        noise = torch.normal(mean, std)
        self._noisy_values = torch.zeros(size, 2)
        self._noisy_values[:] = self._values
        self._noisy_values = self._noisy_values + noise * self._noise_factor

    def _normalize_data(self):
        values_min, values_max = torch.min(self._values), torch.max(self._values)
        self._values = (self._values - values_min) / (values_max - values_min)
        self._values = self._values * 2 - 1

    def __len__(self):
        return self._size  # number of samples in the dataset

    def __getitem__(self, index):
        item = self._values[index, :]
        noisy_item = self._noisy_values[index, :]
        # if self._transform is not None:
        #     noisy_item = self._transform(item)
        return item, noisy_item, self._labels[index]

    @property
    def values(self):
        return self._values

    @property
    def noisy_values(self):
        return self._noisy_values

</code></pre>
<p>Config values (ConfigProvider just returns those as an object)</p>
<pre><code>num_epochs: 15
batch_size: 128
learning_rate: 0.0001
auto_lr_find: False

noise_factor: 0.1
regularization_factor: 0.0

cluster_size: 5000
n_clusters: 5
centers_radius: 4.0
train_size: 0.8

latent_dim: 8

enc_layer_sizes: [2, 200, 200, 200]
dec_layer_sizes: [200, 200, 200, 2]

retrain_vae: False
random_seed: 11
is_deterministic: True
</code></pre>
",913098.0,,,,,2021-06-14 15:55:31,What is hp_metric in TensorBoard and how to get rid of it?,<python><tensorflow><pytorch><tensorboard><pytorch-lightning>,2,0,,,,CC BY-SA 4.0
64195225,1,64196067.0,,2020-10-04 13:29:26,,6,812,"<p>I have a tensor of shape (<em>m*n</em>, <em>m*n</em>) and I want to extract a tensor of size (<em>n</em>, <em>m*n</em>) containing the m blocks of size <em>n*n</em> that are on the diagonal. For example:</p>
<pre><code>&gt;&gt;&gt; a
tensor([[1, 2, 0, 0],
        [3, 4, 0, 0],
        [0, 0, 5, 6],
        [0, 0, 7, 8]])
</code></pre>
<p>I want to have a function <code>extract(a, m, n)</code> that will output:</p>
<pre><code>&gt;&gt;&gt; extract(a, 2, 2)
tensor([[1, 2, 5, 6],
        [3, 4, 7, 8]])
</code></pre>
<p>I've thought of using some kind of slicing, because the blocks can be expressed by:</p>
<pre><code>&gt;&gt;&gt; for i in range(m):
...     print(a[i*m: i*m + n, i*m: i*m + n])
tensor([[1, 2],
        [3, 4]])
tensor([[5, 6],
        [7, 8]])
</code></pre>
",13354561.0,,9067615.0,,2021-03-23 13:08:24,2021-10-21 19:47:31,Extracting blocks from block diagonal PyTorch tensor,<python><pytorch><tensor><diagonal>,2,0,,,,CC BY-SA 4.0
62937388,1,62937460.0,,2020-07-16 14:49:38,,6,5492,"<p>I am trying to specify a dynamic amount of layers, which I seem to be doing wrong.
My issue is that when I define the 100 layers here, I will get an error in the forward step.
But when I define the layer properly it works?
Below simplified example</p>
<pre class=""lang-py prettyprint-override""><code>class PredictFromEmbeddParaSmall(LightningModule):
    def __init__(self, hyperparams={'lr': 0.0001}):
        super(PredictFromEmbeddParaSmall, self).__init__()
        #Input is something like tensor.size=[768*100]
        self.TO_ILLUSTRATE = nn.Linear(768, 5)
        self.enc_ref=[]
        for i in range(100):
            self.enc_red.append(nn.Linear(768, 5))
        # gather the layers output sth
        self.dense_simple1 = nn.Linear(5*100, 2)
        self.output = nn.Sigmoid()
    def forward(self, x):
        # first input to enc_red
        x_vecs = []
        for i in range(self.para_count):
            layer = self.enc_red[i]
            # The first dim is the batch size here, output is correct
            processed_slice = x[:, i * 768:(i + 1) * 768]
            # This works and give the out of size 5
            rand = self.TO_ILLUSTRATE(processed_slice)
            #This will fail? Error below
            ret = layer(processed_slice)
            #more things happening we can ignore right now since we fail earlier
</code></pre>
<p>I get this error when executing &quot;ret = layer.forward(processed_slice)&quot;</p>
<blockquote>
<p>RuntimeError: Expected object of device type cuda but got device type
cpu for argument #1 'self' in call to _th_addmm</p>
</blockquote>
<p>Is there a smarter way to program this? OR solve the error?</p>
",6614410.0,,6370804.0,,2022-01-19 11:13:49,2023-05-02 12:38:20,Pytorch dynamic amount of Layers?,<pytorch><pytorch-lightning>,2,0,,,,CC BY-SA 4.0
71047884,1,,,2022-02-09 10:27:31,,6,2082,"<p>I encountered this error when using torch package.</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
Input In [12], in &lt;module&gt;
      1 import random
----&gt; 2 import torch
      3 from torch import nn, optim
      4 # import skopt

File /usr/lib/python3.10/site-packages/torch/__init__.py:197, in &lt;module&gt;
    195     if USE_GLOBAL_DEPS:
    196         _load_global_deps()
--&gt; 197     from torch._C import *  # noqa: F403
    199 # Appease the type checker; ordinarily this binding is inserted by the
    200 # torch._C module initialization code in C
    201 if TYPE_CHECKING:

ImportError: libcupti.so.11.5: cannot open shared object file: No such file or directory
</code></pre>
<p>My system is manjaro KDE 20.1. I remember I updated the system these days. My CUDA version is as follows:</p>
<pre class=""lang-sh prettyprint-override""><code>Thu Feb 10 02:23:07 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   41C    P0    11W /  N/A |      0MiB /  8192MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

</code></pre>
<p>How should I solve this problem?</p>
",18160180.0,,18160180.0,,2022-02-09 15:14:32,2022-02-09 15:14:32,ImportError: libcupti.so.11.5: cannot open shared object file: No such file or directory,<linux><pytorch><cuda><manjaro>,0,2,,2022-02-09 14:47:26,,CC BY-SA 4.0
68045496,1,68127722.0,,2021-06-19 09:39:04,,6,1061,"<p>I have a dataset where class values go from -2 to 2 by 1 step <code>(i.e., -2,-1,0,1,2)</code> and where 9 identifies the unlabelled data.
Using one hot encode</p>
<pre><code>self._one_hot_encode(labels)
</code></pre>
<p>I get the following error: <code>RuntimeError: index 1 is out of bounds for dimension 1 with size 1</code></p>
<p>due to</p>
<pre><code>self.one_hot_labels = self.one_hot_labels.scatter(1, labels.unsqueeze(1), 1)
</code></pre>
<p>The error should raise from <code>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1]</code>, where I have 9 in the mapping setting equal index 9 to 1. It is unclear to me how to fix it, even after going through past questions and answers to similar problems (e.g., <a href=""https://stackoverflow.com/questions/67185851/index-1-is-out-of-bounds-for-dimension-0-with-size-1"">index 1 is out of bounds for dimension 0 with size 1</a>).
The part of code involved in the error is the following:</p>
<pre><code>def _one_hot_encode(self, labels):
    # Get the number of classes
    classes = torch.unique(labels)
    classes = classes[classes != 9] # unlabelled 
    self.n_classes = classes.size(0)

    # One-hot encode labeled data instances and zero rows corresponding to unlabeled instances
    unlabeled_mask = (labels == 9)
    labels = labels.clone()  # defensive copying
    labels[unlabeled_mask] = 0
    self.one_hot_labels = torch.zeros((self.n_nodes, self.n_classes), dtype=torch.float)
    self.one_hot_labels = self.one_hot_labels.scatter(1, labels.unsqueeze(1), 1)
    self.one_hot_labels[unlabeled_mask, 0] = 0

    self.labeled_mask = ~unlabeled_mask

def fit(self, labels, max_iter, tol):
    
    self._one_hot_encode(labels)

    self.predictions = self.one_hot_labels.clone()
    prev_predictions = torch.zeros((self.n_nodes, self.n_classes), dtype=torch.float)

    for i in range(max_iter):
        # Stop iterations if the system is considered at a steady state
        variation = torch.abs(self.predictions - prev_predictions).sum().item()
        

        prev_predictions = self.predictions
        self._propagate()
</code></pre>
<p>Example of dataset:</p>
<pre><code>ID  Target  Weight  Label   Score   Scale_Cat   Scale_num
0   A   D   65.1    1   87  Up  1
1   A   X   35.8    1   87  Up  1
2   B   C   34.7    1   37.5    Down    -2
3   B   P   33.4    1   37.5    Down    -2
4   C   B   33.1    1   37.5    Down    -2
5   S   X   21.4    0   12.5    NA  9
</code></pre>
<p>The source code I am using as reference is here: <a href=""https://mybinder.org/v2/gh/thibaudmartinez/label-propagation/master?filepath=notebook.ipynb"" rel=""noreferrer"">https://mybinder.org/v2/gh/thibaudmartinez/label-propagation/master?filepath=notebook.ipynb</a></p>
<p>Full track of the error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-126-792a234f63dd&gt; in &lt;module&gt;
      4 label_propagation = LabelPropagation(adj_matrix_t)
----&gt; 6 label_propagation.fit(labels_t) # causing error
      7 label_propagation_output_labels = label_propagation.predict_classes()
      8 

&lt;ipython-input-115-54a7dbc30bd1&gt; in fit(self, labels, max_iter, tol)
    100 
    101     def fit(self, labels, max_iter=1000, tol=1e-3):
--&gt; 102         super().fit(labels, max_iter, tol)
    103 
    104 ## Label spreading

&lt;ipython-input-115-54a7dbc30bd1&gt; in fit(self, labels, max_iter, tol)
     58             Convergence tolerance: threshold to consider the system at steady state.
     59         &quot;&quot;&quot;
---&gt; 60         self._one_hot_encode(labels)
     61 
     62         self.predictions = self.one_hot_labels.clone()

&lt;ipython-input-115-54a7dbc30bd1&gt; in _one_hot_encode(self, labels)
     42         labels[unlabeled_mask] = 0
     43         self.one_hot_labels = torch.zeros((self.n_nodes, self.n_classes), dtype=torch.float)
---&gt; 44         self.one_hot_labels = self.one_hot_labels.scatter(1, labels.unsqueeze(1), 1)
     45         self.one_hot_labels[unlabeled_mask, 0] = 0
     46 

RuntimeError: index 1 is out of bounds for dimension 1 with size 1
</code></pre>
",14695007.0,,14695007.0,,2021-06-23 16:34:59,2021-06-25 08:17:37,RunTimeError during one hot encoding,<python><numpy><pytorch><torch>,1,0,0.0,,,CC BY-SA 4.0
68691450,1,68933588.0,,2021-08-07 10:19:53,,6,3539,"<p>This question is the same with <a href=""https://datascience.stackexchange.com/questions/99815/how-can-i-check-a-confusion-matrix-after-fine-tuning-with-custom-datasets"">How can I check a confusion_matrix after fine-tuning with custom datasets?</a>, on Data Science Stack Exchange.</p>
<h2>Background</h2>
<p>I would like to check a confusion_matrix, including precision, recall, and f1-score like below after fine-tuning with custom datasets.</p>
<p>Fine tuning process and the task are <a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""noreferrer"">Sequence Classification with IMDb Reviews</a> on the <a href=""https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainer"" rel=""noreferrer"">Fine-tuning with custom datasets tutorial on Hugging face</a>.</p>
<p>After finishing the fine-tune with Trainer, how can I check a confusion_matrix in this case?</p>
<p>An image of confusion_matrix, including precision, recall, and f1-score <a href=""http://www.renom.jp/notebooks/product/renom_dl/trainer/notebook.html"" rel=""noreferrer"">original site</a>: just for example output image</p>
<pre><code>predictions = np.argmax(trainer.test(test_x), axis=1)

# Confusion matrix and classification report.
print(classification_report(test_y, predictions))

            precision    recall  f1-score   support

          0       0.75      0.79      0.77      1000
          1       0.81      0.87      0.84      1000
          2       0.63      0.61      0.62      1000
          3       0.55      0.47      0.50      1000
          4       0.66      0.66      0.66      1000
          5       0.62      0.64      0.63      1000
          6       0.74      0.83      0.78      1000
          7       0.80      0.74      0.77      1000
          8       0.85      0.81      0.83      1000
          9       0.79      0.80      0.80      1000

avg / total       0.72      0.72      0.72     10000
</code></pre>
<h2>Code</h2>
<pre class=""lang-py prettyprint-override""><code>from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
)

model = DistilBertForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)

trainer.train()
</code></pre>
<h2>What I did so far</h2>
<p>Data set Preparation for <a href=""https://huggingface.co/transformers/custom_datasets.html#sequence-classification-with-imdb-reviews"" rel=""noreferrer"">Sequence Classification with IMDb Reviews</a>, and I'm fine-tuning with Trainer.</p>
<pre><code>from pathlib import Path

def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [&quot;pos&quot;, &quot;neg&quot;]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is &quot;neg&quot; else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

import torch

class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = IMDbDataset(train_encodings, train_labels)
val_dataset = IMDbDataset(val_encodings, val_labels)
test_dataset = IMDbDataset(test_encodings, test_labels)
</code></pre>
",14497686.0,,6117017.0,,2023-05-12 14:11:03,2023-05-12 14:11:03,How can I check a confusion_matrix after fine-tuning with custom datasets?,<python-3.x><machine-learning><pytorch><nlp><huggingface-transformers>,1,1,0.0,,,CC BY-SA 4.0
69399917,1,69413191.0,,2021-09-30 23:58:24,,6,4238,"<p>how will you decide what precision works best for your inference model? Both BF16 and F16 takes two bytes but they use different number of bits for fraction and exponent.</p>
<p>Range will be different but I am trying to understand why one chose one over other.</p>
<p>Thank you</p>
<pre><code>    |--------+------+----------+----------|
    | Format | Bits | Exponent | Fraction |
    |--------+------+----------+----------|
    | FP32   |   32 |        8 |       23 |
    | FP16   |   16 |        5 |       10 |
    | BF16   |   16 |        8 |        7 |
    |--------+------+----------+----------|

Range
bfloat16: ~1.18e-38 … ~3.40e38 with 3 significant decimal digits.
float16:  ~5.96e−8 (6.10e−5) … 65504 with 4 significant decimal digits precision.

</code></pre>
",496553.0,,496553.0,,2021-10-05 20:46:31,2021-10-05 20:46:31,How to select half precision (BFLOAT16 vs FLOAT16) for your trained model?,<tensorflow><machine-learning><deep-learning><pytorch><half-precision-float>,1,2,0.0,,,CC BY-SA 4.0
68221962,1,68238040.0,,2021-07-02 08:26:40,,6,21614,"<p>I was trying to run <code>nvcc -V</code> to check cuda version but I got the following error message.</p>
<p><em><strong>Command 'nvcc' not found, but can be installed with:
sudo apt install nvidia-cuda-toolkit</strong></em></p>
<p>But gpu acceleration is working fine for training models on cuda. Is there another way to find out cuda <strong>compiler tools version</strong>. I know <code>nvidia-smi</code> doesn't give the right version.
Is there a way to install or configure nvcc. So I don't have to install a whole new toolkit.</p>
",11008364.0,,,,,2022-06-06 19:59:06,nvcc not found but cuda runs fine?,<cuda><pytorch>,2,2,,,,CC BY-SA 4.0
65860764,1,65861961.0,,2021-01-23 15:07:47,,6,14346,"<p>I am new in PyTorch and I have faced one issue, namely I cannot get my torch_sparse module properly installed.
In general, I wanted to use module <code>torch_geometric</code> - this I have installed. However, when during the execution of the program I keep receiving the error ModuleNotFoundError: No module named ‘torch_sparse’ .</p>
<p>I try to intall it, but when I use the command <code>pip install torch-sparse</code> in anaconda, I get an error:</p>
<p>UserWarning: CUDA initialization:Found no NVIDIA driver on your system.</p>
<p>My system does not have a CUDA. So how could I install <code>torch_sparse</code> module without it?</p>
<p>Thank you in advance!</p>
<p>Kind regards</p>
<p>Rostyslav</p>
",14735646.0,,,,,2023-03-14 13:50:53,PyTorch torch_sparse installation without CUDA,<python><module><pip><pytorch>,2,0,0.0,,,CC BY-SA 4.0
65156637,1,,,2020-12-05 11:55:28,,6,648,"<p>In <a href=""https://www.tensorflow.org/tutorials/text/nmt_with_attention"" rel=""noreferrer"">this tutorial in tensorflow site</a> we can see a code for the implementation of an autoencoder which it's Decoder is as follows:</p>
<pre><code>class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):
    super(Decoder, self).__init__()
    self.batch_sz = batch_sz
    self.dec_units = dec_units
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.dec_units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.fc = tf.keras.layers.Dense(vocab_size)

    # used for attention
    self.attention = BahdanauAttention(self.dec_units)

  def call(self, x, hidden, enc_output):
    # enc_output shape == (batch_size, max_length, hidden_size)
    context_vector, attention_weights = self.attention(hidden, enc_output)

    # x shape after passing through embedding == (batch_size, 1, embedding_dim)
    x = self.embedding(x)

    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)
    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)

    # passing the concatenated vector to the GRU
    output, state = self.gru(x)

    # output shape == (batch_size * 1, hidden_size)
    output = tf.reshape(output, (-1, output.shape[2]))

    # output shape == (batch_size, vocab)
    x = self.fc(output)

    return x, state, attention_weights
</code></pre>
<p>the <code>BahdanauAttention</code> is applied to the output of the encoder and previous hidden state then it is concated with the lookup of the input and then is fed to <code>GRU</code>.</p>
<p>And yet <a href=""https://github.com/computationalmedia/semstyle"" rel=""noreferrer"">in another code from this github repository</a> (which is implemented using pytorch) the attention is applied to the output of the <code>GRU</code>:</p>
<pre><code>class DecoderAttn(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, out_bias):
        super(DecoderAttn, self).__init__()
        self.hidden_size = hidden_size
        self.input_size = input_size
        
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.emb_drop = nn.Dropout(0.2)
        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)
        self.gru_drop = nn.Dropout(0.2)
        self.mlp = nn.Linear(hidden_size*2, output_size)
        if out_bias is not None:
            out_bias_tensor = torch.tensor(out_bias, requires_grad=False)
            self.mlp.bias.data[:] = out_bias_tensor
        self.logsoftmax = nn.LogSoftmax(dim=2)
        
        self.att_mlp = nn.Linear(hidden_size, hidden_size, bias=False)
        self.attn_softmax = nn.Softmax(dim=2)
    
    def forward(self, input, hidden, encoder_outs):
        emb = self.embedding(input)
        out, hidden = self.gru(self.emb_drop(emb), hidden)
        
        out_proj = self.att_mlp(out)
        enc_out_perm = encoder_outs.permute(0, 2, 1)
        e_exp = torch.bmm(out_proj, enc_out_perm)
        attn = self.attn_softmax(e_exp)
        
        ctx = torch.bmm(attn, encoder_outs)
        
        full_ctx = torch.cat([self.gru_drop(out), ctx], dim=2)
        
        out = self.mlp(full_ctx)
        out = self.logsoftmax(out)
        return out, hidden, attn

</code></pre>
<p>I wonder if the second case is a mistake? If it is not a mistake what is the difference between it and the first decoder? How changing the attention place affect the output?</p>
",5516760.0,,,,,2020-12-16 16:50:36,Where should we put attention in an autoencoder?,<python><tensorflow><pytorch><attention-model>,0,5,0.0,,,CC BY-SA 4.0
72297590,1,,,2022-05-19 01:13:44,,6,15729,"<p>I get error on line <code>x_stats = dec(z).float()</code>.</p>
<pre><code>import torch.nn.functional as F

z_logits = enc(x)
z = torch.argmax(z_logits, axis=1)
z = F.one_hot(z, num_classes=enc.vocab_size).permute(0, 3, 1, 2).float()

x_stats = dec(z).float()
x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))
x_rec = T.ToPILImage(mode='RGB')(x_rec[0])

display_markdown('Reconstructed image:')
display(x_rec)
</code></pre>
<p>I tried to downgrade and reinstall the <code>torch</code> package but that didn't help the issue. My package version is <code>torch==1.11.0</code></p>
<p>Full traceback:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
/Users/hanpham/github/DALL-E/notebooks/usage.ipynb Cell 4' in &lt;cell line: 7&gt;()
      4 z = torch.argmax(z_logits, axis=1)
      5 z = F.one_hot(z, num_classes=enc.vocab_size).permute(0, 3, 1, 2).float()
----&gt; 7 x_stats = dec(z).float()
      8 x_rec = unmap_pixels(torch.sigmoid(x_stats[:, :3]))
      9 x_rec = T.ToPILImage(mode='RGB')(x_rec[0])

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1106 # If we don't have any hooks, we want to skip the rest of the logic in
   1107 # this function, and just call forward.
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
   1112 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.9/site-packages/dall_e/decoder.py:94, in Decoder.forward(self, x)
     91 if x.dtype != torch.float32:
     92     raise ValueError('input must have dtype torch.float32')
---&gt; 94 return self.blocks(x)

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1106 # If we don't have any hooks, we want to skip the rest of the logic in
   1107 # this function, and just call forward.
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
   1112 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/container.py:141, in Sequential.forward(self, input)
    139 def forward(self, input):
    140     for module in self:
--&gt; 141         input = module(input)
    142     return input

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1106 # If we don't have any hooks, we want to skip the rest of the logic in
   1107 # this function, and just call forward.
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
   1112 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/container.py:141, in Sequential.forward(self, input)
    139 def forward(self, input):
    140     for module in self:
--&gt; 141         input = module(input)
    142     return input

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1110, in Module._call_impl(self, *input, **kwargs)
   1106 # If we don't have any hooks, we want to skip the rest of the logic in
   1107 # this function, and just call forward.
   1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1109         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1110     return forward_call(*input, **kwargs)
   1111 # Do not call functions when jit is used
   1112 full_backward_hooks, non_full_backward_hooks = [], []

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/upsampling.py:154, in Upsample.forward(self, input)
    152 def forward(self, input: Tensor) -&gt; Tensor:
    153     return F.interpolate(input, self.size, self.scale_factor, self.mode, self.align_corners,
--&gt; 154                          recompute_scale_factor=self.recompute_scale_factor)

File /opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1185, in Module.__getattr__(self, name)
   1183     if name in modules:
   1184         return modules[name]
-&gt; 1185 raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
   1186     type(self).__name__, name))

AttributeError: 'Upsample' object has no attribute 'recompute_scale_factor'
</code></pre>
",7873644.0,,7873644.0,,2022-05-19 01:42:31,2023-03-13 06:43:14,AttributeError: 'Upsample' object has no attribute 'recompute_scale_factor',<python-3.x><pytorch><torch>,4,4,,,,CC BY-SA 4.0
63838417,1,63847789.0,,2020-09-10 22:24:09,,6,2524,"<p>I ran a reinforcement learning training script which used Pytorch and logged data to tensorboardX and saved checkpoints. Now I want to continue training. How do I tell tensorboardX to continue from where I left off? Thank you!</p>
",8097220.0,,,,,2020-09-19 09:45:16,Tensorboard resume training plot,<pytorch><tensorboard><tensorboardx>,1,0,0.0,,,CC BY-SA 4.0
68285810,1,68287429.0,,2021-07-07 12:11:55,,6,5406,"<p>I am running segmentation on yolact edge. I am trying to find coordinates of the minimu and maximum x and y pixel coordinated of the mask using my own algorithm.
I am trying to convert the values of a tuple to numpy. However I am getting the follwoing errror</p>
<pre><code>TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
</code></pre>
<p>Code</p>
<pre><code>
    xmin = []
    xmax = []
    y = []
    print(np.shape(t[3]))
    print(type(t[3][:][:][:]))
    #row = (t[3][1][360][:]==1).nonzero(as_tuple=True) 
    for i in range (0, 2):
        t_cpu = t[3].clone().detach().cpu()
        horizontal_translation = torch.where(t[3][i][:][:]==1)
        print(horizontal_translation)
        horizontal_translation_numpy = np.asarray(horizontal_translation[1])
        x_min = np.amin(horizontal_translation_numpy)
        x_max = np.amax(horizontal_translation_numpy)
        np.append(xmin,x_min)
        np.append(xmax, x_max)
    print(xmin)
    print(xmax)
</code></pre>
<p>Note:
t is a pytorch tensor that is output by the default program that contains mask data in t[3]. How do I fix this?</p>
<p>Output:</p>
<pre><code>torch.Size([2, 720, 1280])
&lt;class 'torch.Tensor'&gt;
(tensor([105, 105, 105,  ..., 503, 503, 503]), tensor([427, 428, 429,  ..., 468, 469, 470]))
Traceback (most recent call last):
  File &quot;eval.py&quot;, line 1303, in &lt;module&gt;
    evaluate(net, dataset)
  File &quot;eval.py&quot;, line 928, in evaluate
    evalimage(net, inp, out, detections=detections, image_id=&quot;0&quot;)
  File &quot;eval.py&quot;, line 621, in evalimage
    img_numpy = prep_display(preds, frame, None, None, undo_transform=False)
  File &quot;eval.py&quot;, line 198, in prep_display
    horizontal_translation_numpy = np.asarray(horizontal_translation[1])
  File &quot;/home/nvidia/.local/lib/python3.6/site-packages/numpy/core/_asarray.py&quot;, line 83, in asarray
    return array(a, dtype, copy=False, order=order)
  File &quot;/home/nvidia/.local/lib/python3.6/site-packages/torch/tensor.py&quot;, line 480, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.
</code></pre>
",13017267.0,,,,,2021-07-07 13:55:58,TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first (Segmentation using yolact edge),<python><pytorch><image-segmentation><tensor>,2,0,,,,CC BY-SA 4.0
63842961,1,63882976.0,,2020-09-11 07:54:29,,6,12923,"<p>I have a conda environment with PyTorch and Tensorflow, which both require CUDA 9.0 (~cudatoolkit 9.0 from conda). After installing pytorch with torchvision and the cudatoolkit (like they provided on their website) I wanted to install Tensorflow, the problem here is that I get this error:</p>
<pre><code>Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: / 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed                                                                                                   

UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:

  - tensorflow==1.12.0 -&gt; python[version='2.7.*|3.6.*']
  - tensorflow==1.12.0 -&gt; python[version='&gt;=2.7,&lt;2.8.0a0|&gt;=3.6,&lt;3.7.0a0']

Your python: python=3.5

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

The following specifications were found to be incompatible with your system:

  - feature:/linux-64::__cuda==10.2=0
  - feature:|@/linux-64::__cuda==10.2=0

Your installed version is: 10.2
</code></pre>
<p>If I run <code>nvcc</code> or <code>nvidia-smi</code> on my host or the activated conda environment, I get that I have installed CUDA 10.2, even though <code>conda list</code> shows me that cudatoolkit 9.0 is installed. Any solution to this?</p>
<p>EDIT:</p>
<p>When running this code sample:</p>
<pre><code># setting device on GPU if available, else CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Using device:', device)
print()

#Additional Info when using cuda
if device.type == 'cuda':
    print(torch.cuda.get_device_name(0))
    print('Memory Usage:')
    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')


print(torch.version.cuda)
</code></pre>
<p>I get this output:</p>
<pre><code>GeForce GTX 1050
Memory Usage:
Allocated: 0.0 GB
Cached:    0.0 GB
9.0.176
</code></pre>
<p>So PyTorch does get the correct CUDA version, I just cant get tensorflow-gpu installed.</p>
",9460996.0,,10749432.0,,2020-09-14 11:14:45,2020-09-23 13:35:55,Anaconda reading wrong CUDA version,<tensorflow><pytorch><conda><torch>,3,2,0.0,,,CC BY-SA 4.0
63141267,1,63177834.0,,2020-07-28 19:14:25,,6,11572,"<p>This is literally all the code that I am trying to run:</p>
<pre><code>from transformers import AutoModelWithLMHead, AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
</code></pre>
<p>I am getting this error:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-14-aad2e7a08a74&gt; in &lt;module&gt;
----&gt; 1 from transformers import AutoModelWithLMHead, AutoTokenizer
      2 import torch
      3 
      4 tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)
      5 model = AutoModelWithLMHead.from_pretrained(&quot;microsoft/DialoGPT-small&quot;)

ImportError: cannot import name 'AutoModelWithLMHead' from 'transformers' (c:\python38\lib\site-packages\transformers\__init__.py)
</code></pre>
<p>What do I do about it?</p>
",11037848.0,,,,,2020-07-30 16:53:17,ImportError: cannot import name 'AutoModelWithLMHead' from 'transformers',<python><pytorch><huggingface-transformers>,1,7,,,,CC BY-SA 4.0
72313510,1,,,2022-05-20 04:08:42,,6,3057,"<p>Since Pytorch GPU support for apple silicon was just released, I tried to install PyTorch using the steps on the following link. As of now, only a nightly build is available so I installed it. However, when I run the following code, I get the error.</p>
<h3>Link I followed:</h3>
<ol>
<li><a href=""https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/"" rel=""noreferrer"">https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/</a></li>
<li><a href=""https://pytorch.org/get-started/locally/"" rel=""noreferrer"">https://pytorch.org/get-started/locally/</a></li>
</ol>
<h3>Code:</h3>
<pre><code>from __future__ import print_function
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import StepLR


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output


def train(args, model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
            if args.dry_run:
                break


def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


def main():
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=14, metavar='N',
                        help='number of epochs to train (default: 14)')
    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',
                        help='learning rate (default: 1.0)')
    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',
                        help='Learning rate step gamma (default: 0.7)')
    parser.add_argument('--device', default='cpu',
                        help='choose device')
    parser.add_argument('--dry-run', action='store_true', default=False,
                        help='quickly check a single pass')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                        help='how many batches to wait before logging training status')
    parser.add_argument('--save-model', action='store_true', default=False,
                        help='For Saving the current Model')
    args = parser.parse_args()

    torch.manual_seed(args.seed)

    device = torch.device(args.device)

    train_kwargs = {'batch_size': args.batch_size}
    test_kwargs = {'batch_size': args.test_batch_size}
    #if use_cuda:
    #    cuda_kwargs = {'num_workers': 1,
    #                   'pin_memory': True,
    #                   'shuffle': True}
    #    train_kwargs.update(cuda_kwargs)
    #    test_kwargs.update(cuda_kwargs)

    transform=transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
        ])
    dataset1 = datasets.MNIST('../data', train=True, download=True,
                       transform=transform)
    dataset2 = datasets.MNIST('../data', train=False,
                       transform=transform)
    train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)
    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)

    model = Net().to(device)
    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)

    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)
    for epoch in range(1, args.epochs + 1):
        train(args, model, device, train_loader, optimizer, epoch)
        test(model, device, test_loader)
        scheduler.step()

    if args.save_model:
        torch.save(model.state_dict(), &quot;mnist_cnn.pt&quot;)


if __name__ == '__main__':
    main()
    # run with --device cpu or --device mps
</code></pre>
<h3>Error:</h3>
<h4>RuntimeError: The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using <code>sw_vers</code>. I am not sure how to use this argument <code>sw_vers</code>.</h4>
<h3>My Macbook specification:</h3>
<ul>
<li>Model: M1 Max</li>
<li>OS Version: 12.2.1</li>
</ul>
<p>From my inspection, it looks like my OS needs to at least 12.3 for it to run however it says I can use <code>sw_vers</code> to run it on my current OS. I don't want to update because of other libraries compatibility issue. Can anyone figure this out?</p>
",6117565.0,,,,,2023-03-26 12:33:04,RuntimeError: The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`,<python><pytorch><apple-silicon>,0,2,,,,CC BY-SA 4.0
70768868,1,70770694.0,,2022-01-19 10:22:10,,6,5480,"<p>PyTorch is capable of saving and loading the state of an optimizer. An example is shown <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training"" rel=""noreferrer"">in the PyTorch tutorial</a>. I'm currently just saving and loading the model state but not the optimizer. So what's the point of saving and loading the optimizer state besides not having to remember the optimizers params such as the learningrate. And what's contained in the optimizer state?</p>
",12655717.0,,,,,2022-01-19 12:36:41,PyTorch: What's the purpose of saving the optimizer state?,<pytorch>,2,0,,,,CC BY-SA 4.0
70191012,1,,,2021-12-01 20:48:16,,6,646,"<p>I have implemented PPO for Cartpole-VO environment. However, it does not converge in certain iterations of the game. Sometimes it gets stuck in local optima. I have implemented the algorithm using the TD-0 advantage i.e.</p>
<p><code>A(s_t) = R(t+1) + \gamma V(S_{t+1}) - V(S_t)</code></p>
<p>Here is my code:</p>
<pre><code>def running_average(x, n):
    N = n
    kernel = np.ones(N)
    conv_len = x.shape[0]-N
    y = np.zeros(conv_len)
    for i in range(conv_len):
        y[i] = kernel @ x[i:i+N] # matrix multiplication operator: np.mul
        y[i] /= N
    return y



class ActorNetwork(nn.Module):
    def __init__(self, state_dim, n_actions, learning_rate=0.0003, epsilon_clipping=0.3, update_epochs=10):
        super().__init__()
        self.n_actions = n_actions
        self.model = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, n_actions),
            nn.Softmax(dim=-1)
        ).float()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.epsilon_clipping = epsilon_clipping
        self.update_epochs = update_epochs

    def forward(self, X):
        return self.model(X)

    
    def predict(self, state):
        if state.ndim &lt; 2:
            action_probs = self.model(torch.FloatTensor(state).unsqueeze(0).float())
        else: 
            action_probs = self.model(torch.FloatTensor(state))

        return action_probs.squeeze(0).data.numpy()

   
    def update(self, states, actions, deltas, old_prob):
  
        batch_size = len(states)
        state_batch = torch.Tensor(states)
        action_batch = torch.Tensor(actions)
        delta_batch = torch.Tensor(deltas)
        old_prob_batch = torch.Tensor(old_prob)
        for k in range(self.update_epochs):
            pred_batch = self.model(state_batch)

            prob_batch = pred_batch.gather(dim=1, index=action_batch.long().view(-1, 1)).squeeze()

            ratio = torch.exp(torch.log(prob_batch) - torch.log(old_prob_batch))

            clipped = torch.clamp(ratio, 1 - self.epsilon_clipping, 1 + self.epsilon_clipping) * delta_batch
            loss_r = -torch.min(ratio*delta_batch, clipped)
            loss = torch.mean(loss_r)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()




class CriticNetwork(nn.Module):
    def __init__(self, state_dim, learning_rate=0.001):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
        ).float()
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)


    def forward(self, X):
        return self.model(X)

    def predict(self, state):
        if state.ndim &lt; 2:
            values = self.model(torch.FloatTensor(state).unsqueeze(0).float())
        else:
            values = self.model(torch.FloatTensor(state))

        return values.data.numpy()

  
    def update(self, states, targets):
        
        state_batch = torch.Tensor(states)
        target_batch = torch.Tensor(targets)
        pred_batch = self.model(state_batch)
        loss = torch.nn.functional.mse_loss(pred_batch, target_batch.unsqueeze(1))
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    


def train_ppo_agent(env, episode_length, max_episodes, gamma, visualize_step, learning_rate_actor=0.0003, learning_rate_critic=0.001, epsilon_clipping=0.2, actor_update_epochs=10):

  
    model_actor = ActorNetwork(env.observation_space.shape[0], env.action_space.n, learning_rate=learning_rate_actor,
                               epsilon_clipping=epsilon_clipping, update_epochs=actor_update_epochs)
    model_critic = CriticNetwork(env.observation_space.shape[0], learning_rate=learning_rate_critic)



    EPISODE_LENGTH = episode_length
    MAX_EPISODES = max_episodes
    GAMMA = gamma
    VISUALIZE_STEP = max(1, visualize_step)
    score = []


    for episode in range(MAX_EPISODES):
        curr_state = env.reset()
        done = False
        all_episode_t = []
        score_episode = 0
        for t in range(EPISODE_LENGTH):
            act_prob = model_actor.predict(curr_state)
            action = np.random.choice(np.array(list(range(env.action_space.n))), p=act_prob)
            value = model_critic.predict(curr_state)
            prev_state = curr_state
            curr_state, reward, done, info = env.step(action)
            score_episode += reward
            e_t = {'state': prev_state, 'action':action, 'action_prob':act_prob[action],'reward': reward, 'value': value}
            all_episode_t.append(e_t)
            if done:
                break
        score.append(score_episode)

        episode_values = [all_episode_t[t]['value'] for t in range(len(all_episode_t))]
        next_state_estimates = [episode_values[i].item() for i in range(1, len(episode_values))]
        next_state_estimates.append(0)
        boostrap_estimate = []
        for t in range(len(all_episode_t)):
            G = all_episode_t[t]['reward'] + GAMMA * next_state_estimates[t]
            boostrap_estimate.append(G)

        episode_target = np.array(boostrap_estimate)
        episode_values = np.array(episode_values)
        # compute the advantage for each state in the episode: R_{t+1} + \gamma * V(S_{t+1}) - V_{t}
        adv_batch = episode_target-episode_values
       
        state_batch = np.array([all_episode_t[t]['state'] for t in range(len(all_episode_t))])
        action_batch = np.array([all_episode_t[t]['action'] for t in range(len(all_episode_t))])
        old_actor_prob = np.array([all_episode_t[t]['action_prob'] for t in range(len(all_episode_t))])
       
        model_actor.update(state_batch, action_batch, adv_batch, old_actor_prob)
       
        model_critic.update(state_batch, episode_target)

        # print the status after every VISUALIZE_STEP episodes
        if episode % VISUALIZE_STEP == 0 and episode &gt; 0:
            print('Episode {}\tAverage Score: {:.2f}'.format(episode, np.mean(score[-VISUALIZE_STEP:-1])))
            # domain knowledge applied to stop training: if the average score across last 100 episodes is greater than 195, game is solved
            if np.mean(score[-100:-1]) &gt; 195:
                break


    # Training plot: Episodic reward over Training Episodes
    score = np.array(score)
    avg_score = running_average(score, visualize_step)
    plt.figure(figsize=(15, 7))
    plt.ylabel(&quot;Episodic Reward&quot;, fontsize=12)
    plt.xlabel(&quot;Training Episodes&quot;, fontsize=12)
    plt.plot(score, color='gray', linewidth=1)
    plt.plot(avg_score, color='blue', linewidth=3)
    plt.scatter(np.arange(score.shape[0]), score, color='green', linewidth=0.3)
    plt.savefig(&quot;temp/cartpole_ppo_training_plot.pdf&quot;)

    # return the trained models
    return model_actor, model_critic

def main():
    env = gym.make('CartPole-v0')
    episode_length = 300
    n_episodes = 5000
    gamma = 0.99
    vis_steps = 100
    learning_rate_actor = 0.0003
    actor_update_epochs = 10
    epsilon_clipping = 0.2
    learning_rate_critic = 0.001
   
    # train the PPO agent
    model_actor, model_critic = train_ppo_agent(env, episode_length, n_episodes, gamma, vis_steps,
                                               learning_rate_actor=learning_rate_actor,
                                               learning_rate_critic=learning_rate_critic,
                                               epsilon_clipping=epsilon_clipping,
                                               actor_update_epochs=actor_update_epochs)
</code></pre>
<p>Am I missing something, or is this kind of behaviour expected if one uses simple TD-0 advantages for PPO, given the nature of the Cartpole environment?</p>
",6526722.0,,10312071.0,,2021-12-04 12:13:22,2021-12-10 13:41:23,PyTorch PPO implementation for Cartpole-v0 getting stuck in local optima,<python><machine-learning><pytorch><reinforcement-learning><policy-gradient-descent>,1,1,,,,CC BY-SA 4.0
67461425,1,67461825.0,,2021-05-09 19:07:01,,6,2474,"<p>If I have a model:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim

class net_x(nn.Module): 
        def __init__(self):
            super(net_x, self).__init__()
            self.fc1=nn.Linear(2, 20) 
            self.fc2=nn.Linear(20, 20)
            self.out=nn.Linear(20, 4) 

        def forward(self, x):
            x=self.fc1(x)
            x=self.fc2(x)
            x=self.out(x)
            return x

nx = net_x()
</code></pre>
<p>And then I'm defining my inputs, optimizer (with <code>lr=0.1</code>), scheduler (with <code>base_lr=1e-3</code>), and training:</p>
<pre><code>r = torch.tensor([1.0,2.0])
optimizer = optim.Adam(nx.parameters(), lr = 0.1)
scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-3, max_lr=0.1, step_size_up=1, mode=&quot;triangular2&quot;, cycle_momentum=False)

path = 'opt.pt'
for epoch in range(10):
    optimizer.zero_grad()
    net_predictions = nx(r)
    loss = torch.sum(torch.randint(0,10,(4,)) - net_predictions)
    loss.backward()
    optimizer.step()
    scheduler.step()
    print('loss:' , loss)
    
    #save state dict
    torch.save({    'epoch': epoch,
                    'net_x_state_dict': nx.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler': scheduler.state_dict(),    
                    }, path)
#loading state dict
checkpoint = torch.load(path)        
nx.load_state_dict(checkpoint['net_x_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler'])
</code></pre>
<p>The optimizer seems to take the learning rate of the scheduler</p>
<pre><code>for g in optimizer.param_groups:
    print(g)
&gt;&gt;&gt;
{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'initial_lr': 0.001, 'params': [Parameter containing:
</code></pre>
<p>Does the learning rate scheduler overwrite the optimizer? How does it connect to it? Trying to understand the relation between them (i.e how they interact, etc.)</p>
",14735451.0,,,,,2021-05-09 19:53:37,What is the relation between a learning rate scheduler and an optimizer?,<python><pytorch>,1,0,0.0,,,CC BY-SA 4.0
63185157,1,63205764.0,,2020-07-31 02:59:39,,6,793,"<p>According to the official documents, using <code>train()</code> or <code>eval()</code> will have effects on certain modules. However, now I wish to achieve a similar thing with my custom module, i.e. it does something when <code>train()</code> is turned on, and something different when <code>eval()</code> is turned on. How can I do this?</p>
",11790637.0,,,,,2020-08-01 13:32:53,Can I make my custom pytorch modules behave differently when train() or eval() are called?,<python><pytorch>,1,2,,,,CC BY-SA 4.0
63583062,1,,,2020-08-25 16:16:35,,6,8713,"<pre><code>import os.path as osp
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.datasets import MNISTSuperpixels
import torch_geometric.transforms as T
from torch_geometric.data import DataLoader
from torch_geometric.utils import normalized_cut
from torch_geometric.nn import (NNConv, graclus, max_pool, max_pool_x, global_mean_pool)

path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'MNIST')

transform = T.Cartesian(cat=False)
train_dataset = MNISTSuperpixels(path, True, transform=transform)
test_dataset = MNISTSuperpixels(path, False, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)
d = train_dataset
</code></pre>
<p>I'm trying to use MNISTSuperpixels data for graph convolution, but I have some troubles using the example code.
Most of scripts were using
<code>path = osp.join(osp.dirname(osp.realpath(__file__)), '..', 'data', 'MNIST')</code>
However, they gave me an error
<code>NameError: name '__file__' is not defined</code> and I don't understand what <code>osp.realpath(__file__)</code> really means.</p>
<p>I'm using Jupyter notebook on Ubuntu, and my working directory is</p>
<pre><code>print(os.getcwd())
/home/hkimlx/GDL/pytorch_geometric/examples
</code></pre>
<p>which is the same directory where the sample code <code>mnist_nn_conv.py</code> is located.</p>
<p>Please help me. Thanks!</p>
",11299809.0,,,,,2020-11-13 19:51:36,path problem : NameError: name '__file__' is not defined,<python><pytorch><conv-neural-network>,2,1,,,,CC BY-SA 4.0
65920683,1,65920758.0,,2021-01-27 14:00:23,,6,2348,"<p>When testing a network in PyTorch one can use <code>with torch.no_grad():</code>. What is the Libtorch (C++) equivalent?
Thanks!</p>
",13785949.0,,6331369.0,,2021-01-27 14:13:42,2021-01-27 14:13:42,What is the LibTorch equivalent to PyTorch's torch.no_grad?,<python><c++><pytorch><autograd><libtorch>,1,0,,,,CC BY-SA 4.0
68204161,1,68210747.0,,2021-07-01 04:06:08,,6,6690,"<p>I have a tensor <code>img</code> in PyTorch of size <code>bx2xhxw</code> and want to upsample it using <code>torch.nn.functional.interpolate</code>. But while interpolation I do not wish channel 1 to use information from channel 2. To do this should I do,</p>
<pre><code>img2 = torch.rand(b,2,2*h,2*w) # create a random torch tensor.
img2[:,0,:,:] = nn.functional.interpolate(img[:,0,:,:], [2*h,2*w], mode='bilinear', align_corners=True)
img2[:,1,:,:] = nn.functional.interpolate(img[:,1,:,:], [2*h,2*w], mode='bilinear', align_corners=True)
img=img2
</code></pre>
<p>or simply using</p>
<pre><code>img = nn.functional.interpolate(img, [2*h,2*w], mode='bilinear', align_corners=True)
</code></pre>
<p>will solve my purpose.</p>
",13049379.0,,,,,2021-07-01 13:35:06,What information does Pytorch nn.functional.interpolate use?,<python><pytorch><interpolation>,1,0,,,,CC BY-SA 4.0
66633813,1,66666648.0,,2021-03-15 07:22:13,,6,951,"<p>I'm trying to figure out how sequence to sequence loss is calculated. I am using the huggingface transformers library in this case, but this might actually be relevant to other DL libraries.</p>
<p>So to get the required data we can do:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import EncoderDecoderModel, BertTokenizer
import torch
import torch.nn.functional as F
torch.manual_seed(42)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
MAX_LEN = 128
tokenize = lambda x: tokenizer(x, max_length=MAX_LEN, truncation=True, padding=True, return_tensors=&quot;pt&quot;)

model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints
input_seq = [&quot;Hello, my dog is cute&quot;, &quot;my cat cute&quot;]
output_seq = [&quot;Yes it is&quot;, &quot;ok&quot;]
input_tokens = tokenize(input_seq)
output_tokens = tokenize(output_seq)

outputs = model(
    input_ids=input_tokens[&quot;input_ids&quot;], 
    attention_mask=input_tokens[&quot;attention_mask&quot;],
    decoder_input_ids=output_tokens[&quot;input_ids&quot;], 
    decoder_attention_mask=output_tokens[&quot;attention_mask&quot;],
    labels=output_tokens[&quot;input_ids&quot;], 
    return_dict=True)

idx = output_tokens[&quot;input_ids&quot;]
logits = F.log_softmax(outputs[&quot;logits&quot;], dim=-1)
mask = output_tokens[&quot;attention_mask&quot;]
</code></pre>
<h2>Edit 1</h2>
<p>Thanks to @cronoik I was able to replicate the loss calculated by huggingface as being:</p>
<pre class=""lang-py prettyprint-override""><code>output_logits = logits[:,:-1,:]
output_mask = mask[:,:-1]
label_tokens = output_tokens[&quot;input_ids&quot;][:, 1:].unsqueeze(-1)
select_logits = torch.gather(output_logits, -1, label_tokens).squeeze()
huggingface_loss = -select_logits.mean()
</code></pre>
<p>However, since the last two tokens of the second input is just padding, shouldn't we calculate the loss to be:</p>
<pre class=""lang-py prettyprint-override""><code>seq_loss = (select_logits * output_mask).sum(dim=-1, keepdims=True) / output_mask.sum(dim=-1, keepdims=True)
seq_loss = -seq_loss.mean()
</code></pre>
<p>^This takes into account the length of the sequence of each row of outputs, and the padding by masking it out. Think this is especially useful when we have batches of varying length outputs.</p>
",2530674.0,,2530674.0,,2021-03-16 00:28:55,2022-02-08 18:00:15,Sequence to Sequence Loss,<deep-learning><pytorch><huggingface-transformers>,2,1,0.0,,,CC BY-SA 4.0
70865699,1,70866502.0,,2022-01-26 15:10:49,,6,2243,"<p>I developed a custom dataset by using the PyTorch dataset class. The code is like that:</p>
<pre><code>class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, root_path, transform=None):
        self.path = root_path
        self.mean = mean
        self.std = std
        self.transform = transform
        self.images = []
        self.masks = []

        for add in os.listdir(self.path):
            # Some script to load file from directory and appending address to relative array
            ...

        self.masks.sort()
        self.images.sort()

    def __len__(self):
        return len(self.images)

    def __getitem__(self, item):
        image_address = self.images[item]
        mask_address = self.masks[item]



        if self.transform is not None:
            augment = self.transform(image=np.asarray(Image.open(image_address, 'r', None)),
                                     mask=np.asarray(Image.open(mask_address, 'r', None)))
            image = Image.fromarray(augment['image'])
            mask = augment['mask']

        if self.transform is None:
            image = np.asarray(Image.open(image_address, 'r', None))
            mask = np.asarray(Image.open(mask_address, 'r', None))

        # Handle Augmentation here

        return image, mask
</code></pre>
<p>Then I created an object from this class and passed it to torch.utils.data.DataLoader. Although this works well with DataLoader but with torch.utils.data.DataLoader2 I got a problem. The error is this:</p>
<blockquote>
<p><code>dataloader = torch.utils.data.DataLoader2(dataset=dataset, batch_size=2, pin_memory=True, num_workers=4)</code></p>
<blockquote>
<p>Exception: thread parallelism mode is not supported for old DataSets</p>
</blockquote>
</blockquote>
<p>My question is why DataLoader2 module was added to PyTorch what is different with DataLoader and what are its benefits?</p>
<p>PyTorch Version: <code>1.10.1</code></p>
",13503187.0,,,,,2022-01-26 16:05:19,What is different between DataLoader and DataLoader2 in PyTorch?,<python><deep-learning><pytorch><data-science>,1,0,0.0,,,CC BY-SA 4.0
70065235,1,70066487.0,,2021-11-22 11:43:46,,6,7941,"<p>I'm trying to understanding how <code>torch.nn.LayerNorm</code> works in a nlp model. Asuming the input data is a batch of sequence of word embeddings:</p>
<pre><code>batch_size, seq_size, dim = 2, 3, 4
embedding = torch.randn(batch_size, seq_size, dim)
print(&quot;x: &quot;, embedding)

layer_norm = torch.nn.LayerNorm(dim)
print(&quot;y: &quot;, layer_norm(embedding))

# outputs:
&quot;&quot;&quot;
x:  tensor([[[ 0.5909,  0.1326,  0.8100,  0.7631],
         [ 0.5831, -1.7923, -0.1453, -0.6882],
         [ 1.1280,  1.6121, -1.2383,  0.2150]],

        [[-0.2128, -0.5246, -0.0511,  0.2798],
         [ 0.8254,  1.2262, -0.0252, -1.9972],
         [-0.6092, -0.4709, -0.8038, -1.2711]]])
y:  tensor([[[ 0.0626, -1.6495,  0.8810,  0.7060],
         [ 1.2621, -1.4789,  0.4216, -0.2048],
         [ 0.6437,  1.0897, -1.5360, -0.1973]],

        [[-0.2950, -1.3698,  0.2621,  1.4027],
         [ 0.6585,  0.9811, -0.0262, -1.6134],
         [ 0.5934,  1.0505, -0.0497, -1.5942]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
&quot;&quot;&quot;
</code></pre>
<p>From the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html"" rel=""noreferrer"">document's description</a>, my understanding is that the mean and std are computed by all embedding values per sample. So I try to compute <code>y[0, 0, :]</code> manually:</p>
<pre><code>mean = torch.mean(embedding[0, :, :])
std = torch.std(embedding[0, :, :])
print((embedding[0, 0, :] - mean) / std)
</code></pre>
<p>which gives <code>tensor([ 0.4310, -0.0319,  0.6523,  0.6050])</code> and that's not the right output. I want to know what is the right way to compute <code>y[0, 0, :]</code>?</p>
",6670282.0,,,,,2022-11-17 07:44:01,Understanding torch.nn.LayerNorm in nlp,<python><pytorch><normalization>,1,0,,,,CC BY-SA 4.0
70891128,1,,,2022-01-28 09:01:45,,6,15194,"<p>I am trying to execute LIIF(<a href=""https://github.com/yinboc/liif"" rel=""noreferrer"">https://github.com/yinboc/liif</a>) and the following warning appears:</p>
<pre><code>/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
</code></pre>
",16742019.0,,16742019.0,,2022-03-29 10:48:14,2023-04-23 00:01:20,"torch meshgrid warning: in an upcoming release, it will be required to pass the indexing argument",<python><pytorch><grid><torch><user-warning>,2,3,,,,CC BY-SA 4.0
65540160,1,,,2021-01-02 14:25:26,,6,750,"<p>I am currently writing a C++ program which needs to do some analyses of the structure of a CNN model in torchScript format. I am using the C++ torch library the way it is shown on torch.org, loading in the model like so:</p>
<pre><code>#include &lt;torch/script.h&gt;
#include &lt;torch/torch.h&gt;

#include &lt;iostream&gt;
#include &lt;memory&gt;

int main(int argc, const char* argv[]) {
    if (argc != 2) {
        std::cerr &lt;&lt; &quot;usage: example-app &lt;path-to-exported-script-module&gt;\n&quot;;
        return -1;
    }

    torch::jit::script::Module module;
    try {
        // Deserialize the ScriptModule from a file using torch::jit::load().
        module = torch::jit::load(argv[1]);
    }
    catch (const c10::Error&amp; e) {
        std::cerr &lt;&lt; &quot;error loading the model\n&quot;;
        return -1;
    }

    return 0;
}
</code></pre>
<p>As far as I was able to figure out <code>module</code> consists of a collection of nested <code>torch::jit::script::Module</code>'s the lowest of which represent the built in functions. Im accessing those lowest modules as follows:</p>
<pre><code>void print_modules(const torch::jit::script::Module&amp; imodule) {

    for (const auto&amp; module : imodule.named_children()) {

        if(module.value.children().size() &gt; 0){
            print_modules(module.value);

        }
        else{

            std::cout &lt;&lt; module.name &lt;&lt; &quot;\n&quot;;

        }
    }
}
</code></pre>
<p>This function recursively goes trough the modules and prints the name of the lowest level ones, which correspond to the builtin functions of torch script.</p>
<p><strong>My question now is, how do I access the details for those builtin functions like for example the stride length for convolutions?</strong></p>
<p>I can't figure out for the life of me to get access to those basic attributes of modules.</p>
",5719079.0,,5719079.0,,2021-01-02 14:44:54,2021-01-02 14:44:54,How to access module attributes such as the stride of convolutions from jit::script::module,<pytorch><jit><libtorch><torchscript>,0,2,0.0,,,CC BY-SA 4.0
71957324,1,,,2022-04-21 15:48:34,,6,6883,"<p>I am using the emnist data set via the PyTorch datasets together with a neural network that expects a 3 Channel input.</p>
<p>I would like to use PyTorch transforms to copy my 1D greyscale into 3D so I can use the same net for 1D and 3D data.</p>
<p>Which transform can I use? Or how would I extend PyTorch transforms as suggested here: <a href=""https://stackoverflow.com/a/50530923/18895809"">https://stackoverflow.com/a/50530923/18895809</a></p>
",18895809.0,,,,,2022-12-08 12:42:28,Is there a PyTorch transform to go from 1 Channel data to 3 Channels?,<pytorch><pytorch-lightning><pytorch-dataloader>,3,0,,,,CC BY-SA 4.0
72504734,1,,,2022-06-05 04:12:53,,6,12335,"<p>Consider the following code for Linear Regression implemented using PyTorch:</p>
<h5>X is the input, Y is the output for the training set, w is the parameter that needs to be optimised</h5>
<pre><code>import torch

X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)
Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)

w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)

def forward(x):
    return w * x

def loss(y, y_pred):
    return ((y_pred - y)**2).mean()

print(f'Prediction before training: f(5) = {forward(5).item():.3f}')

learning_rate = 0.01
n_iters = 100

for epoch in range(n_iters):
    # predict = forward pass
    y_pred = forward(X)

    # loss
    l = loss(Y, y_pred)

    # calculate gradients = backward pass
    l.backward()

    # update weights
    #w.data = w.data - learning_rate * w.grad
    with torch.no_grad():
        w -= learning_rate * w.grad
    
    # zero the gradients after updating
    w.grad.zero_()

    if epoch % 10 == 0:
        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')
</code></pre>
<p>What does the 'with' block do? The requires_grad argument for w is already set to True. Why is it then being put under a with torch.no_grad() block?</p>
",19273534.0,,,,,2022-06-05 04:35:39,What is the purpose of with torch.no_grad():,<python><machine-learning><pytorch><linear-regression><gradient>,2,0,,,,CC BY-SA 4.0
63349832,1,,,2020-08-11 00:26:20,,6,11368,"<p>I had some code which worked on colab (gpu runtime) just a short while ago. Suddenly I am getting</p>
<p>The NVIDIA driver on your system is too old (found version 10010).</p>
<p>nvcc shows
Cuda compilation tools, release 10.1, V10.1.243</p>
<p>I tried torch versions 1.5.1, then 1.13.0. Both keep getting this error.</p>
<p>There is a discussion showing other people having doubts. with no clear resolution.
<a href=""https://github.com/pytorch/pytorch/issues/27738"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/27738</a></p>
<p>Anyone having the same problem?</p>
",1058511.0,,681865.0,,2020-08-11 05:50:43,2022-01-19 17:42:22,pytorch code sudden fails on colab with NVIDIA driver on your system is too old,<pytorch><google-colaboratory>,6,2,0.0,,,CC BY-SA 4.0
65619076,1,,,2021-01-07 19:54:35,,6,6204,"<p>I stumbled upon the method <code>add_module()</code> in a Pytorch model.</p>
<p><a href=""http://model%20=%20Sequential()%20model.add(Conv1D(filters=64,%20kernel_size=3,%20activation=%27relu%27,%20input_shape=(n_timesteps,n_features)))%20model.add(Conv1D(filters=64,%20kernel_size=3,%20activation=%27relu%27))%20model.add(Dropout(0.5))%20model.add(MaxPooling1D(pool_size=2))%20model.add(Flatten())%20model.add(Dense(100,%20activation=%27relu%27))%20model.add(Dense(n_outputs,%20activation=%27softmax%27))%20model.compile(loss=%27categorical_crossentropy%27,%20optimizer=%27adam%27,%20metrics=%5B%27accuracy%27%5D)"" rel=""noreferrer"">The doc</a> only states</p>
<blockquote>
<p>Adds a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
</blockquote>
<p>I don't understand what &quot;adding a child module&quot; means.</p>
<p>How is it different from just setting a pointer to the other module using <code>self._other module = other_module</code>?</p>
<p>What are the nuances?</p>
",913098.0,,,,,2021-04-23 17:41:32,what is Pytorch's add_module()?,<python><machine-learning><deep-learning><pytorch>,3,0,0.0,,,CC BY-SA 4.0
72505821,1,,,2022-06-05 08:24:55,,6,3182,"<p>The latest preview build supports <a href=""https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/"" rel=""noreferrer"">accelerated training on M1 MacBook Pros</a>. This feature is currently only supported by the newest preview (nightly) build:</p>
<blockquote>
<p>To get started, just install the latest Preview (Nightly) build on
your Apple silicon Mac running macOS 12.3 or later with a native
version (arm64) of Python.</p>
</blockquote>
<p>According to <a href=""https://pytorch.org/get-started/locally/"" rel=""noreferrer"">the documentation</a>, this is how to install the latest preview build via <code>pip</code>:</p>
<pre><code>pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu
</code></pre>
<p>But I'm using <a href=""https://python-poetry.org"" rel=""noreferrer"">Poetry</a> to manage my Python project dependencies.</p>
<p>It doesn't seem like Poetry supports the <code>--pre</code> option:</p>
<pre><code>$ poetry add --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu 

  Stack trace:

  11  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/console_application.py:123 in run
      io = io_factory(

  10  ~/.poetry/lib/poetry/console/config/application_config.py:221 in create_io
      resolved_command = application.resolve_command(args)

   9  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/console_application.py:110 in resolve_command
      return self._config.command_resolver.resolve(args, self)

   8  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/resolver/default_resolver.py:34 in resolve
      return self.create_resolved_command(result)

   7  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/resolver/default_resolver.py:166 in create_resolved_command
      if not result.is_parsable():

   6  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/resolver/resolve_result.py:43 in is_parsable
      self._parse()

   5  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/resolver/resolve_result.py:49 in _parse
      self._parsed_args = self._command.parse(self._raw_args)

   4  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/api/command/command.py:113 in parse
      return self._config.args_parser.parse(args, self._args_format, lenient)

   3  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/args/default_args_parser.py:53 in parse
      self._parse(args, _fmt, lenient)

   2  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/args/default_args_parser.py:101 in _parse
      self._parse_long_option(token, tokens, fmt, lenient)

   1  ~/.poetry/lib/poetry/_vendor/py3.9/clikit/args/default_args_parser.py:247 in _parse_long_option
      self._add_long_option(name, None, tokens, fmt, lenient)

  NoSuchOptionException

  The &quot;--pre&quot; option does not exist.

  at ~/.poetry/lib/poetry/_vendor/py3.9/clikit/args/default_args_parser.py:300 in _add_long_option
      296│     def _add_long_option(
      297│         self, name, value, tokens, fmt, lenient
      298│     ):  # type: (str, Optional[str], List[str], ArgsFormat, bool) -&gt; None
      299│         if not fmt.has_option(name):
    → 300│             raise NoSuchOptionException(name)
      301│ 
      302│         option = fmt.get_option(name)
      303│ 
      304│         if value is False:
</code></pre>
<p>How can I use Poetry to add the latest preview (nightly) build of PyTorch?</p>
",195964.0,,195964.0,,2022-06-05 08:38:33,2023-06-21 14:47:31,How to install the latest PyTorch preview (nightly) build via Poetry,<python><pip><pytorch><python-poetry><package-management>,2,1,,,,CC BY-SA 4.0
63463510,1,66735140.0,,2020-08-18 07:09:23,,6,1489,"<p>I'm using the following code to find the topk matches using pytorch:</p>
<pre><code>def find_top(self, x, y, n_neighbors, unit_vectors=False, cuda=False):
    if not unit_vectors:
        x = __to_unit_torch__(x, cuda=cuda)
        y = __to_unit_torch__(y, cuda=cuda)
    with torch.no_grad():
        d = 1. - torch.matmul(x, y.transpose(0, 1))
        values, indices = torch.topk(d, n_neighbors, dim=1, largest=False, sorted=True)
        return indices.cpu().numpy()
</code></pre>
<p>Unfortunately, it is throwing the following error:</p>
<pre><code>values, indices = torch.topk(d, n_neighbors, dim=1, largest=False, sorted=True)
RuntimeError: invalid argument 5: k not in range for dimension at /pytorch/aten/src/THC/generic/THCTensorTopK.cu:23
</code></pre>
<p>The size of d is <code>(1793,1) </code> . What am I missing?</p>
",13037510.0,,9067615.0,,2021-03-21 17:02:15,2021-03-21 17:14:57,Finding the top k matches in Pytorch,<python><python-3.x><pytorch><top-n>,1,0,,,,CC BY-SA 4.0
63466204,1,,,2020-08-18 09:56:21,,6,3994,"<p>I tried to use tensorboard in torch.utils, but it says &quot;module 'torch.utils' has no attribute 'tensorboard'&quot;.
My torch version is &quot;1.6.0+cu101&quot;</p>
<pre><code>PS C:\Users\kelekelekle&gt; python
Python 3.7.6 (tags/v3.7.6:43364a7ae0, Dec 19 2019, 01:54:44) [MSC v.1916 64 bit (AMD64)] on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; print(torch.__version__)
1.6.0+cu101
&gt;&gt;&gt; writer = torch.utils.tensorboard.SummaryWriter()
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
AttributeError: module 'torch.utils' has no attribute 'tensorboard'
&gt;&gt;&gt;
</code></pre>
",14124601.0,,,,,2020-08-18 10:55:22,AttributeError: module 'torch.utils' has no attribute 'tensorboard',<python><deep-learning><pytorch><tensorboard>,1,0,,,,CC BY-SA 4.0
66995280,1,,,2021-04-07 23:09:49,,6,2299,"<p>I'm trying to improve a CNN I made by implementing a weighted loss method described in <a href=""https://arxiv.org/pdf/1803.09050.pdf"" rel=""noreferrer"">this paper</a>. To do this, I looked into <a href=""https://github.com/danieltan07/learning-to-reweight-examples/blob/master/Learning%20to%20Reweight%20Examples%20for%20Robust%20Deep%20Learning.ipynb"" rel=""noreferrer"">this notebook</a> which implements the pseudo-code of the method described in the paper.</p>
<p>When translating their code to my model, I ran into the error <code>RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior</code> when using <code>torch.autograd.grad()</code>.</p>
<p>My code and the error is in the second to last line:</p>
<pre><code>for epoch in range(1): #tqdm(range(params['epochs'])):
  model.train()
  text_t, labels_t = next(iter(train_iterator))
  text_t = to_var(text_t, requires_grad=False)
  labels_t = to_var(labels_t, requires_grad=False)
  dummy = L2RWCNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, 
                DROPOUT, PAD_IDX)
  dummy.state_dict(model.state_dict())
  dummy.cuda()
  y_f_hat = dummy(text_t)
  cost = F.binary_cross_entropy_with_logits(y_f_hat.squeeze(), labels_t, reduce = False)
  eps = to_var(torch.zeros(cost.size()))
  l_f_meta = torch.sum(cost * eps)
  dummy.zero_grad()
  num_params = 0
  grads = torch.autograd.grad(l_f_meta, (dummy.params()), create_graph = True)
  with torch.no_grad():
    for p, grad in zip(dummy.parameters(), grads):
      tmp = p - params['lr'] * grad
      p.copy_(tmp)
  text_v, labels_v = next(iter(valid_iterator))
  y_g_hat = dummy(text_v)
  l_g_meta = F.binary_cross_entropy_with_logits(y_g_hat.squeeze(), labels_v, reduce = False)
  l_g_meta = torch.sum(l_g_meta)
  grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]
  print(grad_eps)
</code></pre>
<p>I think the error is because <code>eps</code> was not in any previous <code>torch.autograd.grad()</code> calls. I tried the suggested solution of setting <code>allow_unused=True</code> but that resulted in a <code>None</code> value. I looked at <a href=""https://stackoverflow.com/questions/54763081/using-autograd-grad-as-a-parameter-for-a-loss-function-pytorch"">this post</a> to find a solution, but the method that fixed the problem here (don't slice the tensors) doesn't work for me because I'm not passing in any partial variables. I also tried setting <code>create_graph = False</code> in my first <code>autograd.grad()</code> call, but that didn't fix the issue. Does anyone have a solution?</p>
<p><a href=""https://colab.research.google.com/drive/1aY7XlgUClEe5Ldeu8DjSsxuj9sXTtRw4?usp=sharing"" rel=""noreferrer"">My full code if that's needed</a></p>
<p>EDIT:
Created a new post with a different phrasing to the question <a href=""https://stackoverflow.com/questions/67008171/pytorch-backwards-automatic-diff-with-weighted-loss"">here</a></p>
",11397075.0,,11397075.0,,2021-04-08 16:29:36,2021-04-08 16:29:36,PyTorch Autograd Differentiated Tensors appears to not have been used in the graph,<python><pytorch><conv-neural-network><gradient-descent><autograd>,0,1,0.0,,,CC BY-SA 4.0
66493943,1,66495578.0,,2021-03-05 14:05:03,,6,12798,"<p>Following my previous <a href=""https://stackoverflow.com/questions/66452650/extracting-features-of-the-hidden-layer-of-an-autoencoder-using-pytorch/66453597#66453597"">question</a> , I have written this code to train an autoencoder and then extract the features.
(There might be some changes in the variable names)</p>
<pre><code># Autoencoder class
#https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1
class AE_class(nn.Module):
    def __init__(self, **kwargs):
        super().__init__()
        self.encoder_hidden_layer = nn.Linear(
            in_features=kwargs[&quot;input_shape&quot;], out_features=128
        )
        self.encoder_output_layer = nn.Linear(
            in_features=128, out_features=128
        )
        self.decoder_hidden_layer = nn.Linear(
            in_features=128, out_features=128
        )
        self.decoder_output_layer = nn.Linear(
            in_features=128, out_features=kwargs[&quot;input_shape&quot;]
        )

    def forward(self, features):
        #print(&quot;in forward&quot;)
        #print(type(features))
        activation = self.encoder_hidden_layer(features)
        activation = torch.relu(activation)
        code = self.encoder_output_layer(activation)
        code = torch.relu(code)
        activation = self.decoder_hidden_layer(code)
        activation = torch.relu(activation)
        activation = self.decoder_output_layer(activation)
        reconstructed = torch.relu(activation)
        return reconstructed
    
    def encode(self, features_h):
        activation_h = self.encoder_hidden_layer(features_h)
        activation_h = torch.relu(activation_h)
        code_h = self.encoder_output_layer(activation_h)
        code_h = torch.relu(code_h)
        return code_h
   
</code></pre>
<p>And then, for training:</p>
<pre><code>def retrieve_AE_features(X_before, n_voxel_region):
    
        #  use gpu if available
    #https://discuss.pytorch.org/t/runtimeerror-tensor-for-out-is-on-cpu-tensor-for-argument-1-self-is-on-cpu-but-expected-them-to-be-on-gpu-while-checking-arguments-for-addmm/105453
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    # create a model from `AE` autoencoder class
    # load it to the specified device, either gpu or cpu
    model_AE = AE_class(input_shape=n_voxel_region).to(device)

    # create an optimizer object
    # Adam optimizer with learning rate 1e-3
    optimizer = optim.Adam(model_AE.parameters(), lr=1e-3)

    # mean-squared error loss
    criterion = nn.MSELoss()

    
    
    
    X_tensor = torch.tensor(X_before, dtype=torch.float32)
    
    print(type(X_tensor))
                            
    
    train_loader = torch.utils.data.DataLoader(
        X_tensor, batch_size=64, shuffle=True, num_workers=2, pin_memory=True
    )
    
    test_loader = torch.utils.data.DataLoader(
        X_tensor, batch_size=32, shuffle=False, num_workers=2
    )
    
    print(type(train_loader))

    for epoch in range(epochs_AE):
        loss = 0
        
        for batch_features in train_loader:
            # reshape mini-batch data to [N, 784] matrix
            # load it to the active device
            #batch_features = batch_features.view(-1, 784).to(device)
            
            #print(batch_features.shape)

            # reset the gradients back to zero
            # PyTorch accumulates gradients on subsequent backward passes
            optimizer.zero_grad()

            # compute reconstructions
            outputs = model_AE(batch_features)

            # compute training reconstruction loss
            train_loss = criterion(outputs, batch_features)

            # compute accumulated gradients
            train_loss.backward()

            # perform parameter update based on current gradients
            optimizer.step()

            # add the mini-batch training loss to epoch loss
            loss += train_loss.item()

        # compute the epoch training loss
        loss = loss / len(train_loader)

        # display the epoch training loss
        
        print(&quot;AE, epoch : {}/{}, loss = {:.6f}&quot;.format(epoch + 1, epochs_AE, loss))
        
        
    #After training
    hidden_features = model_AE.encode(X_before)
    return hidden_features
</code></pre>
<p>However, I received the following error:</p>
<blockquote>
<p>Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU
(while checking arguments for addmm)</p>
</blockquote>
<p>It seems some of my variables should be defined in another way to be able to be executed on GPU.</p>
<p>My questions:</p>
<ol>
<li>How can I understand which variables will be executed on GPU and which ones on CPU?</li>
<li>How to fix it? In other words, how to define a variable executable on GPU?</li>
</ol>
<p>Thanks in advance</p>
",2178942.0,,,,,2021-06-09 15:00:12,"Tensor for argument #2 'mat1' is on CPU, but expected it to be on GPU",<python><debugging><pytorch><gpu><autoencoder>,1,0,0.0,,,CC BY-SA 4.0
64631086,1,64632356.0,,2020-11-01 10:56:06,,6,10429,"<p>I am working with Keras and trying to analyze the effects on accuracy that models which are built with some layers with meaningful weights, and some layers with random initializations.</p>
<h2>Keras:</h2>
<p>I load <code>VGG19</code> pre-trained model with <code>include_top = False</code> parameter on load method.</p>
<pre class=""lang-py prettyprint-override""><code>model = keras.applications.VGG19(include_top=False, weights=&quot;imagenet&quot;, input_shape=(img_width, img_height, 3))
</code></pre>
<h2>PyTorch:</h2>
<p>I load <code>VGG19</code> pre-trained model until the same layer with the previous model which loaded with Keras.</p>
<pre class=""lang-py prettyprint-override""><code>model = torch.hub.load('pytorch/vision:v0.6.0', 'vgg19', pretrained=True)
new_base =  (list(model.children())[:-2])[0]
</code></pre>
<p>After loaded models following images shows summary of them. (<code>Pytorch</code>, <code>Keras</code>)</p>
<p><a href=""https://i.stack.imgur.com/i2wzT.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/i2wzT.png"" alt=""PyTorch - VGG19 model without top layers"" /></a></p>
<p><a href=""https://i.stack.imgur.com/sjKD4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/sjKD4.png"" alt=""Keras - VGG19 model without top layers"" /></a></p>
<p>So far there is no problem. After that, I want to add a Flatten layer and a Fully connected layer on these pre-trained models. I did it with Keras but I couldn't with PyTorch.</p>
<p><a href=""https://i.stack.imgur.com/gfMux.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gfMux.png"" alt=""Keras - Add new layers on pretrained model"" /></a></p>
<p>The output of new_model.summary() is that:</p>
<p><a href=""https://i.stack.imgur.com/NnO9y.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NnO9y.png"" alt=""Model summary after adding the new layers"" /></a></p>
<p>My question is, how can I add a new layer in <code>PyTorch</code>?</p>
",6742917.0,,472495.0,,2023-03-30 17:39:11,2023-03-30 17:39:11,How can I add new layers on pre-trained model with PyTorch? (Keras example given),<python><keras><pytorch><vgg-net><pre-trained-model>,2,2,0.0,,,CC BY-SA 4.0
64629702,1,64629989.0,,2020-11-01 07:45:59,,6,31226,"<p>I want to convert images to tensor using <code>torchvision.transforms.ToTensor()</code>, after processing I printed the image but the image became so weird. Here is my code:</p>
<pre><code>trans = transforms.Compose([
    transforms.ToTensor()])

demo = Image.open(img) 
demo_img = trans(demo)
demo_array = demo_img.numpy()*255
print(Image.fromarray(demo_array.astype(np.uint8)))
</code></pre>
<p>The original image is <a href=""https://i.stack.imgur.com/U2aiK.jpg"" rel=""noreferrer"">this</a></p>
<p>But after processing it is showed like <a href=""https://i.stack.imgur.com/XnYAt.png"" rel=""noreferrer"">this</a></p>
<p>Did I write something wrong or miss something?</p>
",12422410.0,,8890604.0,,2020-11-01 08:30:48,2020-11-01 08:41:45,Pytorch transform.ToTensor() changes image,<python><pytorch><python-imaging-library><data-science>,1,0,0.0,,,CC BY-SA 4.0
64685062,1,64703152.0,,2020-11-04 17:38:36,,6,4645,"<p>I can set up a conda environment successfully as follows:</p>
<pre class=""lang-bash prettyprint-override""><code>conda create --name temp python=3.8.5
conda install pytorch==1.6.0 torchvision==0.7.0 cpuonly -c pytorch
</code></pre>
<p>I then save the environment to a YAML config file. The looks like this:</p>
<pre class=""lang-yaml prettyprint-override""><code>name: temp
channels:
  - pytorch
  - defaults
dependencies:
  - _libgcc_mutex=0.1=main
  - blas=1.0=mkl
  - ca-certificates=2020.10.14=0
  - certifi=2020.6.20=pyhd3eb1b0_3
  - cpuonly=1.0=0
  - freetype=2.10.4=h5ab3b9f_0
  - intel-openmp=2020.2=254
  - jpeg=9b=h024ee3a_2
  - lcms2=2.11=h396b838_0
  - ld_impl_linux-64=2.33.1=h53a641e_7
  - libedit=3.1.20191231=h14c3975_1
  - libffi=3.3=he6710b0_2
  - libgcc-ng=9.1.0=hdf63c60_0
  - libpng=1.6.37=hbc83047_0
  - libstdcxx-ng=9.1.0=hdf63c60_0
  - libtiff=4.1.0=h2733197_1
  - lz4-c=1.9.2=heb0550a_3
  - mkl=2020.2=256
  - mkl-service=2.3.0=py38he904b0f_0
  - mkl_fft=1.2.0=py38h23d657b_0
  - mkl_random=1.1.1=py38h0573a6f_0
  - ncurses=6.2=he6710b0_1
  - ninja=1.10.1=py38hfd86e86_0
  - numpy=1.19.2=py38h54aff64_0
  - numpy-base=1.19.2=py38hfa32c7d_0
  - olefile=0.46=py_0
  - openssl=1.1.1h=h7b6447c_0
  - pillow=8.0.1=py38he98fc37_0
  - pip=20.2.4=py38h06a4308_0
  - python=3.8.5=h7579374_1
  - pytorch=1.6.0=py3.8_cpu_0
  - readline=8.0=h7b6447c_0
  - setuptools=50.3.0=py38h06a4308_1
  - six=1.15.0=py_0
  - sqlite=3.33.0=h62c20be_0
  - tk=8.6.10=hbc83047_0
  - torchvision=0.7.0=py38_cpu
  - wheel=0.35.1=py_0
  - xz=5.2.5=h7b6447c_0
  - zlib=1.2.11=h7b6447c_3
  - zstd=1.4.5=h9ceee32_0
prefix: /data/anaconda/envs/temp
</code></pre>
<p>But if I try making a conda environment from the following file:</p>
<pre class=""lang-yaml prettyprint-override""><code>name: temp
channels:
  - defaults
  - pytorch
dependencies:
  - python==3.8.5
  - pytorch==1.6.0=py3.8_cpu_0
</code></pre>
<p>it fails, with the following incompatibilities:</p>
<pre class=""lang-none prettyprint-override""><code>UnsatisfiableError: The following specifications were found to be incompatible with each other:

Package ld_impl_linux-64 conflicts for:
python==3.8.5 -&gt; ld_impl_linux-64
Package sqlite conflicts for:
python==3.8.5 -&gt; sqlite[version='&gt;=3.32.3,&lt;4.0a0|&gt;=3.33.0,&lt;4.0a0']
Package * conflicts for:
pytorch==1.6.0=py3.8_cpu_0 -&gt; *[track_features=cpuonly]
Package ncurses conflicts for:
python==3.8.5 -&gt; ncurses[version='&gt;=6.2,&lt;7.0a0']
Package mkl conflicts for:
pytorch==1.6.0=py3.8_cpu_0 -&gt; mkl[version='&gt;=2018']
Package blas conflicts for:
pytorch==1.6.0=py3.8_cpu_0 -&gt; blas=[build=mkl]
Package zlib conflicts for:
python==3.8.5 -&gt; zlib[version='&gt;=1.2.11,&lt;1.3.0a0']
Package openssl conflicts for:
python==3.8.5 -&gt; openssl[version='&gt;=1.1.1g,&lt;1.1.2a']
Package python conflicts for:
pytorch==1.6.0=py3.8_cpu_0 -&gt; python[version='&gt;=3.8,&lt;3.9.0a0']
Package xz conflicts for:
python==3.8.5 -&gt; xz[version='&gt;=5.2.5,&lt;6.0a0']
Package libffi conflicts for:
python==3.8.5 -&gt; libffi[version='&gt;=3.3,&lt;3.4.0a0']
Package libgcc-ng conflicts for:
python==3.8.5 -&gt; libgcc-ng[version='&gt;=7.3.0']
Package tk conflicts for:
python==3.8.5 -&gt; tk[version='&gt;=8.6.10,&lt;8.7.0a0']
Package pip conflicts for:
python==3.8.5 -&gt; pip
Package ninja conflicts for:
pytorch==1.6.0=py3.8_cpu_0 -&gt; ninja
Package readline conflicts for:
python==3.8.5 -&gt; readline[version='&gt;=8.0,&lt;9.0a0']
Package numpy conflicts for:
pytorch==1.6.0=py3.8_cpu_0 -&gt; numpy[version='&gt;=1.11']
</code></pre>
<p>Why? How could such a simple configuration, which was cut down from a successful environment, fail? How should I specify the CPU-only version of pytorch 1.6.0 in a YAML config file for a conda environment?</p>
",575530.0,,,,,2021-01-04 17:53:39,Specifying cpu-only for pytorch in conda YAML file,<yaml><pytorch><conda><miniconda>,2,2,0.0,,,CC BY-SA 4.0
67746193,1,67746385.0,,2021-05-28 22:48:00,,6,13442,"<p>I am facing a very weird error when I am trying to run the following simple line :</p>
<pre><code>a = torch.Tensor([0,0,0],dtype = torch.int64)

TypeError: new() received an invalid combination of arguments - got (list, dtype=torch.dtype), but expected one of:
 * (*, torch.device device)
      didn't match because some of the keywords were incorrect: dtype
 * (torch.Storage storage)
 * (Tensor other)
 * (tuple of ints size, *, torch.device device)
 * (object data, *, torch.device device)
</code></pre>
<p>whereas if we look at official documentation</p>
<pre><code>torch.tensor(data, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor
</code></pre>
<blockquote>
<p>Parameters</p>
<p>data (array_like) – Initial data for the tensor. Can be a
list, tuple, NumPy ndarray, scalar, and other types.</p>
<p>dtype (torch.dtype, optional) – the desired data type of returned
tensor. Default: if None, infers data type from data.</p>
</blockquote>
<p>Why is the code snippet not working even though the official documentation supports it?</p>
",4726246.0,,4726246.0,,2021-05-28 22:56:15,2022-06-24 18:48:42,"torch.Tensor() new() received an invalid combination of arguments - got (list, dtype=torch.dtype)",<python><pytorch><tensor>,2,0,,,,CC BY-SA 4.0
63552044,1,63552285.0,,2020-08-23 21:11:59,,6,5456,"<p>I am attempting to understand more about computer vision models, and I'm trying to do some exploring of how they work. In an attempt to understand how to interpret feature vectors more I'm trying to use Pytorch to extract a feature vector. Below is my code that I've pieced together from various places.</p>
<pre><code>import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.autograd import Variable
from PIL import Image



img=Image.open(&quot;Documents/01235.png&quot;)

# Load the pretrained model
model = models.resnet18(pretrained=True)

# Use the model object to select the desired layer
layer = model._modules.get('avgpool')

# Set model to evaluation mode
model.eval()

transforms = torchvision.transforms.Compose([
        torchvision.transforms.Resize(256),
        torchvision.transforms.CenterCrop(224),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
def get_vector(image_name):
    # Load the image with Pillow library
    img = Image.open(&quot;Documents/Documents/Driven Data Competitions/Hateful Memes Identification/data/01235.png&quot;)
    # Create a PyTorch Variable with the transformed image
    t_img = transforms(img)
    # Create a vector of zeros that will hold our feature vector
    # The 'avgpool' layer has an output size of 512
    my_embedding = torch.zeros(512)
    # Define a function that will copy the output of a layer
    def copy_data(m, i, o):
        my_embedding.copy_(o.data)
    # Attach that function to our selected layer
    h = layer.register_forward_hook(copy_data)
    # Run the model on our transformed image
    model(t_img)
    # Detach our copy function from the layer
    h.remove()
    # Return the feature vector
    return my_embedding

pic_vector = get_vector(img)
</code></pre>
<p>When I do this I get the following error:</p>
<pre><code>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [64, 3, 7, 7], but got 3-dimensional input of size [3, 224, 224] instead
</code></pre>
<p>I'm sure this is an elementary error, but I can't seem to figure out how to fix this. It was my impression that the &quot;totensor&quot; transformation would make my data 4-d, but it seems it's either not working correctly or I'm misunderstanding it. Appreciate any help or resources I can use to learn more about this!</p>
",2355903.0,,,,,2023-06-02 18:56:45,How to extract feature vector from single image in Pytorch?,<python><computer-vision><pytorch><feature-extraction>,3,0,0.0,,,CC BY-SA 4.0
67164667,1,67166006.0,,2021-04-19 15:14:27,,6,974,"<p>Given input like:</p>
<pre><code>tensor([[[1.9392, -1.9266,  0.9664],
         [0.0000, -1.9266,  0.9664],
         [0.0000, -0.0000,  0.9664]]])
</code></pre>
<p>My desired output is:</p>
<pre><code>tensor([[[0.4596,  0.0096, 0.1737],
         [0.0000,  0.0096, 0.1737],
         [0.0000, -0.0000, 0.1737]]])
</code></pre>
<p>I.e. just calculating the function over the upper triangular elements.</p>
",14520902.0,,9067615.0,,2021-04-20 10:25:52,2021-04-20 10:25:52,How to use PyTorch to softmax only the upper triangular elements of a matrix?,<python><matrix><pytorch><tensor><softmax>,1,0,,,,CC BY-SA 4.0
63645357,1,63684465.0,,2020-08-29 09:17:46,,6,4911,"<p>I'm trying to run a PyTorch model in a Django app. As it is not recommended to execute the models (or any long-running task) in the views, I decided to run it in a Celery task. My model is quite big and it takes about 12 seconds to load and about 3 seconds to infer. That's why I decided that I couldn't afford to load it at every request. So I tried to load it at settings and save it there for the app to use it. So my final scheme is:</p>
<ul>
<li>When the Django app starts, in the settings the PyTorch model is loaded and it's accessible from the app.</li>
<li>When views.py receives a request, it delays a celery task</li>
<li>The celery task uses the settings.model to infer the result</li>
</ul>
<p>The problem here is that the celery task throws the following error when trying to use the model</p>
<pre><code>[2020-08-29 09:03:04,015: ERROR/ForkPoolWorker-1] Task app.tasks.task[458934d4-ea03-4bc9-8dcd-77e4c3a9caec] raised unexpected: RuntimeError(&quot;Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method&quot;)
Traceback (most recent call last):
  File &quot;/home/ubuntu/anaconda3/envs/tensor/lib/python3.7/site-packages/celery/app/trace.py&quot;, line 412, in trace_task
    R = retval = fun(*args, **kwargs)
  File &quot;/home/ubuntu/anaconda3/envs/tensor/lib/python3.7/site-packages/celery/app/trace.py&quot;, line 704, in __protected_call__
    return self.run(*args, **kwargs)
  /*...*/
  File &quot;/home/ubuntu/anaconda3/envs/tensor/lib/python3.7/site-packages/torch/cuda/__init__.py&quot;, line 191, in _lazy_init
    &quot;Cannot re-initialize CUDA in forked subprocess. &quot; + msg)
RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method
</code></pre>
<p>Here's the code in my settings.py loading the model:</p>
<pre><code>if sys.argv and sys.argv[0].endswith('celery') and 'worker' in sys.argv: #In order to load only for the celery worker
    import torch
    torch.cuda.init()
    torch.backends.cudnn.benchmark = True
    load_model_file()
</code></pre>
<p>And the task code</p>
<pre><code>@task
def getResult(name):
    print(&quot;Executing on GPU:&quot;, torch.cuda.is_available())
    if os.path.isfile(name):
        try:
            outpath = model_inference(name)
            os.remove(name)
            return outpath
        except OSError as e:
            print(&quot;Error&quot;, name, &quot;doesn't exist&quot;)
    return &quot;&quot;
</code></pre>
<p>The print in the task shows <code>&quot;Executing on GPU: true&quot;</code></p>
<p>I've tried setting <code>torch.multiprocessing.set_start_method('spawn')</code> in the settings.py before and after the <code>torch.cuda.init()</code> but it gives the same error.</p>
",4788165.0,,4788165.0,,2020-08-31 19:05:14,2020-11-23 21:12:57,Using PyTorch with Celery,<python><django><multiprocessing><pytorch><celery>,3,0,0.0,,,CC BY-SA 4.0
66962837,1,66963266.0,,2021-04-06 04:48:10,,6,979,"<p>I am trying to create a <code>transform</code> that shuffles the patches of each image in a batch.
I aim to use it in the same manner as the rest of the transformations in <code>torchvision</code>:</p>
<pre><code>trans = transforms.Compose([
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
            ShufflePatches(patch_size=(16,16)) # our new transform
        ])
</code></pre>
<p>More specifically, the input is a <code>BxCxHxW</code> tensor. I want to split each image in the batch into non-overlapping patches of size patch_size, shuffle them, and regroup into a single image.</p>
<p>Given the image (of size <code>224x224</code>):</p>
<p><a href=""https://i.stack.imgur.com/zFYKI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zFYKI.png"" alt=""enter image description here"" /></a></p>
<p>Using <code>ShufflePatches(patch_size=(112,112))</code> I would like to produce the output image:</p>
<p><a href=""https://i.stack.imgur.com/io78z.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/io78z.png"" alt=""enter image description here"" /></a></p>
<p>I think the solution has to do with <code>torch.unfold</code> and <code>torch.fold</code>, but didn't manage to get any further.</p>
<p>Any help would be appreciated!</p>
",15327614.0,,,,,2021-04-06 06:31:58,Shuffle patches in image batch,<python><deep-learning><computer-vision><pytorch><torchvision>,1,0,,,,CC BY-SA 4.0
64225965,1,,,2020-10-06 12:38:14,,6,197,"<p>The documentation of torch.Tensor.view says:</p>
<blockquote>
<p>each new view dimension must either be a <em><strong>subspace</strong></em> of an original dimension, or only span across original dimensions ...</p>
</blockquote>
<p><a href=""https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view"" rel=""noreferrer"">https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view</a></p>
<p>What is a subspace of a dimension?</p>
",14400748.0,,,,,2022-01-26 11:27:54,What is a subspace of a dimension in pytorch?,<pytorch>,2,2,0.0,,,CC BY-SA 4.0
67180955,1,67182158.0,,2021-04-20 14:29:56,,6,7468,"<p>There is a <a href=""https://tanelp.github.io/posts/a-bug-that-plagues-thousands-of-open-source-ml-projects/"" rel=""noreferrer"">bug</a> in PyTorch/Numpy where when loading batches in parallel with a <code>DataLoader</code> (i.e. setting <code>num_workers &gt; 1</code>), the same NumPy random seed is used for each worker, resulting in any random functions applied being identical across parallelized batches.</p>
<p>Minimal example:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
from torch.utils.data import Dataset, DataLoader

class RandomDataset(Dataset):
    def __getitem__(self, index):
        return np.random.randint(0, 1000, 2)

    def __len__(self):
        return 9
    
dataset = RandomDataset()
dataloader = DataLoader(dataset, batch_size=1, num_workers=3)

for batch in dataloader:
    print(batch)
</code></pre>
<p>As you can see, for each parallelized set of batches (3), the results are the same:</p>
<pre class=""lang-py prettyprint-override""><code># First 3 batches
tensor([[891, 674]])
tensor([[891, 674]])
tensor([[891, 674]])
# Second 3 batches
tensor([[545, 977]])
tensor([[545, 977]])
tensor([[545, 977]])
# Third 3 batches
tensor([[880, 688]])
tensor([[880, 688]])
tensor([[880, 688]])
</code></pre>
<p>What is the recommended/most elegant way to fix this? i.e. have each batch produce a different randomization, irrespective of the number of workers.</p>
",9067615.0,,,,,2022-01-13 15:03:27,PyTorch DataLoader uses same random seed for batches run in parallel,<python><numpy><parallel-processing><pytorch><dataloader>,2,0,0.0,,,CC BY-SA 4.0
67722328,1,67750851.0,,2021-05-27 12:40:46,,6,13822,"<p>I am working on the pytorch to learn.</p>
<p>And  There is a question how to check the output gradient by each layer in my code.</p>
<p>My code is below</p>
<pre><code>#import the nescessary libs
import numpy as np
import torch
import time

# Loading the Fashion-MNIST dataset
from torchvision import datasets, transforms

# Get GPU Device

device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)


# Define a transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (0.5,))
                                                                   ])
# Download and load the training data
trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)
testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)
testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)

# Examine a sample
dataiter = iter(trainloader)
images, labels = dataiter.next()

# Define the network architecture
from torch import nn, optim
import torch.nn.functional as F

model = nn.Sequential(nn.Linear(784, 128),
                      nn.ReLU(),
                      nn.Linear(128, 10),
                      nn.LogSoftmax(dim = 1)
                     )
model.to(device)

# Define the loss
criterion = nn.CrossEntropyLoss()

# Define the optimizer
optimizer = optim.Adam(model.parameters(), lr = 0.001)

# Define the epochs
epochs = 5

train_losses, test_losses = [], []

# start = time.time()
for e in range(epochs):
    running_loss = 0
    
    for images, labels in trainloader:
    # Flatten Fashion-MNIST images into a 784 long vector
        images = images.to(device)
        labels = labels.to(device)
        images = images.view(images.shape[0], -1)
        

    # Training pass
        optimizer.zero_grad()
    
        output = model.forward(images)
        loss = criterion(output, labels)
        
        loss.backward()
        
#         print(loss.grad)
        
        optimizer.step()

        running_loss += loss.item()
    
    else:
        print(model[0].grad)
</code></pre>
<p>If I print model[0].grad after back-propagation, Is it going to be the output gradient by each layer for every epoches?</p>
<p>Or, If I want to know the output gradient by each layer, where and what am I should print?</p>
<p>Thank you!!</p>
<p>Thank you for reading</p>
",13259452.0,,,,,2021-05-29 11:31:33,How to check the output gradient by each layer in pytorch in my code?,<python><machine-learning><pytorch><artificial-intelligence><gradient>,1,0,0.0,,,CC BY-SA 4.0
65402802,1,65403360.0,,2020-12-22 03:16:46,,6,1258,"<p>There are several scenarios that make me confused about shuffling the data loader, which are as follows.</p>
<p>I set the “shuffle” parameter to False on both train_loader and valid_loader. then the results I get are as follows</p>
<pre><code>Epoch 1/4    loss=0.8802     val_loss=0.8202     train_acc=0.63      val_acc=0.63  
Epoch 2/4    loss=0.6993     val_loss=0.6500     train_acc=0.66      val_acc=0.72 
Epoch 3/4    loss=0.5363     val_loss=0.5385     train_acc=0.76      val_acc=0.80
Epoch 4/4    loss=0.4055     val_loss=0.5130     train_acc=0.85      val_acc=0.81
</code></pre>
<p>I set the “shuffle” parameter to True on train_loader and False to valid_loader. then the results I get are as follows</p>
<pre><code>Epoch 1/4    loss=0.8928     val_loss=0.8284     train_acc=0.63      val_acc=0.63 
Epoch 2/4    loss=0.7308     val_loss=0.6263     train_acc=0.61      val_acc=0.73 
Epoch 3/4    loss=0.5594     val_loss=0.5046     train_acc=0.54      val_acc=0.81 
Epoch 4/4    loss=0.4304     val_loss=0.4525     train_acc=0.49      val_acc=0.82 
</code></pre>
<p>Based on that result, my training accuracy has a worse performance when I shuffle train_loader.</p>
<p>And this is a snippet of my code.</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(n_epochs):
    model.train()
    avg_loss = 0.
    train_preds = np.zeros((len(train_X),len(le.classes_)))

    for i, (x_batch, y_batch) in enumerate(train_loader):
        y_pred = model(x_batch)
        loss = loss_fn(y_pred, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        avg_loss += loss.item() / len(train_loader)
        train_preds[i * batch_size:(i+1) * batch_size] = F.softmax(y_pred).cpu().detach().numpy()            

    train_accuracy = sum(train_preds.argmax(axis=1) == y_train)/len(y_train)

    model.eval()        
    avg_val_loss = 0.
    val_preds = np.zeros((len(x_cv),len(le.classes_)))

    for i, (x_batch, y_batch) in enumerate(valid_loader):
        y_pred = model(x_batch).detach()
        avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)
        val_preds[i * batch_size:(i+1) * batch_size] =F.softmax(y_pred).cpu().numpy()
    val_accuracy = sum(val_preds.argmax(axis=1)==y_test)/len(y_test)
</code></pre>
<p>Did I make a mistake calculating the training accuracy? Thanks in advance</p>
",13912887.0,,3237438.0,,2020-12-22 04:38:04,2020-12-22 04:42:59,PyTorch: Shuffle DataLoader,<pytorch><dataloader>,1,0,0.0,,,CC BY-SA 4.0
69240815,1,72271924.0,,2021-09-19 06:20:14,,6,22934,"<p>I am trying to execute the following code for a nlp proj</p>
<pre><code>import torchtext
from torchtext.legacy.data import Field, BucketIterator, Iterator
from torchtext.legacy import data


----&gt; 6 from torchtext.legacy.data import Field, BucketIterator, Iterator
      7 from torchtext.legacy import data
      8 

ModuleNotFoundError: No module named 'torchtext.legacy'.
</code></pre>
<p>I have tried it on both kaggle notebook and jupyter notebook and found the same error in both.
i even tried to install !pip install -qqq deepmatcher==0.1.1 in kaggle to solve the issue but it still gives the same error.
is there any solution to this?</p>
",13430940.0,,13430940.0,,2021-09-19 06:30:31,2023-02-19 07:21:24,"I am trying to import:from torchtext.legacy.data import Field, BucketIterator,Iterator,data, but get error 'No module named 'torchtext.legacy'",<python><nlp><pytorch>,3,0,,,,CC BY-SA 4.0
68562730,1,,,2021-07-28 14:57:09,,6,21026,"<p>I'm trying to use my GPU to run the YOLOR model, and I keep getting the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\yolor\detect.py&quot;, line 198, in &lt;module&gt;
    detect()
  File &quot;D:\yolor\detect.py&quot;, line 41, in detect
    device = select_device(opt.device)
  File &quot;D:\yolor\utils\torch_utils.py&quot;, line 47, in select_device
    assert torch.cuda.is_available(), 'CUDA unavailable, invalid device %s requested' % device  # check availablity
AssertionError: CUDA unavailable, invalid device 0 requested
</code></pre>
<p>When I try to check if CUDA is available with the following:</p>
<pre><code>python3
&gt;&gt;import torch
&gt;&gt;print(torch.cuda.is_available())
</code></pre>
<p>I get <code>False</code>, which explains the problem. I tried running the command</p>
<pre><code>py -m pip install torch1.9.0+cu111 torchvision0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>I get the error: <code>ERROR: Invalid requirement: 'torch1.9.0+cu111'</code></p>
<p>Running <code>nvcc --version</code>, I get:</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Mon_May__3_19:41:42_Pacific_Daylight_Time_2021
Cuda compilation tools, release 11.3, V11.3.109
Build cuda_11.3.r11.3/compiler.29920130_0
</code></pre>
<p>Thus, I'm not really sure what the issue is, or how to fix it.</p>
<p>EDIT: As @Ivan pointed out, I added the == sign, but still get <code>False</code> when checking if CUDA is available.</p>
",16521752.0,,681865.0,,2021-07-28 23:26:48,2022-12-24 15:27:20,"How to Fix ""AssertionError: CUDA unavailable, invalid device 0 requested""",<python><deep-learning><pytorch>,4,1,0.0,,,CC BY-SA 4.0
70340812,1,,,2021-12-13 20:55:27,,6,47530,"<p>I am trying to install <code>torch with CUDA enabled</code> in Visual Studio environment. I right clicked on <strong>Python Environments</strong> in Solution Explorer, uninstalled the existing version of Torch that is not compiled with CUDA and tried to run this <code>pip command</code> from the official <code>Pytorch</code> website. The command is:</p>
<p><code>pip3 install torch==1.10.0+cu102 torchvision==0.11.1+cu102 torchaudio===0.10.0+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html</code></p>
<p>Visual Studio reports this error <code>Looking in links: https://download.pytorch.org/whl/cu102/torch_stable.html ERROR: Could not find a version that satisfies the requirement pip3 (from versions: none) ERROR: No matching distribution found for pip3</code>.</p>
<p>I have seen similar questions asked on this site but some are circumventing on <code>Conda</code> while others did have unclear answers which were not accepted so I was in doubt whether to follow the answers or not. I have a very important project I need to present and I can't do that unless I install torch with cuda enabled, Please Help me and Thanks.</p>
",16612111.0,,681865.0,,2021-12-14 03:57:28,2023-05-01 01:45:11,How to install pytorch with CUDA support with pip in Visual Studio,<python><pytorch>,4,0,0.0,,,CC BY-SA 4.0
62759281,1,62759641.0,,2020-07-06 15:25:13,,6,9178,"<p>In Pytorch, is there any way of loading a <em>specific</em> single sample using the <code>torch.utils.data.DataLoader</code> class? I'd like to do some testing with it.</p>
<p>The <a href=""https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py"" rel=""noreferrer"">tutorial</a> uses</p>
<pre><code>trainloader = torch.utils.data.DataLoader(...)
images, labels = next(iter(trainloader))
</code></pre>
<p>to fetch a <em>random</em> batch of samples. Is there are way, using <code>DataLoader</code>, to get a <em>specific</em> sample?</p>
<p>Cheers</p>
",12097191.0,,,,,2020-07-06 16:20:49,How to get a specific sample from pytorch DataLoader?,<pytorch>,2,1,0.0,,,CC BY-SA 4.0
72363741,1,72859877.0,,2022-05-24 13:26:13,,6,7874,"<p>I am a beginner with pytorch. I am trying to do an aspect based sentiment analysis. I am facing the error mentioned in the subject. My code is as follows: I request help to resolve this error. Thanks in advance. I will share the entire code and the error stack.
<code>!pip install transformers</code></p>
<pre><code>import transformers
from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup
import torch
import numpy as np
import pandas as pd
import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import defaultdict
from textwrap import wrap
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
%matplotlib inline
%config InlineBackend.figure_format='retina'
sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = [&quot;#01BEFE&quot;, &quot;#FFDD00&quot;, &quot;#FF7D00&quot;, &quot;#FF006D&quot;, &quot;#ADFF02&quot;, &quot;#8F00FF&quot;]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 12, 8
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
</code></pre>
<p><code>df = pd.read_csv(&quot;/Users/user1/Downloads/auto_bio_copy.csv&quot;)</code></p>
<p>I am importing a csv file which has content and label as shown below:</p>
<p><code>df.head()</code></p>
<pre><code>                     content                                      label
0   I told him I would leave the car and come back...   O O O O O O O O O O O O O O O O O O O O O O O ...
1   I had the ignition interlock device installed ...   O O O B-Negative I-Negative I-Negative O O O O...
2   Aug. 23 or 24 I went to Walmart auto service d...   O O O O O O O B-Negative I-Negative I-Negative...
3   Side note This is the same reaction I 'd gotte...   O O O O O O O O O O O O O O O O O O O O O O O ...
4   Locked out of my car . Called for help 215pm w...   O O O O O O O O O O O O O O O O O B-Negative O...
</code></pre>
<p><code>df.shape</code></p>
<p><code>(1999, 2)</code></p>
<p>I am converting the label values into integers as follows:
O=zero(0), B-Positive=1, I-Positive=2, B-Negative=3, I-Negative=4, B-Neutral=5, I-Neutral=6, B-Mixed=7, I-Mixed=8</p>
<pre><code>df['label'] = df.label.str.replace('O', '0')
df['label'] = df.label.str.replace('B-Positive', '1')
df['label'] = df.label.str.replace('I-Positive', '2')
df['label'] = df.label.str.replace('B-Negative', '3')
df['label'] = df.label.str.replace('I-Negative', '4')
df['label'] = df.label.str.replace('B-Neutral', '5')
df['label'] = df.label.str.replace('I-Neutral', '6')
df['label'] = df.label.str.replace('B-Mixed', '7')
df['label'] = df.label.str.replace('I-Mixed', '8')
</code></pre>
<p>Next, converting the string to integer list as follows:</p>
<pre><code>df['label'] = df['label'].str.split(' ').apply(lambda s: list(map(int, s)))
</code></pre>
<pre><code>df.head()
</code></pre>
<pre><code>                     content                                         label
0   I told him I would leave the car and come back...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
1   I had the ignition interlock device installed ...   [0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
2   Aug. 23 or 24 I went to Walmart auto service d...   [0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 0, 0, 0, 0, ...
3   Side note This is the same reaction I 'd gotte...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
4   Locked out of my car . Called for help 215pm w...   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
</code></pre>
<pre><code>PRE_TRAINED_MODEL_NAME = 'bert-base-cased'
</code></pre>
<pre><code>tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)
</code></pre>
<pre><code>token_lens = []
for txt in df.content:
  tokens = tokenizer.encode_plus(txt, max_length=512, add_special_tokens=True, truncation=True, return_attention_mask=True)
  token_lens.append(len(tokens))
MAX_LEN = 512
</code></pre>
<pre><code>class Auto_Bio_Dataset(Dataset):
    def __init__(self, contents, labels, tokenizer, max_len):
        self.contents = contents
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.contents)
    def __getitem__(self, item):
        content = str(self.contents[item])
        label = self.labels[item]
        encoding = self.tokenizer.encode_plus(
          content,
          add_special_tokens=True,
          max_length=self.max_len,
          return_token_type_ids=False,
          #padding='max_length',
          pad_to_max_length=True,
          truncation=True,
          return_attention_mask=True,
          return_tensors='pt'
        )
        return {
          'content_text': content,
          'input_ids': encoding['input_ids'].flatten(),
          'attention_mask': encoding['attention_mask'].flatten(),
          'labels': torch.tensor(label)
        }
</code></pre>
<pre><code>df_train, df_test = train_test_split(
  df,
  test_size=0.1,
  random_state=RANDOM_SEED
)
df_val, df_test = train_test_split(
  df_test,
  test_size=0.5,
  random_state=RANDOM_SEED
)
</code></pre>
<pre><code>df_train.shape, df_val.shape, df_test.shape
</code></pre>
<pre><code>((1799, 2), (100, 2), (100, 2))
</code></pre>
<pre><code>def create_data_loader(df, tokenizer, max_len, batch_size):
    ds = Auto_Bio_Dataset(
        contents=df.content.to_numpy(),
        labels=df.label.to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len
  )
    return DataLoader(
        ds,
        batch_size=batch_size,
        num_workers=2
  )
</code></pre>
<pre><code>BATCH_SIZE = 16
train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)
</code></pre>
<pre><code>data = next(iter(train_data_loader))
data.keys()
</code></pre>
<p>Error is as follows:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-71-e0a71018e473&gt; in &lt;module&gt;
----&gt; 1 data = next(iter(train_data_loader))
      2 data.keys()

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)
    528             if self._sampler_iter is None:
    529                 self._reset()
--&gt; 530             data = self._next_data()
    531             self._num_yielded += 1
    532             if self._dataset_kind == _DatasetKind.Iterable and \

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)
   1222             else:
   1223                 del self._task_info[idx]
-&gt; 1224                 return self._process_data(data)
   1225 
   1226     def _try_put_index(self):

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)
   1248         self._try_put_index()
   1249         if isinstance(data, ExceptionWrapper):
-&gt; 1250             data.reraise()
   1251         return data
   1252 

~/opt/anaconda3/lib/python3.7/site-packages/torch/_utils.py in reraise(self)
    455             # instantiate since we don't know how to
    456             raise RuntimeError(msg) from None
--&gt; 457         raise exception
    458 
    459 

RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&quot;, line 287, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in fetch
    return self.collate_fn(data)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in default_collate
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in &lt;dictcomp&gt;
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 138, in default_collate
    return torch.stack(batch, 0, out=out)
RuntimeError: stack expects each tensor to be equal size, but got [157] at entry 0 and [154] at entry 1
</code></pre>
<p>I found in some github post that this error can be because of batch size, so i changed the batch size to 8 and then the error is as follows:</p>
<pre><code>BATCH_SIZE = 8
train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)
</code></pre>
<pre><code>data = next(iter(train_data_loader))
data.keys()
</code></pre>
<pre><code>RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-73-e0a71018e473&gt; in &lt;module&gt;
----&gt; 1 data = next(iter(train_data_loader))
      2 data.keys()

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)
    528             if self._sampler_iter is None:
    529                 self._reset()
--&gt; 530             data = self._next_data()
    531             self._num_yielded += 1
    532             if self._dataset_kind == _DatasetKind.Iterable and \

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _next_data(self)
   1222             else:
   1223                 del self._task_info[idx]
-&gt; 1224                 return self._process_data(data)
   1225 
   1226     def _try_put_index(self):

~/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py in _process_data(self, data)
   1248         self._try_put_index()
   1249         if isinstance(data, ExceptionWrapper):
-&gt; 1250             data.reraise()
   1251         return data
   1252 

~/opt/anaconda3/lib/python3.7/site-packages/torch/_utils.py in reraise(self)
    455             # instantiate since we don't know how to
    456             raise RuntimeError(msg) from None
--&gt; 457         raise exception
    458 
    459 

RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py&quot;, line 287, in _worker_loop
    data = fetcher.fetch(index)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py&quot;, line 52, in fetch
    return self.collate_fn(data)
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in default_collate
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 157, in &lt;dictcomp&gt;
    return elem_type({key: default_collate([d[key] for d in batch]) for key in elem})
  File &quot;/Users/namrathabhandarkar/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py&quot;, line 137, in default_collate
    out = elem.new(storage).resize_(len(batch), *list(elem.size()))
RuntimeError: Trying to resize storage that is not resizable
</code></pre>
<p>I am not sure what is causing the first error(the one mentioned in subject). I am using padding and truncate in my code, yet the error.</p>
<p>Any help to resolve this issue is highly appreciated.</p>
<p>Thanks in advance.</p>
",15499442.0,,15499442.0,,2022-05-24 16:51:41,2022-07-04 16:47:04,"pytorch dataloader - RuntimeError: stack expects each tensor to be equal size, but got [157] at entry 0 and [154] at entry 1",<pytorch><sentiment-analysis><pytorch-dataloader>,1,0,,,,CC BY-SA 4.0
66648432,1,66659607.0,,2021-03-16 02:46:43,,6,10570,"<p>I am trying to train a deep learning architecture, the model trains perfectly. I am testing after each epoch. For 7 epoch all the loss and accuracy seems okay but at 8 epoch during the testing test loss becomes nan. I have checked my data, it got no nan. Also my test accuracy is higher than train which is weird. Train data size is 37646 and test is 18932 so it should be enough. Before becoming nan test started to become very high around 1.6513713663602217e+30. This is really weird and I don't understand why is happening. Any help or suggestion is much appreciated.</p>
",4278798.0,,,,,2021-03-16 17:22:39,Pytorch: test loss becoming nan after some iteration,<deep-learning><pytorch>,1,6,,,,CC BY-SA 4.0
70386800,1,,,2021-12-16 23:29:23,,6,2685,"<p>I want (the proper and official - bug free way) to do:</p>
<ol>
<li>resume from a checkpoint to continue training on multiple gpus</li>
<li>save checkpoint correctly during training with multiple gpus</li>
</ol>
<p>For that my guess is the following:</p>
<ol>
<li>to do 1 we have all the processes load the checkpoint from the file, then call <code>DDP(mdl)</code> for each process. I assume the checkpoint saved a <code>ddp_mdl.module.state_dict()</code>.</li>
<li>to do 2 simply check who is rank = 0 and have that one do the torch.save({'model': ddp_mdl.module.state_dict()})</li>
</ol>
<p>Approximate code:</p>
<pre><code>def save_ckpt(rank, ddp_model, path):
    if rank == 0:
        state = {'model': ddp_model.module.state_dict(),
             'optimizer': optimizer.state_dict(),
            }
        torch.save(state, path)

def load_ckpt(path, distributed, map_location=map_location=torch.device('cpu')):
    # loads to
    checkpoint = torch.load(path, map_location=map_location)
    model = Net(...)
    optimizer = ...
    model.load_state_dict(checkpoint['model'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    if distributed:
        model = DDP(model, device_ids=[gpu], find_unused_parameters=True)
    return model
</code></pre>
<p>Is this correct?</p>
<hr />
<p>One of the reasons that I am asking is that distributed code can go subtly wrong. I want to make sure this does not happen to me. Of course I want to avoid deadlocks but that would be obvious if it happens to me (e.g. perhaps it could happen if all the processes somehow tried to open the same ckpt file at the same time. In that case I'd somehow make sure that only one of them loads it one at a time or have rank 0 only load it and then send it to the rest of the processes).</p>
<p>I am also asking because <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#save-and-load-checkpoints"" rel=""noreferrer"">the official docs don't make sense to me</a>. I will paste their code and explanation since links can die sometimes:</p>
<blockquote>
<p>Save and Load Checkpoints
It’s common to use torch.save and torch.load to checkpoint modules during training and recover from checkpoints. See SAVING AND LOADING MODELS for more details. When using DDP, one optimization is to save the model in only one process and then load it to all processes, reducing write overhead. This is correct because all processes start from the same parameters and gradients are synchronized in backward passes, and hence optimizers should keep setting parameters to the same values. If you use this optimization, make sure all processes do not start loading before the saving is finished. Besides, when loading the module, you need to provide an appropriate map_location argument to prevent a process to step into others’ devices. If map_location is missing, torch.load will first load the module to CPU and then copy each parameter to where it was saved, which would result in all processes on the same machine using the same set of devices. For more advanced failure recovery and elasticity support, please refer to TorchElastic.</p>
</blockquote>
<pre><code>def demo_checkpoint(rank, world_size):
    print(f&quot;Running DDP checkpoint example on rank {rank}.&quot;)
    setup(rank, world_size)

    model = ToyModel().to(rank)
    ddp_model = DDP(model, device_ids=[rank])

    loss_fn = nn.MSELoss()
    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)

    CHECKPOINT_PATH = tempfile.gettempdir() + &quot;/model.checkpoint&quot;
    if rank == 0:
        # All processes should see same parameters as they all start from same
        # random parameters and gradients are synchronized in backward passes.
        # Therefore, saving it in one process is sufficient.
        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)

    # Use a barrier() to make sure that process 1 loads the model after process
    # 0 saves it.
    dist.barrier()
    # configure map_location properly
    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
    ddp_model.load_state_dict(
        torch.load(CHECKPOINT_PATH, map_location=map_location))

    optimizer.zero_grad()
    outputs = ddp_model(torch.randn(20, 10))
    labels = torch.randn(20, 5).to(rank)
    loss_fn = nn.MSELoss()
    loss_fn(outputs, labels).backward()
    optimizer.step()

    # Not necessary to use a dist.barrier() to guard the file deletion below
    # as the AllReduce ops in the backward pass of DDP already served as
    # a synchronization.

    if rank == 0:
        os.remove(CHECKPOINT_PATH)

    cleanup()
</code></pre>
<hr />
<p>Related:</p>
<ul>
<li><a href=""https://discuss.pytorch.org/t/checkpointing-ddp-module-instead-of-ddp-itself/115714"" rel=""noreferrer"">https://discuss.pytorch.org/t/checkpointing-ddp-module-instead-of-ddp-itself/115714</a></li>
<li><a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""noreferrer"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a></li>
<li><a href=""https://discuss.pytorch.org/t/ddp-and-gradient-checkpointing/132244"" rel=""noreferrer"">https://discuss.pytorch.org/t/ddp-and-gradient-checkpointing/132244</a></li>
<li><a href=""https://github.com/pytorch/pytorch/issues/23138"" rel=""noreferrer"">https://github.com/pytorch/pytorch/issues/23138</a></li>
<li><a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html"" rel=""noreferrer"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html</a></li>
<li><a href=""https://stackoverflow.com/questions/70386800/what-is-the-proper-way-to-checkpoint-during-training-when-using-distributed-data"">What is the proper way to checkpoint during training when using distributed data parallel (DDP) in PyTorch?</a></li>
<li><a href=""https://discuss.pytorch.org/t/what-is-the-proper-way-to-checkpoint-during-training-when-using-distributed-data-parallel-ddp-in-pytorch/139575"" rel=""noreferrer"">https://discuss.pytorch.org/t/what-is-the-proper-way-to-checkpoint-during-training-when-using-distributed-data-parallel-ddp-in-pytorch/139575</a></li>
</ul>
",1601580.0,,,,,2022-12-13 13:53:02,What is the proper way to checkpoint during training when using distributed data parallel (DDP) in PyTorch?,<python><deep-learning><neural-network><pytorch><distributed-computing>,1,0,,,,CC BY-SA 4.0
63062741,1,,,2020-07-23 20:36:06,,6,19841,"<pre><code>RuntimeError: Detected that PyTorch and torchvision were compiled with different CUDA versions. PyTorch has CUDA Version=10.2 and torchvision has CUDA Version=10.1. Please reinstall the torchvision that matches your PyTorch install.
</code></pre>
<p>I am trying to run YOLACT on my Google Colab and found this error. Can someone help solve this issue?</p>
",13665920.0,,,,,2022-04-21 23:31:40,Pytorch and Torchvision are compiled different CUDA versions,<pytorch><torchvision>,1,0,,,,CC BY-SA 4.0
69100302,1,69102733.0,,2021-09-08 09:19:27,,6,1695,"<p>I have a 2D pytorch tensor of shape n by m. I want to index the second dimension using a list of indices (which could be done with torch.gather) then <strong>then also set new values</strong> to the result of the indexing.</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>data = torch.tensor([[0,1,2], [3,4,5], [6,7,8]]) # shape (3,3)
indices = torch.tensor([1,2,1], dtype=torch.long).unsqueeze(-1) # shape (3,1)
# data tensor:
# tensor([[0, 1, 2],
#         [3, 4, 5],
#         [6, 7, 8]])
</code></pre>
<p>I want to select the specified indices per row (which would be <code>[1,5,7]</code> but then also set these values to another number - e.g. 42</p>
<p>I can select the desired columns row wise by doing:</p>
<pre><code>data.gather(1, indices)
tensor([[1],
        [5],
        [7]])
data.gather(1, indices)[:] = 42 # **This does NOT work**, since the result of gather 
                                # does not use the same storage as the original tensor
</code></pre>
<p>which is fine, but I would like to change these values now, and have the change also affect the <code>data</code> tensor.</p>
<p>I can do what I want to achieve using this, but it seems to be very un-pythonic:</p>
<pre class=""lang-py prettyprint-override""><code>max_index = torch.max(indices)
for i in range(0, max_index + 1):
  mask = (indices == i).nonzero(as_tuple=True)[0]
  data[mask, i] = 42
print(data)
# tensor([[ 0, 42,  2],
#         [ 3,  4, 42],
#         [ 6, 42,  8]])
</code></pre>
<p>Any hints on how to do that more elegantly?</p>
",7274172.0,,,,,2021-09-08 12:16:44,Setting results of torch.gather(...) calls,<python><indexing><pytorch><tensor>,1,0,0.0,,,CC BY-SA 4.0
67845882,1,67846587.0,,2021-06-05 03:02:27,,6,15317,"<p>When I run the program below, it gives me an error. The problem seems to be in the loss function but I can't find it. I have read the Pytorch Documentation for nn.CrossEntropyLoss but still can't find the problem.</p>
<p>Image size is (1 x 256 x 256),
Batch size is 1</p>
<p>I am new to PyTorch, thanks.</p>
<pre><code>import torch
import torch.nn as nn
from PIL import Image
import numpy as np
torch.manual_seed(0)

x = np.array(Image.open(&quot;cat.jpg&quot;))
x = np.expand_dims(x, axis = 0)
x = np.expand_dims(x, axis = 0)
x = torch.from_numpy(x)
x = x.type(torch.FloatTensor) # shape = (1, 1, 256, 256)

def Conv(in_channels, out_channels, kernel=3, stride=1, padding=0):
    return nn.Conv2d(in_channels, out_channels, kernel, stride, padding)

class model(nn.Module):
    def __init__(self):
        super(model, self).__init__()

        self.sequential = nn.Sequential(
            Conv(1, 3),
            Conv(3, 5),
            nn.Flatten(),
            nn.Linear(317520, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        y = self.sequential(x)
        return y

def compute_loss(y_hat, y):
    return nn.CrossEntropyLoss()(y_hat, y)

model = model()
y_hat = model(x)

loss = compute_loss(y_hat, torch.tensor([1]))
</code></pre>
<p>Error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:/Me/AI/Models/test.py&quot;, line 38, in &lt;module&gt;
    **loss = compute_loss(y, torch.tensor([1]))**
  File &quot;D:/Me/AI/Models/test.py&quot;, line 33, in compute_loss
    return nn.CrossEntropyLoss()(y_hat, y)
  File &quot;D:\Softwares\Anaconda\envs\deeplearning\lib\site-packages\torch\nn\modules\module.py&quot;, line 1054, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;D:\Softwares\Anaconda\envs\deeplearning\lib\site-packages\torch\nn\modules\loss.py&quot;, line 1120, in forward
    return F.cross_entropy(input, target, weight=self.weight,
  File &quot;D:\Softwares\Anaconda\envs\deeplearning\lib\site-packages\torch\nn\functional.py&quot;, line 2824, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
**IndexError: Target 1 is out of bounds.**

Process finished with exit code 1

</code></pre>
",15166885.0,,,,,2021-11-23 08:02:37,IndexError: Target 1 is out of bounds,<python><python-3.x><pytorch>,3,0,,,,CC BY-SA 4.0
63962561,1,,,2020-09-18 20:40:04,,6,4003,"<p>How can I check if some weights are not changed during training in PyTorch?</p>
<p>As I understand one option can be just dump model weights at some epochs and check if they are changed iterating over weights, but maybe there is some simpler way?</p>
",1179925.0,,,,,2023-02-24 10:57:43,PyTorch: How to check if some weights are not changed during training?,<pytorch>,2,3,0.0,,,CC BY-SA 4.0
67241289,1,,,2021-04-24 09:35:55,,6,22166,"<p>I am testing ZED Camera with the code on <a href=""https://github.com/stereolabs/zed-pytorch"" rel=""noreferrer"">https://github.com/stereolabs/zed-pytorch</a>. While running the final command: python zed_object_detection.py --config-file configs/caffe2/e2e_mask_rcnn_R_50_C4_1x_caffe2.yaml --min-image-size 256</p>
<p>I get the following error:</p>
<p>Traceback (most recent call last):
File &quot;zed_object_detection.py&quot;, line 6, in 
from predictor import COCODemo
File &quot;/home/fypadmin/Desktop/23Apr_ZED/zed-pytorch/predictor.py&quot;, line 4, in 
from torchvision import transforms as T
File &quot;/home/fypadmin/anaconda3/envs/pytorch1/lib/python3.8/site-packages/torchvision/<strong>init</strong>.py&quot;, line 4, in 
from torchvision import datasets
File &quot;/home/fypadmin/anaconda3/envs/pytorch1/lib/python3.8/site-packages/torchvision/datasets/<strong>init</strong>.py&quot;, line 1, in 
from .lsun import LSUN, LSUNClass
File &quot;/home/fypadmin/anaconda3/envs/pytorch1/lib/python3.8/site-packages/torchvision/datasets/lsun.py&quot;, line 19, in 
from .utils import verify_str_arg, iterable_to_str
File &quot;/home/fypadmin/anaconda3/envs/pytorch1/lib/python3.8/site-packages/torchvision/datasets/utils.py&quot;, line 11, in 
from torch._six import PY3
ImportError: cannot import name 'PY3' from 'torch._six' (/home/fypadmin/anaconda3/envs/pytorch1/lib/python3.8/site-packages/torch/_six.py)</p>
<p>I am new to ML and I am running pytorch 1.8.1. Looking forward to any help. Thanks</p>
",15753693.0,,,,,2021-05-06 14:49:18,ImportError: cannot import name 'PY3' from 'torch._six',<python><pytorch>,2,1,0.0,,,CC BY-SA 4.0
63975130,1,63975459.0,,2020-09-20 03:01:15,,6,6190,"<p>The FashionMNIST dataset has 10 different output classes. How can I get a subset of this dataset with only specific classes? In my case, I only want images of sneaker, pullover, sandal and shirt classes (their classes are 7,2,5 and 6 respectively).</p>
<p>This is how I load my dataset.</p>
<p><code>train_dataset_full = torchvision.datasets.FashionMNIST(data_folder, train = True, download = True, transform = transforms.ToTensor())</code></p>
<p>The approach I’ve followed is below.
Iterate through the dataset, one by one, then compare the 1st element (i.e. class) in the returned tuple to my required class. I’m stuck here. If the value returned is true, how can I append/add this observation to an empty dataset?</p>
<pre><code>sneaker = 0
pullover = 0
sandal = 0
shirt = 0
for i in range(60000):
    if train_dataset_full[i][1] == 7:
        sneaker += 1
    elif train_dataset_full[i][1] == 2:
        pullover += 1
    elif train_dataset_full[i][1] == 5:
        sandal += 1
    elif train_dataset_full[i][1] == 6:
        shirt += 1
</code></pre>
<p>Now, in place of <code>sneaker += 1</code>, <code>pullover += 1</code>, <code>sandal += 1</code> and <code>shirt += 1</code> I want to do something like this <code>empty_dataset.append(train_dataset_full[i])</code> or something similar.</p>
<p>If the above approach is incorrect, please suggest another method.</p>
",14307905.0,,4685471.0,,2020-09-21 15:02:22,2023-06-09 03:28:32,How to get only specific classes from PyTorch's FashionMNIST dataset?,<python><pytorch>,5,0,0.0,,,CC BY-SA 4.0
70428140,1,,,2021-12-20 21:16:11,,6,4215,"<p>I am trying to do a text classification using pytorch and torchtext on paperspace.</p>
<p>I get</p>
<pre><code>RuntimeError: ‘lengths’ argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor
</code></pre>
<p>My PyTorch version is 1.10.1+cu102</p>
",17567185.0,,681865.0,,2021-12-20 23:04:53,2021-12-20 23:04:53,"RuntimeError: ‘lengths’ argument should be a 1D CPU int64 tensor, but got 1D cuda:0 Long tensor",<pytorch>,1,0,,,,CC BY-SA 4.0
73029425,1,,,2022-07-18 22:55:01,,6,10848,"<p>I was previously loading a ResNet model with the ResNet50_Weights parameter successfully, but then suddenly I started getting the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;splitting_models.py&quot;, line 3, in &lt;module&gt;
    from torchvision.models.resnet import ResNet50_Weights
ImportError: cannot import name 'ResNet50_Weights' from 'torchvision.models.resnet' (/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torchvision/models/resnet.py)
</code></pre>
<p>Here is the import:</p>
<pre><code>from torchvision.models import ResNet50_Weights
</code></pre>
<p>How could I go about fixing this?</p>
<p>PyTorch version: 1.2.0</p>
<p>TorchVision version: 0.4.0</p>
<h2>EDIT</h2>
<p>Upgrading using</p>
<pre><code>pip install --upgrade torch torchvision
</code></pre>
<p>to the following versions fixed the issue:</p>
<p>PyTorch version: 1.12.0</p>
<p>TorchVision version: 0.13.0</p>
",5322196.0,,5322196.0,,2022-07-18 23:42:25,2023-02-28 10:22:18,Cannot import name 'ResNet50_Weights' from 'torchvision.models.resnet',<pytorch><torchvision>,2,2,,,,CC BY-SA 4.0
63106109,1,63109000.0,,2020-07-26 21:52:47,,6,20913,"<p>I am new to pytorch, and i would like to know how to display graphs of loss and accuraccy And how exactly should i store these values,knowing that i'm applying a cnn model for image classification using CIFAR10.</p>
<p>here is my current implementation :</p>
<pre><code>    def train(num_epochs,optimizer,criterion,model):
        for epoch in range(num_epochs):
            for i, (images, labels) in enumerate(trainloader):
                # origin shape: [4, 3, 32, 32] = 4, 3, 1024
                # input_layer: 3 input channels, 6 output channels, 5 kernel size
                images = images.to(device)
                labels = labels.to(device)
    
                # Forward pass
                outputs = model(images)
                loss = criterion(outputs, labels)
    
                # Backward and optimize
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
    
                if (i+1) % 2000 == 0:
                    print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')
                
        
        PATH = './cnn.pth'
        torch.save(model.state_dict(), PATH)
    
    
    def test ():
        with torch.no_grad():
            n_correct = 0
            n_samples = 0
            n_class_correct = [0 for i in range(10)]
            n_class_samples = [0 for i in range(10)]
        
            for images, labels in testloader:
                images = images.to(device)
                labels = labels.to(device)
                outputs = model(images)
                # max returns (value ,index)
                _, predicted = torch.max(outputs, 1)
                n_samples += labels.size(0)
                n_correct += (predicted == labels).sum().item()
            
                for i in range(batch_size):
                    label = labels[i]
                    pred = predicted[i]
                
                    if (label == pred):
                        n_class_correct[label] += 1
                    n_class_samples[label] += 1
    
            acc = 100.0 * n_correct / n_samples
            print(f'Accuracy of the network: {acc} %')
        
    
            for i in range(10):
                acc = 100.0 * n_class_correct[i] / n_class_samples[i]
                print(f'Accuracy of {classes[i]}: {acc} %')
            
            
            test_score = np.mean([100 * n_class_correct[i] / n_class_samples[i] for i in range(10)])
            print(&quot;the score test is : {0:.3f}%&quot;.format(test_score))
            return acc
</code></pre>
",13999613.0,,,,,2020-07-27 05:06:33,How to display graphs of loss and accuracy on pytorch using matplotlib,<python><matplotlib><pytorch><conv-neural-network>,1,0,0.0,,,CC BY-SA 4.0
63047762,1,63048031.0,,2020-07-23 05:56:12,,6,9395,"<p>I tried to define a simple model in Pytorch. The model computes negative log prob for a gaussian distribution:</p>
<pre><code>import torch
import torch.nn as nn

class GaussianModel(nn.Module):

    def __init__(self):
        super(GaussianModel, self).__init__()

        self.register_parameter('mean', nn.Parameter(torch.zeros(1),
                                                     requires_grad=True))
        
        self.pdf = torch.distributions.Normal(self.state_dict()['mean'],
                                              torch.tensor([1.0]))
    def forward(self, x):
        return -self.pdf.log_prob(x)

model = GaussianModel()
</code></pre>
<p>Then I tried to optimize the <code>mean</code> parameter:</p>
<pre><code>optimizer = torch.optim.SGD(model.parameters(), lr=0.002)
for _ in range(5):
  optimizer.zero_grad()
  nll = model(torch.tensor([3.0], requires_grad=True))
  nll.backward()
  optimizer.step()
  print('mean : ', model.state_dict()['mean'],
                 ' - Negative Loglikelihood : ', nll.item())
</code></pre>
<p>But it seems the gradient is zero and <code>mean</code> does not change:</p>
<pre><code>mean :  tensor([0.])  - Negative Loglikelihood :  5.418938636779785
mean :  tensor([0.])  - Negative Loglikelihood :  5.418938636779785
mean :  tensor([0.])  - Negative Loglikelihood :  5.418938636779785
mean :  tensor([0.])  - Negative Loglikelihood :  5.418938636779785
mean :  tensor([0.])  - Negative Loglikelihood :  5.418938636779785
</code></pre>
<p>Did I register and use the <code>mean</code> parameter correctly? can autograd compute the gradient for <code>torch.distributions.Normal.log_prob</code> or I should implement the <code>backward()</code> for the model?</p>
",4334320.0,,4334320.0,,2020-07-23 06:14:52,2020-07-23 08:57:45,Correct way to register a parameter for model in Pytorch,<python><pytorch>,1,0,0.0,,,CC BY-SA 4.0
64048720,1,64083148.0,,2020-09-24 14:34:01,,6,4416,"<p>I am trying to filter a single channel 2D image of size 256x256 using unfold to create 16x16 blocks with an overlap of 8. This is shown below:</p>
<pre><code># I = [256, 256] image
kernel_size = 16
stride = bx/2
patches = I.unfold(1, kernel_size, 
int(stride)).unfold(0, kernel_size, int(stride)) # size = [31, 31, 16, 16]

 
</code></pre>
<p>I have started to attempt to put the image back together with fold but I’m not quite there yet. I’ve tried to use view to get the image to ‘fit’ the way it’s supposed to but I don’t see how this would preserve the original image. Perhaps I’m overthinking this.</p>
<pre><code># patches.shape = [31, 31, 16, 16]
patches = = filt_data_block.contiguous().view(-1, kernel_size*kernel_size) # [961, 256]
patches = patches.permute(1, 0) # size = [951, 256]
</code></pre>
<p>Any help would be greatly appreciated. Thanks very much.</p>
",12994945.0,,,,,2020-09-26 22:38:30,Pytorch Unfold and Fold: How do I put this image tensor back together again?,<machine-learning><pytorch><artificial-intelligence><vision>,2,0,0.0,,,CC BY-SA 4.0
66746307,1,,,2021-03-22 12:43:40,,6,10341,"<p>I am following this tutorial: <a href=""https://huggingface.co/transformers/torchscript.html"" rel=""noreferrer"">https://huggingface.co/transformers/torchscript.html</a>
to create a trace of my custom BERT model, however when running the exact same <code>dummy_input</code> I receive an error:</p>
<pre><code>TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. 
We cant record the data flow of Python values, so this value will be treated as a constant in the future. 
</code></pre>
<p>Having loaded in my model and tokenizer, the code to create the trace is the following:</p>
<pre><code>text = &quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;
tokenized_text = tokenizer.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = '[MASK]'
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

traced_model = torch.jit.trace(model, dummy_input)
</code></pre>
<p>The <code>dummy_input</code> is a list of tensors so I'm not sure where the <code>Boolean</code> type is coming into play here. Does anyone understand why this error is occurring and whether the Boolean conversion is happening?</p>
<p>Many Thanks</p>
",14973252.0,,10886420.0,,2021-03-22 17:24:47,2021-03-22 17:24:47,Torch JIT Trace = TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect,<tensorflow><pytorch><huggingface-transformers><torchscript>,1,0,,,,CC BY-SA 4.0
75742628,1,,,2023-03-15 09:14:40,,6,175,"<p>I want to implement non-negative matrix factorization using PyTorch.  Here is my initial implement:</p>
<pre class=""lang-py prettyprint-override""><code>def nmf(X, k, lr, epochs):
    # X: input matrix of size (m, n)
    # k: number of latent factors
    # lr: learning rate
    # epochs: number of training epochs
    m, n = X.shape
    W = torch.rand(m, k, requires_grad=True)  # initialize W randomly
    H = torch.rand(k, n, requires_grad=True)  # initialize H randomly
    # training loop
    for i in range(epochs):
        # compute reconstruction error
        loss = torch.norm(X - torch.matmul(W, H), p='fro')
        # compute gradients
        loss.backward()
        # update parameters using additive update rule
        with torch.no_grad():
            W -= lr * W.grad
            H -= lr * H.grad
            W.grad.zero_()
            H.grad.zero_()
        if i % 10 == 0:
            print(f&quot;Epoch {i}: loss = {loss.item()}&quot;)
    return W.detach(), H.detach()
</code></pre>
<p>Lee and Seung in <a href=""https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf"" rel=""nofollow noreferrer"">this paper</a>, proposed to use adaptive learning rates to avoid subtraction and thus the production of negative elements. <a href=""https://stats.stackexchange.com/a/352921/312701"">Here</a> is the stats.SE thread where I get some idea. But I don't know how to implement <strong>multiplicative update rule</strong> for W,H in pytorch, as it need to separate the positive and negative part of their gradient respectively. <strong>Yes, I can manually implement that but I want to leverage this to the torch autograd.</strong></p>
<p><a href=""https://i.stack.imgur.com/H9j37.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/H9j37.png"" alt=""image"" /></a></p>
<p>Any idea how to manage to do so? Thanks in advance.</p>
",9138425.0,,9138425.0,,2023-03-17 15:50:13,2023-03-22 05:30:40,Get positive and negative part of gradient for loss function in PyTorch,<matrix><pytorch><mathematical-optimization><gradient-descent><autograd>,1,0,,,,CC BY-SA 4.0
76067104,1,76074046.0,,2023-04-20 18:14:37,,6,5011,"<p>I want to create a self hosted LLM model that will be able to have a context of my own custom data (Slack conversations for that matter).</p>
<p>I've heard Vicuna is a great alternative to ChatGPT and so I made the below code:</p>
<pre><code>from llama_index import SimpleDirectoryReader, LangchainEmbedding, GPTListIndex, \
    GPTSimpleVectorIndex, PromptHelper, LLMPredictor, Document, ServiceContext
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
import torch
from langchain.llms.base import LLM
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM

!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
    
class CustomLLM(LLM):
    model_name = &quot;eachadea/vicuna-13b-1.1&quot;
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    pipeline = pipeline(&quot;text2text-generation&quot;, model=model, tokenizer=tokenizer, device=0,
                        model_kwargs={&quot;torch_dtype&quot;:torch.bfloat16})

    def _call(self, prompt, stop=None):
        return self.pipeline(prompt, max_length=9999)[0][&quot;generated_text&quot;]
 
    def _identifying_params(self):
        return {&quot;name_of_model&quot;: self.model_name}

    def _llm_type(self):
        return &quot;custom&quot;


llm_predictor = LLMPredictor(llm=CustomLLM())
</code></pre>
<p>But sadly I'm hitting the below error:</p>
<pre><code>OutOfMemoryError: CUDA out of memory. Tried to allocate 270.00 MiB (GPU 0; 22.03 GiB total capacity; 21.65 GiB 
already allocated; 94.88 MiB free; 21.65 GiB reserved in total by PyTorch) If reserved memory is &gt;&gt; allocated 
memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and 
PYTORCH_CUDA_ALLOC_CONF
</code></pre>
<p>Here's the output of <code>!nvidia-smi</code> (before running anything):</p>
<pre><code>Thu Apr 20 18:04:00 2023       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A10G                     Off| 00000000:00:1E.0 Off |                    0 |
|  0%   23C    P0               52W / 300W|      0MiB / 23028MiB |     18%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
</code></pre>
<p>Any idea how to modify my code to make it work?</p>
",1492337.0,,,,,2023-06-05 16:05:24,Using Vicuna + langchain + llama_index for creating a self hosted LLM model,<python><machine-learning><pytorch><chatgpt-api><langchain>,2,0,,,,CC BY-SA 4.0
67821588,1,67843017.0,,2021-06-03 12:35:30,,6,10304,"<p>I am doing an experiment on a chest x-ray Project. and I want multiple versions of the CUDA toolkit but the problem is that my system put the latest version which I installed lastly is appearing.
Is it possible to run any of CUDA like 9.0, 10.2, 11.0 as required to GitHub code?</p>
<p>I have done all the initial steps like path added to an environment variable and added CUDNN copied file and added to the environment.
Now the problem is that I want to use Cuda 9.0 as per my code but my default setting put cuda.11.0 what is the solution or script to switch easily between these version</p>
",7156266.0,,681865.0,,2021-06-03 13:05:28,2021-06-04 19:31:17,Is it possible to run multiple CUDA version on windows?,<cuda><pytorch><nvidia><torchvision>,1,4,0.0,,,CC BY-SA 4.0
65541788,1,65699717.0,,2021-01-02 17:06:02,,6,2478,"<p>Currently Helsinki-NLP/opus-mt-es-en model takes around 1.5sec for inference from transformer. How can that be reduced?
Also when trying to convert it to onxx runtime getting this error:</p>
<blockquote>
<p>ValueError: Unrecognized configuration class &lt;class 'transformers.models.marian.configuration_marian.MarianConfig'&gt; for this kind of AutoModel: AutoModel.
Model type should be one of RetriBertConfig, MT5Config, T5Config, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaConfig, BartConfig, LongformerConfig, RobertaConfig, LayoutLMConfig, SqueezeBertConfig, BertConfig, OpenAIGPTConfig, GPT2Config, MobileBertConfig, TransfoXLConfig, XLNetConfig, FlaubertConfig, FSMTConfig, XLMConfig, CTRLConfig, ElectraConfig, ReformerConfig, FunnelConfig, LxmertConfig, BertGenerationConfig, DebertaConfig, DPRConfig, XLMProphetNetConfig, ProphetNetConfig, MPNetConfig, TapasConfig.</p>
</blockquote>
<p>Is it possible to convert this to onxx runtime?</p>
",7086926.0,,6331369.0,,2021-01-02 17:14:00,2022-01-10 13:56:54,How to reduce the inference time of Helsinki-NLP/opus-mt-es-en (translation model) from transformer,<pytorch><huggingface-transformers><machine-translation>,2,0,,,,CC BY-SA 4.0
66807032,1,,,2021-03-25 20:22:27,,6,11418,"<p>I am trying to execute a python file which has pytorch with lightning and torchvision modules. But after I downloaded and successfully installed whl file of pytorch in pi3 I am getting same error again and again.
The error is</p>
<pre><code> ModuleNotFoundError: No module named 'pytorch_lightning.metrics'
</code></pre>
<p>Help would be highly appreciated as I am stuck for more than 3 days.
I have installed the modules using pip.</p>
",15481650.0,,15481650.0,,2021-04-09 10:02:03,2022-08-31 08:15:58,How to install the module pytorch_lightning.metrics in Raspberry pi3,<python><pytorch><pytorch-lightning>,4,1,,,,CC BY-SA 4.0
72693671,1,,,2022-06-20 22:44:35,,6,3083,"<p>I'm on Windows 11, using WSL2 (Windows Subsystem for Linux). I recently upgraded my RAM from 32 GB to 64 GB.</p>
<p>While I can make my computer use more than 32 GB of RAM, WSL2 seems to be refusing to use more than 32 GB. For example, if I do</p>
<pre><code>&gt;&gt;&gt; import torch
&gt;&gt;&gt; a = torch.randn(100000, 100000) # 40 GB tensor
</code></pre>
<p>Then I see the memory usage go up until it hit's 30-ish GB, at which point, I see &quot;Killed&quot;, and the python process gets killed. Checking <code>dmesg</code>, it says that it killed the process because &quot;Out of memory&quot;.</p>
<p>Any idea what the problem might be, or what the solution is?</p>
",1114253.0,,646382.0,,2022-06-20 23:37:48,2022-06-20 23:37:48,"PyTorch running under WSL2 getting ""Killed"" for Out of memory even though I have a lot of memory left?",<memory><pytorch><out-of-memory><windows-subsystem-for-linux><wsl-2>,1,1,0.0,,,CC BY-SA 4.0
66116155,1,68703285.0,,2021-02-09 09:26:42,,6,15272,"<p>I have two version of CUDA installed on my Ubuntu 16.04 machine: 9.0 and 10.1.
They are located in <code>/usr/local/cuda-9.0</code> and <code>/usr/local/10.1</code> respectively.
If I install PyTorch 1.6.0 (which needs CUDA 10.1) via pip (<code>pip install torch==1.6.0</code>), it uses version 9.0 and thus detects no GPUs. I already changed my <code>LD_LIBRARY_PATH</code> to <code>&quot;/usr/local/cuda-10.1/lib64:/usr/local/cuda-10.1/cuda/extras/CUPTI/lib64&quot;</code> but PyTorch is still using CUDA 9.0.
How do I tell PyTorch to use CUDA 10.1?</p>
",13600920.0,,13600920.0,,2021-02-09 10:18:27,2021-08-08 17:43:08,How to tell PyTorch which CUDA version to take?,<pytorch>,1,6,0.0,,,CC BY-SA 4.0
66109084,1,66117248.0,,2021-02-08 20:44:13,,6,4570,"<p>I am trying to convert the Pegasus newsroom in HuggingFace's transformers model to the ONNX format. I followed <a href=""https://colab.research.google.com/github/huggingface/transformers/blob/master/notebooks/04-onnx-export.ipynb#scrollTo=foYlXrSksR_v"" rel=""noreferrer"">this</a> guide published by Huggingface. After installing the prereqs, I ran this code:</p>
<pre><code>!rm -rf onnx/
from pathlib import Path
from transformers.convert_graph_to_onnx import convert

convert(framework=&quot;pt&quot;, model=&quot;google/pegasus-newsroom&quot;, output=Path(&quot;onnx/google/pegasus-newsroom.onnx&quot;), opset=11)

</code></pre>
<p>and got these errors:</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-3b37ed1ceda5&gt; in &lt;module&gt;()
      3 from transformers.convert_graph_to_onnx import convert
      4 
----&gt; 5 convert(framework=&quot;pt&quot;, model=&quot;google/pegasus-newsroom&quot;, output=Path(&quot;onnx/google/pegasus-newsroom.onnx&quot;), opset=11)
      6 
      7 

6 frames
/usr/local/lib/python3.6/dist-packages/transformers/models/pegasus/modeling_pegasus.py in forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, encoder_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)
    938             input_shape = inputs_embeds.size()[:-1]
    939         else:
--&gt; 940             raise ValueError(&quot;You have to specify either decoder_input_ids or decoder_inputs_embeds&quot;)
    941 
    942         # past_key_values_length

ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds

</code></pre>
<p>I have never seen this error before. Any ideas?</p>
",14882176.0,,13273054.0,,2021-02-14 18:53:25,2023-01-02 16:29:28,how to convert HuggingFace's Seq2seq models to onnx format,<python><tensorflow><pytorch><huggingface-transformers><onnx>,2,0,0.0,,,CC BY-SA 4.0
66744675,1,,,2021-03-22 10:56:35,,6,4217,"<pre><code>with torch.no_grad:AttributeError: __enter__
</code></pre>
<p>I got this error while running pytorch code.</p>
<p>I have torch==0.4.1 torchvision==0.3.0, I run the code in google colab.</p>
",11837152.0,,,,,2021-03-22 11:57:35,with torch.no_grad: AttributeError: __enter__,<machine-learning><deep-learning><pytorch><torch>,1,0,,,,CC BY-SA 4.0
65173493,1,,,2020-12-06 21:54:10,,6,806,"<p>I understand the use of <code>super()</code> to inherit attributes and methods from a class when initializing your class. However, when using this in the context of neural networks such as:</p>
<pre><code>class Net(nn.Module):
      def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,
                 is_training = True):
          super(Net, self).__init__()
</code></pre>
<p>I am confused on why <code>super()</code> is calling the same class. What does this mean and why is this done?</p>
<p>Thanks very much!</p>
",11093050.0,,11093050.0,,2020-12-06 22:03:06,2020-12-06 22:03:06,Confused with the use of super() in Pytorch for neural net construction,<python><class><neural-network><pytorch><superclass>,0,4,0.0,,,CC BY-SA 4.0
68084302,1,,,2021-06-22 13:19:02,,6,2728,"<p>I am trying to finetune a pre-trained GPT2-model. When applying the respective tokenizer, I originally got the error message:</p>
<blockquote>
<p>Using pad_token, but it is not set yet.</p>
</blockquote>
<p>Thus, I changed my code to:</p>
<pre><code>GPT2_tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
GPT2_tokenizer.pad_token = GPT2_tokenizer.eos_token
</code></pre>
<p>When calling the <code>trainer.train()</code> later, I end up with the following error:</p>
<blockquote>
<p>AssertionError: Cannot handle batch sizes &gt; 1 if no padding token is
defined.</p>
</blockquote>
<p>Since I specifically defined the pad_token above, I expect these errors (or rather my fix of the original error and this new error) to be related - although I could be wrong. Is this a known problem that eos_token and pad_token somehow interfer? Is there an easy work-around?</p>
<p>Thanks a lot!</p>
",9806377.0,,,,,2021-12-08 16:06:22,"""AssertionError: Cannot handle batch sizes > 1 if no padding token is > defined"" and pad_token = eos_token",<python><neural-network><pytorch><data-science>,1,0,,,,CC BY-SA 4.0
68401650,1,68401651.0,,2021-07-15 22:56:22,,6,1303,"<p><a href=""https://pytorch.org/tutorials/advanced/cpp_extension.html"" rel=""noreferrer"">This tutorial</a> demonstrates how to make a C++/CUDA-based Python extension for PyTorch. But for ... reasons ... my use-case is more complicated than this and doesn't fit neatly within the Python setuptools framework described by the tutorial.</p>
<p>Is there a way to use cmake to compile a Python library that extends PyTorch?</p>
",752843.0,,,,,2021-07-15 22:56:22,How can I make a PyTorch extension with cmake,<cmake><pytorch>,1,0,0.0,,,CC BY-SA 4.0
68845314,1,68848042.0,,2021-08-19 09:29:24,,6,7070,"<p>I am fairly new to <strong>Docker</strong> and containerisation. I am wanting to decrease the size of <code>my_proj</code> docker container in production.</p>
<p>I prefer installing packages and managing dependencies via <code>Poetry</code>.</p>
<p>How can I specify using <strong>CPU-only</strong> <code>PyTorch</code> in a <code>Dockerfile</code>?</p>
<p>To do this via. <code>bash</code> terminal, it would be:</p>
<pre><code>poetry add pytorch-cpu torchvision-cpu -c pytorch
</code></pre>
<p>(or <code>conda install</code>...)</p>
<hr />
<p>My existing <code>Dockerfile</code>:</p>
<pre><code>FROM python:3.7-slim as base
RUN apt-get update -y \
    &amp;&amp; apt-get -y --no-install-recommends install curl wget\
    &amp;&amp; rm -rf /var/lib/apt/lists/* 
ENV ROOT /home/worker/python/my_proj
WORKDIR $ROOT

ARG ATLASSIAN_TOKEN
ARG POETRY_HTTP_BASIC_AZURE_PASSWORD
ARG ACCESS_KEY
ENV AWS_ACCESS_KEY_ID=$ACCESS_KEY
ARG SECRET_KEY
ENV AWS_SECRET_ACCESS_KEY=$SECRET_KEY
ARG REPO
ENV REPO_URL=$REPO
ENV PYPIRC_PATH=$ROOT/.pypirc

ENV \
    PYTHONFAULTHANDLER=1 \
    POETRY_VERSION=1.1.4 \
    POETRY_HOME=/etc/poetry \
    XDG_CACHE_HOME=/home/worker/.cache \
    POETRY_VIRTUALENVS_IN_PROJECT=true \
    MPLCONFIGDIR=/home/worker/matplotlib \
    PATH=/home/worker/python/my_proj/.venv/bin:/usr/local/bin:/etc/poetry/bin:$PATH

ADD https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py ./
RUN python get-poetry.py &amp;&amp; chmod +x /etc/poetry/bin/poetry
RUN --mount=type=cache,target=/root/.cache pip install twine keyring artifacts-keyring
RUN --mount=type=cache,target=/root/.cache apt update &amp;&amp; apt install gcc -y

FROM base as ws
ARG WS_APIKEY
ARG WS_PROJECTVERSION=
ARG WS_PROJECTNAME=workers-python-my_proj
ARG WS_PRODUCTNAME=HALO
COPY --chown=worker:worker . .
RUN --mount=type=cache,uid=1000,target=/home/worker/.cache poetry install --no-dev
COPY --from=openjdk:15-slim-buster /usr/local/openjdk-15 /usr/local/openjdk-15
ENV JAVA_HOME /usr/local/openjdk-15
ENV PATH $JAVA_HOME/bin:$PATH
RUN --mount=type=cache,uid=1000,target=/home/worker/.cache ./wss_agent.sh

FROM base as test
COPY . .
RUN poetry config experimental.new-installer false
RUN poetry install
RUN cd my_proj &amp;&amp; poetry run invoke deployconfluence_server_pass=$ATLASSIAN_TOKEN

FROM base as package
COPY . .
RUN poetry build
RUN python -m pip install --upgrade pip &amp;&amp; \
pip install twine keyring artifacts-keyring &amp;&amp; \
twine upload -r $REPO_URL --config-file $PYPIRC_PATH dist/* --skip-existing

FROM base as build
COPY . .
RUN poetry config experimental.new-installer false
RUN poetry install --no-dev
RUN pip3 --no-cache-dir install --upgrade awscli
RUN aws s3 cp s3://....tar.gz $ROOT/my_proj # censored url
RUN mkdir $ROOT/my_proj/bert-base-cased &amp;&amp; cd $ROOT/my_proj/bert-base-cased &amp;&amp; \
wget https://huggingface.co/bert-base-cased/resolve/main/config.json &amp;&amp; \
wget https://huggingface.co/bert-base-cased/resolve/main/tokenizer.json &amp;&amp; \
wget https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json 

FROM python:3.7-slim as production
ENV  ROOT=/home/worker/python/my_proj \
     VIRTUAL_ENV=/home/worker/python/my_proj/.venv\
     PATH=/home/worker/python/my_proj/.venv/bin:/home/worker/python/my_proj:$PATH
COPY --from=build /home/worker/python/my_proj/pyproject.toml /home/worker/python/
COPY --from=build /home/worker/python/my_proj/.venv /home/worker/python/my_proj/.venv
COPY --from=build /home/worker/python/my_proj/my_proj /home/worker/python/my_proj
WORKDIR $ROOT
ENV PYTHONPATH=$ROOT:/home/worker/python/
ENTRYPOINT [ &quot;primary_worker&quot;, &quot;--mongo&quot; ]
</code></pre>
",,user16341274,,user16341274,2021-08-19 09:40:11,2021-08-19 12:37:59,install PyTorch CPU-only in Dockerfile,<python-3.x><docker><pytorch><dockerfile><python-poetry>,1,0,0.0,,,CC BY-SA 4.0
67182475,1,67193104.0,,2021-04-20 16:05:49,,6,4157,"<p>Inside a <code>LightningModule</code>,Pycharm allows 2 auto complete methods:</p>
<pre><code>class MyModel(LightningModule):

    def on_validation_epoch_end(self):

    def validation_epoch_end(self, outs):
</code></pre>
<p>with <code>on_validation_epoch_end</code> referenced in <code>hooks.py</code></p>
<pre><code>def on_validation_epoch_end(self) -&gt; None:
    &quot;&quot;&quot;
    Called in the validation loop at the very end of the epoch.
    &quot;&quot;&quot;
    # do something when the epoch ends
</code></pre>
<p>and</p>
<p><code>validation_epoch_end</code> called in <code>evaluation_loop.py</code> as <code>eval_results = model.validation_epoch_end(eval_results)</code> leading to <code>__run_eval_epoch_end</code>.</p>
<hr />
<p>What is the purpose of each of those?</p>
<p>I can only assume one is deprecated. Could not find any relevant docs.</p>
",913098.0,,,,,2021-04-21 09:34:54,What is the difference between `on_validation_epoch_end` and `validation_epoch_end` in Pytorch-lightning?,<python><pytorch><pytorch-lightning>,1,0,0.0,,,CC BY-SA 4.0
67952290,1,,,2021-06-12 19:20:19,,6,2869,"<p>I am trying to use the base images provided by NVIDIA that let us use their GPUs via Docker containers. Because I am using docker, there is no need for me to have CUDA Toolkit or CuDNN on my system. All I need to have is the right driver - which I have.</p>
<p>I can run the official pytorch docker containers and the containers utilize my GPU. However when I run anything using the base images from NVIDIA then I get the following Warning -</p>
<pre><code>$ docker run --gpus all -it --rm -p 8000:8000 ubuntu-cuda-gpu:latest
</code></pre>
<pre><code>/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() &gt; 0
</code></pre>
<p>The application executes, it just uses CPU. But I want to be able to use my GPU like I can when I run the same code(it is a simple pytorch example) using official pytorch docker images.</p>
<p>The <a href=""https://hub.docker.com/r/nvidia/cuda/tags/?page=1&amp;ordering=last_updated"" rel=""noreferrer"">base image</a> used is -</p>
<pre><code>FROM nvidia/cuda:11.3.1-cudnn8-runtime-ubuntu20.04
# Setup
RUN apt update &amp;&amp; \
    apt install -y bash \
                   build-essential \
                   git \
                   curl \
                   ca-certificates \
                   python3 \
                   python3-pip &amp;&amp; \
    rm -rf /var/lib/apt/lists

# Your stuff
RUN python3 -m pip install --no-cache-dir --upgrade pip &amp;&amp; \
    python3 -m pip install --no-cache-dir \
    torch \
    transformers \
...
</code></pre>
<p>If I just run the image without any machine learning code and try to execute nvidia-smi then I get the output as -</p>
<pre><code>$ docker run --gpus all -it --rm -p 8000:8000 ubuntu-cuda-gpu:latest nvidia-smi
</code></pre>
<pre><code>Sat Jun 12 19:15:21 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.80       Driver Version: 460.80       CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 3060    Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   31C    P8     9W / 170W |     14MiB / 12053MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
</code></pre>
<p>This leads me to believe that at least something is right. But why is it that I am not able to use my GPU and how to make sure that I can?</p>
<p>I am on Ubuntu 20.04.</p>
",3647970.0,,681865.0,,2021-06-12 23:29:58,2021-06-12 23:29:58,NVIDIA cuda enabled docker container issue - UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(),<docker><ubuntu><pytorch><nvidia-docker>,0,2,0.0,,,CC BY-SA 4.0
67365237,1,,,2021-05-03 07:46:52,,6,1094,"<p>&quot;Obviously!&quot;, you might say... But there's one significant difference that I have trouble explaining by the difference in random initialization.</p>
<p>Take the two pre-trained basenets (before the average pooling layer) and feed them with the same image, you will notice that the output features don't follow the same distribution. Specifically, <strong>TensorFlow</strong>'s backbone has more inhibited features by the ReLU compared to <strong>Pytorch</strong>'s backbone. Additionally, as shows in the third figure, the dynamic range is different between the two frameworks.</p>
<p><a href=""https://i.stack.imgur.com/N80rP.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/N80rP.png"" alt=""Features distribution"" /></a></p>
<p>Of course, this difference is absorbed by the dense layer addressing the classification task, but: Can that difference be explained by randomness in the training process? Or training time? Or is there something else that would explain the difference?</p>
<p>Code to reproduce:</p>
<pre><code>import imageio
import numpy as np
image = imageio.imread(&quot;/tmp/image.png&quot;).astype(np.float32)/255

import tensorflow as tf
inputs = image[np.newaxis]
model = tf.keras.applications.ResNet50(include_top=False, input_shape=(None, None, 3))
output = model(inputs).numpy()
print(f&quot;TensorFlow features range: [{np.min(output):.02f};{np.max(output):.02f}]&quot;)

import torchvision
import torch
model = torch.nn.Sequential(*list(torchvision.models.resnet50(pretrained=True).children())[0:8])
inputs = torch.tensor(image).permute(2,0,1).unsqueeze(0)
output = model(inputs).detach().permute(0,2,3,1).numpy()
print(f&quot;Pytorch features range: [{np.min(output):.02f};{np.max(output):.02f}]&quot;)
</code></pre>
<p>Outputting</p>
<pre><code>TensorFlow features range: [0.00;25.98]
Pytorch features range: [0.00;12.00]
</code></pre>
<p>Note: it's similar to any image.</p>
",1782553.0,,4332585.0,,2022-04-08 13:31:40,2022-08-05 09:19:02,ImageNet pretrained ResNet50 backbones are different between Pytorch and TensorFlow,<tensorflow><deep-learning><pytorch><resnet><pre-trained-model>,2,0,0.0,,,CC BY-SA 4.0
70510341,1,,,2021-12-28 17:14:23,,6,5638,"<p>I haven't been able to find much in the way of examples on SHAP values with PyTorch. I've used two techniques to generate SHAP values, however, their results don't appear to agree with each other.</p>
<h1>SHAP KernelExplainer with PyTorch</h1>
<pre><code>import torch
from torch.autograd import Variable
import shap
import numpy
import pandas

torch.set_grad_enabled(False)

# Get features
train_features_df = ... # pandas dataframe
test_features_df = ... # pandas dataframe

# Define function to wrap model to transform data to tensor
f = lambda x: model_list[0]( Variable( torch.from_numpy(x) ) ).detach().numpy()

# Convert my pandas dataframe to numpy
data = test_features_df.to_numpy(dtype=np.float32)

# The explainer doesn't like tensors, hence the f function
explainer = shap.KernelExplainer(f, data)

# Get the shap values from my test data
shap_values = explainer.shap_values(data)

# Enable the plots in jupyter
shap.initjs()

feature_names = test_features_df.columns
# Plots
#shap.force_plot(explainer.expected_value, shap_values[0], feature_names)
#shap.dependence_plot(&quot;b1_price_avg&quot;, shap_values[0], data, feature_names)
shap.summary_plot(shap_values[0], data, feature_names)
</code></pre>
<p><a href=""https://i.stack.imgur.com/BuhCX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BuhCX.png"" alt=""SHAP summary plot from KernelExplainer with PyTorch"" /></a></p>
<h1>SHAP DeepExplainer with PyTorch</h1>
<pre><code># It wants gradients enabled, and uses the training set
torch.set_grad_enabled(True)
e = shap.DeepExplainer(model, Variable( torch.from_numpy( train_features_df.to_numpy(dtype=np.float32) ) ) )

# Get the shap values from my test data (this explainer likes tensors)
shap_values = e.shap_values( Variable( torch.from_numpy(data) ) )

# Plots
#shap.force_plot(explainer.expected_value, shap_values, feature_names)
#shap.dependence_plot(&quot;b1_price_avg&quot;, shap_values, data, feature_names)
shap.summary_plot(shap_values, data, feature_names)
</code></pre>
<p><a href=""https://i.stack.imgur.com/lPNpa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/lPNpa.png"" alt=""enter image description here"" /></a></p>
<h1>Comparing results</h1>
<p>As you can see from the summary plots, the value given to the features from the same PyTorch model, with the same test data, are noticeably different.</p>
<p>For example the feature b1_addresses_avg has value one from last with the KernelExplainer. But with the DeepExplainer is ranked third from top.</p>
<p>I'm not sure where to go from here.</p>
",439181.0,,,,,2023-03-31 14:42:49,SHAP values with PyTorch - KernelExplainer vs DeepExplainer,<python><pytorch><shap>,1,2,0.0,,,CC BY-SA 4.0
71328622,1,,,2022-03-02 20:14:33,,6,747,"<p>I have two networks and I'm profiling them to see which operations are taking most of the time. I noticed that the <code>CUDA time avg</code> for the <code>aten::conv2d</code> operation is different for different networks. That too by an order of magnitude. In my first network, it is <code>22us</code>, whereas for the second network it is <code>3ms</code>. My first network has convolution layers with up to <code>512</code> filters, but the second one has only up to <code>192</code> filters. So, I would expect that the average time taken by convolution operation should be lesser in my second network. Instead, it is 3 orders of magnitude higher. Why would this be happening?</p>
<p>The full profiling output is below</p>
<p>Network 1:</p>
<pre><code>                                                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  

                                      cudaLaunchKernel        99.80%     933.739ms        99.80%     933.739ms      20.750ms       0.000us         0.00%       0.000us       0.000us            45  
                                       model_inference         0.05%     453.000us       100.00%     935.567ms     935.567ms       0.000us         0.00%     195.000us     195.000us             1  
                               aten::cudnn_convolution         0.04%     388.000us        99.84%     934.047ms     103.783ms     195.000us       100.00%     195.000us      21.667us             9  
                                    aten::_convolution         0.01%     138.000us        99.88%     934.419ms     103.824ms       0.000us         0.00%     195.000us      21.667us             9  
                                          aten::conv2d         0.01%     122.000us        99.89%     934.592ms     103.844ms       0.000us         0.00%     195.000us      21.667us             9  
                                            aten::add_         0.01%     112.000us         0.02%     155.000us      17.222us       0.000us         0.00%       0.000us       0.000us             9  
                              aten::upsample_nearest2d         0.01%      82.000us         0.01%     105.000us      26.250us       0.000us         0.00%       0.000us       0.000us             4  
                                           aten::empty         0.01%      79.000us         0.01%      79.000us       3.292us       0.000us         0.00%       0.000us       0.000us            24  
                                       aten::threshold         0.01%      74.000us         0.02%     149.000us      18.625us       0.000us         0.00%       0.000us       0.000us             8  
                                            aten::_cat         0.01%      71.000us         0.01%     119.000us      29.750us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::relu         0.01%      57.000us         0.02%     206.000us      25.750us       0.000us         0.00%       0.000us       0.000us             8  
                                     aten::convolution         0.01%      51.000us        99.88%     934.470ms     103.830ms       0.000us         0.00%     195.000us      21.667us             9  
                                            aten::view         0.01%      50.000us         0.01%      50.000us       5.556us       0.000us         0.00%       0.000us       0.000us             9  
                                             aten::cat         0.00%      32.000us         0.02%     151.000us      37.750us       0.000us         0.00%       0.000us       0.000us             4  
                                         aten::reshape         0.00%      29.000us         0.01%      79.000us       8.778us       0.000us         0.00%       0.000us       0.000us             9  
                                         aten::resize_         0.00%      25.000us         0.00%      25.000us       0.962us       0.000us         0.00%       0.000us       0.000us            26  
                                            aten::rsub         0.00%      21.000us         0.00%      33.000us      33.000us       0.000us         0.00%       0.000us       0.000us             1  
                                             aten::mul         0.00%      17.000us         0.00%      27.000us      27.000us       0.000us         0.00%       0.000us       0.000us             1  
                                           aten::zeros         0.00%      13.000us         0.00%      16.000us      16.000us       0.000us         0.00%       0.000us       0.000us             1  
                                       cudaEventRecord         0.00%      12.000us         0.00%      12.000us       1.333us       0.000us         0.00%       0.000us       0.000us             9  
                                       cudaBindTexture         0.00%      11.000us         0.00%      11.000us       2.750us       0.000us         0.00%       0.000us       0.000us             4  
                                   aten::empty_strided         0.00%       6.000us         0.00%       6.000us       6.000us       0.000us         0.00%       0.000us       0.000us             1  
                                           aten::zero_         0.00%       1.000us         0.00%       1.000us       1.000us       0.000us         0.00%       0.000us       0.000us             1  
cudnn::maxwell::gemm::computeOffsetsKernel(cudnn::ma...         0.00%       0.000us         0.00%       0.000us       0.000us     195.000us       100.00%     195.000us     195.000us             1  
                                     cudaUnbindTexture         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             4  
Self CPU time total: 935.583ms
Self CUDA time total: 195.000us
</code></pre>
<p>Network 2:</p>
<pre><code>-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                        cudaMemcpyAsync        42.86%        1.035s        42.86%        1.035s      11.495ms       0.000us         0.00%       0.000us       0.000us            90  
                                       cudaLaunchKernel        34.81%     840.325ms        34.81%     840.325ms     169.969us       0.000us         0.00%       0.000us       0.000us          4944  
                                  cudaStreamSynchronize        15.92%     384.331ms        15.92%     384.331ms       5.736ms       0.000us         0.00%       0.000us       0.000us            67  
                                        model_inference         1.51%      36.559ms       100.00%        2.414s        2.414s       0.000us         0.00%        1.215s        1.215s             1  
                                            aten::fill_         1.03%      24.843ms        34.91%     842.670ms       7.731ms       8.759ms         0.72%       8.759ms      80.358us           109  
                                              aten::sum         0.57%      13.648ms         0.91%      22.019ms      18.123us      57.415ms         4.73%      57.415ms      47.255us          1215  
                                            aten::slice         0.50%      12.124ms         0.59%      14.229ms       3.526us       0.000us         0.00%       0.000us       0.000us          4035  
                                              aten::mul         0.49%      11.935ms         0.88%      21.340ms      17.293us     492.228ms        40.52%     492.228ms     398.888us          1234  
                                            aten::empty         0.44%      10.568ms         0.44%      10.568ms       2.556us       0.000us         0.00%       0.000us       0.000us          4134  
                                            aten::clamp         0.31%       7.455ms         0.84%      20.342ms      19.485us      12.405ms         1.02%      24.810ms      23.764us          1044  
                                              aten::add         0.25%       6.053ms         0.36%       8.615ms      14.334us      33.147ms         2.73%      33.147ms      55.153us           601  
                                aten::cudnn_convolution         0.18%       4.459ms         0.27%       6.549ms      46.779us     423.769ms        34.88%     423.769ms       3.027ms           140  
                                              aten::div         0.16%       3.892ms         0.27%       6.584ms      16.098us       3.225ms         0.27%       3.225ms       7.885us           409  
                                          aten::resize_         0.09%       2.287ms         0.10%       2.445ms       2.582us      75.000us         0.01%      75.000us       0.079us           947  
                                            aten::copy_         0.09%       2.226ms        58.96%        1.423s       6.498ms      80.877ms         6.66%      81.024ms     369.973us           219  
                                             aten::_cat         0.09%       2.087ms         0.12%       2.971ms      34.547us      26.689ms         2.20%      26.689ms     310.337us            86  
                                       aten::as_strided         0.09%       2.082ms         0.10%       2.305ms       0.554us       0.000us         0.00%       0.000us       0.000us          4164  
                                  aten::constant_pad_nd         0.06%       1.497ms        34.09%     822.790ms       9.350ms       0.000us         0.00%      46.706ms     530.750us            88  
                                     aten::_convolution         0.05%       1.113ms         0.38%       9.142ms      65.300us       0.000us         0.00%     440.725ms       3.148ms           140  
                                              aten::sub         0.04%       1.082ms         0.08%       1.905ms      18.676us      16.975ms         1.40%      16.975ms     166.422us           102  
                                       aten::leaky_relu         0.03%     727.000us         0.05%       1.253ms      19.277us      11.039ms         0.91%      11.039ms     169.831us            65  
                                       aten::reciprocal         0.03%     722.000us         0.05%       1.258ms      17.971us      10.340ms         0.85%      10.340ms     147.714us            70  
                                            aten::index         0.03%     707.000us         0.09%       2.140ms      66.875us      16.861ms         1.39%      17.207ms     537.719us            32  
                                             aten::add_         0.03%     672.000us         0.04%       1.027ms      14.671us      16.956ms         1.40%      16.956ms     242.229us            70  
                                           aten::conv2d         0.03%     610.000us         0.43%      10.298ms      73.557us       0.000us         0.00%     440.725ms       3.148ms           140  
                                             aten::view         0.03%     605.000us         0.03%     619.000us       2.623us       0.000us         0.00%       0.000us       0.000us           236  
                                    aten::empty_strided         0.02%     564.000us         0.02%     564.000us       6.409us       0.000us         0.00%       0.000us       0.000us            88  
                                      aten::convolution         0.02%     546.000us         0.40%       9.688ms      69.200us       0.000us         0.00%     440.725ms       3.148ms           140  
                                           aten::narrow         0.02%     534.000us         0.06%       1.388ms       4.131us       0.000us         0.00%       0.000us       0.000us           336  
                                              aten::cat         0.02%     511.000us         0.14%       3.482ms      40.488us       0.000us         0.00%      26.689ms     310.337us            86  
                                               aten::to         0.02%     413.000us        58.86%        1.421s       9.665ms       0.000us         0.00%      42.584ms     289.687us           147  
                                             aten::rsub         0.02%     374.000us         0.03%     616.000us      19.250us      92.000us         0.01%      92.000us       2.875us            32  
                                           aten::select         0.01%     311.000us         0.01%     354.000us       4.023us       0.000us         0.00%       0.000us       0.000us            88  
                                          aten::reshape         0.01%     304.000us         0.03%     660.000us       3.976us       0.000us         0.00%       0.000us       0.000us           166  
                                             aten::ceil         0.01%     265.000us         0.03%     717.000us      21.088us     606.000us         0.05%       1.212ms      35.647us            34  
                                          aten::permute         0.01%     214.000us         0.01%     249.000us       4.446us       0.000us         0.00%       0.000us       0.000us            56  
                              aten::upsample_bilinear2d         0.01%     199.000us         0.03%     629.000us      34.944us       2.185ms         0.18%       2.260ms     125.556us            18  
                                           aten::expand         0.01%     189.000us         0.01%     246.000us       3.417us       0.000us         0.00%       0.000us       0.000us            72  
                                             aten::ones         0.01%     180.000us         1.02%      24.632ms     947.385us       0.000us         0.00%       0.000us       0.000us            26  
                                               aten::gt         0.01%     162.000us         0.02%     474.000us      29.625us     496.000us         0.04%     992.000us      62.000us            16  
                                           aten::repeat         0.01%     154.000us         0.03%     724.000us      60.333us       0.000us         0.00%       0.000us       0.000us            12  
                                        cudaEventRecord         0.01%     146.000us         0.01%     146.000us       1.043us       0.000us         0.00%       0.000us       0.000us           140  
                                        aten::unsqueeze         0.01%     144.000us         0.01%     177.000us       3.404us       0.000us         0.00%       0.000us       0.000us            52  
                                       aten::contiguous         0.01%     139.000us         0.03%     735.000us      22.969us       0.000us         0.00%     346.000us      10.812us            32  
                                             aten::mean         0.01%     137.000us         0.01%     214.000us      23.778us     131.000us         0.01%     131.000us      14.556us             9  
                                           aten::arange         0.01%     124.000us         0.01%     242.000us      10.083us       0.000us         0.00%       0.000us       0.000us            24  
                                       aten::empty_like         0.01%     123.000us         0.01%     284.000us       5.680us       0.000us         0.00%       0.000us       0.000us            50  
                                        cudaBindTexture         0.01%     121.000us         0.01%     121.000us       3.025us       0.000us         0.00%       0.000us       0.000us            40  
                                            aten::stack         0.00%     112.000us         0.03%     802.000us      50.125us       0.000us         0.00%     158.000us       9.875us            16  
                                            aten::floor         0.00%      77.000us         0.01%     191.000us      23.875us      18.000us         0.00%      36.000us       4.500us             8  
                                         aten::moveaxis         0.00%      73.000us         0.01%     276.000us      11.500us       0.000us         0.00%       0.000us       0.000us            24  
                                          aten::movedim         0.00%      67.000us         0.01%     203.000us       8.458us       0.000us         0.00%       0.000us       0.000us            24  
                                           aten::unfold         0.00%      61.000us         0.00%      82.000us       2.562us       0.000us         0.00%       0.000us       0.000us            32  
                                      aten::leaky_relu_         0.00%      51.000us         0.00%     119.000us      23.800us       0.000us         0.00%     789.000us     157.800us             5  
                                         aten::_s_where         0.00%      51.000us         0.00%      91.000us      22.750us     536.000us         0.04%     536.000us     134.000us             4  
                                            aten::clone         0.00%      36.000us         0.01%     159.000us      31.800us       0.000us         0.00%     435.000us      87.000us             5  
                                            aten::where         0.00%      34.000us         0.01%     174.000us      43.500us       0.000us         0.00%     536.000us     134.000us             4  
                                        aten::expand_as         0.00%      27.000us         0.00%      70.000us       4.375us       0.000us         0.00%       0.000us       0.000us            16  
                                            aten::zeros         0.00%      18.000us         0.00%      29.000us      14.500us       0.000us         0.00%       0.000us       0.000us             2  
                                             aten::item         0.00%      16.000us         0.00%      22.000us       2.750us       0.000us         0.00%       0.000us       0.000us             8  
                                          aten::detach_         0.00%      10.000us         0.00%      15.000us       3.750us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::alias         0.00%       8.000us         0.00%       8.000us       0.667us       0.000us         0.00%       0.000us       0.000us            12  
                              aten::_local_scalar_dense         0.00%       6.000us         0.00%       6.000us       0.750us       0.000us         0.00%       0.000us       0.000us             8  
                                                detach_         0.00%       5.000us         0.00%       5.000us       1.250us       0.000us         0.00%       0.000us       0.000us             4  
                                            aten::zero_         0.00%       2.000us         0.00%       2.000us       1.000us       0.000us         0.00%       0.000us       0.000us             2  
                       Memcpy HtoD (Pageable -&gt; Device)         0.00%       0.000us         0.00%       0.000us       0.000us      41.981ms         3.46%      41.981ms     626.582us            67  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       8.759ms         0.72%       8.759ms     105.530us            83  
void at::native::unrolled_elementwise_kernel&lt;at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      37.512ms         3.09%      37.512ms     451.952us            83  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      65.145ms         5.36%      65.145ms     208.131us           313  
void at::native::unrolled_elementwise_kernel&lt;at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     416.783ms        34.31%     416.783ms     494.992us           842  
void at::native::reduce_kernel&lt;256, 2, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       2.070ms         0.17%       2.070ms       8.519us           243  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.051ms         0.99%      12.051ms      24.950us           483  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.225ms         0.27%       3.225ms       7.885us           409  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      12.284ms         1.01%      12.284ms      24.277us           506  
void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      26.580ms         2.19%      26.580ms     359.189us            74  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      11.039ms         0.91%      11.039ms     169.831us            65  
                         Memcpy DtoD (Device -&gt; Device)         0.00%       0.000us         0.00%       0.000us       0.000us     510.000us         0.04%     510.000us      22.174us            23  
cudnn::maxwell::gemm::computeOffsetsKernel(cudnn::ma...         0.00%       0.000us         0.00%       0.000us       0.000us      62.000us         0.01%      62.000us       5.167us            12  
                 maxwell_scudnn_128x32_relu_interior_nn         0.00%       0.000us         0.00%       0.000us       0.000us       1.320ms         0.11%       1.320ms     132.000us            10  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.340ms         0.85%      10.340ms     147.714us            70  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.300ms         0.85%      10.300ms     130.380us            79  
void at::native::unrolled_elementwise_kernel&lt;at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      50.898ms         4.19%      50.898ms     242.371us           210  
void cudnn::winograd::generateWinogradTilesKernel&lt;0,...         0.00%       0.000us         0.00%       0.000us       0.000us       1.166ms         0.10%       1.166ms      13.250us            88  
maxwell_scudnn_winograd_128x128_ldg1_ldg4_tile148n_n...         0.00%       0.000us         0.00%       0.000us       0.000us     150.355ms        12.38%     150.355ms       1.709ms            88  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.775ms         0.31%       3.775ms      78.646us            48  
                maxwell_scudnn_128x128_relu_interior_nn         0.00%       0.000us         0.00%       0.000us       0.000us     106.000us         0.01%     106.000us     106.000us             1  
                   maxwell_scudnn_128x128_relu_small_nn         0.00%       0.000us         0.00%       0.000us       0.000us     104.000us         0.01%     104.000us     104.000us             1  
                                      cudaUnbindTexture         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us            40  
void cudnn::detail::implicit_convolve_sgemm&lt;float, f...         0.00%       0.000us         0.00%       0.000us       0.000us      12.632ms         1.04%      12.632ms     789.500us            16  
void at::native::reduce_kernel&lt;256, 2, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      10.000us         0.00%      10.000us      10.000us             1  
void at::native::(anonymous namespace)::upsample_bil...         0.00%       0.000us         0.00%       0.000us       0.000us       2.185ms         0.18%       2.185ms     121.389us            18  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     606.000us         0.05%     606.000us      35.647us            17  
void at::native::reduce_kernel&lt;128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     121.000us         0.01%     121.000us      15.125us             8  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      18.000us         0.00%      18.000us       4.500us             4  
void at::native::unrolled_elementwise_kernel&lt;at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     103.000us         0.01%     103.000us      12.875us             8  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     121.000us         0.01%     121.000us       7.562us            16  
void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     109.000us         0.01%     109.000us      13.625us             8  
void at::native::unrolled_elementwise_kernel&lt;at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     354.000us         0.03%     354.000us      11.062us            32  
void at::native::vectorized_elementwise_kernel&lt;4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      92.000us         0.01%      92.000us       2.875us            32  
void at::native::unrolled_elementwise_kernel&lt;at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     346.000us         0.03%     346.000us      10.812us            32  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 2.414s
Self CUDA time total: 1.215s
</code></pre>
<p>Profiling code:</p>
<pre><code>with torch.no_grad():
  with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:
    with record_function(&quot;model_inference&quot;):
      output_batch = self.frame_predictor(input_batch)
  print(prof.key_averages().table(sort_by=&quot;self_cuda_time_total&quot;, row_limit=10))
</code></pre>
",3337089.0,,3337089.0,,2022-03-09 13:40:21,2022-03-09 13:40:21,Pytorch profiler shows different average execution time for convolution for two different networks,<pytorch><profiling>,0,4,,,,CC BY-SA 4.0
65623906,1,65625608.0,,2021-01-08 05:33:01,,6,699,"<p>I'm trying to vectorize the following for-loop in Pytorch. I'd be happy with just vectorizing the inner for-loop, but doing the whole batch would also be awesome.</p>
<pre><code># B: the batch size
# N: the number of training examples 
# dim: the dimension of each feature vector
# K: the number of discrete labels. each vector has a single label
# delta: margin for hinge loss

batch_data = torch.tensor(...)  # Tensor of shape [B x N x d]
batch_labels = torch.tensor(...)  # Tensor of shape [B x N x 1], each element is one of K labels (ints)

batch_losses = []  # Ultimately should be [B x 1]
batch_centroids = []  # Ultimately should be [B x K_i x dim]
for i in range(B):
    centroids = []  # Keep track of the means for each class. 
    classes = torch.unique(labels)  # Get the unique labels for the classes.

    # NOTE: The number of classes K for each item in the batch might actually
    # be different. This may complicate batch-level operations.

    total_loss = 0

    # For each class independently. This is the part I want to vectorize.
    for cl in classes:
        # Take the subset of training examples with that label.
        subset = data[torch.where(labels == cl)]

        # Find the centroid of that subset.
        centroid = subset.mean(dim=0)
        centroids.append(centroid)
  
        # Get the distance between each point in the subset and the centroid.
        dists = subset - centroid
        norm = torch.linalg.norm(dists, dim=1)

        # The loss is the mean of the hinge loss across the subset.
        margin = norm - delta
        hinge = torch.clamp(margin, min=0.0) ** 2

        total_loss += hinge.mean()

    # Keep track of everything. If it's too hard to keep track of centroids, that's also OK.
    loss = total_loss.mean()
    batch_losses.append(loss)
    batch_centroids.append(centroids)
   
   
</code></pre>
<p>I've been scratching my head on how to deal with the irregularly sized tensors. The number of classes in each batch <code>K_i</code> is different, and the size of each subset is different.</p>
",975021.0,,466862.0,,2021-01-08 08:51:53,2021-01-16 19:32:56,How to vectorize indexing and computation when indexed tensors are different dimensions?,<python><pytorch><vectorization>,2,0,0.0,,,CC BY-SA 4.0
74027680,1,76528206.0,,2022-10-11 11:53:02,,6,474,"<p>I am trying to learn how to use the Pytorch profiler API to measure the difference in performance when training a model using different methods. In the dedicated <a href=""https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html"" rel=""noreferrer"">tutorial</a>, there is one part where they show how to do just that using the &quot;schedule&quot; parameter of the profiler.</p>
<p>My problem is that when I want to use it in my code, calling step the first &quot;wait&quot; times prints a message</p>
<blockquote>
<p>[W kineto_shim.cpp:337] Profiler is not initialized: skipping step() invocation</p>
</blockquote>
<p>Since I want my profiler to sleep most of the time, my &quot;wait&quot; value is quite high so it pollutes my terminal with a bunch of those lines until the profiler is actually executed for the first time</p>
<p>How can I get rid of it ?</p>
<p>Here's a minimal code sample that reproduces the problem</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from torch.profiler import profile, record_function, ProfilerActivity

with profile(
        activities=[torch.profiler.ProfilerActivity.CUDA],
        schedule=torch.profiler.schedule(wait=15, warmup=1, active=4),        
        profile_memory=False,
        record_shapes=True,
        with_stack=True,
    ) as prof:
        for _ in range(20):
            y = torch.randn(1).cuda() + torch.randn(1).cuda()
            prof.step()
            

print(prof.key_averages())
</code></pre>
",11012043.0,,,,,2023-06-30 23:06:35,Pytorch profiler with scheduler prints unwanted message at step,<python><pytorch><profiler>,1,0,,,,CC BY-SA 4.0
66596699,1,,,2021-03-12 08:42:48,,6,2489,"<p>In PyTorch, the build-in <code>torch.roll</code> function is only able to shift columns (or rows) with same offsets. But I want to shift columns with different offsets. Suppose the input tensor is</p>
<pre><code>[[1,2,3],
 [4,5,6],
 [7,8,9]]
</code></pre>
<p>Let's say, I want to shift with offset <code>i</code> for the i-th column. Thus, the expected output is</p>
<pre><code>[[1,8,6],
 [4,2,9],
 [7,5,3]]
</code></pre>
<p>An option to do so is to separately shift every column using <code>torch.roll</code> and concat each of them. But for the consideration of effectiveness and code compactness, I don't want to introduce the loop structure. Is there a better way？</p>
",13633030.0,,13633030.0,,2021-03-12 08:51:41,2021-08-03 19:51:08,How to shift columns (or rows) in a tensor with different offsets in PyTorch?,<pytorch>,2,0,,,,CC BY-SA 4.0
69718379,1,,,2021-10-26 06:30:58,,6,2383,"<p>I am confused about whether it is possible to run an int8 quantized model on CUDA, or can you only train a quantized model on CUDA with fakequantise for deployment on another backend such as a CPU.</p>
<p>I want to run the model on CUDA with actual int8 instructions instead of FakeQuantised float32 instructions, and enjoy the efficiency gains. Pytorch docs are strangely nonspecific about this. If it is possible to run a quantized model on CUDA with a different framework such as <code>TensorFlow</code> I would love to know.</p>
<p>This is the code to prep my quantized model (using post-training quantization). The model is normal CNN with nn.Conv2d and nn.LeakyRelu and nn.MaxPool modules:</p>
<pre><code>model_fp = torch.load(models_dir+net_file)

model_to_quant = copy.deepcopy(model_fp)
model_to_quant.eval()
model_to_quant = quantize_fx.fuse_fx(model_to_quant)

qconfig_dict = {&quot;&quot;: torch.quantization.get_default_qconfig('qnnpack')}

model_prepped = quantize_fx.prepare_fx(model_to_quant, qconfig_dict)
model_prepped.eval()
model_prepped.to(device='cuda:0')

train_data   = ImageDataset(img_dir, train_data_csv, 'cuda:0')
train_loader = DataLoader(train_data, batch_size=32, shuffle=True, pin_memory=True)

for i, (input, _) in enumerate(train_loader):
    if i &gt; 1: break
    print('batch', i+1, end='\r')
    input = input.to('cuda:0')
    model_prepped(input)
</code></pre>
<p>This actually quantizes the model:</p>
<pre><code>model_quantised = quantize_fx.convert_fx(model_prepped)
model_quantised.eval()
</code></pre>
<p>This is an attempt to run the quantized model on CUDA, and raises a NotImplementedError, when I run it on CPU it works fine:</p>
<pre><code>model_quantised = model_quantised.to('cuda:0')
for i, _ in train_loader:
    input = input.to('cuda:0')
    out = model_quantised(input)
    print(out, out.shape)
    break
</code></pre>
<p>This is the error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/adam/Desktop/thesis/Ship Detector/quantisation.py&quot;, line 54, in &lt;module&gt;
    out = model_quantised(input)
  File &quot;/home/adam/.local/lib/python3.9/site-packages/torch/fx/graph_module.py&quot;, line 513, in wrapped_call
    raise e.with_traceback(None)
NotImplementedError: Could not run 'quantized::conv2d.new' with arguments from the 'QuantizedCUDA' backend. 
This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). 
If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 
'quantized::conv2d.new' is only available for these backends: [QuantizedCPU, BackendSelect, Named, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, UNKNOWN_TENSOR_TYPE_ID, AutogradMLC, Tracer, Autocast, Batched, VmapMode].
</code></pre>
",15818618.0,,681865.0,,2021-10-26 07:18:05,2022-09-23 07:07:51,Running Pytorch Quantized Model on CUDA GPU,<python><machine-learning><pytorch>,1,0,0.0,,,CC BY-SA 4.0
72224866,1,73269143.0,,2022-05-13 05:35:24,,6,958,"<p>I want to know the inference time of a layer in Alexnet. This code measures the inference time of the first fully connected layer of Alexnet as the batch size changes. And I have a few questions about this.</p>
<ol>
<li>Is it possible to measure the inference time accurately with the following code?</li>
<li>Is there a time difference because the CPU and GPU run separately?</li>
<li>Is there a module used to measure layer inference time in Pytorch?</li>
</ol>
<p>Given the following code:</p>
<pre><code>import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
import time
from tqdm import tqdm


class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()

        self.relu = nn.ReLU(inplace=True)
        self.maxpool2D = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
        self.adaptive_avg_polling = nn.AdaptiveAvgPool2d((6, 6))
        self.dropout = nn.Dropout(p=0.5)

        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)
        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)
        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)
        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)
        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(256 * 6 * 6, 4096)
        self.fc2 = nn.Linear(4096, 4096)
        self.fc3 = nn.Linear(4096, 1000)

    def time(self, x):
        x = self.maxpool2D(self.relu(self.conv1(x)))
        x = self.maxpool2D(self.relu(self.conv2(x)))
        x =                self.relu(self.conv3(x))
        x =                self.relu(self.conv4(x))
        x = self.maxpool2D(self.relu(self.conv5(x)))
        x = self.adaptive_avg_polling(x)


        x = x.view(x.size(0), -1)
        x = self.dropout(x)

        start1 = time.time()
        x = self.fc1(x)
        finish1 = time.time()

        x = self.dropout(self.relu(x))
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)

        return finish1 - start1



def layer_time():
     use_cuda = torch.cuda.is_available()
     print(&quot;use_cuda : &quot;, use_cuda)

     FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor
     device= torch.device(&quot;cuda:0&quot; if use_cuda else &quot;cpu&quot;)

     net = AlexNet().to(device)

     test_iter = 10000
     batch_size = 1
     for i in range(10):
         X = torch.randn(size=(batch_size, 3, 227, 227)).type(FloatTensor)
         s = 0.0
         for i in tqdm(range(test_iter)):
             s += net.time(X)
         print(s)
         batch_size *= 2


 layer_time()

</code></pre>
",19106705.0,,19106705.0,,2022-08-07 16:26:17,2022-08-07 17:37:43,How to get time taken for each layer in Pytorch?,<python><pytorch>,1,1,0.0,,,CC BY-SA 4.0
65057895,1,,,2020-11-29 07:27:07,,6,1062,"<p>I'm training my neural network using PyTorch framework. The data is full HD images (1920x1080). But in each iteration, I just need to crop out a random 256x256 patch from these images. My network is relatively small (5 conv layers), and hence the bottleneck is being caused by loading the data. I've provided my current code below. Is there any way to optimize loading the data and speed up the training?</p>
<p><strong>Code</strong>:</p>
<pre><code>from pathlib import Path

import numpy
import skimage.io
import torch.utils.data as data

import Imath
import OpenEXR


class Ours(data.Dataset):
    &quot;&quot;&quot;
    Loads patches of resolution 256x256. Patches are selected such that they contain atleast 1 unknown pixel
    &quot;&quot;&quot;

    def __init__(self, data_dirpath, split_name, patch_size):
        super(Ours, self).__init__()
        self.dataroot = Path(data_dirpath) / split_name
        self.video_names = []
        for video_path in sorted(self.dataroot.iterdir()):
            for i in range(4):
                for j in range(11):
                    view_num = i * 12 + j
                    self.video_names.append((video_path.stem, view_num))
        self.patch_size = patch_size
        return

    def __getitem__(self, index):
        video_name, view_num = self.video_names[index]

        patch_start_pt = (numpy.random.randint(1080), numpy.random.randint(1920))

        frame1_path = self.dataroot / video_name / f'render/rgb/{view_num + 1:04}.png'
        frame2_path = self.dataroot / video_name / f'render/rgb/{view_num + 2:04}.png'
        depth_path = self.dataroot / video_name / f'render/depth/{view_num + 1:04}.exr'
        mask_path = self.dataroot / video_name / f'render/masks/{view_num + 1:04}.png'
        frame1 = self.get_image(frame1_path, patch_start_pt)
        frame2 = self.get_image(frame2_path, patch_start_pt)
        mask = self.get_mask(mask_path, patch_start_pt)
        depth = self.get_depth(depth_path, patch_start_pt, mask)

        data_dict = {
            'frame1': frame1,
            'frame2': frame2,
            'mask': mask,
            'depth': depth,
        }
        return data_dict

    def __len__(self):
        return len(self.video_names)

    @staticmethod
    def get_mask(path: Path, patch_start_point: tuple):
        h, w = patch_start_point
        mask = skimage.io.imread(path.as_posix())[h:h + self.patch_size, w:w + self.patch_size][None]
        return mask

    def get_image(self, path: Path, patch_start_point: tuple):
        h, w = patch_start_point
        image = skimage.io.imread(path.as_posix())
        image = image[h:h + self.patch_size, w:w + self.patch_size, :3]
        image = image.astype(numpy.float32) / 255 * 2 - 1
        image_cf = numpy.moveaxis(image, [0, 1, 2], [1, 2, 0])
        return image_cf

    def get_depth(self, path: Path, patch_start_point: tuple, mask: numpy.ndarray):
        h, w = patch_start_point

        exrfile = OpenEXR.InputFile(path.as_posix())
        raw_bytes = exrfile.channel('B', Imath.PixelType(Imath.PixelType.FLOAT))
        depth_vector = numpy.frombuffer(raw_bytes, dtype=numpy.float32)
        height = exrfile.header()['displayWindow'].max.y + 1 - exrfile.header()['displayWindow'].min.y
        width = exrfile.header()['displayWindow'].max.x + 1 - exrfile.header()['displayWindow'].min.x
        depth = numpy.reshape(depth_vector, (height, width))

        depth = depth[h:h + self.patch_size, w:w + self.patch_size]
        depth = depth[None]
        depth = depth.astype(numpy.float32)
        depth = depth * mask
        return depth
</code></pre>
<p>Finally, I'm creating a DataLoader as follows:</p>
<pre><code>train_data_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)
</code></pre>
<p><strong>What I've tried so far</strong>:</p>
<ol>
<li>I've searched if it is possible to read a part of the image. Unfortunately, I didn't get any leads. Looks like python libraries read the full image.</li>
<li>I'm planning to read more patches from a single image so that I will need to read fewer images. But in PyTorch framework, the <code>get_item()</code> function has to return a single sample, not a batch. So, in each <code>get_item()</code> I can read only a patch.</li>
<li>I'm planning to circumvent this as follows: Read 4 patches in <code>get_item()</code> and return patches of shape <code>(4,3,256,256)</code> instead of <code>(3,256,256)</code>. Later when I read a batch using dataloader, I'll get a batch of shape <code>(BS,4,3,256,256)</code> instead of <code>(BS,3,256,256)</code>. I can then concatenate the data along <code>dim=1</code> to convert <code>(BS,4,3,256,256)</code> to <code>(BS*4,3,256,256)</code>. Thus I can reduce <code>batch_size</code> (<code>BS</code>) by 4 and hopefully this will speed up data loading by 4 times.</li>
</ol>
<p>Are there any other options? I'm open to all kind of suggestions. Thanks!</p>
",3337089.0,,6331369.0,,2021-02-05 12:06:58,2021-02-05 12:06:58,Optimize pytorch data loader for reading small patches in full HD images,<python><performance><pytorch><dataloader><pytorch-dataloader>,0,3,,,,CC BY-SA 4.0
64358283,1,,,2020-10-14 17:08:02,,6,8329,"<p>I have the following pieces of code:</p>
<pre><code># Device configuration
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)

# split the dataset into validation and test sets
len_valid_set = int(0.1*len(dataset))
len_train_set = len(dataset) - len_valid_set

print(&quot;The length of Train set is {}&quot;.format(len_train_set))
print(&quot;The length of Test set is {}&quot;.format(len_valid_set))

train_dataset , valid_dataset,  = torch.utils.data.random_split(dataset , [len_train_set, len_valid_set])

# shuffle and batch the datasets
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)
test_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=8, shuffle=True, num_workers=4)

print(&quot;LOADERS&quot;,
    len(dataloader),
    len(train_loader),
    len(test_loader))
</code></pre>
<p>The length of Train set is 720</p>
<p>The length of Test set is 80</p>
<p>LOADERS 267 90 10</p>
<pre><code>mean = 0.0
std = 0.0
nb_samples = 0.0
for data in train_loader:
    images, landmarks = data[&quot;image&quot;], data[&quot;landmarks&quot;]
    batch_samples = images.size(0)

    images_data = images.view(batch_samples, images.size(1), -1)
    mean += images_data.mean(2).sum(0)
    std += images_data.std(2).sum(0)
    nb_samples += batch_samples

mean /= nb_samples
std /= nb_samples
</code></pre>
<p>And I get this error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-23-9e47ddfeff5e&gt; in &lt;module&gt;
      7 
      8     images_data = images.view(batch_samples, images.size(1), -1)
----&gt; 9     mean += images_data.mean(2).sum(0)
     10     std += images_data.std(2).sum(0)
     11     nb_samples += batch_samples

RuntimeError: Can only calculate the mean of floating types. Got Byte instead.
</code></pre>
<p>The fixed code is taken from <a href=""https://stackoverflow.com/a/64349380/2414957"">https://stackoverflow.com/a/64349380/2414957</a> it worked for dataloader but not train_loader</p>
<p>Also, these are the results of</p>
<pre><code>print(type(images_data))
print(images_data)
</code></pre>
<p>We have:</p>
<pre><code>&lt;class 'torch.Tensor'&gt;
tensor([[[74, 74, 74,  ..., 63, 63, 63],
         [73, 73, 73,  ..., 61, 61, 61],
         [75, 75, 75,  ..., 61, 61, 61],
         ...,
         [74, 74, 74,  ..., 38, 38, 38],
         [75, 75, 75,  ..., 39, 39, 39],
         [72, 72, 72,  ..., 38, 38, 38]],

        [[75, 75, 75,  ..., 65, 65, 65],
         [75, 75, 75,  ..., 62, 62, 62],
         [75, 75, 75,  ..., 63, 63, 63],
         ...,
         [71, 71, 71,  ..., 39, 39, 39],
         [74, 74, 74,  ..., 38, 38, 38],
         [73, 73, 73,  ..., 37, 37, 37]],

        [[72, 72, 72,  ..., 62, 62, 62],
         [74, 74, 74,  ..., 63, 63, 63],
         [75, 75, 75,  ..., 61, 61, 61],
         ...,
         [74, 74, 74,  ..., 38, 38, 38],
         [74, 74, 74,  ..., 39, 39, 39],
         [73, 73, 73,  ..., 37, 37, 37]],

        ...,

        [[75, 75, 75,  ..., 63, 63, 63],
         [73, 73, 73,  ..., 63, 63, 63],
         [74, 74, 74,  ..., 62, 62, 62],
         ...,
         [74, 74, 74,  ..., 38, 38, 38],
         [73, 73, 73,  ..., 39, 39, 39],
         [73, 73, 73,  ..., 37, 37, 37]],

        [[73, 73, 73,  ..., 62, 62, 62],
         [75, 75, 75,  ..., 62, 62, 62],
         [74, 74, 74,  ..., 63, 63, 63],
         ...,
         [73, 73, 73,  ..., 39, 39, 39],
         [74, 74, 74,  ..., 38, 38, 38],
         [74, 74, 74,  ..., 38, 38, 38]],

        [[74, 74, 74,  ..., 62, 62, 62],
         [74, 74, 74,  ..., 63, 63, 63],
         [74, 74, 74,  ..., 62, 62, 62],
         ...,
         [74, 74, 74,  ..., 38, 38, 38],
         [73, 73, 73,  ..., 38, 38, 38],
         [72, 72, 72,  ..., 36, 36, 36]]], dtype=torch.uint8)
</code></pre>
<p>When I tried</p>
<pre><code>images_data = images_data.float()
mean += images_data.mean(2).sum(0)
</code></pre>
<p>I didn't get a tensor for 3 values for mean and 3 values for std like I expected but got a very large tensor (each torch.Size([600]))</p>
<p><a href=""https://i.stack.imgur.com/O9Z4y.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/O9Z4y.png"" alt=""enter image description here"" /></a></p>
",2414957.0,,2414957.0,,2020-10-14 17:38:32,2020-10-14 17:44:36,RuntimeError: Can only calculate the mean of floating types. Got Byte instead. for mean += images_data.mean(2).sum(0),<python><deep-learning><pytorch><dataloader>,1,0,0.0,,,CC BY-SA 4.0
66738473,1,,,2021-03-21 23:03:14,,6,2380,"<p>I'm trying to specify PyTorch with CUDA in install_requires. The command to install with pip is</p>
<p><code>pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio===0.8.0 -f https://download.pytorch.org/whl/torch_stable.html</code></p>
<p>How do I do that in the setup.py install_requires?</p>
",3424455.0,,681865.0,,2021-03-21 23:40:25,2022-12-02 18:48:20,Installing PyTorch with CUDA in setup.py,<python><pytorch><setup.py><install-requires>,1,1,0.0,,,CC BY-SA 4.0
64358372,1,,,2020-10-14 17:14:31,,6,1527,"<p>Is there any way I can run the python library pytorch in pyodide? I tried installing pytorch with micropip but it gives this error message:</p>
<blockquote>
<p>Couldn't find a pure Python 3 wheel for 'pytorch'</p>
</blockquote>
",14139987.0,,9067615.0,,2021-03-21 16:49:14,2021-03-21 16:49:14,Run pytorch in pyodide?,<python><pytorch><pyodide>,1,3,,,,CC BY-SA 4.0
63285197,1,63397197.0,,2020-08-06 13:59:03,,6,8693,"<p>I am trying to implement Bayesian CNN using Mc Dropout on Pytorch,
the main idea is that by applying dropout at test time and running over many forward passes , you get predictions from a variety of different models.
I’ve found an application of the Mc Dropout and I really did not get how they applied this method and how exactly they did choose the correct prediction from the list of predictions</p>
<p>here is the code</p>
<pre><code>
 def mcdropout_test(model):
    model.train()
    test_loss = 0
    correct = 0
    T = 100
    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output_list = []
        for i in xrange(T):
            output_list.append(torch.unsqueeze(model(data), 0))
        output_mean = torch.cat(output_list, 0).mean(0)
        test_loss += F.nll_loss(F.log_softmax(output_mean), target, size_average=False).data[0]  # sum up batch loss
        pred = output_mean.data.max(1, keepdim=True)[1]  # get the index of the max log-probability
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nMC Dropout Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


    train()
    mcdropout_test()
</code></pre>
<p>I have replaced</p>
<blockquote>
<p><code>data, target = Variable(data, volatile=True), Variable(target)</code></p>
</blockquote>
<p>by adding</p>
<blockquote>
<p><code>with torch.no_grad():</code>  at the beginning</p>
</blockquote>
<p>And this is how I have defined my CNN</p>
<pre><code>class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 192, 5, padding=2)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(192, 192, 5, padding=2)
        self.fc1 = nn.Linear(192 * 8 * 8, 1024)
        self.fc2 = nn.Linear(1024, 256)
        self.fc3 = nn.Linear(256, 10)
        self.dropout = nn.Dropout(p=0.3)
        
        nn.init.xavier_uniform_(self.conv1.weight)
        nn.init.constant_(self.conv1.bias, 0.0)
        nn.init.xavier_uniform_(self.conv2.weight)
        nn.init.constant_(self.conv2.bias, 0.0)
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.constant_(self.fc1.bias, 0.0)
        nn.init.xavier_uniform_(self.fc2.weight)
        nn.init.constant_(self.fc2.bias, 0.0)
        nn.init.xavier_uniform_(self.fc3.weight)
        nn.init.constant_(self.fc3.bias, 0.0)


    def forward(self, x):
        x = self.pool(F.relu(self.dropout(self.conv1(x))))  # recommended to add the relu
        x = self.pool(F.relu(self.dropout(self.conv2(x))))  # recommended to add the relu
        x = x.view(-1, 192 * 8 * 8)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(self.dropout(x)))
        x = self.fc3(self.dropout(x))  # no activation function needed for the last layer
        return x
</code></pre>
<p>Can anyone help me to get the right implementation of the Monte Carlo Dropout method on CNN?</p>
",13999613.0,,13999613.0,,2020-08-13 19:02:10,2023-06-07 17:56:27,Measuring uncertainty using MC Dropout on pytorch,<pytorch><bayesian><conv-neural-network><dropout><uncertainty>,1,0,0.0,,,CC BY-SA 4.0
64978232,1,,,2020-11-23 23:42:09,,6,1524,"<p>I have a neural network that's computing a vector quantity <code>u</code>. I'd like to compute first and second-order jacobians with respect to the input <code>x</code>, a single element.</p>
<p>Would anybody know how to do that in PyTorch? Below, the code snippet from my project:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn

class PINN(torch.nn.Module):
    
    def __init__(self, layers:list):
        super(PINN, self).__init__()
        self.linears = nn.ModuleList([])
        for i, dim in enumerate(layers[:-2]):
            self.linears.append(nn.Linear(dim, layers[i+1]))
            self.linears.append(nn.ReLU())
        self.linears.append(nn.Linear(layers[-2], layers[-1]))
        
    def forward(self, x):
        for layer in self.linears:
            x = layer(x)
        return x
</code></pre>
<p>I then instantiate my network:</p>
<pre><code>n_in = 1
units = 50
q = 500

pinn = PINN([n_in, units, units, units, q+1])
pinn
</code></pre>
<p>Which returns</p>
<pre><code>PINN(
  (linears): ModuleList(
    (0): Linear(in_features=1, out_features=50, bias=True)
    (1): ReLU()
    (2): Linear(in_features=50, out_features=50, bias=True)
    (3): ReLU()
    (4): Linear(in_features=50, out_features=50, bias=True)
    (5): ReLU()
    (6): Linear(in_features=50, out_features=501, bias=True)
  )
)
</code></pre>
<p>Then I compute both FO and SO jacobians</p>
<pre><code>x = torch.randn(1, requires_grad=False)

u_x = torch.autograd.functional.jacobian(pinn, x, create_graph=True)
print(&quot;First Order Jacobian du/dx of shape {}, and features\n{}&quot;.format(u_x.shape, u_x)

u_xx = torch.autograd.functional.jacobian(lambda _: u_x, x)
print(&quot;Second Order Jacobian du_x/dx of shape {}, and features\n{}&quot;.format(u_xx.shape, u_xx)
</code></pre>
<p>Returns</p>
<pre><code>First Order Jacobian du/dx of shape torch.Size([501, 1]), and features
tensor([[-0.0310],
        [ 0.0139],
        [-0.0081],
        [-0.0248],
        [-0.0033],
        [ 0.0013],
        [ 0.0040],
        [ 0.0273],
        ...
        [-0.0197]], grad_fn=&lt;ViewBackward&gt;)
</code></pre>
<pre><code>Second Order Jacobian du/dx of shape torch.Size([501, 1, 1]), and features
tensor([[[0.]],

        [[0.]],

        [[0.]],

        [[0.]],

        ...

        [[0.]]])
</code></pre>
<p>Should not <code>u_xx</code> be a <code>None</code> vector if it didn't depend on <code>x</code>?</p>
<p>Thanks in advance</p>
",9380430.0,,9067615.0,,2021-03-31 10:03:19,2022-07-04 01:20:42,PyTorch how to compute second order Jacobian?,<python><pytorch><gradient>,2,6,0.0,,,CC BY-SA 4.0
72776834,1,,,2022-06-27 18:30:23,,6,690,"<p>I have been trying to fine-tune a conversational model of HuggingFace: Blendebot. I have tried the conventional method given on the official hugging face website which asks us to do it using the trainer.train() method. I tried it using the .compile() method. I have tried fine-tuning using PyTorch as well as TensorFlow on my dataset. Both methods seem to fail and give us an error saying that there is no method called compile or train for the Blenderbot model. I have even looked everywhere online to check how Blenderbot could be fine-tuned on my custom data and nowhere does it mention properly that runs without throwing an error. I have gone through Youtube tutorials, blogs, and StackOverflow posts but none answer this question. Hoping someone would respond here and help me out. I am open to using other HuggingFace Conversational Models as well for fine-tuning.</p>
<p>Here is a link I am using to fine-tune the blenderbot model.</p>
<p>Fine-tuning methods: <a href=""https://huggingface.co/docs/transformers/training"" rel=""noreferrer"">https://huggingface.co/docs/transformers/training</a></p>
<p>Blenderbot: <a href=""https://huggingface.co/docs/transformers/model_doc/blenderbot"" rel=""noreferrer"">https://huggingface.co/docs/transformers/model_doc/blenderbot</a></p>
<pre><code>from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration
mname = &quot;facebook/blenderbot-400M-distill&quot;
model = BlenderbotForConditionalGeneration.from_pretrained(mname)
tokenizer = BlenderbotTokenizer.from_pretrained(mname)


#FOR TRAINING: 

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

#OR

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=tf.metrics.SparseCategoricalAccuracy(),
)

model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)
</code></pre>
<p>None of these work.</p>
",12729788.0,,9657861.0,,2022-07-19 17:48:16,2022-07-19 17:48:16,Blenderbot FineTuning,<python><tensorflow><nlp><pytorch><huggingface-transformers>,1,1,0.0,,,CC BY-SA 4.0
69220221,1,,,2021-09-17 08:32:49,,6,2746,"<pre><code>t1 = torch.tensor([1,2,3])
t2 = torch.tensor([4,5,6])
t3 = torch.tensor([7,8,9])

torch.stack((t1,t2,t3),dim=1)
</code></pre>
<p>When implementing the torch.stack(), I can't understand how stacking is done for different dim.
Here stacking is done for columns but I can't understand the details as to how it is done. It becomes more complicated dealing with 2-d or 3-D tensors.</p>
<pre><code>tensor([[1, 4, 7],
        [2, 5, 8],
        [3, 6, 9]])
</code></pre>
",15006725.0,,15327614.0,,2021-09-17 10:37:31,2021-09-17 10:47:18,Use of torch.stack(),<python><pytorch><tensor>,2,0,0.0,,,CC BY-SA 4.0
65321216,1,,,2020-12-16 10:12:49,,6,964,"<p>I have uploaded a transformer roberta model in S3 bucket. Am now trying to run inference against the model using Pytorch with SageMaker Python SDK. I specified the model directory <code>s3://snet101/sent.tar.gz</code> which is a compressed file of the model (pytorch_model.bin) and all its dependencies.  Here is the code</p>
<pre><code>model = PyTorchModel(model_data=model_artifact,
                   name=name_from_base('roberta-model'),
                   role=role, 
                   entry_point='torchserve-predictor2.py',
                   source_dir='source_dir',
                   framework_version='1.4.0',
                   py_version = 'py3',
                   predictor_cls=SentimentAnalysis)
predictor = model.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')
test_data = {&quot;text&quot;: &quot;How many cows are in the farm ?&quot;}
prediction = predictor.predict(test_data)
</code></pre>
<p>I get the following error on the predict method of the predictor object:</p>
<pre><code>ModelError                                Traceback (most recent call last)
&lt;ipython-input-6-bc621eb2e056&gt; in &lt;module&gt;
----&gt; 1 prediction = predictor.predict(test_data)

~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py in predict(self, data, initial_args, target_model, target_variant)
    123 
    124         request_args = self._create_request_args(data, initial_args, target_model, target_variant)
--&gt; 125         response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)
    126         return self._handle_response(response)
    127 

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _api_call(self, *args, **kwargs)
    355                     &quot;%s() only accepts keyword arguments.&quot; % py_operation_name)
    356             # The &quot;self&quot; in this scope is referring to the BaseClient.
--&gt; 357             return self._make_api_call(operation_name, kwargs)
    358 
    359         _api_call.__name__ = str(py_operation_name)

~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py in _make_api_call(self, operation_name, api_params)
    674             error_code = parsed_response.get(&quot;Error&quot;, {}).get(&quot;Code&quot;)
    675             error_class = self.exceptions.from_code(error_code)
--&gt; 676             raise error_class(parsed_response, operation_name)
    677         else:
    678             return parsed_response

ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from model with message &quot;Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.&quot;. See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/roberta-model-2020-12-16-09-42-37-479 in account 165258297056 for more information.
</code></pre>
<p>I checked the server log error</p>
<pre><code>java.lang.IllegalArgumentException: reasonPhrase contains one of the following prohibited characters: \r\n: Can't load config for '/.sagemaker/mms/models/model'. Make sure that:
'/.sagemaker/mms/models/model' is a correct model identifier listed on 'https://huggingface.co/models'
or '/.sagemaker/mms/models/model' is the correct path to a directory containing a config.json file
</code></pre>
<p>How can I fix this?</p>
",9128790.0,,,,,2021-09-30 08:55:03,Amazon Sagemaker ModelError when serving model,<amazon-s3><pytorch><amazon-sagemaker><huggingface-transformers>,1,0,0.0,,,CC BY-SA 4.0
67236480,1,67237834.0,,2021-04-23 20:29:46,,6,12428,"<p>I am learning LSTM with PyTorch from someone's code. Here he uses the <code>clip_grad_norm_</code> function in the training process of a two layer LSTM. I want to know why he uses the <code>clip_grad_norm_</code> function here, so I can understand the whole code properly (he used it in second last line).</p>
<pre><code>for x, y in get_batches(data, batch_size, seq_length):
    counter += 1
                            
    x = one_hot_encode(x, n_chars)
    inputs, targets = torch.from_numpy(x), torch.from_numpy(y)
            
    if(train_on_gpu):
        inputs, targets = inputs.cuda(), targets.cuda()

    h = tuple([each.data for each in h])                    
    net.zero_grad()
                            
    output, h = net(inputs, h)                
            
    loss = criterion(output, targets.view(batch_size*seq_length).long())
    loss.backward()

    nn.utils.clip_grad_norm_(net.parameters(), clip)
    opt.step() 
</code></pre>
<p>If you need more information about question then please let me know.</p>
",14125950.0,,5987698.0,,2021-04-23 23:36:22,2021-04-23 23:36:22,Why is the clip_grad_norm_ function used here?,<python><pytorch><lstm><recurrent-neural-network>,1,0,,,,CC BY-SA 4.0
67948757,1,68241940.0,,2021-06-12 12:15:37,,5,973,"<p>I'm trying to run TensorRT inference in C++. Sometimes the code crashes when trying to build a new engine or load the engine from the file. It happens occasionally (sometimes it runs without any problem). I follow the below steps to prepare network:</p>
<pre><code>initLibNvInferPlugins(&amp;gLogger.getTRTLogger(), &quot;&quot;);

if (mParams.loadEngine.size() &gt; 0)
{
    std::vector&lt;char&gt; trtModelStream;
    size_t size{0};
    std::ifstream file(mParams.loadEngine, std::ios::binary);

    if (file.good())
    {
        file.seekg(0, file.end);
        size = file.tellg();
        file.seekg(0, file.beg);
        trtModelStream.resize(size);
        file.read(trtModelStream.data(), size);
        file.close();
    }
    IRuntime* infer_Runtime = nvinfer1::createInferRuntime(gLogger);
    if (mParams.dlaCore &gt;= 0)
    {
        infer_Runtime-&gt;setDLACore(mParams.dlaCore);
    }


    mEngine = std::shared_ptr&lt;nvinfer1::ICudaEngine&gt;(
                infer_Runtime-&gt;deserializeCudaEngine(trtModelStream.data(), size, nullptr), samplesCommon::InferDeleter());

    gLogInfo &lt;&lt; &quot;TRT Engine loaded from: &quot; &lt;&lt; mParams.loadEngine &lt;&lt; endl;

    infer_Runtime-&gt;destroy();
    if (!mEngine)
    {
        return false;
    }
    else
    {
        return true;
    }
}

auto builder = SampleUniquePtr&lt;nvinfer1::IBuilder&gt;(nvinfer1::createInferBuilder(gLogger.getTRTLogger()));


const auto explicitBatch = 1U &lt;&lt; static_cast&lt;uint32_t&gt;(NetworkDefinitionCreationFlag::kEXPLICIT_BATCH);
auto network = SampleUniquePtr&lt;nvinfer1::INetworkDefinition&gt;(builder-&gt;createNetworkV2(explicitBatch));
auto config = SampleUniquePtr&lt;nvinfer1::IBuilderConfig&gt;(builder-&gt;createBuilderConfig());
auto parser = SampleUniquePtr&lt;nvonnxparser::IParser&gt;(nvonnxparser::createParser(*network, gLogger.getTRTLogger()));

mEngine = nullptr;

parser-&gt;parseFromFile(
        locateFile(mParams.onnxFileName, mParams.dataDirs).c_str(), static_cast&lt;int&gt;(gLogger.getReportableSeverity()));

// Calibrator life time needs to last until after the engine is built.
std::unique_ptr&lt;IInt8Calibrator&gt; calibrator;

config-&gt;setAvgTimingIterations(1);
config-&gt;setMinTimingIterations(1);
config-&gt;setMaxWorkspaceSize(4_GiB);
builder-&gt;setMaxBatchSize(mParams.batchSize);


mEngine = std::shared_ptr&lt;nvinfer1::ICudaEngine&gt;(
            builder-&gt;buildEngineWithConfig(*network, *config), samplesCommon::InferDeleter());

</code></pre>
<p>The error occurs here:</p>
<pre><code>[05/12/2021-16:46:42] [I] [TRT] Detected 1 inputs and 1 output network tensors.

16:46:42: The program has unexpectedly finished.
</code></pre>
<p>This line crashes when loading existing engine:</p>
<pre><code>mEngine = std::shared_ptr&lt;nvinfer1::ICudaEngine(
    infer_Runtime-&gt;deserializeCudaEngine(trtModelStream.data(), size, nullptr), samplesCommon::InferDeleter());
</code></pre>
<p>Or when building the engine:</p>
<pre><code>mEngine = std::shared_ptr&lt;nvinfer1::ICudaEngine&gt;(
            builder-&gt;buildEngineWithConfig(*network, *config), samplesCommon::InferDeleter()); 
</code></pre>
<p>More info:</p>
<blockquote>
<p>TensorRT 7.2.3 <br/>
Ubuntu 18.04 <br/>
cuDNN 8.1.1 <br/>
CUDA 11.1 update1 <br/>
ONNX 1.6.0 <br/>
Pytorch 1.5.0 <br/></p>
</blockquote>
",5257241.0,,,,,2021-07-04 07:02:52,Loading or building cuda engine crashes occassionaly after upgrading to TensorRT 7,<c++><pytorch><onnx><tensorrt>,1,1,,,,CC BY-SA 4.0
67838192,1,,,2021-06-04 13:33:40,,5,10275,"<p>Below is the code I am trying to run. <code>fasterrcnn_foodtracker.pth</code> is the already trained model I am trying to load with PyTorch.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torchvision
import cv2

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

path = '/home/amir/PycharmProjects/Food-Recognition/fasterrcnn_foodtracker.pth'
model.load_state_dict(torch.load(path, map_location=torch.device('cpu')), strict=False)
model.eval()

img = cv2.imread('twodishes.jpg')
prediction = model([img])
print(prediction)
</code></pre>
<p>A runtime error appears with a size mismatch.</p>
<pre><code>RuntimeError: Error(s) in loading state_dict for FasterRCNN:
    size mismatch for roi_heads.box_predictor.cls_score.weight: copying a param with shape torch.Size([100, 1024]) from checkpoint, the shape in current model is torch.Size([91, 1024]).
    size mismatch for roi_heads.box_predictor.cls_score.bias: copying a param with shape torch.Size([100]) from checkpoint, the shape in current model is torch.Size([91]).
    size mismatch for roi_heads.box_predictor.bbox_pred.weight: copying a param with shape torch.Size([400, 1024]) from checkpoint, the shape in current model is torch.Size([364, 1024]).
    size mismatch for roi_heads.box_predictor.bbox_pred.bias: copying a param with shape torch.Size([400]) from checkpoint, the shape in current model is torch.Size([364]).
</code></pre>
",9050888.0,,9050888.0,,2021-06-04 13:40:48,2023-05-25 13:05:47,Size Mismatch Runtime Error When Trying to Load a PyTorch Model,<python><pytorch>,4,0,,,,CC BY-SA 4.0
62728485,1,62749037.0,,2020-07-04 11:22:13,,5,16065,"<p>I have a problem where</p>
<pre class=""lang-py prettyprint-override""><code>import torch
print(torch.cuda_is_available())
</code></pre>
<p>will print False, and I can't use the GPU available. I've tried it on <code>conda</code> environment, where I've installed the PyTorch version corresponding to the NVIDIA driver I have. I've also tried it in docker container, where I've done the same. I've tried both of these options on a remote server, but they both failed. I know that I've installed the correct driver versions because I've checked the version with <code>nvcc --version</code> before installing PyTorch, and I've checked the GPU connection with <code>nvidia-smi</code> which displays the GPUs on the machines correctly.</p>
<p>Also, I've checked <a href=""https://stackoverflow.com/questions/39649102/how-do-i-select-which-gpu-to-run-a-job-on"">this</a> post and tried exporting <code>CUDA_VISIBLE_DEVICES</code>, but had no luck.</p>
<p>On the server I have NVIDIA V100 GPUs with CUDA version 10.0 (for conda environment) and version 10.2 on a docker container I've built. Any help or push in the right direction would be greatly appreciated. Thanks!</p>
",8420223.0,,,,,2020-07-06 03:58:19,PyTorch can't see GPU (torch.cuda.is_availble() returns False),<pytorch><gpu><nvidia>,1,7,0.0,,,CC BY-SA 4.0
66444927,1,,,2021-03-02 18:16:42,,5,1562,"<p>I keep getting this issue when running DDP in pytorch:</p>
<pre><code>Traceback (most recent call last):
  File &quot;ml4coq-proj/embeddings_zoo/tree_nns/main_brando.py&quot;, line 330, in &lt;module&gt;
    main_distributed()
  File &quot;ml4coq-proj/embeddings_zoo/tree_nns/main_brando.py&quot;, line 230, in main_distributed
    mp.spawn(fn=train, args=(opts,), nprocs=opts.world_size)
  File &quot;/home/miranda9/miniconda3/envs/automl-meta-learning/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 199, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File &quot;/home/miranda9/miniconda3/envs/automl-meta-learning/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 157, in start_processes
    while not context.join():
  File &quot;/home/miranda9/miniconda3/envs/automl-meta-learning/lib/python3.8/site-packages/torch/multiprocessing/spawn.py&quot;, line 105, in join
    raise Exception(
Exception: process 1 terminated with signal SIGSEGV
</code></pre>
<p>but this error is rather uninformative (dones't tell me what process or what it was trying to access for example) so I am unsure what I need to do do solve it.</p>
<p><a href=""https://www.suse.com/support/kb/doc/?id=000016786#:%7E:text=On%20a%20Unix%20operating%20system,memory%20address%20that%20does%20not"" rel=""noreferrer"">Some research</a> takes you that:</p>
<blockquote>
<p>SIGSEGV: On a Unix operating system such as Linux, a &quot;segmentation violation&quot; (also known as &quot;signal 11&quot;, &quot;SIGSEGV&quot;, &quot;segmentation fault&quot; or, abbreviated, &quot;sig11&quot; or &quot;segfault&quot;) is a signal sent by the kernel to a process when the system has detected that the process was attempting to access a memory address that does not belong to it. Typically, this results in the offending process being terminated.</p>
</blockquote>
<p>yes I do have multiprocessing code as the usual <code>mp.spawn(fn=train, args=(opts,), nprocs=opts.world_size)</code> requires.</p>
<p>First I read the docs on <a href=""https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies"" rel=""noreferrer"">sharing strategies</a> which talks about how tensors are shared in pytorch:</p>
<blockquote>
<p>Note that it applies only to CPU tensor - CUDA tensors will always use the CUDA API, as that’s the only way they can be shared.</p>
</blockquote>
<p>I was using the <a href=""https://pytorch.org/docs/stable/multiprocessing.html#file-system-file-system"" rel=""noreferrer"">file system sharing memory</a> since it seemed to give me less issue when I needed lots of processes but I went down to only 2 processes and 2 gpus and to the share strategy being <a href=""https://pytorch.org/docs/stable/multiprocessing.html#file-system-file-system"" rel=""noreferrer"">file descriptor</a>. I thought that perhaps if the processes had their own cached file descriptor then there wouldn't be issues.</p>
<p>I did check the cuda devices availabe:</p>
<pre><code> $ echo $CUDA_VISIBLE_DEVICES
1,3
</code></pre>
<p>all seems fine.</p>
<p>I am unsure what might be causing the issue. There are possible issues like:</p>
<ul>
<li>two processes are trying to checkpoint at the same time but I always only let <code>rank=0</code> do the checkpointing so that doesn't make sense.</li>
<li>two processes are writing to tensorboard but I also only allow <code>rank=0</code> to do the logging (or any of the printing).</li>
</ul>
<p>So I am unsure what could be causing the issue. It could be that I have my dataset concatenated all 1 single json file causing the issue, but that wasn't causing issues yesterday with multiple gpus...though, if that is the case it would be hard to fix since <a href=""https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel"" rel=""noreferrer"">DDP (distributed data parallel)</a> uses the <a href=""https://pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler"" rel=""noreferrer"">DistributedSampler</a> which doesn't place any restriction like that on my data-set or dataloaders...or at least as far as I know (afaik).</p>
<p>Last thing is that yesterday I was getting weird error too and somehow it occurred to me to check the gpu type. I was quetting an issue because I was using a k40 gpu. I made sure that was not the case. Yesterday I was using a <code>Quadro 6000 RTX</code>, today it seems these are the GPUs I got:</p>
<pre><code>$ nvidia-smi
Tue Mar  2 12:15:04 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    Off  | 00000000:02:00.0 Off |                  N/A |
| 22%   37C    P0    56W / 250W |      0MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  TITAN X (Pascal)    Off  | 00000000:03:00.0 Off |                  N/A |
| 24%   39C    P0    56W / 250W |      0MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  TITAN X (Pascal)    Off  | 00000000:82:00.0 Off |                  N/A |
| 53%   84C    P2   244W / 250W |  11935MiB / 12196MiB |     57%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  TITAN X (Pascal)    Off  | 00000000:83:00.0 Off |                  N/A |
| 25%   39C    P0    56W / 250W |      0MiB / 12196MiB |      3%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    2   N/A  N/A     31809      C   python                          11933MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>not sure if that is causing the issue but it's not always realistic to get the Quadro's so I want it to work for the Titan's too (and anything that isn't the k40s since the k40s seem to not be supported by pytorch anymore).</p>
<p>There are a few pytorch discussion forum posts and gitissues but none seems very helpful (to me at lest - not clear what they did to solve things despite end of discussion):</p>
<ul>
<li><a href=""https://discuss.pytorch.org/t/multiprocessing-using-torch-multiprocessing/11029/3"" rel=""noreferrer"">https://discuss.pytorch.org/t/multiprocessing-using-torch-multiprocessing/11029/3</a></li>
<li><a href=""https://discuss.pytorch.org/t/using-torch-tensor-over-multiprocessing-queue-process-fails/2847/12"" rel=""noreferrer"">https://discuss.pytorch.org/t/using-torch-tensor-over-multiprocessing-queue-process-fails/2847/12</a></li>
<li><a href=""https://github.com/pytorch/fairseq/issues/1720"" rel=""noreferrer"">https://github.com/pytorch/fairseq/issues/1720</a></li>
<li><a href=""https://github.com/pytorch/fairseq/issues/1308"" rel=""noreferrer"">https://github.com/pytorch/fairseq/issues/1308</a></li>
<li><a href=""https://github.com/facebookresearch/SlowFast/issues/118"" rel=""noreferrer"">https://github.com/facebookresearch/SlowFast/issues/118</a></li>
<li><a href=""https://discuss.pytorch.org/t/segmentation-fault/23489"" rel=""noreferrer"">https://discuss.pytorch.org/t/segmentation-fault/23489</a></li>
</ul>
<p>I also noticed this happens when rank 0 ends before rank 1 (that helped me reproduce it, otherwise a sigabort <code>SIGABRT</code> happens).</p>
<pre><code>    # clean up distributed code
    torch.distributed.barrier()
    if rank == 1:
        time.sleep(1)
    print(f'\n----&gt; about to cleanup worker with rank {rank}')
    # cleanup(rank)
    torch.distributed.destroy_process_group()
    print(f'clean up done successfully! {rank}'
</code></pre>
<hr />
<p>crossposted:</p>
<ul>
<li><a href=""https://discuss.pytorch.org/t/how-to-fix-a-sigsegv-in-pytorch-when-using-distributed-training-e-g-ddp/113518"" rel=""noreferrer"">https://discuss.pytorch.org/t/how-to-fix-a-sigsegv-in-pytorch-when-using-distributed-training-e-g-ddp/113518</a></li>
<li><a href=""https://www.reddit.com/r/pytorch/comments/lwbb72/how_to_fix_a_sigsegv_in_pytorch_when_using/"" rel=""noreferrer"">https://www.reddit.com/r/pytorch/comments/lwbb72/how_to_fix_a_sigsegv_in_pytorch_when_using/</a></li>
<li><a href=""https://www.quora.com/unanswered/How-do-I-fix-a-SIGSEGV-in-PyTorch-when-using-distributed-training-e-g-DDP"" rel=""noreferrer"">https://www.quora.com/unanswered/How-do-I-fix-a-SIGSEGV-in-PyTorch-when-using-distributed-training-e-g-DDP</a></li>
</ul>
",1601580.0,,1601580.0,,2021-03-02 20:42:14,2021-03-02 20:42:14,How to fix a SIGSEGV in pytorch when using distributed training (e.g. DDP)?,<python><neural-network><multiprocessing><pytorch><distributed-computing>,0,0,,,,CC BY-SA 4.0
65226693,1,65226751.0,,2020-12-10 00:02:52,,5,31406,"<p>I am trying to install a package VIBE from a git repo and inistally I was installing its dependencies. The code is located here: <a href=""https://github.com/mkocabas/VIBE"" rel=""noreferrer"">https://github.com/mkocabas/VIBE</a> how should I fix this?</p>
<p>Here's the error I got:</p>
<pre><code>(vibe-env) mona@mona:~/research/VIBE$ pip install -r requirements.txt
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Requirement already satisfied: torchvision==0.5.0 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 19)) (0.5.0)
Collecting git+https://github.com/mattloper/chumpy.git (from -r requirements.txt (line 24))
  Cloning https://github.com/mattloper/chumpy.git to /tmp/pip-req-build-vdh2h3jw
Collecting git+https://github.com/mkocabas/yolov3-pytorch.git (from -r requirements.txt (line 25))
  Cloning https://github.com/mkocabas/yolov3-pytorch.git to /tmp/pip-req-build-ay_gkil2
Collecting git+https://github.com/mkocabas/multi-person-tracker.git (from -r requirements.txt (line 26))
  Cloning https://github.com/mkocabas/multi-person-tracker.git to /tmp/pip-req-build-l9jgk1qb
Requirement already satisfied: six&gt;=1.11.0 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from chumpy==0.70-&gt;-r requirements.txt (line 24)) (1.15.0)
Collecting filterpy==1.4.5
  Using cached filterpy-1.4.5-py3-none-any.whl
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting gdown==3.6.4
  Downloading gdown-3.6.4.tar.gz (5.2 kB)
Requirement already satisfied: six&gt;=1.11.0 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from chumpy==0.70-&gt;-r requirements.txt (line 24)) (1.15.0)
Collecting h5py==2.10.0
  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)
Requirement already satisfied: six&gt;=1.11.0 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from chumpy==0.70-&gt;-r requirements.txt (line 24)) (1.15.0)
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting joblib==0.14.1
  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)
     |████████████████████████████████| 294 kB 5.6 MB/s 
Collecting llvmlite==0.32.1
  Downloading llvmlite-0.32.1-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)
     |████████████████████████████████| 20.2 MB 14.1 MB/s 
Collecting matplotlib==3.1.3
  Using cached matplotlib-3.1.3-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting numba==0.47.0
  Downloading numba-0.47.0-cp37-cp37m-manylinux1_x86_64.whl (3.7 MB)
     |████████████████████████████████| 3.7 MB 33.0 MB/s 
Requirement already satisfied: setuptools in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from numba==0.47.0-&gt;-r requirements.txt (line 6)) (51.0.0.post20201207)
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting opencv-python==4.1.2.30
  Downloading opencv_python-4.1.2.30-cp37-cp37m-manylinux1_x86_64.whl (28.3 MB)
     |████████████████████████████████| 28.3 MB 29.4 MB/s 
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting pillow==6.2.1
  Downloading Pillow-6.2.1-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)
     |████████████████████████████████| 2.1 MB 107.9 MB/s 
Collecting progress==1.5
  Downloading progress-1.5.tar.gz (5.8 kB)
Collecting pyrender==0.1.36
  Downloading pyrender-0.1.36-py3-none-any.whl (1.2 MB)
     |████████████████████████████████| 1.2 MB 23.0 MB/s 
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Requirement already satisfied: six&gt;=1.11.0 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from chumpy==0.70-&gt;-r requirements.txt (line 24)) (1.15.0)
Collecting PyYAML==5.3.1
  Using cached PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl
Collecting scikit-image==0.16.2
  Downloading scikit_image-0.16.2-cp37-cp37m-manylinux1_x86_64.whl (26.5 MB)
     |████████████████████████████████| 26.5 MB 25.7 MB/s 
Collecting scikit-video==1.1.11
  Using cached scikit_video-1.1.11-py2.py3-none-any.whl (2.3 MB)
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting scipy==1.4.1
  Using cached scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting smplx==0.1.13
  Downloading smplx-0.1.13-py3-none-any.whl (26 kB)
Requirement already satisfied: torch&gt;=1.0.1.post2 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from smplx==0.1.13-&gt;-r requirements.txt (line 7)) (1.4.0)
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Collecting tensorboard==2.1.0
  Downloading tensorboard-2.1.0-py3-none-any.whl (3.8 MB)
     |████████████████████████████████| 3.8 MB 29.3 MB/s 
Requirement already satisfied: setuptools in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from numba==0.47.0-&gt;-r requirements.txt (line 6)) (51.0.0.post20201207)
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Requirement already satisfied: six&gt;=1.11.0 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from chumpy==0.70-&gt;-r requirements.txt (line 24)) (1.15.0)
Requirement already satisfied: wheel&gt;=0.26 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from tensorboard==2.1.0-&gt;-r requirements.txt (line 18)) (0.36.1)
Collecting tensorflow==1.15.4
  Downloading tensorflow-1.15.4-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)
     |████████████████████████████████| 110.5 MB 22 kB/s 
Requirement already satisfied: numpy==1.17.5 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (1.17.5)
Requirement already satisfied: six&gt;=1.11.0 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from chumpy==0.70-&gt;-r requirements.txt (line 24)) (1.15.0)
Requirement already satisfied: wheel&gt;=0.26 in /home/mona/anaconda3/envs/vibe-env/lib/python3.7/site-packages (from tensorboard==2.1.0-&gt;-r requirements.txt (line 18)) (0.36.1)
INFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of smplx to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of scikit-video to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of pyrender to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of progress to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of numba to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of multi-person-tracker to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of llvmlite to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of joblib to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of h5py to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of gdown to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of filterpy to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of chumpy to determine which version is compatible with other requirements. This could take a while.
ERROR: Cannot install -r requirements.txt (line 17) and tensorboard==2.1.0 because these package versions have conflicting dependencies.

The conflict is caused by:
    The user requested tensorboard==2.1.0
    tensorflow 1.15.4 depends on tensorboard&lt;1.16.0 and &gt;=1.15.0

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies


</code></pre>
<pre><code>
(vibe-env) mona@mona:~/research/VIBE$ python
Python 3.7.9 (default, Aug 31 2020, 12:42:55) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import torch
&gt;&gt;&gt; torch.__version__
'1.4.0'
</code></pre>
<p>Here are all the commands I ran before this:</p>
<pre><code>(base) mona@mona:~/research/VIBE$ export CONDA_ENV_NAME=vibe-env


(base) mona@mona:~/research/VIBE$ conda create -n $CONDA_ENV_NAME python=3.7


(base) mona@mona:~/research/VIBE$ eval &quot;$(conda shell.bash hook)&quot;


(base) mona@mona:~/research/VIBE$ conda activate $CONDA_ENV_NAME

(vibe-env) mona@mona:~/research/VIBE$ pip install numpy==1.17.5 torch==1.4.0 torchvision==0.5.0

(vibe-env) mona@mona:~/research/VIBE$ pip install git+https://github.com/giacaglia/pytube.git --upgrade
</code></pre>
",2414957.0,,,,,2022-12-05 08:17:15,The conflict is caused by: The user requested tensorboard==2.1.0 tensorflow 1.15.4 depends on tensorboard<1.16.0 and >=1.15.0,<python><tensorflow><pip><pytorch><tensorboard>,3,0,0.0,,,CC BY-SA 4.0
68814074,1,68814420.0,,2021-08-17 08:28:46,,5,647,"<p>I fine tuned the pretrained model <a href=""https://huggingface.co/dbmdz/bert-base-turkish-cased"" rel=""noreferrer"">here</a> by freezing all layers except the classifier layers. And I saved weight file with using pytorch as .bin format.</p>
<p>Now instead of loading the 400mb pre-trained model, is there a way to load the parameters of the just Classifier layer I retrained it? By the way, I know that I have to load the original pretrained model, I just don't want to load the entire fine tuned model. due to memory concerns.</p>
<p>I can access the last layer's parameters from state_dict as below, but how can I save them in a separate file to use them later for less memory usage?</p>
<pre><code>model = PosTaggingModel(num_pos_tag=num_pos_tag)
state_dict = torch.load(&quot;model.bin&quot;)
print(&quot;state dictionary:&quot;,state_dict)
with torch.no_grad():
    model.out_pos_tag.weight.copy_(state_dict['out_pos_tag.weight'])
    model.out_pos_tag.bias.copy_(state_dict['out_pos_tag.bias'])
</code></pre>
<p>Here is the model class:</p>
<pre><code>class PosTaggingModel(nn.Module):
    def __init__(self, num_pos_tag):
        super(PosTaggingModel, self).__init__()
        self.num_pos_tag = num_pos_tag
        self.model = AutoModel.from_pretrained(&quot;dbmdz/bert-base-turkish-cased&quot;)
        for name, param in self.model.named_parameters():
            if 'classifier' not in name: # classifier layer
                param.requires_grad = False
        self.bert_drop = nn.Dropout(0.3)
        self.out_pos_tag = nn.Linear(768, self.num_pos_tag)
        
    def forward(self, ids, mask, token_type_ids, target_pos_tag):
        o1, _ = self.model(ids, attention_mask = mask, token_type_ids = token_type_ids)
        
        bo_pos_tag = self.bert_drop(o1)
        pos_tag = self.out_pos_tag(bo_pos_tag)

        loss = loss_fn(pos_tag, target_pos_tag, mask, self.num_pos_tag)
        return pos_tag, loss
</code></pre>
<p>I don't know if this is possible but I'm just looking for a way to save and reuse the last layer's parameters, without the need for parameters of frozen layers. I couldn't find it in the <a href=""https://pytorch.org/tutorials/beginner/saving_loading_models.html?highlight=save"" rel=""noreferrer"">documentation</a>.
Thanks in advance to those who will help.</p>
",15522580.0,,,,,2021-08-17 08:53:25,How to save parameters just related to classifier layer of pretrained bert model due to the memory concerns?,<python><nlp><pytorch><bert-language-model><transfer-learning>,1,0,0.0,,,CC BY-SA 4.0
72061934,1,,,2022-04-29 17:50:27,,5,10369,"<p>I have this code:</p>
<pre><code>    actual_loes_score_g = actual_loes_score_t.to(self.device, non_blocking=True)

    predicted_loes_score_g = self.model(input_g)

    loss_func = nn.L1Loss()
    loss_g = loss_func(
        predicted_loes_score_g,
        actual_loes_score_g,
    )
</code></pre>
<p>where <code>predicted_loes_score_g</code> is <code>tensor([[-24.9374]], grad_fn=&lt;AddmmBackward0&gt;)</code> and <code>actual_loes_score_g</code> is <code>tensor([20.], dtype=torch.float64)</code>.  (I am using a batch size of 1 for debugging purposes.)</p>
<p>I am getting this warning:</p>
<pre><code>torch/nn/modules/loss.py:96: UserWarning: Using a target size (torch.Size([1])) that is 
different to the input size (torch.Size([1, 1])). This will likely lead to incorrect 
results due to broadcasting. Please ensure they have the same size.
</code></pre>
<p>How do I correctly ensure they have the same size?</p>
<p>I thought this might be the answer:</p>
<pre><code>    predicted_loes_score = predicted_loes_score_g.detach()[0]
    loss_g = loss_func(
        predicted_loes_score,
        actual_loes_score_g,
    )
</code></pre>
<p>but then I get this error later:</p>
<pre><code>torch/autograd/__init__.py&quot;, line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
",7648.0,,7648.0,,2022-04-29 18:15:04,2022-04-29 19:41:55,"UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1]))",<deep-learning><pytorch>,1,0,,,,CC BY-SA 4.0
73656975,1,74160390.0,,2022-09-09 03:13:39,,5,214,"<p>I am on a workstation with 4 A6000 GPUs. Moving a Torch tensor from one GPU to another GPU corrupts the data, silently!!!</p>
<p>See the simple example below.</p>
<pre><code>x
&gt;tensor([1], device='cuda:0')

x.to(1)
&gt;tensor([1], device='cuda:1')

x.to(2)
&gt;tensor([0], device='cuda:2')

x.to(3)
&gt;tensor([0], device='cuda:3')
</code></pre>
<p>Any ideas what is the cause of this issue?</p>
<p>Other info that might be handy:</p>
<p>(there was two nvlinks which I manually removed trying to solve the problem)</p>
<pre><code>        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity
GPU0     X      SYS     SYS     SYS     0-63            N/A
GPU1    SYS      X      SYS     SYS     0-63            N/A
GPU2    SYS     SYS      X      SYS     0-63            N/A
GPU3    SYS     SYS     SYS      X      0-63            N/A
</code></pre>
<pre><code>nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Tue_Mar__8_18:18:20_PST_2022
Cuda compilation tools, release 11.6, V11.6.124
Build cuda_11.6.r11.6/compiler.31057947_0
</code></pre>
<pre><code>NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6
</code></pre>
<p>Edit: adding some screenshots</p>
<p>It seems to be stateful. Changes which GPUs work fine together after starting a new python runtime.</p>
<p><a href=""https://i.stack.imgur.com/so85y.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/so85y.jpg"" alt=""It seems to be stateful, changes which GPUs work fine together after starting a new python runtime."" /></a></p>
",6647166.0,,6647166.0,,2022-09-09 22:20:42,2022-10-22 00:53:55,Pytorch silent data corruption,<python><pytorch><gpu>,1,6,,,,CC BY-SA 4.0
65428216,1,,,2020-12-23 17:12:13,,5,3919,"<p>I'm using AutomaticMixedPrecision feature of PyTorch to train a network with smaller footprint and precision.<br />
At a certain point some embeddings from the network have NaNs in their tensors, so I'd like to replace those with 0s in order to perform online hard negative samples mining.</p>
<p>However, after replacing the NaNs in the tensor like this:</p>
<pre><code>tensor[torch.isnan(tensor)] = 0
</code></pre>
<p>I get the following error while doing the next scaler ste (scaler.step(optimizer):</p>
<pre><code>    assert len(optimizer_state[&quot;found_inf_per_device&quot;]) &gt; 0, &quot;No inf checks were recorded for this optimizer.&quot;
AssertionError: No inf checks were recorded for this optimizer.
</code></pre>
<p>What's the correct way to zero out NaNs while getting rid of this error?</p>
",1334761.0,,,,,2021-08-18 09:35:54,AssertionError: No inf checks were recorded for this optimizer in Pytorch's AutomaticMixedPrecision,<python><deep-learning><pytorch>,1,1,,,,CC BY-SA 4.0
72069125,1,,,2022-04-30 13:58:57,,5,883,"<p>I am working on the ArXiV paper <a href=""https://arxiv.org/pdf/1609.07009.pdf"" rel=""noreferrer"">Is the deconvolution layer the same as a convolutional layer?</a>. The topic is to upsample, by an upscale factor <code>r</code>, a tensor <code>x</code> from shape <code>(N, C, H, W)</code> to shape <code>(N, C, H*r, W*r)</code>.</p>
<p>In that paper they introduce an equivalence between the two following methods (the point being the second one should be more computationally efficient than the first one):</p>
<ul>
<li><strong>SubPixel method</strong>: create sub pixel image, then convolution in sub pixel space with a kernel of shape <code>(C, C, Hk * r, Wk * r)</code></li>
<li><strong>PixelShuffle method</strong>: convolution with kernel <code>(C * r * r, C, Hk, Wk)</code>, then periodic shuffling</li>
</ul>
<p>Those methods are pictured by the Figure 6 and 7 of the paper, that I reproduced and animated below to highlight my interrogations about the equivalence statement. After the figures comes a <code>pytorch</code> implementation to support those interrogations.</p>
<h3>Questions</h3>
<p>Am I missing something or the following is true?:</p>
<ol>
<li>SubPixel output size:
<ul>
<li>SubPixel method gives an output image of size <code>(6, 6)</code> and not <code>(8, 8)</code> as represented in the figure.</li>
<li>SubPixel method can give an output image of size <code>(8, 8)</code> by adding more padding in the subpixel space, but then last row and last column of the output image are full of zeros.</li>
</ul>
</li>
<li>Except for &quot;purple&quot; pixels, there is a spatial shift of values between both output images:
<ul>
<li>Purple pixels are aligned</li>
<li>Blue pixels are shifted on dim W</li>
<li>Green pixels are shifted on dim H</li>
<li>Red pixels are shifted on dim H and dim W</li>
<li>e.g. :
<ul>
<li>PixShuff: Output blue pixel <code>(0, 1)</code> is computed from input pixels <code>[(0, 0), (0, 1), (1, 0), (1, 1)]</code></li>
<li>SubPix: Output blue pixel <code>(0, 1)</code> is computed from input pixels <code>[(0, 1), (0, 2), (1, 1), (1, 2)]</code>. In PixShuff, those input pixels are used to compute the output blue pixel <code>(0, 3)</code>.</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3>Figures</h3>
<h4>SubPixel method</h4>
<p><a href=""https://i.stack.imgur.com/ghvOm.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/ghvOm.gif"" alt=""SubPixel  (Figure 6)"" /></a></p>
<h4>PixelShuffle method</h4>
<p><a href=""https://i.stack.imgur.com/cOqDN.gif"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cOqDN.gif"" alt=""PixelShuffle (Figure 7)"" /></a></p>
<h3>Implementation</h3>
<p>Notes:</p>
<ul>
<li>I added some assertions to make sure I understand the dimensions correctly.</li>
<li>There is still some hard-coded values for padding etc.</li>
</ul>
<p>The main script:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn.functional as F

torch.set_printoptions(precision=2)
torch.manual_seed(34)


def main():
    N, Cin, H, W = 1, 1, 4, 4
    Cout = Cin
    ratio = 2
    Hk, Wk = 2, 2

    x = torch.rand(N, Cin, H, W)
    x_padded = torch.zeros(N, Cin, H + 1, W + 1)
    x_padded[..., :-1, :-1] = x

    kernel_stacked = torch.rand(Cout * ratio * ratio, Cin, Hk, Wk)
    kernel_shuffled = F.pixel_shuffle(kernel_stacked.movedim(0, 1), ratio)

    x_up_pixshuff = upsample_conv_pixshuff(x_padded, ratio, kernel_stacked)
    x_up_subpix = upsample_subpix_conv(x, ratio, kernel_shuffled)
    x_up_subpix_padded = upsample_subpix_conv(x_padded, ratio, kernel_shuffled)

    purple_subpix = x_up_subpix_padded[..., ::2, ::2]
    purple_pixshuff = x_up_pixshuff[..., ::2, ::2]
    blue_subpix = x_up_subpix_padded[..., ::2, 1::2]
    blue_pixshuff = x_up_pixshuff[..., ::2, 1::2]
    green_subpix = x_up_subpix_padded[..., 1::2, ::2]
    green_pixshuff = x_up_pixshuff[..., 1::2, ::2]
    red_subpix = x_up_subpix_padded[..., 1::2, 1::2]
    red_pixshuff = x_up_pixshuff[..., 1::2, 1::2]

    # ... Long list of print statements given below


def upsample_conv_pixshuff(x, upscale_factor, kernel):
    N, Cin, H, W = x.shape
    Cout_r_r, _, H, W = kernel.shape
    assert kernel.shape[1] == Cin
    assert Cout_r_r == Cin * upscale_factor * upscale_factor

    x_up_stacked = F.conv2d(x, kernel)
    return F.pixel_shuffle(x_up_stacked, upscale_factor)


def upsample_subpix_conv(x, upscale_factor, kernel):
    N, Cin, Hin, Win = x.shape
    Cout, _, Hk, Wk = kernel.shape
    assert kernel.shape[1] == Cin
    assert Cout == Cin
    Hout, Wout = Hin * upscale_factor, Win * upscale_factor

    x_interleaved = torch.zeros(N, Cout, Hout + 1, Wout + 1)
    x_interleaved[..., :-1:upscale_factor, :-1:upscale_factor] = x
    return F.conv2d(x_interleaved, kernel)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>The output given by the long list of print statements:</p>
<pre><code>-&gt; SubPixel method gives an output image of size `(6, 6)` and not (8, 8)

x_up_pixshuff.shape = torch.Size([1, 1, 8, 8])
x_up_subpix.shape   = torch.Size([1, 1, 6, 6])

-&gt; SubPixel method can give an output image of size `(8, 8)`
by adding more padding in the subpixel space,
but then ouptput last row/col image are full of zeros

x_up_subpix_padded.shape = torch.Size([1, 1, 8, 8])
torch.all(x_up_subpix_padded[..., :, -1] == 0) = tensor(True)
torch.all(x_up_subpix_padded[..., -1, 0] == 0) = tensor(True)

-&gt; There is a spatial shift between output images
    - Purple pixels are aligned
torch.all(purple_subpix == purple_pixshuff) = tensor(True)
    - Blue pixels are shifted on dim W
torch.all(blue_subpix[..., :-1] == blue_pixshuff[..., 1:]) = tensor(True)
    - Green pixels are shifted on dim H
torch.all(green_subpix[..., :-1, :] == green_pixshuff[..., 1:, :]) = tensor(True)
    - Red pixels are shifted on dim H and dim W
torch.all(red_subpix[..., :-1, :-1] == red_pixshuff[..., 1:, 1:]) = tensor(True)
</code></pre>
<p>The complete matrices:</p>
<pre><code>x_up_pixshuff
tensor([[[[0.73, 0.43, 0.26, 0.33, 0.74, 0.64, 0.36, 0.17],
          [1.17, 1.29, 0.17, 0.82, 0.61, 1.24, 0.79, 0.69],
          [0.88, 0.62, 0.41, 0.21, 0.56, 0.50, 0.56, 0.28],
          [1.15, 1.35, 1.07, 0.87, 0.89, 1.36, 1.12, 1.03],
          [0.92, 0.90, 1.21, 0.93, 0.94, 0.78, 0.64, 0.37],
          [0.84, 1.94, 1.67, 2.35, 1.32, 2.08, 0.96, 1.03],
          [0.36, 0.44, 0.78, 0.71, 0.63, 0.55, 0.34, 0.23],
          [0.23, 1.09, 0.61, 1.57, 0.51, 1.20, 0.30, 0.45]]]])

x_up_subpix
tensor([[[[0.73, 0.33, 0.26, 0.64, 0.74, 0.17],
          [1.15, 0.87, 1.07, 1.36, 0.89, 1.03],
          [0.88, 0.21, 0.41, 0.50, 0.56, 0.28],
          [0.84, 2.35, 1.67, 2.08, 1.32, 1.03],
          [0.92, 0.93, 1.21, 0.78, 0.94, 0.37],
          [0.23, 1.57, 0.61, 1.20, 0.51, 0.45]]]])

x_up_subpix_padded
tensor([[[[0.73, 0.33, 0.26, 0.64, 0.74, 0.17, 0.36, 0.00],
          [1.15, 0.87, 1.07, 1.36, 0.89, 1.03, 1.12, 0.00],
          [0.88, 0.21, 0.41, 0.50, 0.56, 0.28, 0.56, 0.00],
          [0.84, 2.35, 1.67, 2.08, 1.32, 1.03, 0.96, 0.00],
          [0.92, 0.93, 1.21, 0.78, 0.94, 0.37, 0.64, 0.00],
          [0.23, 1.57, 0.61, 1.20, 0.51, 0.45, 0.30, 0.00],
          [0.36, 0.71, 0.78, 0.55, 0.63, 0.23, 0.34, 0.00],
          [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]]]])
</code></pre>
<p>The list of print statements for reference:</p>
<pre class=""lang-py prettyprint-override""><code>print(&quot;-&gt; SubPixel method gives an output image of size `(6, 6)` and not (8, 8)\n&quot;)
    print(f&quot;{x_up_pixshuff.shape = }&quot;)
    print(f&quot;{x_up_subpix.shape   = }&quot;)

    print(
        &quot;\n-&gt; SubPixel method can give an output image of size `(8, 8)`&quot;
        &quot;\nby adding more padding in the subpixel space,&quot;
        &quot;\nbut then ouptput last row/col image are full of zeros\n&quot;
    )
    print(f&quot;{x_up_subpix_padded.shape = }&quot;)
    print(f&quot;{torch.all(x_up_subpix_padded[..., :, -1] == 0) = }&quot;)
    print(f&quot;{torch.all(x_up_subpix_padded[..., -1, 0] == 0) = }&quot;)

    print(&quot;\n-&gt; There is a spatial shift between output images&quot;)
    print(&quot;    - Purple pixels are aligned&quot;)
    print(f&quot;{torch.all(purple_subpix == purple_pixshuff) = }&quot;)
    print(&quot;    - Blue pixels are shifted on dim W&quot;)
    print(f&quot;{torch.all(blue_subpix[..., :-1] == blue_pixshuff[..., 1:]) = }&quot;)
    print(&quot;    - Green pixels are shifted on dim H&quot;)
    print(f&quot;{torch.all(green_subpix[..., :-1, :] == green_pixshuff[..., 1:, :]) = }&quot;)
    print(&quot;    - Red pixels are shifted on dim H and dim W&quot;)
    print(f&quot;{torch.all(red_subpix[..., :-1, :-1] == red_pixshuff[..., 1:, 1:]) = }&quot;)

    print(&quot;\n-&gt; For reference, the complete matrices:&quot;)
    print(&quot;\nx_up_pixshuff&quot;)
    print(x_up_pixshuff)
    print(&quot;\nx_up_subpix&quot;)
    print(x_up_subpix)
    print(&quot;\nx_up_subpix_padded&quot;)
    print(x_up_subpix_padded)
</code></pre>

",13636407.0,,,,,2022-04-30 13:58:57,Is (Convolution + PixelShuffle) the same as SubPixel convolution?,<pytorch><conv-neural-network>,0,0,0.0,,,CC BY-SA 4.0
74844094,1,74881827.0,,2022-12-18 20:12:43,,5,256,"<p>In Professor Boyd <a href=""https://see.stanford.edu/materials/lsocoee364b/hw4sol.pdf"" rel=""nofollow noreferrer"">homework solution</a> for projection onto the unit simplex, he winds up with the following equation:</p>
<pre><code>g_of_nu = (1/2)*torch.norm(-relu(-(x-nu)))**2 + nu*(torch.sum(x) -1) - x.size()[0]*nu**2 
</code></pre>
<p><a href=""https://i.stack.imgur.com/BAetU.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/BAetU.png"" alt=""enter image description here"" /></a></p>
<p>If one calculates <code>nu*</code>, then the projection to unit simplex would be <code>y*=relu(x-nu*1)</code>.</p>
<p>What he suggests is to find the maximizer of <code>g_of_nu</code>. Since <code>g_of_nu</code> is strictly concave, I multiply it by a negative sign (<code>f_of_nu</code>) and find its global minimizer using gradient descent.</p>
<hr />
<p><strong>Question</strong></p>
<p>My final vector <code>y*</code>, does not add up to one, what am I doing wrong?</p>
<hr />
<p><strong>Code for replication</strong></p>
<pre><code>torch.manual_seed(1)
x = torch.randn(10)#.view(-1, 1)
x_list = x.tolist()
print(list(map(lambda x: round(x, 4), x_list)))
nu_0 = torch.tensor(0., requires_grad = True)
nu = nu_0
optimizer = torch.optim.SGD([nu], lr=1e-1)

nu_old = torch.tensor(float('inf'))
steps = 100
eps = 1e-6
i = 1
while torch.norm(nu_old-nu) &gt; eps:
  nu_old = nu.clone()
  optimizer.zero_grad()
  f_of_nu = -( (1/2)*torch.norm(-relu(-(x-nu)))**2 + nu*(torch.sum(x) -1) - x.size()[0]*nu**2 )
  f_of_nu.backward()
  optimizer.step()
  print(f'At step {i+1:2} the function value is {f_of_nu.item(): 1.4f} and nu={nu: 0.4f}' )
  i += 1
y_star = relu(x-nu).cpu().detach()
print(list(map(lambda x: round(x, 4), y_star.tolist())))
print(y_star.sum())
</code></pre>
<pre><code>[0.6614, 0.2669, 0.0617, 0.6213, -0.4519, -0.1661, -1.5228, 0.3817, -1.0276, -0.5631]
At step  1 the function value is -1.9618 and nu= 0.0993
.
.
.
At step 14 the function value is -1.9947 and nu= 0.0665
[0.5948, 0.2004, 0.0, 0.5548, 0.0, 0.0, 0.0, 0.3152, 0.0, 0.0]
tensor(1.6652)
</code></pre>
<p><strong>The function</strong></p>
<pre><code>torch.manual_seed(1)
x = torch.randn(10)
nu = torch.linspace(-1, 1, steps=10000)

f = lambda x, nu: -( (1/2)*torch.norm(-relu(-(x-nu)))**2 + nu*(torch.sum(x) -1) - x.size()[0]*nu**2 )

f_value_list = np.asarray( [f(x, i) for i in nu.tolist()] )

i_min = np.argmin(f_value_list)
print(nu[i_min])

fig, ax = plt.subplots()

ax.plot(nu.cpu().detach().numpy(), f_value_list);
</code></pre>
<p>Here is the minimizer from the graph which is consistent with the gradient descent.</p>
<pre><code>tensor(0.0665)
</code></pre>
<p><a href=""https://i.stack.imgur.com/mCpCV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mCpCV.png"" alt=""enter image description here"" /></a></p>
",11653374.0,,11653374.0,,2022-12-21 18:59:30,2022-12-21 22:13:59,Projection onto unit simplex using gradient decent in Pytorch,<python><pytorch><gradient-descent>,1,0,,,,CC BY-SA 4.0
70483124,1,,,2021-12-25 22:26:46,,5,3150,"<p>I am currently working with <code>torch.nn.CrossEntropyLoss</code>. As far as I know, it is common to compute the loss batch-wise. However, is there a possibility to compute the loss over multiple batches?</p>
<p>More concretely, assume we are given the data</p>
<pre><code>import torch

features = torch.randn(no_of_batches, batch_size, feature_dim)
targets = torch.randint(low=0, high=10, size=(no_of_batches, batch_size))

loss_function = torch.nn.CrossEntropyLoss()
</code></pre>
<p>Is there a way to compute in one line</p>
<pre><code>loss = loss_function(features, targets) # raises RuntimeError: Expected target size [no_of_batches, feature_dim], got [no_of_batches, batch_size]
</code></pre>
<p>?</p>
<p>Thank you in advance!</p>
",14147996.0,,,,,2021-12-25 23:12:25,torch.nn.CrossEntropyLoss over Multiple Batches,<pytorch><loss-function><cross-entropy><pytorch-dataloader>,1,0,,,,CC BY-SA 4.0
73270890,1,73272183.0,,2022-08-07 20:39:01,,5,2172,"<p>I have a torch tensor which I need to convert to a byte object so that I can pass it to starlette's <code>StreamingResponse</code> which will return a reconstructed image from the byte object. I am trying to convert the tensor and return it like so:</p>
<pre class=""lang-py prettyprint-override""><code>def some_unimportant_function(params):
    return_image = io.BytesIO()
    torch.save(some_img, return_image)
    return_image.seek(0)
    return_img = return_image.read()
    
    return StreamingResponse(content=return_img, media_type=&quot;image/jpeg&quot;)

</code></pre>
<p>The below works fine on regular byte objects and my API returns the reconstructed image:</p>
<pre class=""lang-py prettyprint-override""><code>def some_unimportant_function(params):
    image = Image.open(io.BytesIO(some_image))

    return_image = io.BytesIO()
    image.save(return_image, &quot;JPEG&quot;)
    return_image.seek(0)
    return StreamingResponse(content=return_image, media_type=&quot;image/jpeg&quot;)
</code></pre>
<p>Using <code>PIL</code> library for this</p>
<p>what am I doing wrong here?</p>
",19302855.0,,,,,2022-08-08 01:39:03,How do I convert a torch tensor to an image to be returned by FastAPI?,<python><pytorch><fastapi><starlette>,1,6,0.0,,,CC BY-SA 4.0
68396513,1,68430718.0,,2021-07-15 15:18:49,,5,1630,"<p>While following the Jupyter notebooks for the course
I hit upon an error when these lines are run.
I know that the <code>cnn_learner</code> line has got no errors whatsoever, The problem lies in the <code>lr_find()</code> part
It seems that <code>learn.lr_find()</code> does not want to return two values! Although its documentation says that it returns a tuple. That is my problem.</p>
<p>These are the lines of code:</p>
<pre><code>learn = cnn_learner(dls, resnet34, metrics=error_rate)
lr_min,lr_steep = learn.lr_find()
</code></pre>
<p>The error says:</p>
<pre><code>not enough values to unpack (expected 2, got 1)
</code></pre>
<p>for the second line.<br />
Also, I get this graph with one 'marker' which I suppose is either one of the values of <code>lr_min</code> or <code>lr_steep</code>
<a href=""https://i.stack.imgur.com/DZrbU.png"" rel=""noreferrer"">This is the graph</a></p>
<p>When I run <code>learn.lr_find()</code> only, i.e. do not capture the output in <code>lr_min, lr_steep</code>;  it runs well but then I do not get the min and steep learning rates (which is really important for me)</p>
<p>I read through what <code>lr_find</code> does and it is clear that it returns a tuple. Its docstring says</p>
<blockquote>
<p>Launch a mock training to find a good learning rate and return suggestions based on <code>suggest_funcs</code> as a named <strong>tuple</strong></p>
</blockquote>
<p>I had duplicated the original notebook, and when I hit this error, I ran the original notebook, with the same results. I update the notebooks as well, but no change!
Wherever I have searched for this online, any sort of error hasn't popped up. The only relevant thing I found is that <code>lr_find()</code> returns different results of the learning rates after every run, which is perfectly fine.</p>
",13832810.0,,,,,2021-07-18 15:46:58,Problem in lr_find() in Pytorch fastai course,<python><machine-learning><deep-learning><pytorch><fast-ai>,1,0,,,,CC BY-SA 4.0
67881255,1,,,2021-06-08 04:03:39,,5,635,"<p>I'm trying to run some PyTorch models on my Jetson Nano (4GB RAM), but I've learned that PyTorch uses about 2GB of RAM just to initialize anything CUDA related.</p>
<p>I've done some testing (with the help of <a href=""https://github.com/pytorch/pytorch/issues/12873"" rel=""nofollow noreferrer"">this GitHub issue</a>), and got the following script running:</p>
<pre><code>import torch
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('size', type=int)
parser.add_argument('--cpu', action='store_true')
args = parser.parse_args()

@profile
def f():
    torch.set_grad_enabled(False)
    torch.cuda._lazy_init()
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    if args.cpu:
        device = 'cpu'
    model = torch.nn.Conv2d(1, 1, 1).to(device)
    x = torch.rand(1, 1, args.size, args.size).to(device)
    y = model(x)

if __name__ == '__main__':
    f()
</code></pre>
<p>Which can be run with <code>python3 -m memory_profiler torchmemscript.py 100</code>. Here is the output from that:</p>
<pre><code>Line #    Mem usage    Increment  Occurences   Line Contents
============================================================
     9  150.906 MiB  150.906 MiB           1   @profile
    10                                         def f():
    11  150.906 MiB    0.000 MiB           1       torch.set_grad_enabled(False)
    12  155.336 MiB    4.430 MiB           1       torch.cuda._lazy_init()
    13  155.336 MiB    0.000 MiB           1       device = 'cuda' if torch.cuda.is_available() else 'cpu'
    14  155.336 MiB    0.000 MiB           1       if args.cpu:
    15                                                 device = 'cpu'
    16 1889.699 MiB 1734.363 MiB           1       model = torch.nn.Conv2d(1, 1, 1).to(device)
    17 1890.414 MiB    0.715 MiB           1       x = torch.rand(1, 1, args.size, args.size).to(device)
    18 2634.496 MiB  744.082 MiB           1       y = model(x)
</code></pre>
<p>So clearly the model is loaded and uses about ~1.7GB of RAM on my Jetson Nano. Running the same script with the <code>--cpu</code> option gives:</p>
<pre><code>Line #    Mem usage    Increment  Occurences   Line Contents
============================================================
     9  151.055 MiB  151.055 MiB           1   @profile
    10                                         def f():
    11  151.055 MiB    0.000 MiB           1       torch.set_grad_enabled(False)
    12  155.359 MiB    4.305 MiB           1       torch.cuda._lazy_init()
    13  155.359 MiB    0.000 MiB           1       device = 'cuda' if torch.cuda.is_available() else 'cpu'
    14  155.359 MiB    0.000 MiB           1       if args.cpu:
    15  155.359 MiB    0.000 MiB           1           device = 'cpu'
    16  157.754 MiB    2.395 MiB           1       model = torch.nn.Conv2d(1, 1, 1).to(device)
    17  157.754 MiB    0.000 MiB           1       x = torch.rand(1, 1, args.size, args.size).to(device)
    18  160.051 MiB    2.297 MiB           1       y = model(x)
</code></pre>
<p>Is there a way to reduce this overhead? In the GitHub issue there are mentions of compiling pytorch without all the CUDA kernels to reduce the RAM overhead, but I'm unsure which compile options I will need and which ones will actually reduce RAM overhead.</p>
<p>Is there a known way to reduce the RAM usage by PyTorch?</p>
",3116573.0,,10607772.0,,2023-04-19 10:32:43,2023-04-19 10:32:43,How can I reduce the amount of (non-GPU) RAM overhead that PyTorch uses when using CUDA?,<pytorch><cuda><ram><nvidia-jetson>,0,0,,,,CC BY-SA 4.0
67819858,1,67820422.0,,2021-06-03 10:34:56,,5,4797,"<p>I have a set of tensors that I'm padding with <code>pad_sequence</code> but I need to guarantee a fixed length for them. I can't do it right now as <code>pad_sequence</code> will extend the shorter tensors up to the longest, if that longest tensor doesn't reach the length I want them I'm screwed. I thought a solution could be adding zeros to one of the tensors to padd up to the length I want so the result of that padding will have my desired length. I don't know how to do it</p>
<p>So lets say I have a tensor with shape <code>torch.Size([44])</code> and a desired length 50, how can I add zeros to it to reach a shape of <code>torch.Size([50])</code>? This needs to hold regardless of the initial tensor shape.</p>
",6643799.0,,6643799.0,,2021-06-03 11:46:28,2022-07-15 10:09:05,Enforce pad_sequence to a certain length,<python><deep-learning><pytorch><recurrent-neural-network>,1,0,0.0,,,CC BY-SA 4.0
67378655,1,,,2021-05-04 04:04:13,,5,394,"<p>I have a dataset on my own, and the dataset contains two classes, let's say 0 and 1. Besides, there is a large part of nodes which class is unlabeled. My goal is to predict these unlabeled nodes using GCN. But I am confused about how to deal with these unlabeled nodes in Pytorch Geometric.</p>
<p>As far as I can think about, I can label the nodes into 3 classes, 0, 1 and unknown. But if I do it this way, that means I am trying to classify the dataset into three classes? (But that's not what I want since unknown is not a class).</p>
<p>And another way to deal with these node is to ignore them, simply run PyG on the labeled node. But in this way, it seems that these unlabeled node(with feature) is useless in the dataset?</p>
",15828775.0,,11339311.0,,2022-01-08 17:15:48,2022-01-08 17:15:48,How to deal with the unlabeled nodes in Pytorch Geometric?,<pytorch><pytorch-geometric>,1,0,0.0,,,CC BY-SA 4.0
65617755,1,,,2021-01-07 18:09:27,,5,7562,"<p>I need to replicate PyTorch image normalization in OpenCV or NumPy.</p>
<p>Quick backstory: I'm doing a project where I'm training in PyTorch but will have to inference in OpenCV due to deploying to an embedded device where I won't have the storage space to install PyTorch.  After training in PyTorch and saving a PyTorch graph I'm then converting to an ONNX graph.  For inferencing in OpenCV I'm opening the image as an OpenCV image (i.e. NumPy array), then resizing, then successively calling <code>cv2.normalize</code>, <code>cv2.dnn.blobFromImage</code>, <code>net.setInput</code>, and <code>net.forward</code>.</p>
<p>I'm getting slightly different accuracy results when test inferencing in PyTorch vs inferencing in OpenCV, and I suspect the difference is due to the normalization process producing a slightly different result between the two.</p>
<p>Here is a quick script I put together to show the difference on a single image.  Note that I'm using grayscale (single-channel) and I'm normalizing into the -1.0 to +1.0 range:</p>
<pre><code># scratchpad.py

import torch
import torchvision

import cv2
import numpy as np
import PIL
from PIL import Image

TRANSFORM = torchvision.transforms.Compose([
    torchvision.transforms.Resize((224, 224)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize([0.5], [0.5])
])

def main():
    # 1st show PyTorch normalization

    # open the image as an OpenCV image
    openCvImage = cv2.imread('image.jpg', cv2.IMREAD_GRAYSCALE)
    # convert OpenCV image to PIL image
    pilImage = PIL.Image.fromarray(openCvImage)
    # convert PIL image to a PyTorch tensor
    ptImage = TRANSFORM(pilImage).unsqueeze(0)
    # show the PyTorch tensor info
    print('\nptImage.shape = ' + str(ptImage.shape))
    print('ptImage max = ' + str(torch.max(ptImage)))
    print('ptImage min = ' + str(torch.min(ptImage)))
    print('ptImage avg = ' + str(torch.mean(ptImage)))
    print('ptImage: ')
    print(str(ptImage))

    # 2nd show OpenCV normalization

    # resize the image
    openCvImage = cv2.resize(openCvImage, (224, 224))
    # convert to float 32 (necessary for passing into cv2.dnn.blobFromImage which is not show here)
    openCvImage = openCvImage.astype('float32')
    # use OpenCV version of normalization, could also do this with numpy
    cv2.normalize(openCvImage, openCvImage, 1.0, -1.0, cv2.NORM_MINMAX)
    # show results
    print('\nopenCvImage.shape = ' + str(openCvImage.shape))
    print('openCvImage max = ' + str(np.max(openCvImage)))
    print('openCvImage min = ' + str(np.min(openCvImage)))
    print('openCvImage avg = ' + str(np.mean(openCvImage)))
    print('openCvImage: ')
    print(str(openCvImage))

    print('\ndone !!\n')
# end function

if __name__ == '__main__':
    main()
</code></pre>
<p>Here is the test image that I'm using:</p>
<p><a href=""https://i.stack.imgur.com/CPMhH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CPMhH.jpg"" alt=""enter image description here"" /></a></p>
<p>and here are the results I'm getting currently:</p>
<pre><code>$ python3 scratchpad.py 

ptImage.shape = torch.Size([1, 1, 224, 224])
ptImage max = tensor(0.9608)
ptImage min = tensor(-0.9686)
ptImage avg = tensor(0.1096)
ptImage: 
tensor([[[[ 0.0431, -0.0431,  0.1294,  ...,  0.8510,  0.8588,  0.8588],
          [ 0.0510, -0.0510,  0.0980,  ...,  0.8353,  0.8510,  0.8431],
          [ 0.0588, -0.0431,  0.0745,  ...,  0.8510,  0.8588,  0.8588],
          ...,
          [ 0.6157,  0.6471,  0.5608,  ...,  0.6941,  0.6627,  0.6392],
          [ 0.4902,  0.3961,  0.3882,  ...,  0.6627,  0.6471,  0.6706],
          [ 0.3725,  0.4039,  0.5451,  ...,  0.6549,  0.6863,  0.6549]]]])

openCvImage.shape = (224, 224)
openCvImage max = 1.0000001
openCvImage min = -1.0
openCvImage avg = 0.108263366
openCvImage: 
[[ 0.13725497 -0.06666661  0.20000008 ...  0.8509805   0.8666668
   0.8509805 ]
 [ 0.15294124 -0.06666661  0.09019614 ...  0.8274511   0.8431374
   0.8274511 ]
 [ 0.12156869 -0.06666661  0.0196079  ...  0.8509805   0.85882366
   0.85882366]
 ...
 [ 0.5843138   0.74117655  0.5450981  ...  0.83529425  0.59215695
   0.5764707 ]
 [ 0.6862746   0.34117654  0.39607853 ...  0.67843145  0.6705883
   0.6470589 ]
 [ 0.34117654  0.4117648   0.5215687  ...  0.5607844   0.74117655
   0.59215695]]

done !!
</code></pre>
<p>As you can see the results are similar but definitely not exactly the same.</p>
<p>How can I do the normalization in OpenCV and have it come out exactly or almost exactly the same as the PyTorch normalization?  I've tried various options in both OpenCV and with NumPy but could not get it any closer than the above results, which are substantially different.</p>
<p>-- Edit ---------------------------</p>
<p>In response to Ivan, I also tried this:</p>
<pre><code># resize the image
openCvImage = cv2.resize(openCvImage, (224, 224))
# convert to float 32 (necessary for passing into cv2.dnn.blobFromImage which is not show here)
openCvImage = openCvImage.astype('float32')
mean = np.mean(openCvImage)
stdDev = np.std(openCvImage)
openCvImage = (openCvImage - mean) / stdDev
# show results
print('\nopenCvImage.shape = ' + str(openCvImage.shape))
print('openCvImage max = ' + str(np.max(openCvImage)))
print('openCvImage min = ' + str(np.min(openCvImage)))
print('openCvImage avg = ' + str(np.mean(openCvImage)))
print('openCvImage: ')
print(str(openCvImage))
</code></pre>
<p>Which results in:</p>
<pre><code>openCvImage.shape = (224, 224)
openCvImage max = 2.1724665
openCvImage min = -2.6999729
openCvImage avg = 7.298528e-09
openCvImage: 
[[ 0.07062991 -0.42616782  0.22349077 ...  1.809422    1.8476373
   1.809422  ]
 [ 0.10884511 -0.42616782 -0.04401573 ...  1.7520993   1.7903144
   1.7520993 ]
 [ 0.0324147  -0.42616782 -0.21598418 ...  1.809422    1.8285296
   1.8285296 ]
 ...
 [ 1.1597633   1.5419154   1.0642253  ...  1.7712069   1.178871
   1.1406558 ]
 [ 1.4081622   0.56742764  0.70118093 ...  1.3890547   1.3699471
   1.3126242 ]
 [ 0.56742764  0.7393961   1.0069026  ...  1.1024406   1.5419154
   1.178871  ]]
</code></pre>
<p>Which is similar to the PyTorch normalization but clearly not the same.</p>
<p>I'm attempting to achieve normalization in OpenCV that produces the same result as the PyTorch normalization.</p>
<p>I realize that due to slight differences in the resizing operation (and possibly very small rounding differences) I'll probably never get exactly the same normalized result but I'd like to get as close as possible to the PyTorch result.</p>
",4835204.0,,4835204.0,,2021-01-07 19:58:20,2022-03-31 13:05:12,How to replicate PyTorch normalization in OpenCV or NumPy?,<python><numpy><opencv><pytorch>,5,0,,,,CC BY-SA 4.0
67819077,1,,,2021-06-03 09:40:37,,5,2559,"<p>I'm not so experienced in Data Science and pytorch and I have problems with implementing at least anything here(currently I'm making a NN for segmentation tasks). There is some kind of memory problem, although it doesn't meen anything - every epoch takes a lot less memory than it is in the risen</p>
<pre><code>import torch
from torch import nn
from torch.autograd import Variable
from torch.nn import Linear, ReLU6, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, Softplus ,BatchNorm2d, Dropout, ConvTranspose2d
import torch.nn.functional as F
from torch.nn import LeakyReLU,Tanh
from torch.optim import Adam, SGD
import numpy as np
import cv2 as cv
def train(epoch,model,criterion, x_train, y_train, loss_val):
    model.train()
    tr_loss = 0
    # getting the training set
    x_train, y_train = Variable(x_train), Variable(y_train)
    # converting the data into GPU format

    # clearing the Gradients of the model parameters
    optimizer.zero_grad()
    
    # prediction for training and validation set
    output_train = model(x_train)
    # computing the training and validation loss
    loss_train = criterion(output_train, y_train)
    train_losses.append(loss_train)
    # computing the updated weights of all the model parameters
    loss_train.backward()
    optimizer.step()
    tr_loss = loss_train.item()
    return loss_train
        # printing the validation loss
        
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 96, (3,3), padding=1)
        self.conv11= nn.Conv2d(96, 96, (3,3), padding=1)
        self.conv12= nn.Conv2d(96, 96, (3,3), padding=1)
        self.pool  = nn.MaxPool2d((2,2), 2)
        self.conv2 = nn.Conv2d(96, 192, (3,3), padding=1)
        self.conv21 = nn.Conv2d(192, 192, (3,3), padding=1)
        self.conv22 = nn.Conv2d(192, 192, (3,3), padding=1)
        self.b = BatchNorm2d(96)
        self.b1 = BatchNorm2d(192)
        self.b2 = BatchNorm2d(384)
        self.conv3 = nn.Conv2d(192,384,(3,3), padding=1)
        self.conv31= nn.Conv2d(384,384,(3,3), padding=1)
        self.conv32= nn.Conv2d(384,384,(3,3), padding=1)
        self.lin1   = nn.Linear(384*16*16, 256*2*2, 1)
        self.lin2   = nn.Linear(256*2*2, 16*16, 1)
        self.uppool = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.upconv1= nn.ConvTranspose2d(385,192,(3,3), padding=1)
        self.upconv11=nn.ConvTranspose2d(192,32,(3,3), padding=1)
        self.upconv12=nn.ConvTranspose2d(32,1,(3,3), padding=1)
        self.upconv2= nn.ConvTranspose2d(193,96,(3,3), padding=1)
        self.upconv21= nn.ConvTranspose2d(96,16,(3,3), padding=1)
        self.upconv22= nn.ConvTranspose2d(16,1,(3,3), padding=1)
        self.upconv3= nn.ConvTranspose2d(97,16,(3,3), padding=1)
        self.upconv4= nn.ConvTranspose2d(16,8,(3,3), padding=1)
        self.upconv6= nn.ConvTranspose2d(8,1,(3,3), padding=1)
    def forward(self, x):
        m=Tanh()
        x1=self.b(m(self.conv12(m(self.conv11(m(self.conv1(x)))))))
        x = self.pool(x1)
        x2=self.b1(m(self.conv22(m(self.conv21(m(self.conv2(x)))))))
        x = self.pool(x2)
        x3=self.b2(m(self.conv32(m(self.conv31(m(self.conv3(x)))))))
        x=self.pool(x3)
        x = x.view(-1, 16*16*384)
        x = m(self.lin1(x))
        x = m(self.lin2(x))
        x = x.view(1, 1, 16, 16)
        x=torch.cat((x,self.pool(x3)),1)
        x = self.uppool(m(self.upconv12(m(self.upconv11(m(self.upconv1(x)))))))
        
        x=torch.cat((x,self.pool(x2)),1)
        x = self.uppool(m(self.upconv22(m(self.upconv21(m(self.upconv2(x)))))))
        
        x=torch.cat((x,self.pool(x1)),1)
        x = (self.uppool(m(self.upconv3(x))))
        x = (m(self.upconv4(x)))
        l=Softplus()
        x= l(self.upconv6(x))
        return x
train_data=[]
for path in range(1000):
    n=&quot;&quot;.join([&quot;0&quot; for i in range(5-len(str(path)))])+str(path)
    paths=&quot;00000\\&quot;+n+&quot;.png&quot;
    train_data.append(cv.imread(paths))
for path in range(2000,3000):
    n=&quot;&quot;.join([&quot;0&quot; for i in range(5-len(str(path)))])+str(path)
    paths=&quot;02000\\&quot;+n+&quot;.png&quot;
    train_data.append(cv.imread(paths))
train_output=[]
for path in range(1,2001):
    n=&quot;outputs\\&quot;+str(path)+&quot;.jpg&quot;
    train_output.append(cv.imread(n))
data=torch.from_numpy((np.array(train_data,dtype=float).reshape(2000,3,128,128)/255)).reshape(2000,3,128,128)
data_cuda=torch.tensor(data.to('cuda'), dtype=torch.float32)

output=torch.from_numpy(np.array(train_output,dtype=float).reshape(2000,3,128,128))[:,2].view(2000,1,128,128)*2
output_cuda=torch.tensor(output.to('cuda'),dtype=torch.float32)
model=Net()
optimizer = Adam(model.parameters(), lr=0.1)
criterion = nn.BCEWithLogitsLoss()
if torch.cuda.is_available():
    model = model.cuda()
    criterion = criterion.cuda()
print(model)
epochs=3
n_epochs = 1
train_losses = []
val_losses = []
for epoch in range(n_epochs):
    loss_train=0
    for i in range(data.shape[0]):
        loss_train1=train(epoch,model,criterion,data_cuda[i].reshape(1,3,128,128),output_cuda[i].reshape(1,1,128,128),train_losses)
        loss_train+=loss_train1
    print('Epoch : ',epoch+1, '\t', 'loss :', loss_train/data.shape[0])
with torch.no_grad():
    torch.save(model.state_dict(), &quot;C:\\Users\\jugof\\Desktop\\Python\\pytorch_models&quot;)
    a=np.array(model(data_cuda).to('cpu').numpy())*255
    cv.imshow('',a.reshape(128,128))
    cv.waitKey(0)&quot;&quot;&quot;
</code></pre>
<p>Here is the error:</p>
<pre><code>&gt; PS C:\Users\jugof\Desktop\Python&gt; &amp; C:/Users/jugof/anaconda3/python.exe c:/Users/jugof/Desktop/Python/3d_visual_effect1.py
c:/Users/jugof/Desktop/Python/3d_visual_effect1.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  data_cuda=torch.tensor(data.to('cuda'), dtype=torch.float32)
c:/Users/jugof/Desktop/Python/3d_visual_effect1.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  output_cuda=torch.tensor(output.to('cuda'),dtype=torch.float32)
Epoch :  1       loss : tensor(0.6933, device='cuda:0', grad_fn=&lt;DivBackward0&gt;)
Traceback (most recent call last):
  File &quot;c:/Users/jugof/Desktop/Python/3d_visual_effect1.py&quot;, line 120, in &lt;module&gt;
    a=np.array(model(data_cuda).to('cpu').numpy())*255
  File &quot;C:\Users\jugof\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;c:/Users/jugof/Desktop/Python/3d_visual_effect1.py&quot;, line 62, in forward
    x1=self.b(m(self.conv12(m(self.conv11(m(self.conv1(x)))))))
  File &quot;C:\Users\jugof\anaconda3\lib\site-packages\torch\nn\modules\module.py&quot;, line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\jugof\anaconda3\lib\site-packages\torch\nn\modules\conv.py&quot;, line 399, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File &quot;C:\Users\jugof\anaconda3\lib\site-packages\torch\nn\modules\conv.py&quot;, line 395, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 11.72 GiB (GPU 0; 6.00 GiB total capacity; 2.07 GiB already allocated; 1.55 GiB free; 2.62 GiB reserved in total by PyTorch)
I feed a numpy array (an image) of 128*128 shape and recieve another of the same shape, it's a segmentation model(again)
</code></pre>
<p>I was using Flickr-Faces-HQ Dataset (FFHQ) and used downsampled 128*128 labels - I used 00000, 01000 and 02000 files and masks were recieved by opencv haarscascades_eye</p>
",14945024.0,,756233.0,,2023-05-18 15:08:56,2023-05-18 15:08:56,Runtime error: CUDA out of memory by the end of training and doesn’t save model; pytorch,<python><pytorch>,2,0,,,,CC BY-SA 4.0
67633879,1,67639226.0,,2021-05-21 09:07:56,,5,1887,"<p>I'm trying to implement a gaussian-like blurring of a 3D volume in pytorch. I can do a 2D blur of a 2D image by convolving with a 2D gaussian kernel easy enough, and the same approach seems to work for 3D with a 3D gaussian kernel. However, it is very slow in 3D (especially with larger sigmas/kernel sizes). I understand this can also be done instead by convolving 3 times with the 2D kernel which should be much faster, but I can't get this to work. My test case is below.</p>
<pre><code>import torch
import torch.nn.functional as F

VOL_SIZE = 21


def make_gaussian_kernel(sigma):
    ks = int(sigma * 5)
    if ks % 2 == 0:
        ks += 1
    ts = torch.linspace(-ks // 2, ks // 2 + 1, ks)
    gauss = torch.exp((-(ts / sigma)**2 / 2))
    kernel = gauss / gauss.sum()

    return kernel


def test_3d_gaussian_blur(blur_sigma=2):
    # Make a test volume
    vol = torch.zeros([VOL_SIZE] * 3)
    vol[VOL_SIZE // 2, VOL_SIZE // 2, VOL_SIZE // 2] = 1

    # 3D convolution
    vol_in = vol.reshape(1, 1, *vol.shape)
    k = make_gaussian_kernel(blur_sigma)
    k3d = torch.einsum('i,j,k-&gt;ijk', k, k, k)
    k3d = k3d / k3d.sum()
    vol_3d = F.conv3d(vol_in, k3d.reshape(1, 1, *k3d.shape), stride=1, padding=len(k) // 2)

    # Separable 2D convolution
    vol_in = vol.reshape(1, *vol.shape)
    k2d = torch.einsum('i,j-&gt;ij', k, k)
    k2d = k2d / k2d.sum()
    k2d = k2d.expand(VOL_SIZE, 1, *k2d.shape)
    for i in range(3):
        vol_in = vol_in.permute(0, 3, 1, 2)
        vol_in = F.conv2d(vol_in, k2d, stride=1, padding=len(k) // 2, groups=VOL_SIZE)
    vol_3d_sep = vol_in

    torch.allclose(vol_3d, vol_3d_sep)  # --&gt; False
</code></pre>
<p>Any help would be very much appreciated!</p>
",612911.0,,,,,2022-03-19 08:00:43,Implementing a 3D gaussian blur using separable 2D convolutions in pytorch,<pytorch><convolution><gaussianblur>,2,3,0.0,,,CC BY-SA 4.0
73264234,1,,,2022-08-07 00:59:55,,5,3402,"<p>I'm using Anaconda and I have installed PyTorch using the following command: <code>pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116</code></p>
<p>Now I'm getting the following error in torch/utils/cpp_extension.py:</p>
<pre><code>UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified
</code></pre>
<p>I'm using Windows 10 and I have installed Visual Studio Community 2022 and Visual Studio Build Tools 2022, please see screenshots below.</p>
<p>Does somebody what is wrong or missing?</p>
<p><a href=""https://i.stack.imgur.com/9NmOX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/9NmOX.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.stack.imgur.com/V8dDS.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/V8dDS.png"" alt=""enter image description here"" /></a></p>
<p>Edit: I'm using Cuda 11.6. I have now also installed Visual Studio 2019 including the build tools. Now the above error is gone but I have a new error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\utils\cpp_extension.py&quot;, line 1808, in _run_ninja_build
    subprocess.run(
  File &quot;C:\Users\myUser\Anaconda3\envs\parlai\lib\subprocess.py&quot;, line 528, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['ninja', '-v']' returned non-zero exit status 1.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;C:\Users\myUser\ParlAI\server\server.py&quot;, line 3, in &lt;module&gt;
    from parlai.utils.safety import OffensiveStringMatcher, OffensiveLanguageClassifier
  File &quot;c:\users\myUser\parlai\parlai\utils\safety.py&quot;, line 10, in &lt;module&gt;
    from parlai.agents.transformer.transformer import TransformerClassifierAgent
  File &quot;c:\users\myUser\parlai\parlai\agents\transformer\transformer.py&quot;, line 15, in &lt;module&gt;
    from parlai.core.torch_generator_agent import TorchGeneratorAgent
  File &quot;c:\users\myUser\parlai\parlai\core\torch_generator_agent.py&quot;, line 48, in &lt;module&gt;
    from parlai.ops.ngram_repeat_block import NGramRepeatBlock
  File &quot;c:\users\myUser\parlai\parlai\ops\ngram_repeat_block.py&quot;, line 23, in &lt;module&gt;
    ngram_repeat_block_cuda = load(
  File &quot;C:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\utils\cpp_extension.py&quot;, line 1202, in load
    return _jit_compile(
  File &quot;C:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\utils\cpp_extension.py&quot;, line 1425, in _jit_compile
    _write_ninja_file_and_build_library(
  File &quot;C:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\utils\cpp_extension.py&quot;, line 1537, in _write_ninja_file_and_build_library
    _run_ninja_build(
  File &quot;C:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\utils\cpp_extension.py&quot;, line 1824, in _run_ninja_build
    raise RuntimeError(message) from e
RuntimeError: Error building extension 'ngram_repeat_block_cuda': [1/2] C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin\nvcc --generate-dependencies-with-compile --dependency-output ngram_repeat_block_cuda_kernel.cuda.o.d -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcompiler /EHsc -Xcompiler /wd4190 -Xcompiler /wd4018 -Xcompiler /wd4275 -Xcompiler /wd4267 -Xcompiler /wd4244 -Xcompiler /wd4251 -Xcompiler /wd4819 -Xcompiler /MD -DTORCH_EXTENSION_NAME=ngram_repeat_block_cuda -DTORCH_API_INCLUDE_EXTENSION_H -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\torch\csrc\api\include -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\TH -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\THC &quot;-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\include&quot; -IC:\Users\myUser\Anaconda3\envs\parlai\Include -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -c c:\users\myUser\parlai\parlai\clib\cuda\ngram_repeat_block_cuda_kernel.cu -o ngram_repeat_block_cuda_kernel.cuda.o
FAILED: ngram_repeat_block_cuda_kernel.cuda.o
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\bin\nvcc --generate-dependencies-with-compile --dependency-output ngram_repeat_block_cuda_kernel.cuda.o.d -Xcudafe --diag_suppress=dll_interface_conflict_dllexport_assumed -Xcudafe --diag_suppress=dll_interface_conflict_none_assumed -Xcudafe --diag_suppress=field_without_dll_interface -Xcudafe --diag_suppress=base_class_has_different_dll_interface -Xcompiler /EHsc -Xcompiler /wd4190 -Xcompiler /wd4018 -Xcompiler /wd4275 -Xcompiler /wd4267 -Xcompiler /wd4244 -Xcompiler /wd4251 -Xcompiler /wd4819 -Xcompiler /MD -DTORCH_EXTENSION_NAME=ngram_repeat_block_cuda -DTORCH_API_INCLUDE_EXTENSION_H -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\torch\csrc\api\include -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\TH -IC:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\THC &quot;-IC:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.6\include&quot; -IC:\Users\myUser\Anaconda3\envs\parlai\Include -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 -c c:\users\myUser\parlai\parlai\clib\cuda\ngram_repeat_block_cuda_kernel.cu -o ngram_repeat_block_cuda_kernel.cuda.o
C:/Users/myUser/Anaconda3/envs/parlai/lib/site-packages/torch/include\c10/macros/Macros.h(143): warning C4067: unexpected tokens following preprocessor directive - expected a newline
C:/Users/myUser/Anaconda3/envs/parlai/lib/site-packages/torch/include\c10/macros/Macros.h(143): warning C4067: unexpected tokens following preprocessor directive - expected a newline
C:/Users/myUser/Anaconda3/envs/parlai/lib/site-packages/torch/include\c10/core/SymInt.h(84): warning #68-D: integer conversion resulted in a change of sign

C:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\pybind11\cast.h(1429): error: too few arguments for template template parameter &quot;Tuple&quot;
          detected during instantiation of class &quot;pybind11::detail::tuple_caster&lt;Tuple, Ts...&gt; [with Tuple=std::pair, Ts=&lt;T1, T2&gt;]&quot;
(1507): here

C:\Users\myUser\Anaconda3\envs\parlai\lib\site-packages\torch\include\pybind11\cast.h(1503): error: too few arguments for template template parameter &quot;Tuple&quot;
          detected during instantiation of class &quot;pybind11::detail::tuple_caster&lt;Tuple, Ts...&gt; [with Tuple=std::pair, Ts=&lt;T1, T2&gt;]&quot;
(1507): here

2 errors detected in the compilation of &quot;c:/users/myUser/parlai/parlai/clib/cuda/ngram_repeat_block_cuda_kernel.cu&quot;.
ngram_repeat_block_cuda_kernel.cu
ninja: build stopped: subcommand failed.
</code></pre>
",3591044.0,,3591044.0,,2022-08-08 00:14:33,2022-08-08 00:14:33,PyTorch Error checking compiler version for cl (cpp_extension.py),<python><windows><visual-studio><visual-c++><pytorch>,0,0,,,,CC BY-SA 4.0
70051750,1,75653514.0,,2021-11-21 04:04:01,,5,970,"<p>I'm running Python code from an old repo that seems to no longer work but I can't figure out why or how to fix it.</p>
<p><strong>SETUP CODE</strong> (Google Colab Notebook)</p>
<pre><code>#@title Setup
import pkg_resources
print(pkg_resources.get_distribution(&quot;torch&quot;).version)
from IPython.utils import io
with io.capture_output() as captured:
  !git clone https://github.com/openai/CLIP
  # !pip install taming-transformers
  !git clone https://github.com/CompVis/taming-transformers.git
  !rm -Rf clipit
  !git clone https://github.com/mfrashad/clipit.git
  !pip install ftfy regex tqdm omegaconf pytorch-lightning
  !pip install kornia
  !pip install imageio-ffmpeg   
  !pip install einops
  !pip install torch-optimizer
  !pip install easydict
  !pip install braceexpand
  !pip install git+https://github.com/pvigier/perlin-numpy

  # ClipDraw deps
  !pip install svgwrite
  !pip install svgpathtools
  !pip install cssutils
  !pip install numba
  !pip install torch-tools
  !pip install visdom

  !pip install gradio

  !git clone https://github.com/BachiLi/diffvg
  %cd diffvg
  # !ls
  !git submodule update --init --recursive
  !python setup.py install
  %cd ..
  
  !mkdir -p steps
  !mkdir -p models
import sys
sys.path.append(&quot;clipit&quot;)
</code></pre>
<p><strong>THEN ERRORS ON SECOND TO LAST INE</strong></p>
<pre><code>import clipit
# To reset settings to default
clipit.reset_settings()
# You can use &quot;|&quot; to separate multiple prompts
prompts = &quot;underwater city&quot;
# You can trade off speed for quality: draft, normal, better, best
quality = &quot;normal&quot;
# Aspect ratio: widescreen, square
aspect = &quot;widescreen&quot;
# Add settings
clipit.add_settings(prompts=prompts, quality=quality, aspect=aspect)
# Apply these settings and run
settings = clipit.apply_settings()
clipit.do_init(settings) # generates error
clipit.do_run(settings)
</code></pre>
<blockquote>
<p>Working with z of shape (1, 256, 16, 16) = 65536 dimensions. loaded
pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth
VQLPIPSWithDiscriminator running with hinge loss. Restored from
models/vqgan_imagenet_f16_16384.ckpt
--------------------------------------------------------------------------- RuntimeError                              Traceback (most recent call
last)  in ()
12 # Apply these settings and run
13 settings = clipit.apply_settings()
---&gt; 14 clipit.do_init(settings)
15 clipit.do_run(settings)</p>
<p>1 frames /usr/local/lib/python3.7/dist-packages/torch/jit/_script.py
in fail(self, *args, **kwargs)
912     def _make_fail(name):
913         def fail(self, *args, **kwargs):
--&gt; 914             raise RuntimeError(name + &quot; is not supported on ScriptModules&quot;)
915
916         return fail</p>
<p>RuntimeError: requires_grad_ is not supported on ScriptModules</p>
</blockquote>
",1060489.0,,,,,2023-03-06 16:42:11,PyTorch RuntimeError: requires_grad_ is not supported on ScriptModules,<python><pytorch>,1,2,,,,CC BY-SA 4.0
66478043,1,,,2021-03-04 15:23:11,,5,4119,"<p>While executing iNltk library, I am getting an error. I have latest versions of pytorch and torchvision.</p>
<pre><code>'LSTM' object has no attribute '_flat_weights_names'
</code></pre>
<p>After re-searching on some blogs some people suggested to downgrade the version to 1.2 So i tried below installation from <a href=""https://pytorch.org/get-started/previous-versions/"" rel=""noreferrer"">https://pytorch.org/get-started/previous-versions/</a></p>
<pre><code>pip install torch==1.2.0+cu92 torchvision==0.4.0+cu92 -f https://download.pytorch.org/whl/torch_stable.html
</code></pre>
<p>However, getting errors</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement torch==1.2.0+cpu
ERROR: No matching distribution found for torch==1.2.0+cpu
</code></pre>
<p>Also, 1.3.1 version is missing.</p>
<p>Anybody has any idea about how to downgrade to 1.3.1 or 1.2.0?</p>
<p>Thanks in advance</p>
<p>PD</p>
",15226396.0,,,,,2022-06-04 13:27:08,'LSTM' object has no attribute '_flat_weights_names',<python><pytorch><nltk><torch>,2,0,0.0,,,CC BY-SA 4.0
65378968,1,,,2020-12-20 10:33:03,,5,1369,"<p>Let <code>a</code> be a <code>(n, d, l)</code> tensor. Let <code>indices</code> be a <code>(n, 1)</code> tensor, containing indices. I want to gather from <code>a</code> in the middle dimension tensors from indices given by <code>indices</code>. The resulting tensor would therefore be of shape <code>(n, l)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>n = 3
d = 2
l = 3

a = tensor([[[ 0,  1,  2],
             [ 3,  4,  5]],

            [[ 6,  7,  8],
             [ 9, 10, 11]],

            [[12, 13, 14],
             [15, 16, 17]]])

indices = tensor([[0],
                  [1],
                  [0]])

# Shape of result is (n, l)
result = tensor([[ 0,  1,  2],  # a[0, 0, :] since indices[0] == 0

                 [ 9, 10, 11],  # a[1, 1, :] since indices[1] == 1

                 [12, 13, 14]]) # a[2, 0, :] since indices[2] == 0
</code></pre>
<p>This is indeed similar to <code>a.gather(1, indices)</code>, but <code>gather</code> won't work since <code>indices</code> does not have the same shape as <code>a</code>. How can I use <code>gather</code> in this setting? Or what should I use?</p>
",8737016.0,,,,,2021-06-30 16:00:37,Torch gather middle dimension,<python><pytorch><torch>,3,0,0.0,,,CC BY-SA 4.0
72239086,1,72258827.0,,2022-05-14 09:52:45,,5,520,"<p>With even very simple example, <code>backward()</code> cannot work if <code>sparse_grad=True</code>, please see the error below.</p>
<p>Is this error expected, or I'm using <code>gather</code> in a wrong way?</p>
<pre class=""lang-py prettyprint-override""><code>In [1]: import torch as th

In [2]: x = th.rand((3,3), requires_grad=True)

# sparse_grad = False, the backward could work as expetecd
In [3]: th.gather(x @ x, 1, th.LongTensor([[0], [1]]), sparse_grad=False).sum().backward()

# sparse_grad = True, backward CANNOT work
In [4]: th.gather(x @ x, 1, th.LongTensor([[0], [1]]), sparse_grad=True).sum().backward()
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
----&gt; 1 th.gather(x @ x, 1, th.LongTensor([[0], [1]]), sparse_grad=True).sum().backward()

~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py in backward(self, gradient, retain_graph, create_graph, inputs)
    305                 create_graph=create_graph,
    306                 inputs=inputs)
--&gt; 307         torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
    308
    309     def register_hook(self, hook):

~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    152         retain_graph = create_graph
    153
--&gt; 154     Variable._execution_engine.run_backward(
    155         tensors, grad_tensors_, retain_graph, create_graph, inputs,
    156         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag

RuntimeError: sparse tensors do not have strides
</code></pre>
",2235936.0,,9747182.0,,2022-05-23 14:19:31,2022-05-23 14:19:31,pytorch gather failed with sparse_grad=True,<python><pytorch><backpropagation>,2,0,,,,CC BY-SA 4.0
63265560,1,,,2020-08-05 12:47:01,,5,347,"<p>I implemented a custom PyTorch Dataset for my project's Dataloader to use. However, it is running slower than expected so profiling was chosen to troubleshoot the bottleneck.</p>
<p>Looked into vprof but I am unsure of the function to be profiled if I am interested in only profiling the Dataset implementation. Using PyTorch 1.5 with Python 3.7 on Ubuntu 18.04.</p>
<p>How can we perform performance profiling (CPU and memory) on just the custom Dataset implementation?</p>
<p>Thanks!</p>
",3023615.0,,,,,2020-08-05 12:47:01,Profiling of PyTorch Custom Dataset for Dataloader,<python><python-3.x><pytorch><profiling><flamegraph>,0,0,,,,CC BY-SA 4.0
74852107,1,74930159.0,,2022-12-19 14:50:44,,5,272,"<p>I am mastering pytorch here, and decided to implement very simple 1 to 1 linear regression, from height to weight.</p>
<p>Got dataset: <a href=""https://www.kaggle.com/datasets/mustafaali96/weight-height"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/mustafaali96/weight-height</a> but any other would do nicely.</p>
<p>Lets import libraries and information about females:</p>
<pre><code>import torch
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
df = pd.read_csv('weight-height.csv',sep=',')
#https://www.kaggle.com/datasets/mustafaali96/weight-height
height_f=df[df['Gender']=='Female']['Height'].to_numpy()
weight_f=df[df['Gender']=='Female']['Weight'].to_numpy()
plt.scatter(height_f, weight_f, c =&quot;red&quot;,alpha=0.1)
plt.show()
</code></pre>
<p>Which gives nice scatter of measured females:
<a href=""https://i.stack.imgur.com/58B6a.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/58B6a.png"" alt=""distribution"" /></a></p>
<p>So far, so good.</p>
<p>Lets make Dataloader:</p>
<pre><code>class Data(Dataset):
  def __init__(self, X: np.ndarray, y: np.ndarray) -&gt; None:
    # need to convert float64 to float32 else
    # will get the following error
    # RuntimeError: expected scalar type Double but found Float
    self.X = torch.from_numpy(X.reshape(-1, 1).astype(np.float32))
    self.y = torch.from_numpy(y.reshape(-1, 1).astype(np.float32))    
    self.len = self.X.shape[0]  
  def __getitem__(self, index: int) -&gt; tuple:
    return self.X[index], self.y[index]  
  def __len__(self) -&gt; int:
    return self.len

traindata = Data(height_f, weight_f)
batch_size = 500
num_workers = 2
trainloader = DataLoader(traindata, 
                         batch_size=batch_size, 
                         shuffle=True, 
                         num_workers=num_workers)
</code></pre>
<p>...linear regression model...</p>
<pre><code>class linearRegression(torch.nn.Module):
    def __init__(self, inputSize, outputSize):
        super(linearRegression, self).__init__()
        self.linear = torch.nn.Linear(inputSize, outputSize)
        

    def forward(self, x):
        out = self.linear(x)
        return out
model = linearRegression(1, 1)
criterion = torch.nn.MSELoss() 
optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)

</code></pre>
<p>.. lets train it:</p>
<pre><code>epochs=10
for epoch in range(epochs):
    print(epoch)
    for i, (inputs, labels) in enumerate(trainloader):
        
        outputs=model(inputs)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
</code></pre>
<p>gives 0,1,2,3,4,5,6,7,8,9
now lets see what our model gives:</p>
<pre><code>range_height_f=torch.linspace(height_f.min(),height_f.max(),150)

plt.scatter(height_f, weight_f, c =&quot;red&quot;,alpha=0.1)
pred=model(range_height_f.reshape(-1, 1))
plt.scatter(range_height_f, pred.detach().numpy(), c =&quot;green&quot;,alpha=0.1)
</code></pre>
<p>...
<a href=""https://i.stack.imgur.com/Y5LHv.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y5LHv.png"" alt=""wrong model"" /></a></p>
<p>Why does it do this? Why wrong slope?
consistently wrong slope, I might add
Whatever I change, optimizer, batch size, epochs, females to males.. it gives me this very wrong slope, and I really don't get - why?</p>
<p>Edit 1: Added loss, here is plot
<a href=""https://i.stack.imgur.com/hOGWy.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/hOGWy.png"" alt=""loss"" /></a></p>
<p>Edit 2: Have decided to explore a bit, and made regression with skilearn:</p>
<pre><code>from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression
X_train, X_test, y_train, y_test = train_test_split(height_f, weight_f, test_size = 0.25)

regr = LinearRegression()
regr.fit(X_train.reshape(-1,1), y_train)
plt.scatter(height_f, weight_f, c =&quot;red&quot;,alpha=0.1)
range_pred=regr.predict(range_height_f.reshape(-1, 1))
range_pred
plt.scatter(range_height_f, range_pred, c =&quot;green&quot;,alpha=0.1)
</code></pre>
<p>which gives following regression, which looks nice:
<a href=""https://i.stack.imgur.com/LqUf9.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LqUf9.png"" alt=""skilearn regression"" /></a></p>
<pre><code>t = torch.from_numpy(height_f.astype(np.float32))
p=regr.predict(t.reshape(-1,1))
p=torch.from_numpy(p).reshape(-1,1)


w= torch.from_numpy(weight_f.astype(np.float32)).reshape(-1,1)

print(criterion(p,w).item())
</code></pre>
<p>However in this case criterion=100.65161998527695</p>
<p>Pytorch in own turn converges to about 210</p>
<p>Edit 3
Changed optimisation to Adam from SGD:</p>
<pre><code>#optimizer = torch.optim.SGD(model.parameters(), lr=0.00001)
optimizer = torch.optim.Adam(model.parameters(), lr=0.5)
</code></pre>
<p>lr is larger in this case, which yields interesting, but consistent result.
Here is loss:
<a href=""https://i.stack.imgur.com/SYqWR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/SYqWR.png"" alt=""Adam loss"" /></a>,
And here is proposed regression:
<a href=""https://i.stack.imgur.com/0iTFr.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/0iTFr.png"" alt=""Adam regression loss"" /></a></p>
<p>And, here is log of loss criterion as well for Adam optimizer:
<a href=""https://i.stack.imgur.com/vZWwI.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vZWwI.png"" alt=""Last epochs"" /></a></p>
",4327368.0,,10858321.0,,2023-01-01 15:24:30,2023-01-02 04:49:30,"Pytorch Linear regression 1x1d, consistantly wrong slope",<python><numpy><pytorch><linear-regression>,4,4,,,,CC BY-SA 4.0
67692437,1,,,2021-05-25 16:55:41,,5,892,"<p>I tried to implement the most simple Deep Q Learning algorithm. I think, I've implemented it right and know that Deep Q Learning struggles with divergences but the reward is declining very fast and the loss is diverging. I would be grateful if someone could help me pointing out the right hyperparameters or if I implemented the algorithm wrong. I've tried a lot of hyperparameter combinations and also changing the complexity of the QNet.</p>
<pre><code>import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import collections
import numpy as np
import matplotlib.pyplot as plt
import gym
from torch.nn.modules.linear import Linear
from torch.nn.modules.loss import MSELoss


class ReplayBuffer:
  def __init__(self, max_replay_size, batch_size):
    self.max_replay_size = max_replay_size
    self.batch_size      = batch_size
    self.buffer          = collections.deque()


def push(self, *transition):
    if len(self.buffer) == self.max_replay_size:
        self.buffer.popleft()
    self.buffer.append(transition)


def sample_batch(self):
    indices = np.random.choice(len(self.buffer), self.batch_size, replace = False)
    batch   = [self.buffer[index] for index in indices]
    
    state, action, reward, next_state, done = zip(*batch)
    
    state      = np.array(state)
    action     = np.array(action)
    reward     = np.array(reward)
    next_state = np.array(next_state)
    done       = np.array(done)
    
    return state, action, reward, next_state, done


def __len__(self):
    return len(self.buffer)


class QNet(nn.Module):
  def __init__(self, state_dim, action_dim):
    super(QNet, self).__init__()

    self.linear1 = Linear(in_features = state_dim, out_features = 64)
    self.linear2 = Linear(in_features = 64, out_features = action_dim)


  def forward(self, x):
    x = self.linear1(x)
    x = F.relu(x)
    x = self.linear2(x)
    return x


def train(replay_buffer, model, target_model, discount_factor, mse, optimizer):
  state, action, reward, next_state, _ = replay_buffer.sample_batch()
  state, next_state = torch.tensor(state, dtype = torch.float), torch.tensor(next_state, 
  dtype = torch.float)

  # Compute Q Value and Target Q Value
  q_values = model(state).gather(1, torch.tensor(action, dtype = torch.int64).unsqueeze(-1))

  with torch.no_grad():
    max_next_q_values = target_model(next_state).detach().max(1)[0]
    q_target_value = torch.tensor(reward, dtype = torch.float) + discount_factor * 
                     max_next_q_values

  optimizer.zero_grad()
  loss = mse(q_values, q_target_value.unsqueeze(1))
  loss.backward()
  optimizer.step()

  return loss.item()


def main():
  # Define Hyperparameters and Parameters
  EPISODES        = 10000
  MAX_REPLAY_SIZE = 10000
  BATCH_SIZE      = 32
  EPSILON         = 1.0
  MIN_EPSILON     = 0.05
  DISCOUNT_FACTOR = 0.95
  DECAY_RATE      = 0.99
  LEARNING_RATE   = 1e-3
  SYNCHRONISATION = 33
  EVALUATION      = 32

  # Initialize Environment, Model, Target-Model, Optimizer, Loss Function and Replay Buffer
  env = gym.make(&quot;CartPole-v0&quot;)

  model        = QNet(state_dim = env.observation_space.shape[0], action_dim = 
                 env.action_space.n)
  target_model = QNet(state_dim = env.observation_space.shape[0], action_dim = 
                 env.action_space.n)
  target_model.load_state_dict(model.state_dict())

  optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)
  mse       = MSELoss()

  replay_buffer = ReplayBuffer(max_replay_size = MAX_REPLAY_SIZE, batch_size = BATCH_SIZE)

  while len(replay_buffer) != MAX_REPLAY_SIZE:
    state = env.reset()
    done  = False
    while done != True:
        action = env.action_space.sample()

        next_state, reward, done, _ = env.step(action)

        replay_buffer.push(state, action, reward, next_state, done)

        state = next_state

  # Begin with the Main Loop where the QNet is trained
  count_until_synchronisation = 0
  count_until_evaluation      = 0
  history = {'Episode': [], 'Reward': [], 'Loss': []}
  for episode in range(EPISODES):
    total_reward = 0.0
    total_loss   = 0.0
    state        = env.reset()
    iterations   = 0
    done         = False
    while done != True:
        count_until_synchronisation += 1
        count_until_evaluation      += 1

        # Take an action
        if np.random.rand(1) &lt; EPSILON:
            action = env.action_space.sample()
        else:
            with torch.no_grad():
                output = model(torch.tensor(state, dtype = torch.float)).numpy()
            action = np.argmax(output)

        # Observe new state and reward + store into replay_buffer
        next_state, reward, done, _ = env.step(action)
        total_reward += reward

        replay_buffer.push(state, action, reward, next_state, done)

        state = next_state

        if count_until_synchronisation % SYNCHRONISATION == 0:
            target_model.load_state_dict(model.state_dict())

        if count_until_evaluation % EVALUATION == 0:
            loss = train(replay_buffer = replay_buffer, model = model, target_model = 
                         target_model, discount_factor = DISCOUNT_FACTOR,
                         mse = mse, optimizer = optimizer)
            total_loss += loss

        iterations += 1

    print (f&quot;Episode {episode} is concluded in {iterations} iterations with a total reward 
           of {total_reward}&quot;)

    if EPSILON &gt; MIN_EPSILON:
        EPSILON *= DECAY_RATE

    history['Episode'].append(episode)
    history['Reward'].append(total_reward)
    history['Loss'].append(total_loss)

# Plot the Loss + Reward per Episode
fig, ax = plt.subplots(figsize = (10, 6))
ax.plot(history['Episode'], history['Reward'], label = &quot;Reward&quot;)
ax.set_xlabel('Episodes', fontsize = 15)
ax.set_ylabel('Total Reward per Episode', fontsize = 15)
plt.legend(prop = {'size': 15})
plt.show()

fig, ax = plt.subplots(figsize = (10, 6))
ax.plot(history['Episode'], history['Loss'], label = &quot;Loss&quot;)
ax.set_xlabel('Episodes', fontsize = 15)
ax.set_ylabel('Total Loss per Episode', fontsize = 15)
plt.legend(prop = {'size': 15})
plt.show()


if __name__ == &quot;__main__&quot;:
  main()
</code></pre>
",15995948.0,,15995948.0,,2021-05-27 08:09:46,2022-04-26 23:48:52,Deep Reinforcement Learning - CartPole Problem,<python><deep-learning><pytorch><reinforcement-learning><q-learning>,3,0,0.0,,,CC BY-SA 4.0
