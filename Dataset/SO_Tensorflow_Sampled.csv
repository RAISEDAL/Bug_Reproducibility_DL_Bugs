Id,PostTypeId,AcceptedAnswerId,ParentId,CreationDate,DeletionDate,Score,ViewCount,Body,OwnerUserId,OwnerDisplayName,LastEditorUserId,LastEditorDisplayName,LastEditDate,LastActivityDate,Title,Tags,AnswerCount,CommentCount,FavoriteCount,ClosedDate,CommunityOwnedDate,ContentLicense
72441758,1,,,2022-05-31 02:47:35,,165,207861,"<p>I tried to install <a href=""https://docs.ray.io/en/latest/"" rel=""noreferrer"">Ray</a>, but it gave an error:</p>
<pre class=""lang-none prettyprint-override""><code>TypeError: Descriptors cannot not be created directly.
If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc &gt;= 3.19.0.
If you cannot immediately regenerate your protos, some other possible workarounds are:
 1. Downgrade the protobuf package to 3.20.x or lower.
 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).
</code></pre>
<p>I tried to solve the problem and downgraded <em>protobuf</em>:</p>
<pre class=""lang-none prettyprint-override""><code>Name: protobuf
Version: 3.20.0
Summary: Protocol Buffers
Home-page: https://developers.google.com/protocol-buffers/
Author:
Author-email:
License: BSD-3-Clause
Location: d:\opencv\lib\site-packages
Requires:
Required-by: ray, tensorboard, tensorflow
</code></pre>
<p>But still the problem persists in Ray, <a href=""https://en.wikipedia.org/wiki/TensorFlow"" rel=""noreferrer"">TensorFlow</a>, and <a href=""https://en.wikipedia.org/wiki/Keras"" rel=""noreferrer"">Keras</a>. My application isn't working any more. How can I fix it?</p>
",17115086.0,,63550.0,,2022-06-26 13:01:10,2023-06-25 17:42:08,TypeError: Descriptors cannot not be created directly,<python><tensorflow><ray>,12,2,0.0,,,CC BY-SA 4.0
65298241,1,65333085.0,,2020-12-15 00:05:16,,115,192132,"<p>I just installed tensorflow v2.3 on anaconda python. I tried to test out the installation using the python command below;</p>
<pre><code>$ python -c &quot;import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))&quot;
</code></pre>
<p>I got the following message;</p>
<pre><code>2020-12-15 07:59:12.411952: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
hello, [[4.]]
</code></pre>
<p>From the message, it seems that the installation was installed successfully. But what does <code>This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2</code> mean exactly?</p>
<p>Am I using a tensorflow version with some limited features? Any side effects?</p>
<p>I am using Windows 10.</p>
",7518091.0,,,,,2023-04-16 18:51:49,What does this tensorflow message mean? Any side effect? Was the installation successful?,<python><tensorflow><anaconda>,7,1,0.0,,,CC BY-SA 4.0
72964800,1,,,2022-07-13 10:32:08,,57,50198,"<p>I am facing 4 problems when I tried to install TensorFlow on Apple M1:</p>
<ol>
<li><p><a href=""https://www.anaconda.com/blog/new-release-anaconda-distribution-now-supporting-m1"" rel=""noreferrer"">Conda has supported M1 since 2022.05.06</a> but most of articles I googled talk about using Miniforge, e.g. So I feel they are all kind of outdated.</p>
<ol>
<li><a href=""https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706"" rel=""noreferrer"">How To Install TensorFlow on M1 Mac (The Easy Way)</a></li>
<li><a href=""https://makeoptim.com/en/deep-learning/tensorflow-metal"" rel=""noreferrer"">AI - Apple Silicon Mac M1 natively supports TensorFlow 2.8 GPU acceleration</a></li>
<li><a href=""https://www.mrdbourke.com/setup-apple-m1-pro-and-m1-max-for-machine-learning-and-data-science/"" rel=""noreferrer"">How to Setup TensorFlow on Apple M1 Pro and M1 Max (works for M1 too)</a></li>
<li><a href=""https://betterdatascience.com/install-tensorflow-2-7-on-macbook-pro-m1-pro/"" rel=""noreferrer"">How To Install TensorFlow 2.7 on MacBook Pro M1 Pro With Ease</a></li>
</ol>
</li>
<li><p>I used the latest conda 4.13 to setup my python environment(3.8, 3.9 and 3.10) successfully but when I tried to install tensorflow I got the error &quot;<strong>No matching distribution found for tensorflow</strong>&quot; (all failed).</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow 
</code></pre>
</li>
</ol>
<p>The answers in <a href=""https://stackoverflow.com/questions/48720833/could-not-find-a-version-that-satisfies-the-requirement-tensorflow"">Could not find a version that satisfies the requirement tensorflow</a> didn't help. I can't find useful information on <a href=""https://www.tensorflow.org/"" rel=""noreferrer"">https://www.tensorflow.org/</a> too, actually <a href=""https://www.tensorflow.org/install"" rel=""noreferrer"">https://www.tensorflow.org/install</a> just said <code>pip install tensorflow</code>.</p>
<ol start=""3"">
<li><p>I tried to run <code>pip install tensorflow-macos</code> and it succeeded.
I read from the above &quot;works for M1 too&quot; article mentioned &quot;<strong>Apple's fork of TensorFlow is called tensorflow-macos</strong>&quot; although I can't find much information about that. For example, <a href=""https://www.tensorflow.org/"" rel=""noreferrer"">https://www.tensorflow.org/</a> does not mention that. I also found from <a href=""https://developer.apple.com/forums/thread/686926"" rel=""noreferrer"">https://developer.apple.com/forums/thread/686926</a> that someone hit that &quot;<strong>ERROR: No matching distribution found for tensorflow-macos</strong>&quot; (but I didn't).</p>
</li>
<li><p>All the articles I googled, including above 4 articles and this <a href=""https://stackoverflow.com/questions/69215644/tensorflow-on-macos-apple-m1"">Tensorflow on macOS Apple M1</a>, all say I also need to run the following 2 commands</p>
<p><code>conda install -c apple tensorflow-deps</code></p>
<p><code>pip install tensorflow-metal</code></p>
</li>
</ol>
<p>But do I really need to that? I can't find this information from <a href=""https://www.tensorflow.org/"" rel=""noreferrer"">https://www.tensorflow.org/</a>.
What are these 2 packages <code>tensorflow-deps</code> and <code>tensorflow-metal</code> ?</p>
",301513.0,,301513.0,,2022-07-14 08:34:24,2023-06-25 13:07:49,What is the proper way to install TensorFlow on Apple M1 in 2022,<tensorflow><conda><apple-m1>,13,3,0.0,,,CC BY-SA 4.0
70537488,1,70538761.0,,2021-12-30 22:55:22,,48,67019,"<p>I got below error message when I run <code>model_main_tf2.py</code> on Object Detection API:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/content/models/research/object_detection/model_main_tf2.py&quot;, line 32, in &lt;module&gt;
    from object_detection import model_lib_v2
  File &quot;/usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py&quot;, line 29, in &lt;module&gt;
    from object_detection import eval_util
  File &quot;/usr/local/lib/python3.7/dist-packages/object_detection/eval_util.py&quot;, line 36, in &lt;module&gt;
    from object_detection.metrics import lvis_evaluation
  File &quot;/usr/local/lib/python3.7/dist-packages/object_detection/metrics/lvis_evaluation.py&quot;, line 23, in &lt;module&gt;
    from lvis import results as lvis_results
  File &quot;/usr/local/lib/python3.7/dist-packages/lvis/__init__.py&quot;, line 5, in &lt;module&gt;
    from lvis.vis import LVISVis
  File &quot;/usr/local/lib/python3.7/dist-packages/lvis/vis.py&quot;, line 1, in &lt;module&gt;
    import cv2
  File &quot;/usr/local/lib/python3.7/dist-packages/cv2/__init__.py&quot;, line 9, in &lt;module&gt;
    from .cv2 import _registerMatType
ImportError: cannot import name '_registerMatType' from 'cv2.cv2' (/usr/local/lib/python3.7/dist-packages/cv2/cv2.cpython-37m-x86_64-linux-gnu.so)
</code></pre>
<p>The weird thing is I run the same code before, it worked well but now it gives me an error.</p>
",16422667.0,,651246.0,,2022-01-17 22:24:06,2022-04-22 09:55:57,cannot import name '_registerMatType' from 'cv2.cv2',<python><tensorflow><object-detection-api>,8,0,0.0,,,CC BY-SA 4.0
66977227,1,69302029.0,,2021-04-06 22:23:10,,47,59906,"<p>Note: there are many similar questions but for different versions of ubuntu and somewhat different specific libraries.  I have not been able to figure out what combination of symbolic links, additional environment variables such as <code>LD_LIBRARY_PATH</code> would work</p>
<p>Here is my <em>nvidia</em> configuration</p>
<pre><code>$ nvidia-smi
Tue Apr  6 11:35:54 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 2070    Off  | 00000000:01:00.0 Off |                  N/A |
| 18%   25C    P8     9W / 175W |     25MiB /  7982MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1081      G   /usr/lib/xorg/Xorg                 20MiB |
|    0   N/A  N/A      1465      G   /usr/bin/gnome-shell                3MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>When running a TF program the following happened:</p>
<pre><code>2021-04-06 14:35:01.589906: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory
2021-04-06 14:35:01.589914: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
</code></pre>
<p>Has anyone seen this particular mix and how did you resolve it?</p>
<p>Here is one of the additional fixes attempted, but with no change:</p>
<pre><code>conda install cudatoolkit=11.0
</code></pre>
",1056563.0,,681865.0,,2021-04-06 22:35:54,2023-03-08 20:31:22,"""Could not load dynamic library 'libcudnn.so.8'"" when running tensorflow on ubuntu 20.04",<python><tensorflow>,3,3,0.0,,,CC BY-SA 4.0
70520120,1,70563364.0,,2021-12-29 13:30:10,,40,48139,"<p>I was trying to train a model using tensorboard.
While executing, I got this error:</p>
<p><code>$ python train.py  Traceback (most recent call last): File &quot;train.py&quot;, line 6, in &lt;module&gt; from torch.utils.tensorboard import SummaryWriter   File &quot;C:\Users\91960\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\utils\tensorboard\__init__.py&quot;, line 4, in &lt;module&gt; LooseVersion = distutils.version.LooseVersion </code></p>
<p><code>AttributeError: module 'setuptools._distutils' has no attribute 'version'</code>.</p>
<p>I'm using python 3.8.9 64-bit &amp; tensorflow with distutils is already installed which is required by tensorboard.</p>
<p>Why is this happening ? Please help !</p>
",13896155.0,,,,,2023-05-02 17:57:27,AttributeError: module 'setuptools._distutils' has no attribute 'version',<python><tensorflow><tensorboard><distutils>,4,1,0.0,,,CC BY-SA 4.0
66083545,1,70992274.0,,2021-02-07 00:57:32,,37,60815,"<p>Using <code>tensorflow 2.4.1</code></p>
<p>When I run my program, I'm getting this error and can't use my <code>gpu</code>.</p>
<p>I'm using <code>CUDA 11.0</code>, <code>cudnn 8.0</code></p>
<pre><code>2021-02-07 03:36:18.132005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
WARNING:tensorflow:From D:/PycharmProjects/pythonProject/models/kpş,i.py:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-02-07 03:36:19.735127: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-07 03:36:19.739052: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-02-07 03:36:20.715634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.56GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 119.24GiB/s
2021-02-07 03:36:20.716281: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-02-07 03:36:20.723519: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-02-07 03:36:20.724040: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-02-07 03:36:20.729436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-02-07 03:36:20.731800: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-02-07 03:36:20.741580: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-02-07 03:36:20.745576: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-02-07 03:36:20.746657: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2021-02-07 03:36:20.746971: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2021-02-07 03:36:20.836861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-02-07 03:36:20.837144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-02-07 03:36:20.837314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-02-07 03:36:20.837493: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
</code></pre>
",15160674.0,,1832058.0,,2021-02-07 03:08:54,2022-11-30 00:16:24,Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found,<python><tensorflow>,8,1,0.0,,,CC BY-SA 4.0
65383338,1,68214296.0,,2020-12-20 18:26:16,,37,98959,"<p>I'm trying to get tensorflow working on my MacBook pro M1. However, I keep getting the following error when trying to import: <code>zsh: illegal hardware instruction  python</code></p>
<p>I have downloaded and installed tensorflow via this <a href=""https://github.com/apple/tensorflow_macos/releases/tag/v0.1alpha1"" rel=""nofollow noreferrer"">link</a>.</p>
<p>These were my installation steps:</p>
<ul>
<li>install a venv: <code>python3 -m venv venv</code>.</li>
<li>drag the <code>install_venv.sh</code> (which is located within the downloaded folder) file to the terminal, add <code>-p</code> at the end.</li>
<li>select the directory of the venv as the location where tensorflow should be installed.</li>
<li>activate the venv.</li>
<li>type &quot;python&quot;.</li>
<li>try to import tensorflow: <code>import tensorflow as tf</code>.</li>
</ul>
<p>I'm using Python 3.8.2.</p>
",14861092.0,,8293309.0,,2023-04-12 22:33:34,2023-04-12 22:33:34,"""zsh: illegal hardware instruction python"" when installing Tensorflow on macbook pro M1",<python><python-3.x><tensorflow><macos-big-sur><apple-silicon>,3,2,0.0,2021-09-05 10:25:03,,CC BY-SA 4.0
67120450,1,70700690.0,,2021-04-16 06:55:22,,33,94361,"<p>I was working on a sign language detection project on jupyter notebook. While running the code for live detection I encountered an error as shown below:</p>
<blockquote>
<p>OpenCV(4.5.1) C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-1drr4hl0\opencv\modules\highgui\src\window.cpp:651: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'</p>
</blockquote>
<p>The code that caused this error is:</p>
<pre class=""lang-py prettyprint-override""><code>while True: 
    ret, frame = cap.read()
    image_np = np.array(frame)
    
    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)
    detections = detect_fn(input_tensor)
    
    num_detections = int(detections.pop('num_detections'))
    detections = {key: value[0, :num_detections].numpy()
                  for key, value in detections.items()}
    detections['num_detections'] = num_detections

    # detection_classes should be ints.
    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)

    label_id_offset = 1
    image_np_with_detections = image_np.copy()

    viz_utils.visualize_boxes_and_labels_on_image_array(
                image_np_with_detections,
                detections['detection_boxes'],
                detections['detection_classes']+label_id_offset,
                detections['detection_scores'],
                category_index,
                use_normalized_coordinates=True,
                max_boxes_to_draw=5,
                min_score_thresh=.5,
                agnostic_mode=False)

    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))
    
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        cap.release()
        break
</code></pre>
<p>NB: I installed OpenCV using using pip install.</p>
",15657332.0,,10669875.0,,2022-06-12 05:48:56,2023-04-07 13:54:48,"error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support",<python><tensorflow><opencv><machine-learning><computer-vision>,13,1,0.0,,,CC BY-SA 4.0
74852225,1,,,2022-12-19 14:59:12,,32,44961,"<p>I am trying to install TensorFlow in Python. I am getting the following error message, I tried uninstalling NumPy and re-installing NumPy but still getting the same error message. How to resolve this issue?</p>
<pre><code>AttributeError: module 'numpy' has no attribute 'typeDict'
</code></pre>
",11861874.0,,4685471.0,,2023-05-23 01:00:49,2023-05-23 01:00:49,AttributeError: module 'numpy' has no attribute 'typeDict',<python><numpy><tensorflow>,6,1,,,,CC BY-SA 4.0
66022256,1,,,2021-02-03 06:20:42,,32,48056,"<p>When using tensorflow, I have the following error messages</p>
<pre><code>ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.'

File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 965, in _find_and_load_unlocked
ModuleNotFoundError: No module named 'tensorflow_core.estimator'
</code></pre>
<p>The installed tensorflow related packages are shown as following. Do I need to update the estimator's version? If that's the case, how to install the estimator with right version?</p>
<p><a href=""https://i.stack.imgur.com/1iGd8.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1iGd8.jpg"" alt=""enter image description here"" /></a></p>
",297850.0,,5516760.0,,2022-10-11 04:41:32,2022-10-11 04:41:32,ModuleNotFoundError: No module named 'tensorflow_core.estimator' for tensorflow 2.1.0,<python><tensorflow><tensorflow2.0><tensorflow-estimator>,8,1,0.0,,,CC BY-SA 4.0
65273118,1,65319255.0,,2020-12-13 06:54:08,,30,61734,"<p>I am new to deep learning and I have been trying to install tensorflow-gpu version in my pc in vain for the last 2 days. I avoided installing CUDA and cuDNN drivers since several forums online don't recommend it due to numerous compatibility issues. Since I was already using the conda distribution of python before, I went for the <code>conda install -c anaconda tensorflow-gpu</code> as written in their official website here: <a href=""https://anaconda.org/anaconda/tensorflow-gpu"" rel=""noreferrer"">https://anaconda.org/anaconda/tensorflow-gpu</a> .</p>
<p>However even after installing the gpu version in a fresh virtual environment (to avoid potential conflicts with pip installed libraries in the base env), tensorflow doesn't seem to even recognize my GPU for some mysterious reason.</p>
<p>Some of the code snippets I ran(in anaconda prompt) to understand that it wasn't recognizing my GPU:-</p>
<p>1.</p>
<pre><code>&gt;&gt;&gt;from tensorflow.python.client import device_lib
        &gt;&gt;&gt;print(device_lib.list_local_devices())
                    [name: &quot;/device:CPU:0&quot;
                device_type: &quot;CPU&quot;
                memory_limit: 268435456
                locality {
                }
                incarnation: 7692219132769779763
                ]
</code></pre>
<p>As you can see it completely ignores the GPU.</p>
<p>2.</p>
<pre><code>&gt;&gt;&gt;tf.debugging.set_log_device_placement(True)
    &gt;&gt;&gt;a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
2020-12-13 10:11:30.902956: I tensorflow/core/platform/cpu_feature_guard.cc:142] This 
TensorFlow 
binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU 
instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
&gt;&gt;&gt;b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
&gt;&gt;&gt;c = tf.matmul(a, b)
&gt;&gt;&gt;print(c)
tf.Tensor(
[[22. 28.]
[49. 64.]], shape=(2, 2), dtype=float32)
</code></pre>
<p>Here, it was supposed to indicate that it ran with a GPU by showing <code>Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0</code> (as written here: <a href=""https://www.tensorflow.org/guide/gpu"" rel=""noreferrer"">https://www.tensorflow.org/guide/gpu</a>) but nothing like that is present. Also I am not sure what the message after the 2nd line means.</p>
<p>I have also searched for several solutions online including here but almost all of the issues are related to the first manual installation method which I haven't tried yet since everyone recommended this approach.</p>
<p>I don't use cmd anymore since the environment variables somehow got messed up after uninstalling tensorflow-cpu from the base env and on re-installing, it worked perfectly with anaconda prompt but not cmd. This is a separate issue (and widespread also) but I mentioned it in case that has a role to play here. I installed the gpu version in a fresh virtual environment to ensure a clean installation and as far as I understand path variables need to be set up only for manual installation of CUDA and cuDNN libraries.</p>
<p>The card which I use:-(which is CUDA enabled)</p>
<pre><code>C:\WINDOWS\system32&gt;wmic path win32_VideoController get name
Name
NVIDIA GeForce 940MX
Intel(R) HD Graphics 620
</code></pre>
<p>Tensorflow and python version I am using currently:-</p>
<pre><code>&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.__version__
'2.3.0'

Python 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
</code></pre>
<p>System information: Windows 10 Home, 64-bit operating system, x64-based processor.</p>
<p>Any help would be really appreciated. Thanks in advance.</p>
",14372142.0,,,,,2022-04-27 14:53:21,Why is Tensorflow not recognizing my GPU after conda install?,<python><tensorflow><anaconda><gpu><path-variables>,11,4,0.0,,,CC BY-SA 4.0
65608713,1,65608751.0,,2021-01-07 08:10:51,,29,38917,"<p>When i run</p>
<pre><code>import tensorflow as tf 
tf.test.is_gpu_available(
    cuda_only=False, min_cuda_compute_capability=None
)
</code></pre>
<p>I get the following error</p>
<p><a href=""https://i.stack.imgur.com/Mv2p5.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Mv2p5.jpg"" alt=""enter image description here"" /></a></p>
",10900548.0,,,,,2021-05-31 16:23:57,Tensorflow GPU Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found,<python><tensorflow><gpu>,4,0,0.0,,,CC BY-SA 4.0
63199164,1,63203772.0,,2020-07-31 21:14:41,,29,28480,"<p>I am trying to install Tensorflow but it is asking for libcusolver.so.11 and I only have libcusolver.so.10. Can someone tell me what I am doing wrong</p>
<p>Here are my Ubuntu, nvidia and CUDA versions</p>
<pre><code>$ uname -a
$ Linux *****-dev-01 5.4.0-42-generic #46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

$nvidia-smi --query-gpu=gpu_name --format=csv|tail -n 1
GeForce GTX 1650

$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Thu_Jun_11_22:26:38_PDT_2020
Cuda compilation tools, release 11.0, V11.0.194
Build cuda_11.0_bu.TC445_37.28540450_0
</code></pre>
<p>Here is how I am building tensorflow</p>
<pre><code>$git clone https://github.com/tensorflow/tensorflow.git
$cd ./tensorflow
$git checkout tags/v2.2.0
$./configure
$bazel build --config=v2 --config=cuda --config=monolithic --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2 --copt=-Wno-sign-compare //        tensorflow:libtensorflow_cc.so
</code></pre>
<p>Here is the error I am receiving</p>
<pre><code>ERROR: An error occurred during the fetch of repository 'local_config_cuda':
    Traceback (most recent call last):
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 1210
         _create_local_cuda_repository(&lt;1 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 934, in _create_local_cuda_repository
         _find_libs(repository_ctx, &lt;2 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 577, in _find_libs
         _check_cuda_libs(repository_ctx, &lt;2 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 479, in _check_cuda_libs
         execute(repository_ctx, &lt;1 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/remote_config/common.bzl&quot;, line 208, in execute
         fail(&lt;1 more arguments&gt;)
 Repository command failed
 No library found under: /usr/local/cuda/lib64/libcusolver.so.11
 ERROR: Skipping '//tensorflow:libtensorflow_cc.so': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 1210
         _create_local_cuda_repository(&lt;1 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 934, in _create_local_cuda_repository
         _find_libs(repository_ctx, &lt;2 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 577, in _find_libs
         _check_cuda_libs(repository_ctx, &lt;2 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 479, in _check_cuda_libs
         execute(repository_ctx, &lt;1 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/remote_config/common.bzl&quot;, line 208, in execute
         fail(&lt;1 more arguments&gt;)
 Repository command failed
 No library found under: /usr/local/cuda/lib64/libcusolver.so.11
 WARNING: Target pattern parsing failed.
 ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 1210
         _create_local_cuda_repository(&lt;1 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 934, in _create_local_cuda_repository
         _find_libs(repository_ctx, &lt;2 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 577, in _find_libs
         _check_cuda_libs(repository_ctx, &lt;2 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl&quot;, line 479, in _check_cuda_libs
         execute(repository_ctx, &lt;1 more arguments&gt;)
     File &quot;/home/********/Documents/foo/.temp_install_dir/tensorflow/tensorflow/third_party/remote_config/common.bzl&quot;, line 208, in execute
         fail(&lt;1 more arguments&gt;)
 Repository command failed
 No library found under: /usr/local/cuda/lib64/libcusolver.so.11
 INFO: Elapsed time: 1.998s
 INFO: 0 processes.
 FAILED: Build did NOT complete successfully (0 packages loaded)
     currently loading: tensorflow
 NORMAL   test.log
</code></pre>
",654789.0,,654789.0,,2020-07-31 21:23:00,2022-08-24 20:39:30,How to install libcusolver.so.11,<tensorflow><cuda>,3,6,0.0,,,CC BY-SA 4.0
65697623,1,,,2021-01-13 07:49:22,,29,35071,"<p>I'm using tensorflow2.4, and new to tensorflow</p>
<p>Here's the code</p>
<pre><code>model = Sequential()
model.add(LSTM(32, input_shape=(X_train.shape[1:])))
model.add(Dropout(0.2))
model.add(Dense(1, activation='linear'))

model.compile(optimizer='rmsprop', loss='mean_absolute_error', metrics='mae')
model.summary()

save_weights_at = 'basic_lstm_model'
save_best = ModelCheckpoint(save_weights_at, monitor='val_loss', verbose=0,
                        save_best_only=True, save_weights_only=False, mode='min',
                        period=1)
history = model.fit(x=X_train, y=y_train, batch_size=16, epochs=20,
         verbose=1, callbacks=[save_best], validation_data=(X_val, y_val),
         shuffle=True)
</code></pre>
<p>And in some epochs, got this warning:
<a href=""https://i.stack.imgur.com/cWBlr.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/cWBlr.png"" alt=""enter image description here"" /></a></p>
<p>Do you know why did I get this warning?</p>
",2085454.0,,,,,2023-02-23 04:02:05,tensorflow warning - Found untraced functions such as lstm_cell_6_layer_call_and_return_conditional_losses,<tensorflow><warnings><lstm>,6,4,0.0,,,CC BY-SA 4.0
64193633,1,64193672.0,,2020-10-04 10:43:16,,28,79451,"<p>When I try to run a python script , which uses tensorflow, it shows following error ...</p>
<pre><code>2020-10-04 16:01:44.994797: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-04 16:01:46.780656: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-10-04 16:01:46.795642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:03:00.0 name: TITAN X (Pascal) computeCapability: 6.1
coreClock: 1.531GHz coreCount: 28 deviceMemorySize: 11.91GiB deviceMemoryBandwidth: 447.48GiB/s
2020-10-04 16:01:46.795699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-10-04 16:01:46.795808: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64/:/usr/local/cuda-10.0/lib64
2020-10-04 16:01:46.797391: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-10-04 16:01:46.797707: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-10-04 16:01:46.799529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-10-04 16:01:46.800524: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-10-04 16:01:46.804150: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-10-04 16:01:46.804169: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
</code></pre>
<p>Output of nvidia-smi</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  TITAN X (Pascal)    On   | 00000000:03:00.0 Off |                  N/A |
| 23%   28C    P8     9W / 250W |     18MiB / 12194MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1825      G   /usr/lib/xorg/Xorg                  9MiB |
|    0   N/A  N/A      1957      G   /usr/bin/gnome-shell                6MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>Tensorflow version 2.3.1,
Ubuntu - 18.04</p>
<p>I tried to completely remove cuda toolkit and install from scratch but the error remains.
Anybody could help me to identify the source of problem??</p>
",14191514.0,,681865.0,,2020-10-04 10:58:46,2023-03-07 17:53:01,Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory;,<tensorflow><gpu><ubuntu-18.04><nvidia>,7,1,0.0,,,CC BY-SA 4.0
66207609,1,66207610.0,,2021-02-15 11:55:42,,26,56649,"<p>tensorflow version 2.3.1
numpy version 1.20</p>
<p>below the code</p>
<pre><code># define model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
</code></pre>
<p>we got</p>
<blockquote>
<p>NotImplementedError: Cannot convert a symbolic Tensor
(lstm_2/strided_slice:0) to a numpy array. This error may indicate
that you're trying to pass a Tensor to a NumPy call, which is not
supported</p>
</blockquote>
<p>it seems to me a crazy error!</p>
",1645339.0,,,,,2022-03-13 02:12:52,NotImplementedError: Cannot convert a symbolic Tensor (lstm_2/strided_slice:0) to a numpy array. T,<python><numpy><tensorflow>,8,0,0.0,,,CC BY-SA 4.0
74956134,1,74976131.0,,2022-12-29 21:18:00,,23,35306,"<p>I know that this question has been asked a lot, but none of the suggestions seem to work, probably since my setup is somewhat different:</p>
<pre><code>Ubuntu          22.04
python          3.10.8
tensorflow      2.11.0
cudatoolkit     11.2.2
cudnn           8.1.0.77
nvidia-tensorrt 8.4.3.1
nvidia-pyindex  1.0.9
</code></pre>
<p>Having created a conda environment 'tf', in the directory <code>home/dan/anaconda3/envs/tf/lib/python3.10/site-packages/tensorrt</code> I have</p>
<pre><code>libnvinfer_builder_resource.so.8.4.3
libnvinfer_plugin.so.8
libnvinfer.so.8
libnvonnxparser.so.8
libnvparsers.so.8
tensorrt.so
</code></pre>
<p>When running <code>python3 -c &quot;import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))&quot;</code> I get</p>
<pre><code>tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7';
dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory;
LD_LIBRARY_PATH: :/home/dan/anaconda3/envs/tf/lib

tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7';
dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory;
LD_LIBRARY_PATH: :/home/dan/anaconda3/envs/tf/lib

tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
</code></pre>
<p>I'm guessing I should downgrade <code>nvidia-tensorrt</code>, but nothing I've tried seems to work, any advice would be much appreciated.</p>
",13220100.0,,1695960.0,,2022-12-29 23:00:31,2023-06-06 02:20:37,Could not load dynamic library 'libnvinfer.so.7',<tensorflow><nvidia><cudnn><tensorrt>,3,7,,,,CC BY-SA 4.0
68614547,1,68677582.0,,2021-08-01 21:37:27,,23,22516,"<p>Win 10 64-bit 21H1; TF2.5, CUDA 11 installed in environment (Python 3.9.5 Xeus)</p>
<p>I am not the only one seeing this error; see also (unanswered) <a href=""https://stackoverflow.com/questions/68289133/tensorflow-driver-issue-internal-libdevice-not-found-at-libdevice-10-bc"">here</a> and <a href=""https://stackoverflow.com/questions/62670495/using-xla-in-tensorflow-libdevice-10-bc-internaler"">here</a>.
The issue is obscure and the proposed resolutions are unclear/don't seem to work (see e.g. <a href=""https://stackoverflow.com/questions/54189268/how-to-let-tensorflow-xla-know-the-cuda-path"">here</a>)</p>
<p><strong>Issue</strong> Using the TF Linear_Mixed_Effects_Models.ipynb example (download from TensorFlow github <a href=""https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/jupyter_notebooks"" rel=""noreferrer"">here</a>) execution reaches the point of performing the &quot;warm up stage&quot; then throws the error:</p>
<pre><code>InternalError: libdevice not found at ./libdevice.10.bc [Op:__inference_one_e_step_2806]
</code></pre>
<p>The console contains this output showing that it finds the GPU but XLA initialisation fails to find the - existing! - libdevice in the specified paths</p>
<pre><code>2021-08-01 22:04:36.691300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9623 MB memory) -&gt; physical GPU (device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-08-01 22:04:37.080007: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
2021-08-01 22:04:54.122528: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x1d724940130 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-08-01 22:04:54.127766: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1080 Ti, Compute Capability 6.1
2021-08-01 22:04:54.215072: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:241] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired.
2021-08-01 22:04:55.506464: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
2021-08-01 22:04:55.512876: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:74] Searched for CUDA in the following directories:
2021-08-01 22:04:55.517387: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:77]   C:/Users/Julian/anaconda3/envs/TF250_PY395_xeus/Library/bin
2021-08-01 22:04:55.520773: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:77]   C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2
2021-08-01 22:04:55.524125: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:77]   .
2021-08-01 22:04:55.526349: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:79] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
</code></pre>
<p>Now the interesting thing is that the paths searched includes &quot;C:/Users/Julian/anaconda3/envs/TF250_PY395_xeus/Library/bin&quot;</p>
<p>the content of that folder includes all the (successfully loaded at TF startup) DLLs, including cudart64_110.dll, dudnn64_8.dll... and of course <strong>libdevice.10.bc</strong></p>
<p><strong>Question</strong> Since TF says it is searching this location for this file and the file exists there, what is wrong and how do I fix it?</p>
<p>(NB <code>C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.2</code> does not exist... CUDA is intalled in the environment; this path must be a best guess for an OS installation)</p>
<p>Info: I am setting the path by</p>
<pre><code>aPath = '--xla_gpu_cuda_data_dir=C:/Users/Julian/anaconda3/envs/TF250_PY395_xeus/Library/bin'
print(aPath)
os.environ['XLA_FLAGS'] = aPath
</code></pre>
<p>but I have also set an OS environment variable XLA_FLAGS to the same string value... I don't know which one is actually working yet, but the fact that the console output says it searched the intended path is good enough</p>
",4931852.0,,4931852.0,,2021-08-02 07:25:21,2023-04-12 21:10:19,TensorFlow libdevice not found. Why is it not found in the searched path?,<python><tensorflow><configuration>,8,2,,,,CC BY-SA 4.0
63078695,1,63106499.0,,2020-07-24 17:33:24,,22,10912,"<p>When trying to use the <code>hub.load</code> function from <code>tensorflow_hub</code>, I get an <code>OSError: SavedModel file does not exist at:</code> error.</p>
<p>The weird thing is that it worked a few days ago, so I don't quite understand why I'm getting this error now.</p>
<p>Code to reproduce:</p>
<pre><code>import tensorflow as tf
import tensorflow_hub as hub

URL = 'https://tfhub.dev/google/universal-sentence-encoder/4'
embed = hub.load(URL)
</code></pre>
<p>Specific error received:</p>
<pre><code>OSError                                   Traceback (most recent call last)
&lt;ipython-input-11-dfb80f0299b2&gt; in &lt;module&gt;
      1 URL = 'https://tfhub.dev/google/universal-sentence-encoder/4'
----&gt; 2 embed = hub.load(URL)

~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_hub/module_v2.py in load(handle, tags)
    100   if tags is None and is_hub_module_v1:
    101       tags = []
--&gt; 102   obj = tf_v1.saved_model.load_v2(module_path, tags=tags)
    103   obj._is_hub_module_v1 = is_hub_module_v1  # pylint: disable=protected-access
    104   return obj

~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags)
    576     ValueError: If `tags` don't match a MetaGraph in the SavedModel.
    577   &quot;&quot;&quot;
--&gt; 578   return load_internal(export_dir, tags)
    579 
    580 

~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, loader_cls)
    586     tags = nest.flatten(tags)
    587   saved_model_proto, debug_info = (
--&gt; 588       loader_impl.parse_saved_model_with_debug_info(export_dir))
    589 
    590   if (len(saved_model_proto.meta_graphs) == 1 and

~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in parse_saved_model_with_debug_info(export_dir)
     54     parsed. Missing graph debug info file is fine.
     55   &quot;&quot;&quot;
---&gt; 56   saved_model = _parse_saved_model(export_dir)
     57 
     58   debug_info_path = os.path.join(

~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in parse_saved_model(export_dir)
    111                   (export_dir,
    112                    constants.SAVED_MODEL_FILENAME_PBTXT,
--&gt; 113                    constants.SAVED_MODEL_FILENAME_PB))
    114 
    115 

OSError: SavedModel file does not exist at: /var/folders/77/rvfl368x44s51r8dc3b6l2rh0000gn/T/tfhub_modules/063d866c06683311b44b4992fd46003be952409c/{saved_model.pbtxt|saved_model.pb}
</code></pre>
",4517091.0,,,,,2022-01-21 18:33:39,SavedModel file does not exist when using Tensorflow hub,<file><tensorflow><directory><tensorflow-hub><oserror>,4,0,0.0,,,CC BY-SA 4.0
71000120,1,71039779.0,,2022-02-05 16:53:19,,20,41050,"<p>I have pretrained model for object detection (Google Colab + TensorFlow) inside Google Colab and I run it two-three times per week for new images I have and everything was fine for the last year till this week. Now when I try to run model I have this message:</p>
<pre><code>Graph execution error:

2 root error(s) found.
  (0) UNIMPLEMENTED:  DNN library is not found.
     [[{{node functional_1/conv1_conv/Conv2D}}]]
     [[StatefulPartitionedCall/SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/MultiClassNonMaxSuppression/Reshape_5/_126]]
  (1) UNIMPLEMENTED:  DNN library is not found.
     [[{{node functional_1/conv1_conv/Conv2D}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_restored_function_body_27380] ***
</code></pre>
<p>Never happended before.</p>
<p>Before I can run my model I have to install Tensor Flow object detection API with this command:</p>
<pre><code>import os

os.chdir('/project/models/research')

!protoc object_detection/protos/*.proto --python_out=.
!cp object_detection/packages/tf2/setup.py .
!python -m pip install .
</code></pre>
<p>This is the output of command:</p>
<pre><code>Processing /content/gdrive/MyDrive/models/research
  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.
   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.
Collecting avro-python3
  Downloading avro-python3-1.10.2.tar.gz (38 kB)
Collecting apache-beam
  Downloading apache_beam-2.35.0-cp37-cp37m-manylinux2010_x86_64.whl (9.9 MB)
     |████████████████████████████████| 9.9 MB 1.6 MB/s
Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)
Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)
Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)
Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.27)
Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)
Collecting tf-slim
  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)
     |████████████████████████████████| 352 kB 50.5 MB/s
Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)
Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.4)
Collecting lvis
  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)
Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)
Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.3.5)
Collecting tf-models-official&gt;=2.5.1
  Downloading tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)
     |████████████████████████████████| 2.2 MB 38.3 MB/s
Collecting tensorflow_io
  Downloading tensorflow_io-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.4 MB)
     |████████████████████████████████| 23.4 MB 1.7 MB/s
Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.7.0)
Collecting opencv-python-headless
  Downloading opencv_python_headless-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.7 MB)
     |████████████████████████████████| 47.7 MB 74 kB/s
Collecting sacrebleu
  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)
     |████████████████████████████████| 90 kB 10.4 MB/s
Requirement already satisfied: kaggle&gt;=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.5.12)
Requirement already satisfied: psutil&gt;=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (5.4.8)
Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (4.1.3)
Collecting tensorflow-addons
  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)
     |████████████████████████████████| 1.1 MB 37.8 MB/s
Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.5.0)
Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (4.0.1)
Collecting sentencepiece
  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
     |████████████████████████████████| 1.2 MB 37.5 MB/s
Collecting tensorflow-model-optimization&gt;=0.4.1
  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)
     |████████████████████████████████| 213 kB 42.7 MB/s
Collecting pyyaml&lt;6.0,&gt;=5.1
  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)
     |████████████████████████████████| 636 kB 53.3 MB/s
Collecting tensorflow-text~=2.8.0
  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)
     |████████████████████████████████| 4.9 MB 46.1 MB/s
Requirement already satisfied: google-api-python-client&gt;=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.12.10)
Requirement already satisfied: numpy&gt;=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.19.5)
Requirement already satisfied: tensorflow-hub&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.12.0)
Collecting seqeval
  Downloading seqeval-1.2.2.tar.gz (43 kB)
     |████████████████████████████████| 43 kB 2.1 MB/s
Collecting tensorflow~=2.8.0
  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)
     |████████████████████████████████| 497.5 MB 28 kB/s
Collecting py-cpuinfo&gt;=3.3.0
  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)
     |████████████████████████████████| 99 kB 10.1 MB/s
Requirement already satisfied: google-auth&lt;3dev,&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.35.0)
Requirement already satisfied: uritemplate&lt;4dev,&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.0.1)
Requirement already satisfied: httplib2&lt;1dev,&gt;=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.17.4)
Requirement already satisfied: google-auth-httplib2&gt;=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.0.4)
Requirement already satisfied: google-api-core&lt;3dev,&gt;=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.26.3)
Requirement already satisfied: setuptools&gt;=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (57.4.0)
Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2018.9)
Requirement already satisfied: googleapis-common-protos&lt;2.0dev,&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.54.0)
Requirement already satisfied: requests&lt;3.0.0dev,&gt;=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2.23.0)
Requirement already satisfied: packaging&gt;=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (21.3)
Requirement already satisfied: protobuf&gt;=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.17.3)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3dev,&gt;=1.16.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.2.8)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3dev,&gt;=1.16.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (4.8)
Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth&lt;3dev,&gt;=1.16.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (4.2.4)
Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2021.10.8)
Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.24.3)
Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2.8.2)
Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (4.62.3)
Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (5.0.2)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=14.3-&gt;google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.0.7)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;3dev,&gt;=1.16.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.4.8)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2.10)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.0.4)
Requirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.1.0)
Requirement already satisfied: libclang&gt;=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (13.0.0)
Requirement already satisfied: h5py&gt;=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.1.0)
Requirement already satisfied: astunparse&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.6.3)
Requirement already satisfied: gast&gt;=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.4.0)
Requirement already satisfied: google-pasta&gt;=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.2.0)
Requirement already satisfied: typing-extensions&gt;=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.10.0.2)
Requirement already satisfied: wrapt&gt;=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.13.3)
Requirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.23.1)
Collecting tf-estimator-nightly==2.8.0.dev2021122109
  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)
     |████████████████████████████████| 462 kB 49.5 MB/s
Requirement already satisfied: keras-preprocessing&gt;=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.1.2)
Collecting tensorboard&lt;2.9,&gt;=2.8
  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)
     |████████████████████████████████| 5.8 MB 41.2 MB/s
Requirement already satisfied: flatbuffers&gt;=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2.0)
Collecting keras
  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)
     |████████████████████████████████| 1.4 MB 41.2 MB/s
Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.3.0)
Collecting numpy&gt;=1.15.4
  Downloading numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)
     |████████████████████████████████| 15.7 MB 41.4 MB/s
Requirement already satisfied: absl-py&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.0.0)
Requirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.43.0)
Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse&gt;=1.6.0-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.37.1)
Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py&gt;=2.9.0-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.5.2)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.6.1)
Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.0.1)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.4.6)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.8.1)
Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.3.6)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.3.1)
Requirement already satisfied: importlib-metadata&gt;=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown&gt;=2.6.8-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (4.10.1)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata&gt;=4.4-&gt;markdown&gt;=2.6.8-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.7.0)
Requirement already satisfied: oauthlib&gt;=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.2.0)
Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization&gt;=0.4.1-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.1.6)
Requirement already satisfied: crcmod&lt;2.0,&gt;=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (1.7)
Collecting fastavro&lt;2,&gt;=0.21.4
  Downloading fastavro-1.4.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)
     |████████████████████████████████| 2.3 MB 38.1 MB/s
Requirement already satisfied: pyarrow&lt;7.0.0,&gt;=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (6.0.1)
Requirement already satisfied: pydot&lt;2,&gt;=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam-&gt;object-detection==0.1) (1.3.0)
Collecting proto-plus&lt;2,&gt;=1.7.1
  Downloading proto_plus-1.19.9-py3-none-any.whl (45 kB)
     |████████████████████████████████| 45 kB 3.2 MB/s
Collecting requests&lt;3.0.0dev,&gt;=2.18.0
  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)
     |████████████████████████████████| 63 kB 1.8 MB/s
Collecting dill&lt;0.3.2,&gt;=0.3.1.1
  Downloading dill-0.3.1.1.tar.gz (151 kB)
     |████████████████████████████████| 151 kB 44.4 MB/s
Collecting numpy&gt;=1.15.4
  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)
     |████████████████████████████████| 15.3 MB 21.1 MB/s
Collecting orjson&lt;4.0
  Downloading orjson-3.6.6-cp37-cp37m-manylinux_2_24_x86_64.whl (245 kB)
     |████████████████████████████████| 245 kB 53.2 MB/s
Collecting hdfs&lt;3.0.0,&gt;=2.1.0
  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)
Collecting pymongo&lt;4.0.0,&gt;=3.8.0
  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)
     |████████████████████████████████| 508 kB 44.3 MB/s
Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs&lt;3.0.0,&gt;=2.1.0-&gt;apache-beam-&gt;object-detection==0.1) (0.6.2)
Collecting protobuf&gt;=3.12.0
  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)
     |████████████████████████████████| 1.1 MB 47.3 MB/s
Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests&lt;3.0.0dev,&gt;=2.18.0-&gt;google-api-core&lt;3dev,&gt;=1.21.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2.0.11)
Requirement already satisfied: opencv-python&gt;=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis-&gt;object-detection==0.1) (4.1.2.30)
Requirement already satisfied: cycler&gt;=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis-&gt;object-detection==0.1) (0.11.0)
Requirement already satisfied: kiwisolver&gt;=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis-&gt;object-detection==0.1) (1.3.2)
Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle&gt;=1.3.9-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.3)
Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2019.12.20)
Requirement already satisfied: tabulate&gt;=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.8.9)
Collecting portalocker
  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)
Collecting colorama
  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)
Requirement already satisfied: scikit-learn&gt;=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.0.2)
Requirement already satisfied: joblib&gt;=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.21.3-&gt;seqeval-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.1.0)
Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn&gt;=0.21.3-&gt;seqeval-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (3.1.0)
Requirement already satisfied: typeguard&gt;=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2.7.1)
Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (2.3)
Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (0.16.0)
Requirement already satisfied: attrs&gt;=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (21.4.0)
Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (5.4.0)
Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets-&gt;tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.6.0)
Collecting tensorflow-io-gcs-filesystem&gt;=0.23.1
  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)
     |████████████████████████████████| 2.1 MB 40.9 MB/s
Building wheels for collected packages: object-detection, py-cpuinfo, dill, avro-python3, seqeval
  Building wheel for object-detection (setup.py) ... done
  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1686316 sha256=775b8c34c800b3b3139d1067abd686af9ce9158011fccfb5450ccfd9bf424a5a
  Stored in directory: /tmp/pip-ephem-wheel-cache-rmw0fvil/wheels/d0/e3/e9/b9ffe85019ec441e90d8ff9eddee9950c4c23b7598204390b9
  Building wheel for py-cpuinfo (setup.py) ... done
  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=ac956c4c039868fdba78645bea056754e667e8840bea783ad2ca75e4d3e682c6
  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401
  Building wheel for dill (setup.py) ... done
  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=d9c6cdfd69aea2b4d78e6afbbe2bc530394e4081eb186eb4f4cd02373ca739fd
  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f
  Building wheel for avro-python3 (setup.py) ... done
  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=4eca8b4f30e4850d5dabccee36c40c8dda8a6c7e7058cfb7f0258eea5ce7b2b3
  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da
  Building wheel for seqeval (setup.py) ... done
  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=0ddfa46d0e36e9be346a90833ef11cc0d38cc7e744be34c5a0d321f997a30cba
  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7
Successfully built object-detection py-cpuinfo dill avro-python3 seqeval
Installing collected packages: requests, protobuf, numpy, tf-estimator-nightly, tensorflow-io-gcs-filesystem, tensorboard, keras, tensorflow, portalocker, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, py-cpuinfo, proto-plus, orjson, opencv-python-headless, hdfs, fastavro, tf-models-official, tensorflow-io, lvis, avro-python3, apache-beam, object-detection
  Attempting uninstall: requests
    Found existing installation: requests 2.23.0
    Uninstalling requests-2.23.0:
      Successfully uninstalled requests-2.23.0
  Attempting uninstall: protobuf
    Found existing installation: protobuf 3.17.3
    Uninstalling protobuf-3.17.3:
      Successfully uninstalled protobuf-3.17.3
  Attempting uninstall: numpy
    Found existing installation: numpy 1.19.5
    Uninstalling numpy-1.19.5:
      Successfully uninstalled numpy-1.19.5
  Attempting uninstall: tensorflow-io-gcs-filesystem
    Found existing installation: tensorflow-io-gcs-filesystem 0.23.1
    Uninstalling tensorflow-io-gcs-filesystem-0.23.1:
      Successfully uninstalled tensorflow-io-gcs-filesystem-0.23.1
  Attempting uninstall: tensorboard
    Found existing installation: tensorboard 2.7.0
    Uninstalling tensorboard-2.7.0:
      Successfully uninstalled tensorboard-2.7.0
  Attempting uninstall: keras
    Found existing installation: keras 2.7.0
    Uninstalling keras-2.7.0:
      Successfully uninstalled keras-2.7.0
  Attempting uninstall: tensorflow
    Found existing installation: tensorflow 2.7.0
    Uninstalling tensorflow-2.7.0:
      Successfully uninstalled tensorflow-2.7.0
  Attempting uninstall: dill
    Found existing installation: dill 0.3.4
    Uninstalling dill-0.3.4:
      Successfully uninstalled dill-0.3.4
  Attempting uninstall: pyyaml
    Found existing installation: PyYAML 3.13
    Uninstalling PyYAML-3.13:
      Successfully uninstalled PyYAML-3.13
  Attempting uninstall: pymongo
    Found existing installation: pymongo 4.0.1
    Uninstalling pymongo-4.0.1:
      Successfully uninstalled pymongo-4.0.1
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
yellowbrick 1.3.post1 requires numpy&lt;1.20,&gt;=1.16.0, but you have numpy 1.20.3 which is incompatible.
multiprocess 0.70.12.2 requires dill&gt;=0.3.4, but you have dill 0.3.1.1 which is incompatible.
google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.
datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.
albumentations 0.1.12 requires imgaug&lt;0.2.7,&gt;=0.2.5, but you have imgaug 0.2.9 which is incompatible.
Successfully installed apache-beam-2.35.0 avro-python3-1.10.2 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.9 hdfs-2.6.0 keras-2.8.0 lvis-0.5.3 numpy-1.20.3 object-detection-0.1 opencv-python-headless-4.5.5.62 orjson-3.6.6 portalocker-2.3.2 proto-plus-1.19.9 protobuf-3.19.4 py-cpuinfo-8.0.0 pymongo-3.12.3 pyyaml-5.4.1 requests-2.27.1 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorboard-2.8.0 tensorflow-2.8.0 tensorflow-addons-0.15.0 tensorflow-io-0.24.0 tensorflow-io-gcs-filesystem-0.24.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.8.1 tf-estimator-nightly-2.8.0.dev2021122109 tf-models-official-2.8.0 tf-slim-1.1.0
</code></pre>
<p>I am noticing that this command uninstalling tensorflow 2.7 and installing tensorflow 2.8. I am not sure it was happening before. Maybe it's the reason DNN library link is missing o something?</p>
<p>I can confirm these:</p>
<ol>
<li>Nothing was changed inside pretrained model or already installed model or object_detection source files I downloaded a year ago.</li>
<li>I tried to run command !pip install dnn - not working</li>
<li>I tried to restart runtime (without disconnecting) - not working</li>
</ol>
<p>Somebody can help? Thanks.</p>
",12974287.0,,,,,2022-10-04 19:28:36,Colab: (0) UNIMPLEMENTED: DNN library is not found,<tensorflow><google-colaboratory><object-detection>,6,3,0.0,,,CC BY-SA 4.0
70967651,1,,,2022-02-03 07:59:47,,20,74027,"<p>I am trying to use Tensorflow 2.7.0 with GPU, but I am constantly running into the same issue:</p>
<pre><code>2022-02-03 08:32:31.822484: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/username/.cache/pypoetry/virtualenvs/poetry_env/lib/python3.7/site-packages/cv2/../../lib64:/home/username/miniconda3/envs/project/lib/
2022-02-03 08:32:31.822528: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
</code></pre>
<p>This issue has already appeared multiple times here &amp; on github. However, the solutions usually proposed to a) download the missing CUDA files, b) downgrade/upgrade to the correct CUDA version, c) set the correct <code>LD_LIBRARY_PATH</code>.</p>
<p>I have been already using my PC with CUDA-enabled PyTorch, and I did not have a single issue there. My <code>nvidia-smi</code> returns 11.0 version, which is exactly the only I want to have. Also, if I try to run:</p>
<pre><code>import os
LD_LIBRARY_PATH = '/home/username/miniconda3/envs/project/lib/'
print(os.path.exists(os.path.join(LD_LIBRARY_PATH, &quot;libcudart.so.11.0&quot;)))
</code></pre>
<p>it returns <code>True</code>. This is exactly the part of <code>LD_LIBRARY_PATH</code> from the error message, where Tensorflow, apparently, cannot see the <code>libcudart.so.11.0</code> (which IS there).</p>
<p>Is there something really obvious that I am missing?</p>
<p><code>nvidia-smi</code> output:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.156.00   Driver Version: 450.156.00   CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
</code></pre>
<p><code>nvcc</code>:</p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
</code></pre>
",8037521.0,,681865.0,,2022-02-03 09:24:44,2023-04-29 12:27:20,Could not load dynamic library 'libcudart.so.11.0';,<tensorflow>,4,1,0.0,,,CC BY-SA 4.0
65230805,1,,,2020-12-10 08:19:15,,19,27700,"<p>I'm working with tensorflow. Recently Arch replaced Python 3.8 with 3.9 and at the moment there is no tensorflow build for Python 3.9. Downgrading Python version for the whole system for that single reason do not looks like good idea for me. My goal is to create virtual environment with python 3.8.
Is there a way to have both (3.8 and 3.9) versions available in the system? Python page of arch wiki doesn't mention that.</p>
<p>EDIT:</p>
<p>I know, I can use: <code>virtualenv -p python3.8 py38</code> but I need an interpreter in the system.</p>
",3247880.0,,3247880.0,,2020-12-10 11:06:16,2021-09-30 20:02:01,How to install Python 3.8 along with Python 3.9 in Arch Linux?,<python><linux><tensorflow><virtualenv><archlinux>,2,7,0.0,,,CC BY-SA 4.0
75614728,1,75762138.0,,2023-03-02 11:19:35,,19,21896,"<ul>
<li><strong>tf-nightly version</strong> = 2.12.0-dev2023203</li>
<li><strong>Python version</strong> = 3.10.6</li>
<li><strong>CUDA drivers version</strong> = 525.85.12</li>
<li><strong>CUDA version</strong> = 12.0</li>
<li><strong>Cudnn version</strong> = 8.5.0</li>
<li>I am using <strong>Linux</strong> (x86_64, Ubuntu 22.04)</li>
<li>I am coding in <strong>Visual Studio Code</strong> on a <strong>venv</strong> virtual environment</li>
</ul>
<p>I am trying to run some models on the GPU (NVIDIA GeForce RTX 3050) using tensorflow nightly 2.12 (to be able to use Cuda 12.0). The problem that I have is that apparently every checking that I am making seems to be correct, but in the end the script is not able to detect the GPU. I've dedicated a lot of time trying to see what is happening and nothing seems to work, so any advice or solution will be more than welcomed. The GPU seems to be working for torch as you can see at the very end of the question.</p>
<p>I will show some of the most common checkings regarding CUDA that I did (Visual Studio Code terminal), I hope you find them useful:</p>
<ol>
<li><p><strong>Check CUDA version:</strong></p>
<p><code>$ nvcc --version</code></p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Fri_Jan__6_16:45:21_PST_2023
Cuda compilation tools, release 12.0, V12.0.140
Build cuda_12.0.r12.0/compiler.32267302_0
</code></pre>
</li>
<li><p><strong>Check if the connection with the CUDA libraries is correct:</strong></p>
<p><code>$ echo $LD_LIBRARY_PATH</code></p>
<pre><code>/usr/cuda/lib
</code></pre>
</li>
<li><p><strong>Check nvidia drivers for the GPU and check if GPU is readable for the venv:</strong></p>
<p><code>$ nvidia-smi</code></p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |
| N/A   40C    P5     6W /  20W |     46MiB /  4096MiB |     22%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1356      G   /usr/lib/xorg/Xorg                 45MiB |
+-----------------------------------------------------------------------------+
</code></pre>
</li>
<li><p><strong>Add cuda/bin PATH and Check it:</strong></p>
<p><code>$ export PATH=&quot;/usr/local/cuda/bin:$PATH&quot;</code></p>
<p><code>$ echo $PATH</code></p>
<pre><code>/usr/local/cuda-12.0/bin:/home/victus-linux/Escritorio/MasterThesis_CODE/to_share/venv_master/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin
</code></pre>
</li>
<li><p><strong>Custom function to check if CUDA is correctly installed: [<a href=""https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation"">function by Sherlock</a>]</strong></p>
<pre class=""lang-bash prettyprint-override""><code>function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' &lt;&lt;&lt; $LD_LIBRARY_PATH) 2&gt;/dev/null | grep $1; }
function check() { lib_installed $1 &amp;&amp; echo &quot;$1 is installed&quot; || echo &quot;ERROR: $1 is NOT installed&quot;; }
check libcuda
check libcudart
</code></pre>
<pre><code>libcudart.so.12 -&gt; libcudart.so.12.0.146
        libcuda.so.1 -&gt; libcuda.so.525.85.12
        libcuda.so.1 -&gt; libcuda.so.525.85.12
        libcudadebugger.so.1 -&gt; libcudadebugger.so.525.85.12
libcuda is installed
        libcudart.so.12 -&gt; libcudart.so.12.0.146
libcudart is installed
</code></pre>
</li>
<li><p><strong>Custom function to check if Cudnn is correctly installed: [<a href=""https://stackoverflow.com/questions/31326015/how-to-verify-cudnn-installation"">function by Sherlock</a>]</strong></p>
<pre class=""lang-bash prettyprint-override""><code>function lib_installed() { /sbin/ldconfig -N -v $(sed 's/:/ /' &lt;&lt;&lt; $LD_LIBRARY_PATH) 2&gt;/dev/null | grep $1; }
function check() { lib_installed $1 &amp;&amp; echo &quot;$1 is installed&quot; || echo &quot;ERROR: $1 is NOT installed&quot;; }
check libcudnn 
</code></pre>
<pre><code>        libcudnn_cnn_train.so.8 -&gt; libcudnn_cnn_train.so.8.8.0
        libcudnn_cnn_infer.so.8 -&gt; libcudnn_cnn_infer.so.8.8.0
        libcudnn_adv_train.so.8 -&gt; libcudnn_adv_train.so.8.8.0
        libcudnn.so.8 -&gt; libcudnn.so.8.8.0
        libcudnn_ops_train.so.8 -&gt; libcudnn_ops_train.so.8.8.0
        libcudnn_adv_infer.so.8 -&gt; libcudnn_adv_infer.so.8.8.0
        libcudnn_ops_infer.so.8 -&gt; libcudnn_ops_infer.so.8.8.0
libcudnn is installed
</code></pre>
</li>
</ol>
<p>So, once I did these previous checkings I used a script to evaluate if everything was finally ok and then the following error appeared:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf

print(f'\nTensorflow version = {tf.__version__}\n')
print(f'\n{tf.config.list_physical_devices(&quot;GPU&quot;)}\n')
</code></pre>
<pre><code>2023-03-02 12:05:09.463343: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-03-02 12:05:09.489911: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.
2023-03-02 12:05:09.490522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-02 12:05:10.066759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

Tensorflow version = 2.12.0-dev20230203

2023-03-02 12:05:10.748675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-03-02 12:05:10.771263: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...

[]
</code></pre>
<p><strong>Extra check:</strong> I tried to run a checking script on torch and in here it worked so I guess the problem is related with tensorflow/tf-nightly</p>
<pre class=""lang-py prettyprint-override""><code>import torch

print(f'\nAvailable cuda = {torch.cuda.is_available()}')

print(f'\nGPUs availables = {torch.cuda.device_count()}')

print(f'\nCurrent device = {torch.cuda.current_device()}')

print(f'\nCurrent Device location = {torch.cuda.device(0)}')

print(f'\nName of the device = {torch.cuda.get_device_name(0)}')
</code></pre>
<pre><code>Available cuda = True

GPUs availables = 1

Current device = 0

Current Device location = &lt;torch.cuda.device object at 0x7fbe26fd2ec0&gt;

Name of the device = NVIDIA GeForce RTX 3050 Laptop GPU
</code></pre>
<p>Please, if you know something that might help solve this issue, don't hesitate on telling me.</p>
",21117172.0,,21117172.0,,2023-05-09 09:23:17,2023-05-09 09:23:17,"Cuda 12 + tf-nightly 2.12: Could not find cuda drivers on your machine, GPU will not be used, while every checking is fine and in torch it works",<python><tensorflow><gpu>,3,4,,,,CC BY-SA 4.0
64880546,1,66632156.0,,2020-11-17 17:45:29,,19,61954,"<p>I have</p>
<pre><code>$ python3 -c &quot;import tensorflow as tf;print(tf.__version__)&quot;
1.15.0  
</code></pre>
<p>and</p>
<pre><code>$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
</code></pre>
<p>with</p>
<pre><code>python --version
Python 3.6.9
pip --version
pip 19.3.1 from /usr/local/lib/python3.6/dist-packages/pip (python 3.6)
</code></pre>
<p>but I see <code>CUDA 10.2</code> from <code>nvidia-smi</code></p>
<pre><code>$ nvidia-smi
Tue Nov 17 18:40:54 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 2080    On   | 00000000:01:00.0 Off |                  N/A |
| 32%   42C    P2    56W / 215W |    265MiB /  7979MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1840      G   /usr/lib/xorg/Xorg                            57MiB |
|    0      1895      G   /usr/bin/gnome-shell                          85MiB |
|    0     29999      C   /usr/bin/python                              109MiB |
+-----------------------------------------------------------------------------+
</code></pre>
<p>I can see</p>
<pre><code>$ ls /usr/local/
bin  cuda  cuda-10.1  cuda-10.2  etc  games  include  lib  man  sbin  share  src
</code></pre>
<p>and in the <code>.profile</code> I can see</p>
<pre><code># set PATH for cuda 10.2 installation
if [ -d &quot;/usr/local/cuda-10.2/bin/&quot; ]; then
    export PATH=/usr/local/cuda-10.2/bin${PATH:+:${PATH}}
    export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
fi
</code></pre>
<p>so I did override the <code>PATH</code> and <code>LD_LIBRARY_PATH</code> to</p>
<pre><code>export PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}
export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
</code></pre>
<p>but it does not seem to fix.</p>
<pre><code>2020-11-17 18:38:39.470074: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
    2020-11-17 18:38:39.487544: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3000000000 Hz
    2020-11-17 18:38:39.489215: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47007e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
    2020-11-17 18:38:39.489273: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
    2020-11-17 18:38:39.494309: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
    2020-11-17 18:38:39.542010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2020-11-17 18:38:39.542387: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4b1bf40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
    2020-11-17 18:38:39.542399: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
    2020-11-17 18:38:39.542519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
    2020-11-17 18:38:39.542788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
    name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.71
    pciBusID: 0000:01:00.0
    2020-11-17 18:38:39.542872: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
    2020-11-17 18:38:39.542919: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
    2020-11-17 18:38:39.543012: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
    2020-11-17 18:38:39.543059: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
    2020-11-17 18:38:39.543093: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
    2020-11-17 18:38:39.543125: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
    2020-11-17 18:38:39.545590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
    2020-11-17 18:38:39.545617: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
    Skipping registering GPU devices...
    2020-11-17 18:38:39.545653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
    2020-11-17 18:38:39.545658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
    2020-11-17 18:38:39.545662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
    ['/device:CPU:0', '/device:XLA_CPU:0', '/device:XLA_GPU:0']
</code></pre>
",758836.0,,758836.0,,2020-11-30 09:30:24,2021-06-13 00:45:47,Tensorflow Could not load dynamic library 'libcudart.so.10.0 on ubuntu 18.04,<tensorflow><ubuntu-18.04>,1,4,0.0,,,CC BY-SA 4.0
68844792,1,,,2021-08-19 08:54:48,,18,22264,"<p>I am running the following code for LSTM on Databricks with GPU</p>
<pre><code>model = Sequential()
model.add(LSTM(64, activation=LeakyReLU(alpha=0.05), batch_input_shape=(1, timesteps, n_features), 
    stateful=False, return_sequences = True))
model.add(Dropout(0.2))
model.add(LSTM(32))
model.add(Dropout(0.2))
model.add(Dense(n_features))
model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate = 0.001), metrics='acc')
model.fit(generator, epochs=epochs, verbose=0, shuffle=False)
</code></pre>
<p>but the following warning keeps appearing</p>
<pre><code>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
</code></pre>
<p>It trains much slower than it does without a GPU.
I'm using DBR 9.0 ML (includes Apache Spark 3.1.2, GPU, Scala 2.12)
Do I need any additional libraries for this?</p>
",16492842.0,,,,,2022-03-18 08:21:03,lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU,<python><tensorflow><lstm><databricks>,2,2,0.0,,,CC BY-SA 4.0
63059979,1,63924057.0,,2020-07-23 17:33:57,,18,15199,"<p>I am trying to install Tensorflow 1.14 for a package that I am trying to use. I tried:
<code>pip3 uninstall tensorflow</code></p>
<p>Then I tried to install Tensorflow 1.14 using:
<code>pip3 install tensorflow==1.14</code></p>
<p>and I get the following error
<code>ERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2) ERROR: No matching distribution found for tensorflow==1.14</code></p>
<p>I also tried making a new virtual env and tried the following commands but it didn't work. Is there any way to install Tensorflow 1?</p>
",12248034.0,,,,,2022-11-16 09:01:38,Cannot install TensorFlow 1.x,<python><tensorflow>,6,2,,,,CC BY-SA 4.0
67457480,1,,,2021-05-09 12:05:21,,18,11324,"<p>I'm using the huggingface Trainer with <code>BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)</code> model.</p>
<p>Simplified, it looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>model = BertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)

training_args = TrainingArguments(
        output_dir=&quot;bert_results&quot;,
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=32,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir=&quot;bert_results/logs&quot;,
        logging_steps=10
        )

trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
        )
</code></pre>
<p>The logs contain the loss for each 10 steps, but I can't seem to find the training accuracy. Does anyone know how to get the accuracy, for example by changing the verbosity of the logger? I can't seem to find anything about it online.</p>
",12456230.0,,5446749.0,,2023-03-29 07:12:21,2023-03-29 07:12:21,How to get the accuracy per epoch or step for the huggingface.transformers Trainer?,<python><tensorflow><logging><huggingface-transformers>,4,0,0.0,,,CC BY-SA 4.0
66271988,1,,,2021-02-19 04:54:20,,17,14197,"<p>Anyone know the reason for this error?</p>
<pre><code>WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:11 out of the last 11 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001F9D1C05EE0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
WARNING:tensorflow:11 out of the last 11 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x000001F9D5604670&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
C:\Users\User\anaconda3\lib\site-packages\sklearn\cluster\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.
  warnings.warn(&quot;'n_jobs' was deprecated in version 0.23 and will be&quot;
</code></pre>
",14598274.0,,,,,2023-01-31 06:52:24,WARNING:tensorflow:11 out of the last 11 calls to triggered tf.function retracing,<python><tensorflow><jupyter-notebook><warnings>,3,0,0.0,,,CC BY-SA 4.0
66092421,1,66646326.0,,2021-02-07 19:50:56,,17,67174,"<p>I am trying to run binary classification with a tensorflow backend but I keep receiving an error that I believe asks me to rebuild tensorflow with the right compiler flags. I know that my code and data is functionally, so I think the problem is with virtual environment. I have tried finding solutions on tensorflow's website, ibm's website, and stack overflow, but I haven't been successful. I have also tried reinstalling tensorflow and python.</p>
<p>Full Traceback:</p>
<p><code>I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set</code></p>
<p><code>I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA</code></p>
<p><code>To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.</code></p>
<p>Virtual Environment Info:</p>
<p>-using a anaconda environment</p>
<p>-Python 3.7.9</p>
<p>-tensorflow 2.4.1</p>
",15047180.0,,4685471.0,,2021-02-07 20:44:30,2023-02-02 14:22:16,How to rebuild tensorflow with the compiler flags?,<python><tensorflow>,2,0,0.0,,,CC BY-SA 4.0
63601580,1,63601677.0,,2020-08-26 16:11:34,,17,46571,"<p>I'm trying to use opencv-python with GPU on windows 10.</p>
<p>I installed opencv-contrib-python using pip and it's v4.4.0.42, I also have Cuda on my computer and in path.</p>
<p>Anyway, here is a (simple) code that I'm trying to compile:</p>
<pre><code>import cvlib as cv
from cvlib.object_detection import draw_bbox

bbox, label, conf = cv.detect_common_objects(img,confidence=0.5,model='yolov3-worker',enable_gpu=True)

output_image = draw_bbox(img, bbox, label, conf)
</code></pre>
<p>First, here is the line that tell me that tf is ok with cuda:</p>
<pre><code>2020-08-26 5:51:55.718555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
</code></pre>
<p>but when I try to use my GPU to analyse the image, here is what happen:</p>
<pre><code>[ WARN:0] global C:\Users\appveyor\AppData\Local\Temp\1\pip-req-build-j8nxabm_\opencv\modules\dnn\src\dnn.cpp (1429) cv::dnn::dnn4_v20200609::Net::Impl::setUpNet DNN module was not built with CUDA backend; switching to CPU
</code></pre>
<p>Is there a way to solve this without install opencv using cmake? It's a mess on windows...</p>
",13350827.0,,6885902.0,,2022-08-26 14:03:13,2023-06-16 08:46:16,Use GPU with opencv-python,<python><python-3.x><tensorflow><opencv>,4,0,0.0,,,CC BY-SA 4.0
64337550,1,64351300.0,,2020-10-13 14:49:30,,16,20892,"<p>I am trying to install transformers using pip</p>
<pre><code>pip install transformers
</code></pre>
<p>after import transformers</p>
<p>this error show</p>
<pre><code>Neither PyTorch nor TensorFlow &gt;= 2.0 have been found.Models won't be available and only tokenizers, configuration, and file/data utilities can be used.
</code></pre>
<p>although I install TensorFlow-GPU= 2.3.1 and using conda</p>
<p>system info</p>
<pre><code>Windows 10 
python 3.6
cuda 10.1
tensorflow-gpu= 2.3.1
</code></pre>
",4509339.0,,4509339.0,,2020-10-13 14:55:27,2023-05-24 16:02:39,"Neither PyTorch nor TensorFlow >= 2.0 have been found.Models won't be available and only tokenizers, configuration and file/data utilities can be used",<python><tensorflow><nlp>,3,0,,,,CC BY-SA 4.0
65907365,1,66028149.0,,2021-01-26 18:43:01,,16,27309,"<p>I run drive.py program from <a href=""https://www.codeproject.com/Articles/1273179/A-Complete-guide-to-self-driving-Car?fbclid=IwAR1L7mOaPTZ7-vwWicXcAS7FLhVLcU9BUgaivhK0P9tbXAGe0zOThAuktMs"" rel=""noreferrer"">Code Project | A Complete guide to self driving car</a></p>
<p>but when i start program i have error:</p>
<p>Not creating XLA devices, tf_xla_enable_xla_devices not set</p>
<p>Does anyone know how I can fix this problem? What should I download or reinstall?</p>
<p>I use:</p>
<p>Python 3.8.7</p>
<p>CUDA 11.0</p>
<p>tensorflow 2.4.1
On <a href=""http://0.0.0.0:4567/"" rel=""noreferrer"">http://0.0.0.0:4567/</a> of course I see nothing</p>
<p><a href=""https://i.stack.imgur.com/3pXqE.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3pXqE.jpg"" alt=""enter image description here"" /></a></p>
",9800633.0,,12340179.0,,2021-02-11 11:36:17,2021-11-23 18:41:17,"tensorflow: Not creating XLA devices, tf_xla_enable_xla_devices not set",<python><tensorflow>,4,0,0.0,,,CC BY-SA 4.0
70240506,1,70255105.0,,2021-12-06 03:16:05,,16,12198,"<p>I just got my new MacBook Pro with M1 Max chip and am setting up Python. I've tried several combinational settings to test speed - now I'm quite confused. First put my questions here:</p>
<ul>
<li>Why python run natively on M1 Max is greatly (~100%) slower than on my old MacBook Pro 2016 with Intel i5?</li>
<li>On M1 Max, why there isn't significant speed difference between native run (by miniforge) and run via Rosetta (by anaconda) - which is supposed to be slower ~20%?</li>
<li>On M1 Max and native run, why there isn't significant speed difference between conda installed Numpy and TensorFlow installed Numpy - which is supposed to be faster?</li>
<li>On M1 Max, why run in PyCharm IDE is constantly slower ~20% than run from terminal, which doesn't happen on my old Intel Mac.</li>
</ul>
<p>Evidence supporting my questions is as follows:</p>
<hr />
<p>Here are the settings I've tried:</p>
<p><strong>1. Python installed by</strong></p>
<ul>
<li><a href=""https://github.com/conda-forge/miniforge"" rel=""noreferrer"">Miniforge-arm64</a>, so that python is natively run on M1 Max Chip. (Check from Activity Monitor, <code>Kind</code> of python process is <code>Apple</code>).</li>
<li><a href=""https://docs.anaconda.com/anaconda/install/mac-os/"" rel=""noreferrer"">Anaconda</a>. Then python is run via Rosseta. (Check from Activity Monitor, <code>Kind</code> of python process is <code>Intel</code>).</li>
</ul>
<p><strong>2. Numpy installed by</strong></p>
<ul>
<li><code>conda install numpy</code>: numpy from original conda-forge channel, or pre-installed with anaconda.</li>
<li>Apple-TensorFlow: with python installed by miniforge, I directly install tensorflow, and numpy will also be installed. It's said that, numpy installed in this way is optimized for Apple M1 and will be faster. Here is the installation commands:</li>
</ul>
<pre><code>conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal
</code></pre>
<p><strong>3. Run from</strong></p>
<ul>
<li>Terminal.</li>
<li>PyCharm (<a href=""https://www.jetbrains.com/pycharm/download/download-thanks.html?platform=macM1"" rel=""noreferrer"">Apple Silicon version</a>).</li>
</ul>
<hr />
<p>Here is the test code:</p>
<pre class=""lang-py prettyprint-override""><code>import time
import numpy as np
np.random.seed(42)
a = np.random.uniform(size=(300, 300))
runtimes = 10

timecosts = []
for _ in range(runtimes):
    s_time = time.time()
    for i in range(100):
        a += 1
        np.linalg.svd(a)
    timecosts.append(time.time() - s_time)

print(f'mean of {runtimes} runs: {np.mean(timecosts):.5f}s')
</code></pre>
<p>and here are the results:</p>
<pre><code>+-----------------------------------+-----------------------+--------------------+
|   Python installed by (run on)→   | Miniforge (native M1) | Anaconda (Rosseta) |
+----------------------+------------+------------+----------+----------+---------+
| Numpy installed by ↓ | Run from → |  Terminal  |  PyCharm | Terminal | PyCharm |
+----------------------+------------+------------+----------+----------+---------+
|          Apple Tensorflow         |   4.19151  |  4.86248 |     /    |    /    |
+-----------------------------------+------------+----------+----------+---------+
|        conda install numpy        |   4.29386  |  4.98370 |  4.10029 | 4.99271 |
+-----------------------------------+------------+----------+----------+---------+
</code></pre>
<p>This is quite slow. For comparison,</p>
<ul>
<li>run the same code on my old MacBook Pro 2016 with i5 chip - it costs <strong><code>2.39917s</code></strong>.</li>
<li>another <a href=""https://www.zhihu.com/question/431073191/answer/1657202944"" rel=""noreferrer"">post (but not in English)</a> reports that run with M1 chip (not Pro or Max), miniforge+conda_installed_numpy is <strong><code>2.53214s</code></strong>, and miniforge+apple_tensorflow_numpy is <strong><code>1.00613s</code></strong>.</li>
<li>you may also try on it your own.</li>
</ul>
<p>Here is the CPU information details:</p>
<ul>
<li>My old i5:</li>
</ul>
<pre><code>$ sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Intel(R) Core(TM) i5-6360U CPU @ 2.00GHz
machdep.cpu.core_count: 2
</code></pre>
<ul>
<li>My new M1 Max:</li>
</ul>
<pre><code>% sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Apple M1 Max
machdep.cpu.core_count: 10
</code></pre>
<hr />
<p>I follow instructions strictly from tutorials - but why would all these happen? Is it because of my installation flaws, or because of M1 Max chip? Since my work relies heavily on local runs, local speed is very important to me. Any suggestions to possible solution, or any data points on your own device would be greatly appreciated :)</p>
",13571357.0,,7483211.0,,2023-02-23 01:40:55,2023-05-29 13:28:06,Why is numpy native on M1 Max greatly slower than on old Intel i5?,<python><numpy><tensorflow><anaconda><apple-m1>,5,2,0.0,,,CC BY-SA 4.0
63552169,1,63680485.0,,2020-08-23 21:29:17,,16,8756,"<p>I am trying to get started with Tensorflow 2.0 <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection"" rel=""noreferrer"">Object Detection API</a>. I have gone through the installation following <a href=""https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html"" rel=""noreferrer"">the official tutorial</a> and I pass all the tests. However, I keep getting an error message that I don't understand when I try to run the main module. This is how I run it:</p>
<pre><code>python model_main_tf2.py --model_dir=ssd_resnet50_v1_fpn_640x640_coco17_tpu-8 --pipeline_config_path=ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/pipeline.config
</code></pre>
<p>This is the beginning of the error message:</p>
<pre><code>Traceback (most recent call last):
  File &quot;model_main_tf2.py&quot;, line 113, in &lt;module&gt;
    tf.compat.v1.app.run()
  File &quot;/home/hd/hd_hd/hd_rs239/.conda/envs/jan_tf2/lib/python3.7/site-packages/tensorflow/python/platform/app.py&quot;, line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File &quot;/home/hd/hd_hd/hd_rs239/.conda/envs/jan_tf2/lib/python3.7/site-packages/absl/app.py&quot;, line 299, in run
    _run_main(main, args)
  File &quot;/home/hd/hd_hd/hd_rs239/.conda/envs/jan_tf2/lib/python3.7/site-packages/absl/app.py&quot;, line 250, in _run_main
    sys.exit(main(argv))
  File &quot;model_main_tf2.py&quot;, line 110, in main
    record_summaries=FLAGS.record_summaries)
  File &quot;/home/hd/hd_hd/hd_rs239/.conda/envs/jan_tf2/lib/python3.7/site-packages/object_detection/model_lib_v2.py&quot;, line 569, in train_loop
    unpad_groundtruth_tensors)
  File &quot;/home/hd/hd_hd/hd_rs239/.conda/envs/jan_tf2/lib/python3.7/site-packages/object_detection/model_lib_v2.py&quot;, line 383, in load_fine_tune_checkpoint
    ckpt.restore(checkpoint_path).assert_existing_objects_matched()
  File &quot;/home/hd/hd_hd/hd_rs239/.conda/envs/jan_tf2/lib/python3.7/site-packages/tensorflow/python/training/tracking/util.py&quot;, line 791, in assert_existing_objects_matched
    (list(unused_python_objects),))
AssertionError: Some Python objects were not bound to checkpointed values, likely due to changes in the Python program: [SyncOnReadVariable:{
  0: &lt;tf.Variable 'conv2_block1_0_bn/moving_variance:0' shape=(256,) dtype=float32, numpy=
array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
</code></pre>
<p>In the pipeline.config, I specify a checkpoint like this:</p>
<pre><code>  fine_tune_checkpoint: &quot;ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/ckpt-0&quot; 
</code></pre>
<p>These are the contents of <code>ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint/</code> :</p>
<pre><code>checkpoint  
ckpt-0.data-00000-of-00001  
ckpt-0.index
</code></pre>
<p>I have searched Google but couldn't find any answer. In <a href=""https://github.com/tensorflow/models/issues/8399"" rel=""noreferrer"">this issue</a>, the suggested solution is outdated (the code they suggest to replace is not there anymore).</p>
<p><strong>Question:</strong> What is the problem and how can I solve it?</p>
<p>I am doing this on a server with CentOS Linux 7. I am using Python 3.7. I am new to Tensorflow so please if I am missing any important information, let me know.</p>
",7838925.0,,,,,2022-11-22 08:42:37,Some Python objects were not bound to checkpointed values,<python><tensorflow><deep-learning><tensorflow2.0><object-detection-api>,4,2,,,,CC BY-SA 4.0
64312153,1,64312636.0,,2020-10-12 05:28:57,,15,12386,"<p><code>x_train = x_train[..., tf.newaxis].astype(&quot;float32&quot;)</code></p>
<p><code>x_test = x_test[..., tf.newaxis].astype(&quot;float32&quot;)</code></p>
<p>Can someone please explain how <code>tf.newaxis</code> works ?</p>
<p>I found a brief mention in the documentation</p>
<p><a href=""https://www.tensorflow.org/api_docs/python/tf/strided_slice"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/strided_slice</a></p>
<p>but I could not properly understand.</p>
",14408736.0,,6117017.0,,2020-10-12 06:22:52,2021-12-09 17:45:38,tf.newaxis operation in TensorFlow,<python><tensorflow>,2,3,0.0,,,CC BY-SA 4.0
64305438,1,,,2020-10-11 14:57:15,,15,7852,"<p>This occurred while I was using <code>tf.data.Dataset</code>:</p>
<blockquote>
<p>The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to <code>dataset.cache().take(k).repeat()</code>. You should use <code>dataset.take(k).cache().repeat()</code> instead.</p>
</blockquote>
<p>According to other questions, for example <a href=""https://stackoverflow.com/questions/50519343/how-to-cache-data-during-the-first-epoch-correctly-tensorflow-dataset"">this one</a>, it has something to do with where <code>cache()</code> is in the sequence of methods, but I can't understand what to do concretely.</p>
<p>Here's how to reproduce the warning:</p>
<pre><code>import tensorflow_datasets as tfds

ds = tfds.load('iris', split='train')

ds = ds.take(100)

for elem in ds:
    pass
</code></pre>
<p>Seems like no matter what I do, and no matter where I use <code>cache()</code>, the warning pops up.</p>
",10908375.0,,,,,2020-11-09 16:11:44,Warning: The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset,<python><tensorflow><tensorflow-datasets>,1,0,,,,CC BY-SA 4.0
72505051,1,,,2022-06-05 05:40:00,,15,5314,"<p>Tensorflow 2.9.1 on Ubuntu 20.04 with CUDA 11.2 keeps writing to the stdout the following error whenever I try to build a few layers:</p>
<pre><code>2022-06-05 08:32:29.319040: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100
2022-06-05 08:32:29.527708: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
</code></pre>
<p>No traceback or anything. The model seems to build normally, though, but I wonder if there are any performance implications.</p>
<p>What can be the reason of this error? Is there a way to increase the verbosity of the output without rebuilding the Tensorflow binary?</p>
",6187172.0,,6187172.0,,2022-06-19 06:16:26,2023-01-16 15:49:30,The meaning of 'Start cannot spawn child process: No such file or directory' upon running Tensorflow,<python><tensorflow><posix>,1,6,0.0,,,CC BY-SA 4.0
62870656,1,63147614.0,,2020-07-13 06:54:37,,15,11958,"<p>I am using TPU runtime in Google Colab, but having problems in reading files (not sure). I initialized TPU using:</p>
<pre><code>import tensorflow as tf
import os
import tensorflow_datasets as tfds

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print(&quot;All devices: &quot;, tf.config.list_logical_devices('TPU'))
</code></pre>
<p>I have many images in a folder in Google Colab storage ( e.g. <code>'/content/train2017/000000000009.jpg'</code>). I run the following code:</p>
<pre><code>import tensorflow as tf
def load_image(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (299, 299))
    img = tf.keras.applications.inception_v3.preprocess_input(img)
    return img, image_path
load_image('/content/train2017/000000000009.jpg')
</code></pre>
<p>But, I am getting the following error:</p>
<pre><code>---------------------------------------------------------------------------
UnimplementedError                        Traceback (most recent call last)
&lt;ipython-input-33-a7fbb45f3b76&gt; in &lt;module&gt;()
----&gt; 1 load_image('/content/train2017/000000000009.jpg')

5 frames
&lt;ipython-input-7-862c73d29b96&gt; in load_image(image_path)
      2     img = tf.io.read_file(image_path)
      3     img = tf.image.decode_jpeg(img, channels=3)
----&gt; 4     img = tf.image.resize(img, (299, 299))
      5     img = tf.keras.applications.inception_v3.preprocess_input(img)
      6     return img, image_path

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py in resize_images_v2(images, size, method, preserve_aspect_ratio, antialias, name)
   1515       preserve_aspect_ratio=preserve_aspect_ratio,
   1516       name=name,
-&gt; 1517       skip_resize_if_same=False)
   1518 
   1519 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/image_ops_impl.py in _resize_images_common(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)
   1183   with ops.name_scope(name, 'resize', [images, size]):
   1184     images = ops.convert_to_tensor(images, name='images')
-&gt; 1185     if images.get_shape().ndims is None:
   1186       raise ValueError('\'images\' contains no shape.')
   1187     # TODO(shlens): Migrate this functionality to the underlying Op's.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_shape(self)
   1071   def get_shape(self):
   1072     &quot;&quot;&quot;Alias of Tensor.shape.&quot;&quot;&quot;
-&gt; 1073     return self.shape
   1074 
   1075   def _shape_as_list(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in shape(self)
   1065         self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())
   1066       except core._NotOkStatusException as e:
-&gt; 1067         six.raise_from(core._status_to_exception(e.code, e.message), None)
   1068 
   1069     return self._tensor_shape

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnimplementedError: File system scheme '[local]' not implemented (file: '/content/train2017/000000000009.jpg')
</code></pre>
<p>How should I solve it? I found something like a gs bucket, but it is paid. Is there any other way to solve this?</p>
",13720565.0,,8841057.0,,2020-07-13 15:01:59,2021-02-13 05:11:59,File system scheme '[local]' not implemented in Google Colab TPU,<python><tensorflow><google-colaboratory><tpu><google-cloud-tpu>,2,0,0.0,,,CC BY-SA 4.0
72255562,1,72336599.0,,2022-05-16 07:29:59,,15,37433,"<p>I am having problems trying to run TensorFlow on my Windows 10 machine. Code runs fine on my MacOS machine.</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:\Users\Fynn\Documents\GitHub\AlpacaTradingBot\ai.py&quot;, line 15, in &lt;module&gt;
    from keras.models import Sequential, load_model
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\__init__.py&quot;, line 24, in &lt;module&gt;
    from keras import models
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\models\__init__.py&quot;, line 18, in &lt;module&gt;
    from keras.engine.functional import Functional
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\engine\functional.py&quot;, line 24, in &lt;module&gt;
    from keras.dtensor import layout_map as layout_map_lib
  File &quot;C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\keras\dtensor\__init__.py&quot;, line 22, in &lt;module&gt;
    from tensorflow.compat.v2.experimental import dtensor as dtensor_api  # pylint: disable=g-import-not-at-top
ImportError: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (C:\Users\Fynn\AppData\Local\Programs\Python\Python39\lib\site-packages\tensorflow\_api\v2\compat\v2\experimental\__init__.py)
</code></pre>
",15956485.0,,4685471.0,,2022-11-03 03:10:41,2022-11-03 03:10:41,Cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental',<python><tensorflow>,4,0,,,,CC BY-SA 4.0
63111115,1,65333073.0,,2020-07-27 08:05:25,,14,44033,"<p>Recently, I upgraded to Anaconda3 2020.07 which uses python 3.8. In past versions of anaconda, tensorflow was installed successfully. Tensorflow failed to be installed successfully in this version.</p>
<p>I ran the command below;</p>
<pre><code>conda install tensorflow-gpu
</code></pre>
<p>The error message that I received is shown below;</p>
<pre><code>UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:

  - tensorflow-gpu -&gt; python[version='3.5.*|3.6.*|3.7.*|&gt;=3.7,&lt;3.8.0a0|&gt;=3.6,&lt;3.7.0a0|&gt;=3.5,&lt;3.6.0a0|&gt;=2.7,&lt;2.8.0a0']

Your python: python=3.8

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

The following specifications were found to be incompatible with your CUDA driver:

  - feature:/win-64::__cuda==11.0=0

Your installed CUDA driver is: 11.0
</code></pre>
<p>Is there a conda command with the right parameters to get tensorflow installed successfully?</p>
",1709088.0,,,,,2021-12-14 19:50:06,Unable to install tensorflow using conda with python 3.8,<python-3.x><tensorflow><installation><anaconda><conda>,13,3,0.0,,,CC BY-SA 4.0
65242614,1,,,2020-12-10 21:31:42,,14,27117,"<p>I am using a MacBook Pro with M1 processor, macOS version 11.0.1, Python 3.8 in PyCharm, Tensorflow version 2.4.0rc4 (also tried 2.3.0, 2.3.1, 2.4.0rc0). I am trying to run the following code:</p>
<pre><code>import tensorflow
</code></pre>
<p>This causes the error message:</p>
<pre><code>Process finished with exit code 132 (interrupted by signal 4: SIGILL)
</code></pre>
<p>The code runs fine on my Windows and Linux machines.
What does the error message mean and how can I fix it?</p>
",14067415.0,,,,,2023-04-12 22:41:16,"Why does loading tensorflow on Mac lead to ""Process finished with exit code 132 (interrupted by signal 4: SIGILL)""?",<python-3.x><macos><tensorflow>,6,4,0.0,,,CC BY-SA 4.0
63404192,1,63712528.0,,2020-08-13 22:46:27,,14,8641,"<p>I keep failing to run <code>pip install</code> on the <strong>tensorflow</strong> package. First it downloads the .whl file, then goes through a bunch of already satisfied requirements until it gets to <code>installing collected packages: tensorflow</code>, at which point here's the error I get:</p>
<pre><code>ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\Borik\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\include\\external\\com_github_grpc_grpc\\src\\core\\ext\\filters\\client_channel\\lb_policy\\grpclb\\client_load_reporting_filter.h'
</code></pre>
<p>I've never seen anything like this before and can't seem to find anything on the net. I'm using Windows 10 and the latest versions of Python and pip.</p>
",13977239.0,,3700738.0,,2021-12-20 14:54:40,2022-07-10 19:24:26,pip install tensorflow cannot find file called client_load_reporting_filter.h,<python><tensorflow><installation><pip><package>,2,7,0.0,,,CC BY-SA 4.0
62931610,1,,,2020-07-16 09:29:06,,13,13861,"<p>My question is related to <a href=""https://github.com/tensorflow/tensorflow/issues/36921"" rel=""noreferrer"">this</a> and <a href=""https://github.com/tensorflow/tensorflow/issues/35101"" rel=""noreferrer"">this</a> one here. I am using PyCharm on Windows and Python 3.7.8 and Tensorflow 2.2.0:</p>
<pre><code>print (sys.version)
3.7.8 (tags/v3.7.8:4b47a5b6ba, Jun 28 2020, 08:53:46) [MSC v.1916 64 bit (AMD64)]

print(tf.__version__)
2.2.0
</code></pre>
<p>When I run this code from this <a href=""https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub"" rel=""noreferrer"">colab</a> tutorial:</p>
<pre><code>import os

import numpy as np

import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds

(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)
IMG_SIZE = 160 # All images will be resized to 160x160

def format_example(image, label):
  image = tf.cast(image, tf.float32)
  image = (image/127.5) - 1
  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
  return image, label
</code></pre>
<p>and then try to run this line:</p>
<pre><code>train = raw_train.map(format_example)
</code></pre>
<p>I get the WARNING:</p>
<pre><code>WARNING:tensorflow:AutoGraph could not transform &lt;function format_example at 0x00000265A2DB4E58&gt; and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: Unable to locate the source code of &lt;function format_example at 0x00000265A2DB4E58&gt;. Note that functions defined in certain environments, like the interactive Python shell do not expose their source code. If that is the case, you should to define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
</code></pre>
<p>I only get this WARNING when using PyCharm on my local pc. When I run it in colab no problems. What is the issue here and is this relevant or can I ignore the WARNING?</p>
",2165335.0,,,,,2021-01-20 07:08:43,WARNING:tensorflow:AutoGraph could not transform <function format_example at ...> and will run it as-is,<python><tensorflow>,2,3,,,,CC BY-SA 4.0
69879188,1,,,2021-11-08 06:00:07,,13,30024,"<pre><code>Could not load library cudnn_cnn_infer64_8.dll. Error code 126
Please make sure cudnn_cnn_infer64_8.dll is in your library path!
</code></pre>
<p>I keep getting this error when I try to use TensorFlow with GPU, I've installed CUDA, cuDNN, and all the drivers multiple times according to the instructions. But nothing seems to work.
If I use notebook then TensorFlow uses the CPU, with VS code notebook extension i can use the gpu but it stops the session at 1st epoch, when I tried to run it as a normal python file. the above error occurred.</p>
<p>Complete terminal output:</p>
<pre><code>Found 14630 validated image filenames belonging to 3 classes.
Found 1500 validated image filenames belonging to 3 classes.
2021-11-08 11:03:58.000354: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-11-08 11:03:58.603592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2775 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1        
Epoch 1/10
2021-11-08 11:04:07.306011: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8300
Could not load library cudnn_cnn_infer64_8.dll. Error code 126
Please make sure cudnn_cnn_infer64_8.dll is in your library path!
E:\MyWorkSpace\animal_detect&gt;
</code></pre>
<p>The code snippet:</p>
<pre><code>import tensorflow as tf 
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from tensorflow.keras import layers 
from tensorflow.keras import Model 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.vgg16 import VGG16
import pandas as pd
import numpy as np

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
train_gen = ImageDataGenerator(rescale = 1./255.,rotation_range = 40, width_shift_range = 0.2, height_shift_range = 0.2, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)
test_gen = ImageDataGenerator( rescale = 1.0/255. )
train_set = train_gen.flow_from_dataframe(train_df,x_col='loc',y_col='label',batch_size=20,target_size=(224,224))
test_set = train_gen.flow_from_dataframe(test_df,x_col='loc',y_col='label',batch_size=20,target_size=(224,224))
base_model = VGG16(input_shape = (224, 224, 3),
include_top = False,
weights = 'imagenet')
for layer in base_model.layers:
    layer.trainable = False
x = layers.Flatten()(base_model.output)
x = layers.Dense(512, activation='relu')(x)
x = layers.Dropout(0.5)(x)
x = layers.Dense(3, activation='sigmoid')(x)

model = tf.keras.models.Model(base_model.input, x)

model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001), loss = 'categorical_crossentropy',metrics = ['acc'])
vgghist = model.fit(train_set, validation_data = test_set, steps_per_epoch = 100, epochs = 10)
</code></pre>
<p>the same code has been used for Jupyter-notebook, VS code notebook extension and as a normal python file</p>
<p>Device specifications:</p>
<p>processor: Intel i5
gpu: Nvidia Geforce 1050ti</p>
<p>Cuda version: 11.5
cuDNN version: 8.3</p>
",11659317.0,,,,,2022-09-15 03:53:45,Could not load library cudnn_cnn_infer64_8.dll. Error code 126,<python><tensorflow><cuda><cudnn>,2,2,0.0,,,CC BY-SA 4.0
70622895,1,,,2022-01-07 14:43:30,,13,16402,"<p>Hi after running this code below, I get the following error.</p>
<p><em>ValueError: Could not load model facebook/bart-large-mnli with any of the following classes: (&lt;class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSequenceClassification'&gt;,).</em></p>
<pre><code>import tensorflow as tf
from transformers import pipeline

classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)
</code></pre>
<p>Could someone please help.
Thank you!</p>
",15190107.0,,15190107.0,,2022-01-07 14:48:43,2022-12-26 17:22:37,Transformers model from Hugging-Face throws error that specific classes couldn t be loaded,<python><tensorflow><nlp><huggingface-transformers>,3,0,,,,CC BY-SA 4.0
71575380,1,,,2022-03-22 16:12:50,,13,11377,"<p>While using <code>pip install tf-models-official</code> I found the following problem while the library is getting installed:-</p>
<pre><code>Collecting tf-models-official
  Using cached tf_models_official-2.8.0-py2.py3-none-any.whl (2.2 MB)
Requirement already satisfied: Pillow in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (9.0.1)
Collecting gin-config
  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)
Collecting seqeval
  Using cached seqeval-1.2.2-py3-none-any.whl
Requirement already satisfied: tensorflow~=2.8.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (2.8.0)
Collecting tensorflow-datasets
  Using cached tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)
Requirement already satisfied: six in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (1.16.0)
Requirement already satisfied: scipy&gt;=0.19.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (1.8.0)
Collecting pandas&gt;=0.22.0
  Using cached pandas-1.4.1-cp310-cp310-win_amd64.whl (10.6 MB)
Collecting py-cpuinfo&gt;=3.3.0
  Using cached py_cpuinfo-8.0.0-py3-none-any.whl
Collecting google-api-python-client&gt;=1.6.7
  Downloading google_api_python_client-2.42.0-py2.py3-none-any.whl (8.3 MB)
     ---------------------------------------- 8.3/8.3 MB 7.6 MB/s eta 0:00:00
Requirement already satisfied: tf-slim&gt;=1.1.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (1.1.0)
Requirement already satisfied: psutil&gt;=5.4.3 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (5.9.0)
Collecting tensorflow-hub&gt;=0.6.0
  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)
Requirement already satisfied: matplotlib in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (3.5.1)
Collecting kaggle&gt;=1.3.9
  Using cached kaggle-1.5.12-py3-none-any.whl
Collecting sacrebleu
  Using cached sacrebleu-2.0.0-py3-none-any.whl (90 kB)
Requirement already satisfied: numpy&gt;=1.15.4 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (1.22.3)
Collecting tensorflow-addons
  Using cached tensorflow_addons-0.16.1-cp310-cp310-win_amd64.whl (755 kB)
Collecting oauth2client
  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)
Collecting pyyaml&lt;6.0,&gt;=5.1
  Using cached PyYAML-5.4.1-cp310-cp310-win_amd64.whl
Collecting sentencepiece
  Using cached sentencepiece-0.1.96.tar.gz (508 kB)
  Preparing metadata (setup.py) ... done
Collecting opencv-python-headless
  Using cached opencv_python_headless-4.5.5.64-cp36-abi3-win_amd64.whl (35.3 MB)
Requirement already satisfied: Cython in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (0.29.28)
Requirement already satisfied: pycocotools in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tf-models-official) (2.0.4)
Collecting tensorflow-text~=2.8.0
  Using cached tensorflow_text-2.8.1-cp310-cp310-win_amd64.whl (2.5 MB)
Collecting tensorflow-model-optimization&gt;=0.4.1
  Using cached tensorflow_model_optimization-0.7.2-py2.py3-none-any.whl (237 kB)
Requirement already satisfied: google-auth&lt;3.0.0dev,&gt;=1.16.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from google-api-python-client&gt;=1.6.7-&gt;tf-models-official) (2.6.2)
Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0dev,&gt;=1.31.5
  Using cached google_api_core-2.7.1-py3-none-any.whl (114 kB)
Collecting uritemplate&lt;5,&gt;=3.0.1
  Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)
Collecting google-auth-httplib2&gt;=0.1.0
  Using cached google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)
Collecting httplib2&lt;1dev,&gt;=0.15.0
  Using cached httplib2-0.20.4-py3-none-any.whl (96 kB)
Requirement already satisfied: tqdm in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official) (4.63.0)
Requirement already satisfied: urllib3 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official) (1.26.9)
Collecting python-slugify
  Using cached python_slugify-6.1.1-py2.py3-none-any.whl (9.1 kB)
Requirement already satisfied: python-dateutil in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official) (2.8.2)
Requirement already satisfied: requests in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official) (2.27.1)
Requirement already satisfied: certifi in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from kaggle&gt;=1.3.9-&gt;tf-models-official) (2021.10.8)
Collecting pytz&gt;=2020.1
  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)
Requirement already satisfied: h5py&gt;=2.9.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (3.6.0)
Requirement already satisfied: setuptools in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (58.1.0)
Requirement already satisfied: gast&gt;=0.2.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (0.5.3)
Requirement already satisfied: flatbuffers&gt;=1.12 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (2.0)
Requirement already satisfied: libclang&gt;=9.0.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (13.0.0)
Requirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (0.24.0)
Requirement already satisfied: opt-einsum&gt;=2.3.2 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (3.3.0)
Requirement already satisfied: protobuf&gt;=3.9.2 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (3.19.4)
Requirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (1.44.0)
Requirement already satisfied: keras-preprocessing&gt;=1.1.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (1.1.2)
Requirement already satisfied: absl-py&gt;=0.4.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (1.0.0)
Requirement already satisfied: wrapt&gt;=1.11.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (1.14.0)
Requirement already satisfied: keras&lt;2.9,&gt;=2.8.0rc0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (2.8.0)
Requirement already satisfied: termcolor&gt;=1.1.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (1.1.0)
Requirement already satisfied: google-pasta&gt;=0.1.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (0.2.0)
Requirement already satisfied: astunparse&gt;=1.6.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (1.6.3)
Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (2.8.0.dev2021122109)
Requirement already satisfied: tensorboard&lt;2.9,&gt;=2.8 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (2.8.0)
Requirement already satisfied: typing-extensions&gt;=3.6.6 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorflow~=2.8.0-&gt;tf-models-official) (4.1.1)
Collecting dm-tree~=0.1.1
  Using cached dm_tree-0.1.6-cp310-cp310-win_amd64.whl (91 kB)
Requirement already satisfied: pyparsing&gt;=2.2.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from matplotlib-&gt;tf-models-official) (3.0.7)
Requirement already satisfied: cycler&gt;=0.10 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from matplotlib-&gt;tf-models-official) (0.11.0)
Requirement already satisfied: kiwisolver&gt;=1.0.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from matplotlib-&gt;tf-models-official) (1.4.0)
Requirement already satisfied: packaging&gt;=20.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from matplotlib-&gt;tf-models-official) (21.3)
Requirement already satisfied: fonttools&gt;=4.22.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from matplotlib-&gt;tf-models-official) (4.31.1)
Requirement already satisfied: pyasn1-modules&gt;=0.0.5 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from oauth2client-&gt;tf-models-official) (0.2.8)
Requirement already satisfied: rsa&gt;=3.1.4 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from oauth2client-&gt;tf-models-official) (4.8)
Requirement already satisfied: pyasn1&gt;=0.1.7 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from oauth2client-&gt;tf-models-official) (0.4.8)
Requirement already satisfied: tabulate&gt;=0.8.9 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from sacrebleu-&gt;tf-models-official) (0.8.9)
Collecting regex
  Using cached regex-2022.3.15-cp310-cp310-win_amd64.whl (274 kB)
Requirement already satisfied: colorama in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from sacrebleu-&gt;tf-models-official) (0.4.4)
Collecting portalocker
  Using cached portalocker-2.4.0-py2.py3-none-any.whl (16 kB)
Collecting scikit-learn&gt;=0.21.3
  Using cached scikit_learn-1.0.2-cp310-cp310-win_amd64.whl (7.2 MB)
Collecting typeguard&gt;=2.7
  Using cached typeguard-2.13.3-py3-none-any.whl (17 kB)
Collecting dill
  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)
Collecting promise
  Using cached promise-2.3-py3-none-any.whl
Collecting tensorflow-metadata
  Using cached tensorflow_metadata-1.7.0-py3-none-any.whl (48 kB)
Requirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (0.37.1)
Collecting googleapis-common-protos&lt;2.0dev,&gt;=1.52.0
  Using cached googleapis_common_protos-1.56.0-py2.py3-none-any.whl (241 kB)
Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from google-auth&lt;3.0.0dev,&gt;=1.16.0-&gt;google-api-python-client&gt;=1.6.7-&gt;tf-models-official) (5.0.0)
Requirement already satisfied: charset-normalizer~=2.0.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from requests-&gt;kaggle&gt;=1.3.9-&gt;tf-models-official) (2.0.12)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from requests-&gt;kaggle&gt;=1.3.9-&gt;tf-models-official) (3.3)
Collecting threadpoolctl&gt;=2.0.0
  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)
Collecting joblib&gt;=0.11
  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)
Requirement already satisfied: tensorboard-data-server&lt;0.7.0,&gt;=0.6.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (0.6.1)
Requirement already satisfied: tensorboard-plugin-wit&gt;=1.6.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (1.8.1)
Requirement already satisfied: werkzeug&gt;=0.11.15 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (2.0.3)
Requirement already satisfied: markdown&gt;=2.6.8 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (3.3.6)
Requirement already satisfied: google-auth-oauthlib&lt;0.5,&gt;=0.4.1 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (0.4.6)
Requirement already satisfied: pywin32&gt;=226 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from portalocker-&gt;sacrebleu-&gt;tf-models-official) (303)
Requirement already satisfied: text-unidecode&gt;=1.3 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from python-slugify-&gt;kaggle&gt;=1.3.9-&gt;tf-models-official) (1.3)
Requirement already satisfied: requests-oauthlib&gt;=0.7.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (1.3.1)
Requirement already satisfied: oauthlib&gt;=3.0.0 in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&lt;2.9,&gt;=2.8-&gt;tensorflow~=2.8.0-&gt;tf-models-official) (3.2.0)
Building wheels for collected packages: sentencepiece
  Building wheel for sentencepiece (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [22 lines of output]
      C:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\lib\site-packages\setuptools\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead
        warnings.warn(
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-3.10
      creating build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/__init__.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_model_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      running build_ext
      building 'sentencepiece._sentencepiece' extension
      creating build\temp.win-amd64-3.10
      creating build\temp.win-amd64-3.10\Release
      creating build\temp.win-amd64-3.10\Release\src
      creating build\temp.win-amd64-3.10\Release\src\sentencepiece
      C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include -IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\cppwinrt /EHsc /Tpsrc/sentencepiece/sentencepiece_wrap.cxx /Fobuild\temp.win-amd64-3.10\Release\src/sentencepiece/sentencepiece_wrap.obj /MT /I..\build\root\include
      cl : Command line warning D9025 : overriding '/MD' with '/MT'
      sentencepiece_wrap.cxx
      src/sentencepiece/sentencepiece_wrap.cxx(2809): fatal error C1083: Cannot open include file: 'sentencepiece_processor.h': No such file or directory
      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for sentencepiece
  Running setup.py clean for sentencepiece
Failed to build sentencepiece
Installing collected packages: sentencepiece, pytz, py-cpuinfo, gin-config, dm-tree, uritemplate, typeguard, threadpoolctl, tensorflow-model-optimization, tensorflow-hub, regex, pyyaml, python-slugify, promise, portalocker, opencv-python-headless, joblib, httplib2, googleapis-common-protos, dill, tensorflow-metadata, tensorflow-addons, scikit-learn, sacrebleu, pandas, oauth2client, kaggle, tensorflow-datasets, seqeval, google-auth-httplib2, google-api-core, google-api-python-client, tensorflow-text, tf-models-official
  Running setup.py install for sentencepiece ... error
  error: subprocess-exited-with-error

  × Running setup.py install for sentencepiece did not run successfully.
  │ exit code: 1
  ╰─&gt; [22 lines of output]
      C:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\lib\site-packages\setuptools\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead
        warnings.warn(
      running install
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-3.10
      creating build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/__init__.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_model_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      running build_ext
      building 'sentencepiece._sentencepiece' extension
      creating build\temp.win-amd64-3.10
      creating build\temp.win-amd64-3.10\Release
      creating build\temp.win-amd64-3.10\Release\src
      creating build\temp.win-amd64-3.10\Release\src\sentencepiece
      C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include -IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\cppwinrt /EHsc /Tpsrc/sentencepiece/sentencepiece_wrap.cxx /Fobuild\temp.win-amd64-3.10\Release\src/sentencepiece/sentencepiece_wrap.obj /MT /I..\build\root\include
      cl : Command line warning D9025 : overriding '/MD' with '/MT'
      sentencepiece_wrap.cxx
      src/sentencepiece/sentencepiece_wrap.cxx(2809): fatal error C1083: Cannot open include file: 'sentencepiece_processor.h': No such file or directory
      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─&gt; sentencepiece 
</code></pre>
<p>Hence, figured the problem might lie with sentencepiece. Hence, tried installing sentencepiece with two commands <code>pip install sentencepiece</code> as well as <code>pip install sentencepiece==0.1.92</code>. However the following error is coming:-</p>
<pre><code>   Collecting sentencepiece
  Using cached sentencepiece-0.1.96.tar.gz (508 kB)
  Preparing metadata (setup.py) ... done
Building wheels for collected packages: sentencepiece
  Building wheel for sentencepiece (setup.py) ... error
  error: subprocess-exited-with-error

  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─&gt; [22 lines of output]
      C:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\lib\site-packages\setuptools\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead
        warnings.warn(
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-3.10
      creating build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/__init__.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_model_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      running build_ext
      building 'sentencepiece._sentencepiece' extension
      creating build\temp.win-amd64-3.10
      creating build\temp.win-amd64-3.10\Release
      creating build\temp.win-amd64-3.10\Release\src
      creating build\temp.win-amd64-3.10\Release\src\sentencepiece
      C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include -IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\cppwinrt /EHsc /Tpsrc/sentencepiece/sentencepiece_wrap.cxx /Fobuild\temp.win-amd64-3.10\Release\src/sentencepiece/sentencepiece_wrap.obj /MT /I..\build\root\include
      cl : Command line warning D9025 : overriding '/MD' with '/MT'
      sentencepiece_wrap.cxx
      src/sentencepiece/sentencepiece_wrap.cxx(2809): fatal error C1083: Cannot open include file: 'sentencepiece_processor.h': No such file or directory
      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for sentencepiece
  Running setup.py clean for sentencepiece
Failed to build sentencepiece
Installing collected packages: sentencepiece
  Running setup.py install for sentencepiece ... error
  error: subprocess-exited-with-error

  × Running setup.py install for sentencepiece did not run successfully.
  │ exit code: 1
  ╰─&gt; [22 lines of output]
      C:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\lib\site-packages\setuptools\dist.py:717: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead
        warnings.warn(
      running install
      running build
      running build_py
      creating build
      creating build\lib.win-amd64-3.10
      creating build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/__init__.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_model_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      copying src\sentencepiece/sentencepiece_pb2.py -&gt; build\lib.win-amd64-3.10\sentencepiece
      running build_ext
      building 'sentencepiece._sentencepiece' extension
      creating build\temp.win-amd64-3.10
      creating build\temp.win-amd64-3.10\Release
      creating build\temp.win-amd64-3.10\Release\src
      creating build\temp.win-amd64-3.10\Release\src\sentencepiece
      C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\bin\HostX86\x64\cl.exe /c /nologo /Ox /W3 /GL /DNDEBUG /MD -IC:\Users\USER\Documents\Python Scripts\Number_Plate_Recognition\anprsys\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\include -IC:\Users\USER\AppData\Local\Programs\Python\Python310\Include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\ATLMFC\include -IC:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30133\include -IC:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\ucrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\shared -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\um -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\winrt -IC:\Program Files (x86)\Windows Kits\10\include\10.0.22000.0\cppwinrt /EHsc /Tpsrc/sentencepiece/sentencepiece_wrap.cxx /Fobuild\temp.win-amd64-3.10\Release\src/sentencepiece/sentencepiece_wrap.obj /MT /I..\build\root\include
      cl : Command line warning D9025 : overriding '/MD' with '/MT'
      sentencepiece_wrap.cxx
      src/sentencepiece/sentencepiece_wrap.cxx(2809): fatal error C1083: Cannot open include file: 'sentencepiece_processor.h': No such file or directory
      error: command 'C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe' failed with exit code 2
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: legacy-install-failure

× Encountered error while trying to install package.
╰─&gt; sentencepiece

note: This is an issue with the package mentioned above, not pip.
hint: See above for output from the failure.
</code></pre>
<p>I have already tried to see if wheels is installed through <code>pip install wheels</code> and</p>
<pre><code>Requirement already satisfied: wheel in c:\users\user\documents\python scripts\number_plate_recognition\anprsys\lib\site-packages (0.37.1)
</code></pre>
<p>I have tried to find a solution to this but to no avail.</p>
",13314132.0,,,,,2023-03-26 20:46:36,sentencepiece library is not being installed in the system,<python><tensorflow><python-wheel><automatic-license-plate-recognition><sentencepiece>,1,0,,,,CC BY-SA 4.0
65383964,1,65415097.0,,2020-12-20 19:31:18,,13,19915,"<p>I am a newbie to deep learning so while I am trying to build a <strong>Masked R-CNN</strong> model for training my <strong>Custom Dataset</strong> I am getting an error which reads:</p>
<pre><code>TypeError: Could not build a TypeSpec for &lt;KerasTensor: shape=(None, None, 4) dtype=float32 (created by layer 'tf.math.truediv')&gt; with type KerasTensor
</code></pre>
<p>Below is the <strong>PYTHON CODE</strong> I am trying to implement for building my ** Masked R-CNN model**:</p>
<pre><code>Mask R-CNN
Configurations and data loading code for MS COCO.

Copyright (c) 2017 Matterport, Inc.
Licensed under the MIT License (see LICENSE for details)
Written by Waleed Abdulla

------------------------------------------------------------

Usage: import the module (see Jupyter notebooks for examples), or run from
       the command line as such:

    # Train a new model starting from pre-trained COCO weights
    python3 coco.py train --dataset=/path/to/coco/ --model=coco

    # Train a new model starting from ImageNet weights. Also auto download COCO dataset
    python3 coco.py train --dataset=/path/to/coco/ --model=imagenet --download=True

    # Continue training a model that you had trained earlier
    python3 coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5

    # Continue training the last model you trained
    python3 coco.py train --dataset=/path/to/coco/ --model=last

    # Run COCO evaluatoin on the last model you trained
    python3 coco.py evaluate --dataset=/path/to/coco/ --model=last
&quot;&quot;&quot;

import os
import sys
import time
import numpy as np
import imgaug  # https://github.com/aleju/imgaug (pip3 install imgaug)

# Download and install the Python COCO tools from https://github.com/waleedka/coco
# That's a fork from the original https://github.com/pdollar/coco with a bug
# fix for Python 3.
# I submitted a pull request https://github.com/cocodataset/cocoapi/pull/50
# If the PR is merged then use the original repo.
# Note: Edit PythonAPI/Makefile and replace &quot;python&quot; with &quot;python3&quot;.
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval
from pycocotools import mask as maskUtils

import zipfile
import urllib.request
import shutil

# Root directory of the project
ROOT_DIR = os.path.abspath(&quot;../../&quot;)

# Import Mask RCNN
sys.path.append(ROOT_DIR)  # To find local version of the library
from mrcnn.config import Config
from mrcnn import model as modellib, utils

# Path to trained weights file
COCO_MODEL_PATH = os.path.join(ROOT_DIR, &quot;mask_rcnn_coco.h5&quot;)

# Directory to save logs and model checkpoints, if not provided
# through the command line argument --logs
DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, &quot;Mask_RCNN\\logs&quot;)
DEFAULT_DATASET_YEAR = &quot;2014&quot;

############################################################
#  Configurations
############################################################


class CocoConfig(Config):
    &quot;&quot;&quot;Configuration for training on MS COCO.
    Derives from the base Config class and overrides values specific
    to the COCO dataset.
    &quot;&quot;&quot;
    # Give the configuration a recognizable name
    NAME = &quot;coco&quot;

    # We use a GPU with 12GB memory, which can fit two images.
    # Adjust down if you use a smaller GPU.
    IMAGES_PER_GPU = 2

    # Uncomment to train on 8 GPUs (default is 1)
    # GPU_COUNT = 8

    # Number of classes (including background)
    NUM_CLASSES = 1 + 80  # COCO has 80 classes


############################################################
#  Dataset
############################################################

class CocoDataset(utils.Dataset):
    def load_coco(self, dataset_dir, subset, year=DEFAULT_DATASET_YEAR, class_ids=None,
                  class_map=None, return_coco=False, auto_download=False):
        &quot;&quot;&quot;Load a subset of the COCO dataset.
        dataset_dir: The root directory of the COCO dataset.
        subset: What to load (train, val, minival, valminusminival)
        year: What dataset year to load (2014, 2017) as a string, not an integer
        class_ids: If provided, only loads images that have the given classes.
        class_map: TODO: Not implemented yet. Supports maping classes from
            different datasets to the same class ID.
        return_coco: If True, returns the COCO object.
        auto_download: Automatically download and unzip MS-COCO images and annotations
        &quot;&quot;&quot;

        if auto_download is True:
            self.auto_download(dataset_dir, subset, year)

        coco = COCO(&quot;{}/annotations/instances_{}{}.json&quot;.format(dataset_dir, subset, year))
        if subset == &quot;minival&quot; or subset == &quot;valminusminival&quot;:
            subset = &quot;val&quot;
        image_dir = &quot;{}/{}{}&quot;.format(dataset_dir, subset, year)

        # Load all classes or a subset?
        if not class_ids:
            # All classes
            class_ids = sorted(coco.getCatIds())

        # All images or a subset?
        if class_ids:
            image_ids = []
            for id in class_ids:
                image_ids.extend(list(coco.getImgIds(catIds=[id])))
            # Remove duplicates
            image_ids = list(set(image_ids))
        else:
            # All images
            image_ids = list(coco.imgs.keys())

        # Add classes
        for i in class_ids:
            self.add_class(&quot;coco&quot;, i, coco.loadCats(i)[0][&quot;name&quot;])

        # Add images
        for i in image_ids:
            self.add_image(
                &quot;coco&quot;, image_id=i,
                path=os.path.join(image_dir, coco.imgs[i]['file_name']),
                width=coco.imgs[i][&quot;width&quot;],
                height=coco.imgs[i][&quot;height&quot;],
                annotations=coco.loadAnns(coco.getAnnIds(
                    imgIds=[i], catIds=class_ids, iscrowd=None)))
        if return_coco:
            return coco

    def auto_download(self, dataDir, dataType, dataYear):
        &quot;&quot;&quot;Download the COCO dataset/annotations if requested.
        dataDir: The root directory of the COCO dataset.
        dataType: What to load (train, val, minival, valminusminival)
        dataYear: What dataset year to load (2014, 2017) as a string, not an integer
        Note:
            For 2014, use &quot;train&quot;, &quot;val&quot;, &quot;minival&quot;, or &quot;valminusminival&quot;
            For 2017, only &quot;train&quot; and &quot;val&quot; annotations are available
        &quot;&quot;&quot;

        # Setup paths and file names
        if dataType == &quot;minival&quot; or dataType == &quot;valminusminival&quot;:
            imgDir = &quot;{}/{}{}&quot;.format(dataDir, &quot;val&quot;, dataYear)
            imgZipFile = &quot;{}/{}{}.zip&quot;.format(dataDir, &quot;val&quot;, dataYear)
            imgURL = &quot;http://images.cocodataset.org/zips/{}{}.zip&quot;.format(&quot;val&quot;, dataYear)
        else:
            imgDir = &quot;{}/{}{}&quot;.format(dataDir, dataType, dataYear)
            imgZipFile = &quot;{}/{}{}.zip&quot;.format(dataDir, dataType, dataYear)
            imgURL = &quot;http://images.cocodataset.org/zips/{}{}.zip&quot;.format(dataType, dataYear)
        # print(&quot;Image paths:&quot;); print(imgDir); print(imgZipFile); print(imgURL)

        # Create main folder if it doesn't exist yet
        if not os.path.exists(dataDir):
            os.makedirs(dataDir)

        # Download images if not available locally
        if not os.path.exists(imgDir):
            os.makedirs(imgDir)
            print(&quot;Downloading images to &quot; + imgZipFile + &quot; ...&quot;)
            with urllib.request.urlopen(imgURL) as resp, open(imgZipFile, 'wb') as out:
                shutil.copyfileobj(resp, out)
            print(&quot;... done downloading.&quot;)
            print(&quot;Unzipping &quot; + imgZipFile)
            with zipfile.ZipFile(imgZipFile, &quot;r&quot;) as zip_ref:
                zip_ref.extractall(dataDir)
            print(&quot;... done unzipping&quot;)
        print(&quot;Will use images in &quot; + imgDir)

        # Setup annotations data paths
        annDir = &quot;{}/annotations&quot;.format(dataDir)
        if dataType == &quot;minival&quot;:
            annZipFile = &quot;{}/instances_minival2014.json.zip&quot;.format(dataDir)
            annFile = &quot;{}/instances_minival2014.json&quot;.format(annDir)
            annURL = &quot;https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?dl=0&quot;
            unZipDir = annDir
        elif dataType == &quot;valminusminival&quot;:
            annZipFile = &quot;{}/instances_valminusminival2014.json.zip&quot;.format(dataDir)
            annFile = &quot;{}/instances_valminusminival2014.json&quot;.format(annDir)
            annURL = &quot;https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?dl=0&quot;
            unZipDir = annDir
        else:
            annZipFile = &quot;{}/annotations_trainval{}.zip&quot;.format(dataDir, dataYear)
            annFile = &quot;{}/instances_{}{}.json&quot;.format(annDir, dataType, dataYear)
            annURL = &quot;http://images.cocodataset.org/annotations/annotations_trainval{}.zip&quot;.format(dataYear)
            unZipDir = dataDir
        # print(&quot;Annotations paths:&quot;); print(annDir); print(annFile); print(annZipFile); print(annURL)

        # Download annotations if not available locally
        if not os.path.exists(annDir):
            os.makedirs(annDir)
        if not os.path.exists(annFile):
            if not os.path.exists(annZipFile):
                print(&quot;Downloading zipped annotations to &quot; + annZipFile + &quot; ...&quot;)
                with urllib.request.urlopen(annURL) as resp, open(annZipFile, 'wb') as out:
                    shutil.copyfileobj(resp, out)
                print(&quot;... done downloading.&quot;)
            print(&quot;Unzipping &quot; + annZipFile)
            with zipfile.ZipFile(annZipFile, &quot;r&quot;) as zip_ref:
                zip_ref.extractall(unZipDir)
            print(&quot;... done unzipping&quot;)
        print(&quot;Will use annotations in &quot; + annFile)

    def load_mask(self, image_id):
        &quot;&quot;&quot;Load instance masks for the given image.

        Different datasets use different ways to store masks. This
        function converts the different mask format to one format
        in the form of a bitmap [height, width, instances].

        Returns:
        masks: A bool array of shape [height, width, instance count] with
            one mask per instance.
        class_ids: a 1D array of class IDs of the instance masks.
        &quot;&quot;&quot;
        # If not a COCO image, delegate to parent class.
        image_info = self.image_info[image_id]
        if image_info[&quot;source&quot;] != &quot;coco&quot;:
            return super(CocoDataset, self).load_mask(image_id)

        instance_masks = []
        class_ids = []
        annotations = self.image_info[image_id][&quot;annotations&quot;]
        # Build mask of shape [height, width, instance_count] and list
        # of class IDs that correspond to each channel of the mask.
        for annotation in annotations:
            class_id = self.map_source_class_id(
                &quot;coco.{}&quot;.format(annotation['category_id']))
            if class_id:
                m = self.annToMask(annotation, image_info[&quot;height&quot;],
                                   image_info[&quot;width&quot;])
                # Some objects are so small that they're less than 1 pixel area
                # and end up rounded out. Skip those objects.
                if m.max() &lt; 1:
                    continue
                # Is it a crowd? If so, use a negative class ID.
                if annotation['iscrowd']:
                    # Use negative class ID for crowds
                    class_id *= -1
                    # For crowd masks, annToMask() sometimes returns a mask
                    # smaller than the given dimensions. If so, resize it.
                    if m.shape[0] != image_info[&quot;height&quot;] or m.shape[1] != image_info[&quot;width&quot;]:
                        m = np.ones([image_info[&quot;height&quot;], image_info[&quot;width&quot;]], dtype=bool)
                instance_masks.append(m)
                class_ids.append(class_id)

        # Pack instance masks into an array
        if class_ids:
            mask = np.stack(instance_masks, axis=2).astype(np.bool)
            class_ids = np.array(class_ids, dtype=np.int32)
            return mask, class_ids
        else:
            # Call super class to return an empty mask
            return super(CocoDataset, self).load_mask(image_id)

    def image_reference(self, image_id):
        &quot;&quot;&quot;Return a link to the image in the COCO Website.&quot;&quot;&quot;
        info = self.image_info[image_id]
        if info[&quot;source&quot;] == &quot;coco&quot;:
            return &quot;http://cocodataset.org/#explore?id={}&quot;.format(info[&quot;id&quot;])
        else:
            super(CocoDataset, self).image_reference(image_id)

    # The following two functions are from pycocotools with a few changes.

    def annToRLE(self, ann, height, width):
        &quot;&quot;&quot;
        Convert annotation which can be polygons, uncompressed RLE to RLE.
        :return: binary mask (numpy 2D array)
        &quot;&quot;&quot;
        segm = ann['segmentation']
        if isinstance(segm, list):
            # polygon -- a single object might consist of multiple parts
            # we merge all parts into one mask rle code
            rles = maskUtils.frPyObjects(segm, height, width)
            rle = maskUtils.merge(rles)
        elif isinstance(segm['counts'], list):
            # uncompressed RLE
            rle = maskUtils.frPyObjects(segm, height, width)
        else:
            # rle
            rle = ann['segmentation']
        return rle

    def annToMask(self, ann, height, width):
        &quot;&quot;&quot;
        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.
        :return: binary mask (numpy 2D array)
        &quot;&quot;&quot;
        rle = self.annToRLE(ann, height, width)
        m = maskUtils.decode(rle)
        return m


############################################################
#  COCO Evaluation
############################################################

def build_coco_results(dataset, image_ids, rois, class_ids, scores, masks):
    &quot;&quot;&quot;Arrange resutls to match COCO specs in http://cocodataset.org/#format
    &quot;&quot;&quot;
    # If no results, return an empty list
    if rois is None:
        return []

    results = []
    for image_id in image_ids:
        # Loop through detections
        for i in range(rois.shape[0]):
            class_id = class_ids[i]
            score = scores[i]
            bbox = np.around(rois[i], 1)
            mask = masks[:, :, i]

            result = {
                &quot;image_id&quot;: image_id,
                &quot;category_id&quot;: dataset.get_source_class_id(class_id, &quot;coco&quot;),
                &quot;bbox&quot;: [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]],
                &quot;score&quot;: score,
                &quot;segmentation&quot;: maskUtils.encode(np.asfortranarray(mask))
            }
            results.append(result)
    return results


def evaluate_coco(model, dataset, coco, eval_type=&quot;bbox&quot;, limit=0, image_ids=None):
    &quot;&quot;&quot;Runs official COCO evaluation.
    dataset: A Dataset object with valiadtion data
    eval_type: &quot;bbox&quot; or &quot;segm&quot; for bounding box or segmentation evaluation
    limit: if not 0, it's the number of images to use for evaluation
    &quot;&quot;&quot;
    # Pick COCO images from the dataset
    image_ids = image_ids or dataset.image_ids

    # Limit to a subset
    if limit:
        image_ids = image_ids[:limit]

    # Get corresponding COCO image IDs.
    coco_image_ids = [dataset.image_info[id][&quot;id&quot;] for id in image_ids]

    t_prediction = 0
    t_start = time.time()

    results = []
    for i, image_id in enumerate(image_ids):
        # Load image
        image = dataset.load_image(image_id)

        # Run detection
        t = time.time()
        r = model.detect([image], verbose=0)[0]
        t_prediction += (time.time() - t)

        # Convert results to COCO format
        # Cast masks to uint8 because COCO tools errors out on bool
        image_results = build_coco_results(dataset, coco_image_ids[i:i + 1],
                                           r[&quot;rois&quot;], r[&quot;class_ids&quot;],
                                           r[&quot;scores&quot;],
                                           r[&quot;masks&quot;].astype(np.uint8))
        results.extend(image_results)

    # Load results. This modifies results with additional attributes.
    coco_results = coco.loadRes(results)

    # Evaluate
    cocoEval = COCOeval(coco, coco_results, eval_type)
    cocoEval.params.imgIds = coco_image_ids
    cocoEval.evaluate()
    cocoEval.accumulate()
    cocoEval.summarize()

    print(&quot;Prediction time: {}. Average {}/image&quot;.format(
        t_prediction, t_prediction / len(image_ids)))
    print(&quot;Total time: &quot;, time.time() - t_start)


############################################################
#  Training
############################################################


if __name__ == '__main__':
    import argparse

    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description='Train Mask R-CNN on MS COCO.')
    parser.add_argument(&quot;command&quot;,
                        metavar=&quot;&lt;command&gt;&quot;,
                        help=&quot;'train' or 'evaluate' on MS COCO&quot;)
    parser.add_argument('--dataset', required=True,
                        metavar=&quot;C:\\Users\\HP\\TEST\\Train&quot;,
                        help='Directory of the MS-COCO dataset')
    parser.add_argument('--year', required=False,
                        default=DEFAULT_DATASET_YEAR,
                        metavar=&quot;&lt;year&gt;&quot;,
                        help='Year of the MS-COCO dataset (2014 or 2017) (default=2014)')
    parser.add_argument('--model', required=False,
                        #metavar=&quot;C:\\Users\\HP\\mask_rcnn_coco.h5&quot;
            metavar=&quot;C:\\Users\\HP\\Mask_RCNN\\samples\\coco\\coco.py&quot;,
                        help=&quot;Path to weights .h5 file or 'coco'&quot;)
    parser.add_argument('--logs', required=False,
                        default=DEFAULT_LOGS_DIR,
                        metavar=&quot;/path/to/logs/&quot;,
                        help='Logs and checkpoints directory (default=logs/)')
    parser.add_argument('--limit', required=False,
                        default=500,
                        metavar=&quot;&lt;image count&gt;&quot;,
                        help='Images to use for evaluation (default=500)')
    parser.add_argument('--download', required=False,
                        default=False,
                        metavar=&quot;&lt;True|False&gt;&quot;,
                        help='Automatically download and unzip MS-COCO files (default=False)',
                        type=bool)
    args = parser.parse_args()
    print(&quot;Command: &quot;, args.command)
    print(&quot;Model: &quot;, args.model)
    print(&quot;Dataset: &quot;, args.dataset)
    print(&quot;Year: &quot;, args.year)
    print(&quot;Logs: &quot;, args.logs)
    print(&quot;Auto Download: &quot;, args.download)

    # Configurations
    if args.command == &quot;train&quot;:
        config = CocoConfig()
    else:
        class InferenceConfig(CocoConfig):
            # Set batch size to 1 since we'll be running inference on
            # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU
            GPU_COUNT = 1
            IMAGES_PER_GPU = 1
            DETECTION_MIN_CONFIDENCE = 0
        config = InferenceConfig()
    config.display()

    # Create model
    if args.command == &quot;train&quot;:
        model = modellib.MaskRCNN(mode=&quot;training&quot;, config=config,
                                  model_dir=args.logs)
    else:
        model = modellib.MaskRCNN(mode=&quot;inference&quot;, config=config,
                                  model_dir=args.logs)

    # Select weights file to load
    if args.model.lower() == &quot;coco&quot;:
        model_path = COCO_MODEL_PATH
    elif args.model.lower() == &quot;last&quot;:
        # Find last trained weights
        model_path = model.find_last()
    elif args.model.lower() == &quot;imagenet&quot;:
        # Start from ImageNet trained weights
        model_path = model.get_imagenet_weights()
    else:
        model_path = args.model

    # Load weights
    print(&quot;Loading weights &quot;, model_path)
    model.load_weights(model_path, by_name=True)

    # Train or evaluate
    if args.command == &quot;train&quot;:
        # Training dataset. Use the training set and 35K from the
        # validation set, as as in the Mask RCNN paper.
        dataset_train = CocoDataset()
        dataset_train.load_coco(args.dataset, &quot;train&quot;, year=args.year, auto_download=args.download)
        if args.year in '2014':
            dataset_train.load_coco(args.dataset, &quot;valminusminival&quot;, year=args.year, auto_download=args.download)
        dataset_train.prepare()

        # Validation dataset
        dataset_val = CocoDataset()
        val_type = &quot;val&quot; if args.year in '2017' else &quot;minival&quot;
        dataset_val.load_coco(args.dataset, val_type, year=args.year, auto_download=args.download)
        dataset_val.prepare()

        # Image Augmentation
        # Right/Left flip 50% of the time
        augmentation = imgaug.augmenters.Fliplr(0.5)

        # *** This training schedule is an example. Update to your needs ***

        # Training - Stage 1
        print(&quot;Training network heads&quot;)
        model.train(dataset_train, dataset_val,
                    learning_rate=config.LEARNING_RATE,
                    epochs=40,
                    layers='heads',
                    augmentation=augmentation)

        # Training - Stage 2
        # Finetune layers from ResNet stage 4 and up
        print(&quot;Fine tune Resnet stage 4 and up&quot;)
        model.train(dataset_train, dataset_val,
                    learning_rate=config.LEARNING_RATE,
                    epochs=120,
                    layers='4+',
                    augmentation=augmentation)

        # Training - Stage 3
        # Fine tune all layers
        print(&quot;Fine tune all layers&quot;)
        model.train(dataset_train, dataset_val,
                    learning_rate=config.LEARNING_RATE / 10,
                    epochs=160,
                    layers='all',
                    augmentation=augmentation)

    elif args.command == &quot;evaluate&quot;:
        # Validation dataset
        dataset_val = CocoDataset()
        val_type = &quot;val&quot; if args.year in '2017' else &quot;minival&quot;
        coco = dataset_val.load_coco(args.dataset, val_type, year=args.year, return_coco=True,auto_download=args.download)
        dataset_val.prepare()
        print(&quot;Running COCO evaluation on {} images.&quot;.format(args.limit))
        evaluate_coco(model, dataset_val, coco, &quot;bbox&quot;, limit=int(args.limit))
    else:
        print(&quot;'{}' is not recognized. &quot;
              &quot;Use 'train' or 'evaluate'&quot;.format(args.command))
</code></pre>
<p>Now after I saved this code as a <strong>.py</strong> file and executed the following command on my terminal:</p>
<pre><code>(base) C:\Users\HP&gt;python C:\Users\HP\Mask_RCNN\samples\coco\coco.py train --dataset=C:\Users\HP\Test\Train --model=coco
</code></pre>
<p>I got the following:</p>
<pre><code>2020-12-21 00:41:06.252236: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2020-12-21 00:41:06.260248: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

(base) C:\Users\HP&gt;python C:\Users\HP\Desktop\try.py train --dataset=C:\Users\HP\Test\Train --model=C:\Users\HP\mask_rcnn_coco.h5
2020-12-21 00:42:34.586446: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2020-12-21 00:42:34.594568: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

(base) C:\Users\HP&gt;python C:\Users\HP\Mask_RCNN\samples\coco\coco.py train --dataset=C:\Users\HP\Test\Train --model=coco
2020-12-21 00:44:41.479421: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2020-12-21 00:44:41.490317: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Command:  train
Model:  coco
Dataset:  C:\Users\HP\Test\Train
Year:  2014
Logs:  C:\Mask_RCNN\logs
Auto Download:  False

Configurations:
BACKBONE                       resnet101
BACKBONE_STRIDES               [4, 8, 16, 32, 64]
BATCH_SIZE                     2
BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]
COMPUTE_BACKBONE_SHAPE         None
DETECTION_MAX_INSTANCES        100
DETECTION_MIN_CONFIDENCE       0.7
DETECTION_NMS_THRESHOLD        0.3
FPN_CLASSIF_FC_LAYERS_SIZE     1024
GPU_COUNT                      1
GRADIENT_CLIP_NORM             5.0
IMAGES_PER_GPU                 2
IMAGE_MAX_DIM                  1024
IMAGE_META_SIZE                93
IMAGE_MIN_DIM                  800
IMAGE_MIN_SCALE                0
IMAGE_RESIZE_MODE              square
IMAGE_SHAPE                    [1024 1024    3]
LEARNING_MOMENTUM              0.9
LEARNING_RATE                  0.001
LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}
MASK_POOL_SIZE                 14
MASK_SHAPE                     [28, 28]
MAX_GT_INSTANCES               100
MEAN_PIXEL                     [123.7 116.8 103.9]
MINI_MASK_SHAPE                (56, 56)
NAME                           coco
NUM_CLASSES                    81
POOL_SIZE                      7
POST_NMS_ROIS_INFERENCE        1000
POST_NMS_ROIS_TRAINING         2000
ROI_POSITIVE_RATIO             0.33
RPN_ANCHOR_RATIOS              [0.5, 1, 2]
RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)
RPN_ANCHOR_STRIDE              1
RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]
RPN_NMS_THRESHOLD              0.7
RPN_TRAIN_ANCHORS_PER_IMAGE    256
STEPS_PER_EPOCH                1000
TOP_DOWN_PYRAMID_SIZE          256
TRAIN_BN                       False
TRAIN_ROIS_PER_IMAGE           200
USE_MINI_MASK                  True
USE_RPN_ROIS                   True
VALIDATION_STEPS               50
WEIGHT_DECAY                   0.0001


Traceback (most recent call last):
  File &quot;C:\Users\HP\Mask_RCNN\samples\coco\coco.py&quot;, line 456, in &lt;module&gt;
    model_dir=args.logs)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\mrcnn\model.py&quot;, line 1832, in __init__
    self.keras_model = self.build(mode=mode, config=config)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\mrcnn\model.py&quot;, line 1871, in build
    x, K.shape(input_image)[1:3]))(input_gt_boxes)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 952, in __call__
    input_list)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 1091, in _functional_construction_call
    inputs, input_masks, args, kwargs)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 822, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer.py&quot;, line 869, in _infer_output_signature
    keras_tensor.keras_tensor_from_tensor, outputs)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\util\nest.py&quot;, line 659, in map_structure
    structure[0], [func(*x) for x in entries],
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\util\nest.py&quot;, line 659, in &lt;listcomp&gt;
    structure[0], [func(*x) for x in entries],
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\keras\engine\keras_tensor.py&quot;, line 606, in keras_tensor_from_tensor
    out = keras_tensor_cls.from_tensor(tensor)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\keras\engine\keras_tensor.py&quot;, line 205, in from_tensor
    type_spec = type_spec_module.type_spec_from_value(tensor)
  File &quot;C:\Users\HP\anaconda3\lib\site-packages\tensorflow\python\framework\type_spec.py&quot;, line 554, in type_spec_from_value
    (value, type(value).__name__))
TypeError: Could not build a TypeSpec for &lt;KerasTensor: shape=(None, None, 4) dtype=float32 (created by layer 'tf.math.truediv')&gt; with type KerasTensor
</code></pre>
",14851693.0,,,,,2023-03-16 22:11:25,TypeError: Could not build a TypeSpec with type KerasTensor,<python><tensorflow><machine-learning><image-processing><deep-learning>,6,0,0.0,,,CC BY-SA 4.0
67557515,1,67749628.0,,2021-05-16 13:59:01,,13,40897,"<p><strong>Background</strong></p>
<p>I am totally new to Python and to machine learning. I just tried to set up a UNet from code I found on the internet and wanted to adapt it to the case I'm working on bit for bit. When trying to <code>.fit</code> the UNet to the training data, I received the following error:</p>
<pre><code>InvalidArgumentError:  required broadcastable shapes at loc(unknown)
     [[node Equal (defined at &lt;ipython-input-68-f1422c6f17bb&gt;:1) ]] [Op:__inference_train_function_3847]
</code></pre>
<p>I get a lot of results when I search for it, but mostly they are different errors.</p>
<p>What does this mean? And, more importantly, how can I fix it?</p>
<p><strong>The code that caused the error</strong></p>
<p>The context of this error is as follows:
I want to segment images and label the different classes.
I set up directories &quot;trn&quot;, &quot;tst&quot; and &quot;val&quot; for training, test, and validation data. The <code>dir_dat()</code> function applies <code>os.path.join()</code> to get the full path to the respective <a href=""https://www.dropbox.com/sh/g4yr4qjgjtsytmi/AABJXVehoCw5In9wQvL7Llu2a?dl=0"" rel=""noreferrer"">data set</a>. Each of the 3 folders has sub directories for each class labeled with integers. In each folder, there are some <code>.tif</code> images for the respective class.</p>
<p>I defined the following image data generators (training data is sparse, hence the augmentation):</p>
<pre><code>classes = np.array([ 0,  2,  4,  6,  8, 11, 16, 21, 29, 30, 38, 39, 51])
bs = 15 # batch size

augGen = ks.preprocessing.image.ImageDataGenerator(rotation_range = 365,
                                                   width_shift_range = 0.05,
                                                   height_shift_range = 0.05,
                                                   horizontal_flip = True,
                                                   vertical_flip = True,
                                                   fill_mode = &quot;nearest&quot;) \
    .flow_from_directory(directory = dir_dat(&quot;trn&quot;),
                         classes = [str(x) for x in classes.tolist()],
                         class_mode = &quot;categorical&quot;,
                         batch_size = bs, seed = 42)
    
tst_batches = ks.preprocessing.image.ImageDataGenerator() \
    .flow_from_directory(directory = dir_dat(&quot;tst&quot;),
                         classes = [str(x) for x in classes.tolist()],
                         class_mode = &quot;categorical&quot;,
                         batch_size = bs, shuffle = False)

val_batches = ks.preprocessing.image.ImageDataGenerator() \
    .flow_from_directory(directory = dir_dat(&quot;val&quot;),
                         classes = [str(x) for x in classes.tolist()],
                         class_mode = &quot;categorical&quot;,
                         batch_size = bs)
</code></pre>
<p>Then I set up the UNet based on <a href=""https://github.com/bnsreenu/python_for_microscopists/blob/master/074-Defining%20U-net%20in%20Python%20using%20Keras.py"" rel=""noreferrer"">this example</a>. Here, I altered a few parameters to adapt the UNet to the situation (multiple classes), namely activation in the last layer and the loss function:</p>
<pre><code>layer_in = ks.layers.Input(shape = (imgr, imgc, imgdim))
# convert pixel integer values to float
inVals = ks.layers.Lambda(lambda x: x / 255)(layer_in)

# Contraction path
c1 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(inVals)
c1 = ks.layers.Dropout(0.1)(c1)
c1 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c1)
p1 = ks.layers.MaxPooling2D((2, 2))(c1)

c2 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p1)
c2 = ks.layers.Dropout(0.1)(c2)
c2 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c2)
p2 = ks.layers.MaxPooling2D((2, 2))(c2)
 
c3 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p2)
c3 = ks.layers.Dropout(0.2)(c3)
c3 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c3)
p3 = ks.layers.MaxPooling2D((2, 2))(c3)
 
c4 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p3)
c4 = ks.layers.Dropout(0.2)(c4)
c4 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c4)
p4 = ks.layers.MaxPooling2D(pool_size = (2, 2))(c4)
 
c5 = ks.layers.Conv2D(256, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(p4)
c5 = ks.layers.Dropout(0.3)(c5)
c5 = ks.layers.Conv2D(256, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c5)

# Expansive path 
u6 = ks.layers.Conv2DTranspose(128, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c5)
u6 = ks.layers.concatenate([u6, c4])
c6 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u6)
c6 = ks.layers.Dropout(0.2)(c6)
c6 = ks.layers.Conv2D(128, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c6)
 
u7 = ks.layers.Conv2DTranspose(64, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c6)
u7 = ks.layers.concatenate([u7, c3])
c7 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u7)
c7 = ks.layers.Dropout(0.2)(c7)
c7 = ks.layers.Conv2D(64, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c7)
 
u8 = ks.layers.Conv2DTranspose(32, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c7)
u8 = ks.layers.concatenate([u8, c2])
c8 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u8)
c8 = ks.layers.Dropout(0.1)(c8)
c8 = ks.layers.Conv2D(32, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c8)
 
u9 = ks.layers.Conv2DTranspose(16, (2, 2), strides = (2, 2), padding = &quot;same&quot;)(c8)
u9 = ks.layers.concatenate([u9, c1], axis = 3)
c9 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(u9)
c9 = ks.layers.Dropout(0.1)(c9)
c9 = ks.layers.Conv2D(16, (3, 3), activation = &quot;relu&quot;,
                            kernel_initializer = &quot;he_normal&quot;, padding = &quot;same&quot;)(c9)
 
out = ks.layers.Conv2D(1, (1, 1), activation = &quot;softmax&quot;)(c9)
 
model = ks.Model(inputs = layer_in, outputs = out)
model.compile(optimizer = &quot;adam&quot;, loss = &quot;sparse_categorical_crossentropy&quot;, metrics = [&quot;accuracy&quot;])
model.summary()
</code></pre>
<p>Finally, I defined callbacks and ran the training, which produced the error:</p>
<pre><code>cllbs = [
    ks.callbacks.EarlyStopping(patience = 4),
    ks.callbacks.ModelCheckpoint(dir_out(&quot;Checkpoint.h5&quot;), save_best_only = True),
    ks.callbacks.TensorBoard(log_dir = './logs'),# log events for TensorBoard
    ]

model.fit(augGen, epochs = 5, validation_data = val_batches, callbacks = cllbs)
</code></pre>
<p><strong>Full console output</strong></p>
<p>This is the full output when running the last line (in case it helps solving the issue):</p>
<pre><code>trained = model.fit(augGen, epochs = 5, validation_data = val_batches, callbacks = cllbs)
Epoch 1/5
Traceback (most recent call last):

  File &quot;&lt;ipython-input-68-f1422c6f17bb&gt;&quot;, line 1, in &lt;module&gt;
    trained = model.fit(augGen, epochs = 5, validation_data = val_batches, callbacks = cllbs)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\keras\engine\training.py&quot;, line 1183, in fit
    tmp_logs = self.train_function(iterator)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 889, in __call__
    result = self._call(*args, **kwds)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 950, in _call
    return self._stateless_fn(*args, **kwds)

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 3023, in __call__
    return graph_function._call_flat(

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 1960, in _call_flat
    return self._build_call_outputs(self._inference_function.call(

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 591, in call
    outputs = execute.execute(

  File &quot;c:\users\manuel\python\lib\site-packages\tensorflow\python\eager\execute.py&quot;, line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,

InvalidArgumentError:  required broadcastable shapes at loc(unknown)
     [[node Equal (defined at &lt;ipython-input-68-f1422c6f17bb&gt;:1) ]] [Op:__inference_train_function_3847]

Function call stack:
train_function
</code></pre>
",11611246.0,,11611246.0,,2021-05-16 18:45:35,2023-01-31 09:35:32,InvalidArgumentError: required broadcastable shapes at loc(unknown),<python><tensorflow><neural-network><conv-neural-network><tf.keras>,6,0,,,,CC BY-SA 4.0
63796936,1,,,2020-09-08 15:21:07,,13,16691,"<p>I have read TF pages and some posts about the use of prefetch() and cache() to speed up the model input pipeline and tried to implement it on my data. Cache() worked for me as expected, i.e. reading data from the dist in the first epoch, and in all subsequent epochs it is just reading data from memory. But I have many difficulties using prefetch() and I really don't understand when and how to use it. Can anybody help me with it? I really need some help.
My application is like this: I have a set of large TFRecord files, and each includes some raw records to be processed before feeding my net. They are going to be mixed (different sample streams) so what I do is:</p>
<pre><code>def read_datasets(pattern, numFiles, numEpochs=125, batchSize=1024, take=dataLength):

    files = tf.data.Dataset.list_files(pattern)

    def _parse(x):
        x = tf.data.TFRecordDataset(x, compression_type='GZIP')
    return x

    np = 4 # half of the number of CPU cores
    dataset = files.interleave(_parse, cycle_length=numFiles, block_length=1, num_parallel_calls=np)\
    .map(lambda x: parse_tfrecord(x), num_parallel_calls=np)
    dataset = dataset.take(take)
    dataset = dataset.batch(batchSize)
    dataset = dataset.cache()
    dataset = dataset.prefetch(buffer_size=10)
    dataset = dataset.repeat(numEpochs)
    return dataset
</code></pre>
<p>parse_tfrecord(x) function in interleave function is the required preprocessing of data before it applies to the model, and my guess is that the preprocessing time is comparable to the batch processing time by the network. My whole dataset (including all input files) contains about 500 batches of 1024 samples. My questions are:</p>
<p>1- If I do caching, do I really need prefetching?</p>
<p>2- Is it the right sequence to do mapping, batching, caching, prefetching, and repeating?</p>
<p>3- Tensorflow documentation says that the buffer size of prefetch refers to the dataset elements and if it is batched, to the number of batches. So in this case I will read 10 batches of 1024 examples, right? My problem is that I don't see any difference in run time by changing prefetching buffer size and the memory consumption is not changed much even by setting the buffer size to 1000 or bigger.</p>
",6450489.0,,1165181.0,,2023-07-02 04:11:06,2023-07-02 04:11:06,What is the proper use of Tensorflow dataset prefetch and cache options?,<python><tensorflow><tensorflow-datasets>,2,0,0.0,,,CC BY-SA 4.0
65780506,1,,,2021-01-18 18:56:39,,12,22437,"<h3>TL;DR:</h3>
<ul>
<li>Tensorflow 1.15 crashes on my virtual machine when imported by Python (error message is <code>Illegal instruction (core dumped)</code>), very probably thanks to AVX and AVX2 being disabled on it.</li>
<li>My host machine (Windows 10 64bit) <strong>has</strong> AVX and AVX2 (validated using Cygwin, see more details below) [CPU is Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz]</li>
<li>On guest machine (Ubuntu 20.04 64bit using VirtualBox 6.1.16, same happens with 18.04), AVX and AVX2 are missing.</li>
<li>Following advice from existing threads, I ran <code>VBoxManage setextradata &quot;Ubuntu20&quot; VBoxInternal/CPUM/IsaExts/AVX 1</code> and <code>VBoxManage setextradata &quot;Ubuntu20&quot; VBoxInternal/CPUM/IsaExts/AVX2 1</code> on the host machine and restarted the guest machine - nothing changed (the guest machine's name is Ubuntu20).</li>
</ul>
<p>any advice?</p>
<h2>For more details:</h2>
<p>Output of <code>cat /proc/cpuinfo</code> on the host machine using Cygwin - it has avx in avx2:</p>
<pre><code>.....
processor       : 7
vendor_id       : GenuineIntel
cpu family      : 6
model           : 158
model name      : Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz
stepping        : 9
cpu MHz         : 3600.000
cache size      : 256 KB
fpu             : yes
fpu_exception   : yes
cpuid level     : 22
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts
 acpi mmx fxsr sse sse2 ss ht tm pbe pni dtes64 est tm2 ssse3 fma cx16 xtpr pdcm sse4_1 sse4_2 movbe 
popcnt aes xsave osxsave avx f16c rdrand hypervisor lahf_lm ida xsaveopt pln pts dtherm fsgsbase 
tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt
TLB size        : 0 4K pages
clflush size    : 64
cache_alignment : 64
address sizes   : 39 bits physical, 48 bits virtual
power management:
</code></pre>
<p>Output of <code>cat /proc/cpuinfo</code> on the guest machine - avx and avx2 are missing:</p>
<pre><code>processor   : 0
vendor_id   : GenuineIntel
cpu family  : 6
model       : 158
model name  : Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz
stepping    : 9
microcode   : 0xffffffff
cpu MHz     : 3599.996
cache size  : 8192 KB
physical id : 0
siblings    : 1
core id     : 0
cpu cores   : 1
apicid      : 0
initial apicid  : 0
fpu     : yes
fpu_exception   : yes
cpuid level : 22
wp      : yes
flags       : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx 
fxsr sse sse2 ht syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid 
tsc_known_freq pni ssse3 pcid sse4_1 sse4_2 hypervisor lahf_lm invpcid_single pti fsgsbase invpcid 
md_clear flush_l1d arch_capabilities
bugs        : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs itlb_multihit
bogomips    : 7199.99
clflush size    : 64
cache_alignment : 64
address sizes   : 39 bits physical, 48 bits virtual
power management:
</code></pre>
",3322273.0,,224132.0,,2021-01-19 21:26:38,2022-11-16 12:16:31,How to enable AVX / AVX2 in VirtualBox 6.1.16 with Ubuntu 20.04 64bit?,<tensorflow><virtual-machine><virtualbox><avx><avx2>,3,7,0.0,,,CC BY-SA 4.0
66778153,1,,,2021-03-24 09:29:18,,12,6160,"<p>My current understanding is:</p>
<p><strong>Different map_func</strong>: Both  <code>interleave</code> and <code>flat_map</code> expect &quot;A function mapping a dataset element to a <strong>dataset</strong>&quot;. In contrast, <code>map</code> expects    &quot;A function mapping a dataset element to another <strong>dataset element</strong>&quot;.</p>
<p><strong>Arguments</strong>: Both <code>interleave</code> and <code>map</code> offer the argument num_parallel_calls, whereas <code>flat_map</code> does not. Moreover, interleave offers these magical arguments block_length and cycle_length. For cycle_length=1, the documentation states that  the outputs of interleave and flat_map are equal.</p>
<p>Last, I have seen <a href=""https://cs230.stanford.edu/blog/datapipeline/#building-an-image-data-pipeline"" rel=""noreferrer"">data loading pipelines without interleave</a> as well as <a href=""https://www.tensorflow.org/guide/data_performance"" rel=""noreferrer"">ones with interleave</a>. Any advice when to use <code>interleave</code> vs. <code>map</code> or <code>flat_map</code> would be greatly appreciated</p>
<hr />
<p>//EDIT: I do see the value of interleave, if we start out with different datasets, such as in <a href=""https://docs.w3cub.com/tensorflow%7Eguide/performance/datasets_performance"" rel=""noreferrer"">the code below</a></p>
<pre><code>  files = tf.data.Dataset.list_files(&quot;/path/to/dataset/train-*.tfrecord&quot;)
  dataset = files.interleave(tf.data.TFRecordDataset)
</code></pre>
<p>However, is there any benefit of using <code>interleave</code> over <code>map</code> in a scenario such as the one below?</p>
<pre><code>files = tf.data.Dataset.list_files(&quot;/path/to/dataset/train-*.png&quot;)
dataset = files.map(load_img, num_parallel_calls=tf.data.AUTOTUNE)
</code></pre>
",2135504.0,,2135504.0,,2021-03-24 10:02:10,2021-03-26 12:50:53,How exactly does tf.data.Dataset.interleave() differ from map() and flat_map()?,<tensorflow><tf.data.dataset>,1,0,0.0,,,CC BY-SA 4.0
63679865,1,63715153.0,,2020-09-01 00:41:30,,12,19353,"<p>Has anyone successfully installed Tensorflow-GPU on WSL2 with NVIDIA GPUs? I have Ubuntu 18.04 on WSL2, but am struggling to get NVIDIA drivers installed. Any help would be appreciated as I'm lost.</p>
",8748308.0,,,,,2022-02-10 00:36:21,Install Tensorflow-GPU on WSL2,<tensorflow><nvidia><wsl-2>,4,2,0.0,,,CC BY-SA 4.0
68027646,1,,,2021-06-17 23:31:03,,12,2494,"<p>I am trying to convert my object detector python project into an executable file but I always get these warnings and my executable file would not run.</p>
<pre><code>64422 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.estimator&quot; not found!
64425 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v2.compat.v2.keras.metrics&quot; not found!
64843 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.keras.applications.resnet50&quot; not found!
64844 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.keras.applications.resnet&quot; not found!
64845 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v2.compat.v2.keras.backend&quot; not found!
64857 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.estimator.tpu&quot; not found!
64859 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v2.compat.v1.keras.applications.mobilenet&quot; not found!
64892 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.keras.applications.vgg19&quot; not found!
64894 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v2.compat.v2.keras.preprocessing.text&quot; not found!
64896 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.estimator.tpu.experimental&quot; not found!
64899 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.keras.applications.resnet_v2&quot; not found!
64956 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v2.compat.v1.keras.wrappers.scikit_learn&quot; not found!
64957 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v2.compat.v2.keras.applications.resnet50&quot; not found!
64958 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.keras.wrappers.scikit_learn&quot; not found!
65073 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v2.keras.applications.imagenet_utils&quot; not found!
65073 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v2.keras.datasets.cifar100&quot; not found!
65238 WARNING: Hidden import &quot;tensorflow._api.v2.compat.v1.compat.v1.keras.optimizers&quot; not found!
</code></pre>
<p>My project structure is</p>
<pre><code>- project folder
  - venv
  - main.py
  - detect.py
</code></pre>
<p>Inside the <code>detect.py</code> I have the following imports</p>
<pre><code>import tensorflow as tf
from tensorflow.python.saved_model import tag_constants
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession
</code></pre>
<p>The <code>tensorflow module</code> can be found in the <code>site-packages</code> inside the <code>venv</code> folder</p>
<p>The solutions that I have tried are adding the <code>--hidden-import tensorflow</code> flag as said in this <a href=""https://stackoverflow.com/questions/57227191/pyinstaller-hidden-import-not-found"">Question</a></p>
<pre><code>pyinstaller --hidden-import tensorflow --onefile main.py
</code></pre>
<p>I have also tried  <a href=""https://stackoverflow.com/questions/60384288/pyinstaller-modulenotfounderror"">this approach</a> by creating a <code>hooks</code> directory with <code>hook-tensorflow.py</code> file</p>
<pre><code>- project folder
   - venv
   - hooks
      - hook-tensorflow.py
   - main.py
   - detect.py
</code></pre>
<p>hook-tensorflow.py</p>
<pre><code>from PyInstaller.utils.hooks import collect_all


def hook(hook_api):
    packages = [
        'tensorflow'
    ]
    for package in packages:
        datas, binaries, hiddenimports = collect_all(package)
        hook_api.add_datas(datas)
        hook_api.add_binaries(binaries)
        hook_api.add_imports(*hiddenimports)
</code></pre>
<p>And then issuing this terminal command</p>
<pre><code>pyinstaller --additional-hooks-dir=hooks --onefile main.py
</code></pre>
<p>But still, the same warning still persists and my executable file would not run.</p>
",14063087.0,,14063087.0,,2021-06-18 00:14:11,2022-03-02 09:05:19,Hidden import Tensorflow package not found when using Pyinstaller,<python><tensorflow><pycharm><pyinstaller>,1,1,0.0,,,CC BY-SA 4.0
65271399,1,65312471.0,,2020-12-13 00:51:20,,12,25370,"<h3>The Summary</h3>
<p>I have a python import that works when run from the VS Code terminal, but that VS Code's editor is giving warnings about. Also, &quot;Go to Definition&quot; doesn't work.</p>
<h3>The Problem</h3>
<p>I have created a docker container from the image <code>tensorflow/tensorflow:1.15.2-py3</code>, then attach to it using VS Code's &quot;Remote- Containers&quot; extension. Then I've created the following file in the container.</p>
<p>main.py:</p>
<pre><code>import tensorflow.compat.v1 as tf
print(tf.__version__)
</code></pre>
<p>This runs fine in the VS Code terminal, but the Editor and the Problems pane both give me an <code>unresolved import 'tensorflow.compat'</code> warning. Also &quot;Go to Definition&quot; doesn't work on <code>tf.__version__</code>.</p>
<p>I'm using several extensions but I believe the relevant ones are the Microsoft Python extension (installed in the container), as well as the Remote - Containers extension, and now the Pylance extension (installed in the container).</p>
<h3>The Things I've Tried</h3>
<p>I've tried this with the default <code>pylint</code>, and then also after installing <code>pylance</code> with similar results. I've also seen some <a href=""https://github.com/microsoft/pylance-release/blob/master/TROUBLESHOOTING.md#unresolved-import-warnings"" rel=""noreferrer"">docs</a> about similar issues, but they were related to setting the correct source folder location for modules that were part of a project. In contrast, my code <em>within my project</em> seems to work fine with imports/go-to-definition. It's <em>external libraries</em> that don't seem to work.</p>
<p>Also, for the sake of this minimal example, I've attached to the container as root, so I am guessing it's not an issue of elevated permissions.</p>
<p>I've also tried disabling all the extensions except the following, but got the same results:</p>
<ul>
<li>Remote - Containers (local)</li>
<li>Remote - WSL (local)</li>
<li>Python (on container)</li>
<li>Jupyter (on container, required by Python for some reason)</li>
</ul>
<p>All the extensions above are on the latest versions.</p>
<p>I've also fiddled around with setting <code>python.autocomplete.extraPaths</code>, but I'm not sure what the right path is. It also seems like the wrong thing to have to add libraries to the path that are installed in the global python installation, especially since I'm not using a virtual environment (it being in a docker container and all).</p>
<h3>The Question</h3>
<p>How do I fix VS Code so that it recognizes this import and I can use &quot;Go to Definition&quot; to explore these tensorflow functions/classes/etc?</p>
",356887.0,,356887.0,,2020-12-15 00:03:47,2022-11-30 06:53:59,VS Code / Pylance / Pylint Cannot resolve import,<python><tensorflow><visual-studio-code><pylint><pylance>,5,3,0.0,,,CC BY-SA 4.0
64197155,1,,,2020-10-04 16:31:10,,12,2721,"<p>I'm getting this message:</p>
<p><code>Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.</code></p>
<p>How do I do that in Tensorflow 2.3?</p>
<p>Over the past few days this turned out to be a surprisingly frustrating issue. There appears to be no working example of how to do this in TF2.</p>
",39590.0,,,,,2021-01-27 14:43:40,TF2 add report_tensor_allocations_upon_oom to RunOptions,<tensorflow><tensorflow2.0>,2,3,0.0,,,CC BY-SA 4.0
66355477,1,,,2021-02-24 17:02:17,,12,31792,"<p><strong>Could not load library cudnn_ops_infer64_8.dll. Error code 126 Please make sure cudnn_ops_infer64_8.dll is in your library path.</strong>
I've tried searching online but it's been hours and I haven't found anything. I would really appreciate anyone sharing his thoughts. I'm trying to run ai-benchmark library which internally tests for performance of gpu against popular datasets. (see image)
<a href=""https://i.stack.imgur.com/fgWcP.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fgWcP.png"" alt="""" /></a></p>
",6737387.0,,,,,2023-02-01 13:08:42,Could not load library cudnn_ops_infer64_8.dll. Error code 126 Please make sure cudnn_ops_infer64_8.dll is in your library path,<python><tensorflow><nvidia>,6,1,0.0,,,CC BY-SA 4.0
64462347,1,,,2020-10-21 11:13:42,,12,5710,"<p>I am running a model written with TensorFlow 1.x on 4x RTX 3090 and it is taking a long time <strong>to start up the training</strong> than as in 1x RTX 3090. Although, as training starts, it gets finished up earlier in 4x than in 1x. I am using CUDA 11.1 and TensorFlow 1.14 in both the GPUs.</p>
<p>Secondly, When I am using 1x RTX 2080ti, with CUDA 10.2 and TensorFlow 1.14, it is taking less amount <strong>to start the training</strong> as compared to 1x RTX 3090 with 11.1 CUDA and Tensorflow 1.14. Tentatively, it is taking 5 min in 1x RTX 2080ti, 30-35 minutes in 1x RTX 3090, and 1.5 hrs in 4x RTX 3090 <strong>to start the training</strong> for one of the datasets.</p>
<p>I'll be grateful if anyone can help me to resolve this issue.</p>
<p>I am using Ubuntu 16.04, Core™ i9-10980XE CPU, and 32 GB ram both in 2080ti and 3090 machines.</p>
<p>EDIT: I found out that TF takes a long start-up time in Ampere architecture GPUs, according <a href=""https://www.tensorflow.org/install/gpu"" rel=""noreferrer"">to this</a>, but I'm still unclear if this is the case; and, if this <em>is</em> the case, does any solution exist for it?</p>
",13398643.0,,13398643.0,,2021-01-16 09:02:20,2022-01-06 14:54:50,Tensorflow 1.14 performance issue on rtx 3090,<tensorflow><nvidia><stylegan>,3,1,0.0,,,CC BY-SA 4.0
66718335,1,,,2021-03-20 04:48:07,,12,11143,"<p>I have been working on a project for estimating the traffic flow using time series data combine with weather data. I am using a window of 30 values for my time series and I am using 20 weather related features. I have used the functional API to implement this, but I keep getting the same error and I do not know how it can be solved. I have looked at other similar threads such as this one <a href=""https://stackoverflow.com/questions/57430717/input-0-of-layer-conv1d-1-is-incompatible-with-the-layer-expected-ndim-3-found"">Input 0 of layer conv1d_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 200]</a>, but it has not helped.</p>
<p>This is my model,</p>
<pre><code>series_input = Input(shape = (series_input_train.shape[1], ), name = 'series_input')
x = Conv1D(filters=32, kernel_size=5, strides=1, padding=&quot;causal&quot;, activation=&quot;relu&quot;)(series_input)
x = LSTM(32, return_sequences = True)(x)
x = LSTM(32, return_sequences = True)(x)
x = Dense(1, activation = 'relu')(x)
series_output = Lambda(lambda w: w * 200)(x)

weather_input = Input(shape = (weather_input_train.shape[1], ), name = 'weather_input')
x = Dense(32, activation = 'relu')(weather_input)
x = Dense(32, activation = 'relu')(x)
weather_output = Dense(1, activation = 'relu')(x)

concatenate = concatenate([series_output, weather_output], axis=1, name = 'concatenate')

output = Dense(1, name = 'output')(concatenate)

model = Model([series_input, weather_input], output)
</code></pre>
<p>The shapes of <code>series_input_train</code> and <code>weather_input_train</code> are (34970, 30) and (34970, 20) respetively.</p>
<p>The error I keep getting is this one,</p>
<pre><code>ValueError: Input 0 of layer conv1d is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: (None, 30)
</code></pre>
<p>What am I doing wrong?</p>
<p>Honestly, I have always had trouble figuring out how the shape of the inputs work in TensorFlow. If you could point me in the right direction, it would be appreciated but what I need right now is a fix for my model.</p>
",9542989.0,,,,,2022-09-10 22:03:50,"Input 0 of layer conv1d is incompatible with the layer: : expected min_ndim=3, found ndim=2. Full shape received: (None, 30)",<python><tensorflow><time-series><lstm><convolution>,2,1,0.0,,,CC BY-SA 4.0
63139273,1,63240453.0,,2020-07-28 17:05:02,,11,4985,"<p>I am trying to use the Tensorflow Lite ML Model with my project, and, unfortunately, I face an error while running my project:</p>
<pre><code>↳
    ** BUILD FAILED **
Xcode's output:
↳
    /Users/tejasravishankar/Developer/flutter/.pub-cache/hosted/pub.dartlang.org/tflite-1.1.1/ios/Classes/TflitePlugin.mm:21:9: fatal error: 'metal_delegate.h' file not found
    #import &quot;metal_delegate.h&quot;
            ^~~~~~~~~~~~~~~~~~
    1 error generated.
    note: Using new build system
    note: Building targets in parallel
    note: Planning build
    note: Constructing build description
Could not build the application for the simulator.
Error launching application on iPhone 11 Pro Max.
</code></pre>
<p>I have tried <code>flutter clean</code>, and have tried removing the <code>Podfile</code> and <code>Podfile.lock</code> from the <code>ios</code> directory, though that didn't change anything.</p>
<p>Here is my code:</p>
<pre><code>import 'dart:io';
import 'package:flutter/material.dart';
import 'package:tflite/tflite.dart';
import 'package:image_picker/image_picker.dart';

void main() =&gt; runApp(TensorflowApp());

const String pet = 'Pet Recognizer';

class TensorflowApp extends StatefulWidget {
  @override
  _TensorflowAppState createState() =&gt; _TensorflowAppState();
}

class _TensorflowAppState extends State&lt;TensorflowApp&gt; {
  String _model = pet;
  File _image;
  double _imageWidth;
  double _imageHeight;
  // ignore: unused_field
  bool _isLoading = false;
  List _predictions;

  _selectFromImagePicker() async {
    PickedFile _pickedImage =
        await ImagePicker().getImage(source: ImageSource.gallery);
    File _pickedImageFile = _pickedFileFormatter(_pickedImage);
    if (_pickedImage == null) {
      return;
    } else {
      setState(() {
        _isLoading = true;
      });
      _predictImage(_pickedImageFile);
    }
  }

  _predictImage(File image) async {
    await _petRecognizerV1(image);
    FileImage(image).resolve(ImageConfiguration()).addListener(
      ImageStreamListener(
        (ImageInfo info, bool _) {
          setState(() {
            _imageWidth = info.image.height.toDouble();
            _imageHeight = info.image.height.toDouble();
          });
        },
      ),
    );

    setState(() {
      _image = image;
      _isLoading = false;
    });
  }

  _petRecognizerV1(File image) async {
    List&lt;dynamic&gt; _modelPredictions = await Tflite.detectObjectOnImage(
      path: image.path,
      model: pet,
      threshold: 0.3,
      imageMean: 0.0,
      imageStd: 255.0,
      numResultsPerClass: 1,
    );
    setState(() {
      _predictions = _modelPredictions;
    });
  }

  _pickedFileFormatter(PickedFile pickedFile) {
    File formattedFile = File(pickedFile.path);
    return formattedFile;
  }

  renderBoxes(Size screen) {
    if (_predictions == null) {
      return [];
    } else {
      if (_imageHeight == null || _imageWidth == null) {
        return [];
      }
      double factorX = screen.width;
      double factorY = _imageHeight / _imageHeight * screen.width;

      return _predictions.map((prediction) {
        return Positioned(
          left: prediction['rect']['x'] * factorX,
          top: prediction['rect']['y'] * factorY,
          width: prediction['rect']['w'] * factorX,
          height: prediction['rect']['h'] * factorY,
          child: Container(
            decoration: BoxDecoration(
              border: Border.all(color: Colors.green, width: 3.0),
            ),
            child: Text(
              '${prediction[&quot;detectedClass&quot;]} ${(prediction[&quot;confidenceInClass&quot;]) * 100.toStringAsFixed(0)}',
              style: TextStyle(
                background: Paint()..color = Colors.green,
                color: Colors.white,
                fontSize: 15.0,
              ),
            ),
          ),
        );
      }).toList();
    }
  }

  @override
  void initState() {
    super.initState();
    _isLoading = true;
    _loadModel().then((value) {
      setState(() {
        _isLoading = false;
      });
    });
  }

  _loadModel() async {
    Tflite.close();
    try {
      String response;
      if (_model == pet) {
        response = await Tflite.loadModel(
          model: 'assets/pet_recognizer.tflite',
          labels: 'assets/pet_recognizer.txt',
        );
      }
    } catch (error) {
      print(error);
    }
  }

  @override
  Widget build(BuildContext context) {
    Size size = MediaQuery.of(context).size;

    return MaterialApp(
      debugShowCheckedModeBanner: false,
      home: Scaffold(
        appBar: AppBar(
          backgroundColor: Colors.white,
          title: Text('TFLite Test'),
        ),
        floatingActionButton: FloatingActionButton(
          child: Icon(Icons.image),
          tooltip: 'Pick Image From Gallery',
          onPressed: () =&gt; _selectFromImagePicker,
        ),
        body: Stack(
          children: &lt;Widget&gt;[
            Positioned(
              top: 0.0,
              left: 0.0,
              width: size.width,
              child: _image == null
                  ? Text('No Image Selected')
                  : Image.file(_image),
            ),
            renderBoxes(size),
          ],
        ),
      ),
    );
  }
}


</code></pre>
<p>I personally don't think that there is a problem with my code, and I tried running</p>
<pre><code>flutter pub get
</code></pre>
<p>which has worked successfully with success code 0 , a few more times, though it hasn't fixed the problem...</p>
<p>I am not very sure what to do in order to move on with this and would really appreciate any help I receive! Thanks, cheers and I appreciate your help :)</p>
",13411726.0,,,,,2021-10-21 18:06:05,"Flutter TFLite Error: ""metal_delegate.h"" File Not Found",<tensorflow><flutter><machine-learning><dart><tensor>,3,0,0.0,,,CC BY-SA 4.0
69794473,1,,,2021-11-01 08:41:47,,11,6125,"<p>Running estimator from tensorflow today and came up with this error, any idea how to solve it?</p>
<pre><code>File &quot;C:\Users\ASUS Laptop\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py&quot;, line 70, in &lt;module&gt;
    @doc_controls.inheritable_header(&quot;&quot;&quot;\
AttributeError: module 'tensorflow.tools.docs.doc_controls' has no attribute 'inheritable_header'
</code></pre>
<p>This is in my estimator.py file:
@doc_controls.inheritable_header(&quot;&quot;&quot;<br />
Warning: Estimators are not recommended for new code.  Estimators run
<code>v1.Session</code>-style code which is more difficult to write correctly, and
can behave unexpectedly, especially when combined with TF 2 code. Estimators
do fall under our
<a href=""https://tensorflow.org/guide/versions"" rel=""noreferrer"">compatibility guarantees</a>, but will
receive no fixes other than security vulnerabilities. See the
<a href=""https://tensorflow.org/guide/migrate"" rel=""noreferrer"">migration guide</a> for details.
&quot;&quot;&quot;)</p>
",17298247.0,,17298247.0,,2021-11-01 10:17:58,2022-09-16 05:48:51,Module 'tensorflow.tools.docs.doc_controls' has no attribute 'inheritable_header',<python><tensorflow><machine-learning><tensorflow2.0><tensorflow-estimator>,2,0,0.0,,,CC BY-SA 4.0
63884339,1,,,2020-09-14 12:28:18,,11,25657,"<p>I followed instructions given in the TensorFlow website to install tensorflow_hub and installed it within a conda environment.</p>
<pre><code>$ pip install &quot;tensorflow&gt;=2.0.0&quot;
$ pip install --upgrade tensorflow-hub
</code></pre>
<p>I ran the above in anaconda prompt</p>
<p><a href=""https://i.stack.imgur.com/xkqVJ.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/xkqVJ.png"" alt=""enter image description here"" /></a></p>
<p>But I'm still getting <code>ModuleNotFoundError</code> for 'tensorflow_hub'.</p>
<p>Any help here is appreciated. Thanks in advance</p>
",10699452.0,,7013263.0,,2020-09-14 15:23:28,2021-04-10 14:34:41,ModuleNotFoundError: No module named 'tensorflow_hub',<python><tensorflow><tensorflow2.0><tensorflow-hub><modulenotfounderror>,1,4,0.0,,,CC BY-SA 4.0
70839312,1,70924045.0,,2022-01-24 19:18:17,,11,12835,"<p>I'm trying to study the neural-network-and-deep-learning (<a href=""http://neuralnetworksanddeeplearning.com/chap1.html"" rel=""noreferrer"">http://neuralnetworksanddeeplearning.com/chap1.html</a>). Using the updated version for Python 3 by MichalDanielDobrzanski (<a href=""https://github.com/MichalDanielDobrzanski/DeepLearningPython"" rel=""noreferrer"">https://github.com/MichalDanielDobrzanski/DeepLearningPython</a>). Tried to run it in my command console and it gives an error below. I've tried uninstalling and reinstalling setuptools, theano, and numpy but none have worked thus far. Any help is very appreciated!!</p>
<p>Here's the full error log:</p>
<pre><code>WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`
C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory
  warnings.warn(&quot;DeprecationWarning: there is no c++ compiler.&quot;
WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.
Traceback (most recent call last):
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\configparser.py&quot;, line 168, in fetch_val_for_key
    return theano_cfg.get(section, option)
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\configparser.py&quot;, line 781, in get
    d = self._unify_values(section, vars)
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\configparser.py&quot;, line 1149, in _unify_values
    raise NoSectionError(section) from None
configparser.NoSectionError: No section: 'blas'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\configparser.py&quot;, line 327, in __get__
    val_str = fetch_val_for_key(self.fullname,
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\configparser.py&quot;, line 172, in fetch_val_for_key
    raise KeyError(key)
KeyError: 'blas.ldflags'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;C:\Users\ASUS\Documents\GitHub\Neural-network-and-deep-learning-but-for-python-3\test.py&quot;, line 156, in &lt;module&gt;
    import network3
  File &quot;C:\Users\ASUS\Documents\GitHub\Neural-network-and-deep-learning-but-for-python-3\network3.py&quot;, line 37, in &lt;module&gt;
    import theano
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\__init__.py&quot;, line 124, in &lt;module&gt;
    from theano.scan_module import (scan, map, reduce, foldl, foldr, clone,
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\scan_module\__init__.py&quot;, line 41, in &lt;module&gt;
    from theano.scan_module import scan_opt
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\scan_module\scan_opt.py&quot;, line 60, in &lt;module&gt;
    from theano import tensor, scalar
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\tensor\__init__.py&quot;, line 17, in &lt;module&gt;
    from theano.tensor import blas
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\tensor\blas.py&quot;, line 155, in &lt;module&gt;
    from theano.tensor.blas_headers import blas_header_text
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\tensor\blas_headers.py&quot;, line 987, in &lt;module&gt;
    if not config.blas.ldflags:
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\configparser.py&quot;, line 332, in __get__
    val_str = self.default()
  File &quot;C:\Users\ASUS\AppData\Local\Programs\Python\Python39\lib\site-packages\theano\configdefaults.py&quot;, line 1284, in default_blas_ldflags
    blas_info = np.distutils.__config__.blas_opt_info
AttributeError: module 'numpy.distutils.__config__' has no attribute 'blas_opt_info'
</code></pre>
",18020911.0,,1367454.0,,2022-01-24 19:25:00,2023-03-24 06:30:52,module 'numpy.distutils.__config__' has no attribute 'blas_opt_info',<python><numpy><tensorflow><deep-learning><theano>,3,1,0.0,,,CC BY-SA 4.0
63656778,1,63737268.0,,2020-08-30 11:06:20,,11,4315,"<p><em><strong>Update:</strong> This is a bug in tensorflow. Track progress <a href=""https://github.com/tensorflow/tensorflow/issues/42980"" rel=""noreferrer"">here</a>.</em></p>
<p>I have created and trained a model using stable-baselines, which uses Tensorflow 1.
Now I need to use this trained model in an environment where I only have access to Tensorflow 2 or PyTorch.
I figured I would go with Tensorflow 2 as the <a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/load"" rel=""noreferrer"">documentation says</a> I should be able to load models created with Tensorflow 1.</p>
<p><em>I can load the pb file without a problem in Tensorflow 1</em>:</p>
<pre><code>global_session = tf.Session()

with global_session.as_default():
    model_loaded = tf.saved_model.load_v2('tensorflow_model')
    model_loaded = model_loaded.signatures['serving_default']

init = tf.global_variables_initializer()
global_session.run(init)
</code></pre>
<p><em>However in Tensorflow 2 I get the following error</em>:</p>
<pre><code>can_be_imported = tf.saved_model.contains_saved_model('tensorflow_model')
assert(can_be_imported)
model_loaded = tf.saved_model.load('tensorflow_model/')

ValueError: Node 'loss/gradients/model/batch_normalization_3/FusedBatchNormV3_1_grad/FusedBatchNormGradV3' has an _output_shapes attribute inconsistent with the GraphDef for output #3: Dimension 0 in both shapes must be equal, but are 0 and 64. Shapes are [0] and [64].
</code></pre>
<p><em>Model definition</em>:</p>
<pre><code>NUM_CHANNELS = 64

BN1 = BatchNormalization()
BN2 = BatchNormalization()
BN3 = BatchNormalization()
BN4 = BatchNormalization()
BN5 = BatchNormalization()
BN6 = BatchNormalization()
CONV1 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')
CONV2 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')
CONV3 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)
CONV4 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)
FC1 = Dense(128)
FC2 = Dense(64)
FC3 = Dense(7)

def modified_cnn(inputs, **kwargs):
    relu = tf.nn.relu
    log_softmax = tf.nn.log_softmax
    
    layer_1_out = relu(BN1(CONV1(inputs)))
    layer_2_out = relu(BN2(CONV2(layer_1_out)))
    layer_3_out = relu(BN3(CONV3(layer_2_out)))
    layer_4_out = relu(BN4(CONV4(layer_3_out)))
    
    flattened = tf.reshape(layer_4_out, [-1, NUM_CHANNELS * 3 * 2]) 
    
    layer_5_out = relu(BN5(FC1(flattened)))
    layer_6_out = relu(BN6(FC2(layer_5_out)))
    
    return log_softmax(FC3(layer_6_out))

class CustomCnnPolicy(CnnPolicy):
    def __init__(self, *args, **kwargs):
        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)

model = PPO2(CustomCnnPolicy, env, verbose=1)
</code></pre>
<p><em>Model saving in TF1:</em></p>
<pre><code>with model.graph.as_default():
    tf.saved_model.simple_save(model.sess, 'tensorflow_model', inputs={&quot;obs&quot;: model.act_model.obs_ph},
                                   outputs={&quot;action&quot;: model.act_model._policy_proba})
</code></pre>
<p>Fully reproducible code can be found in the following 2 google colab notebooks:
<a href=""https://colab.research.google.com/drive/1ftm9vgiJA8LGwLuUFtaG0aYCS9lUrzET?usp=sharing"" rel=""noreferrer"">Tensorflow 1 saving and loading</a>
<a href=""https://colab.research.google.com/drive/18IWxx-eppX2Sjoo2h1hGe0TzCHNiykF1?usp=sharing"" rel=""noreferrer"">Tensorflow 2 loading</a></p>
<p>Direct link to the saved model:
<a href=""https://drive.google.com/drive/folders/1KmfLDIlAqX80Ha1F7MmC314loI2ayiFY?usp=sharing"" rel=""noreferrer"">model</a></p>
",1564252.0,,1564252.0,,2020-09-10 20:33:23,2020-09-10 20:33:23,How to load a trained TF1 protobuf model into TF2?,<tensorflow><tensorflow2.0><stable-baselines>,1,0,0.0,,,CC BY-SA 4.0
67694895,1,67732583.0,,2021-05-25 20:02:24,,10,22539,"<p>I am running a tensorflow model on google colab. Today, I got this error:</p>
<pre><code> Using TensorFlow backend.
    Traceback (most recent call last):
      File &quot;train.py&quot;, line 6, in &lt;module&gt;
        from yolo import create_yolov3_model, dummy_loss
      File &quot;/content/drive/MyDrive/yolo/yolo_plz_work/yolo.py&quot;, line 1, in &lt;module&gt;
        from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/__init__.py&quot;, line 3, in &lt;module&gt;
        from . import utils
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py&quot;, line 26, in &lt;module&gt;
        from .vis_utils import model_to_dot
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py&quot;, line 7, in &lt;module&gt;
        from ..models import Model
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/models.py&quot;, line 10, in &lt;module&gt;
        from .engine.input_layer import Input
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/__init__.py&quot;, line 3, in &lt;module&gt;
        from .input_layer import Input
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py&quot;, line 7, in &lt;module&gt;
        from .base_layer import Layer
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py&quot;, line 12, in &lt;module&gt;
        from .. import initializers
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py&quot;, line 124, in &lt;module&gt;
        populate_deserializable_objects()
      File &quot;/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py&quot;, line 49, in populate_deserializable_objects
        LOCAL.GENERATED_WITH_V2 = tf.__internal__.tf2.enabled()
      File &quot;/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/module_wrapper.py&quot;, line 193, in __getattr__
        attr = getattr(self._tfmw_wrapped_module, name)
    AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__'
</code></pre>
<p>Previously, things had been running smoothly, so I'm not sure why this happened.
I am using Python 3.7.10, and these are the packages I am supposed to use:</p>
<pre><code>absl-py==0.9.0
astor==0.8.1
gast==0.2.2
google-pasta==0.1.8
grpcio==1.26.0
h5py==2.10.0
Keras==2.3.1
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
numpy==1.18.1
opencv-contrib-python==4.1.2.30
opt-einsum==3.1.0
protobuf==3.11.2
PyYAML==5.3
scipy==1.4.1
six==1.14.0
tensorboard==1.15.0
tensorflow==1.15.0
tensorflow-estimator==1.15.1
termcolor==1.1.0
tqdm==4.41.1
Werkzeug==0.16.0
wrapt==1.11.2
</code></pre>
<p>Perhaps colab recently upgraded some libraries? I am sure that I followed the same installation steps as I usually do.</p>
<p>EDIT:
I think there may be an issue in the keras version.
Here are the first few lines of the file I am running:</p>
<pre><code>from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda
from keras.layers.merge import add, concatenate
from keras.models import Model
from keras.engine.topology import Layer
import tensorflow as tf
</code></pre>
<p>If I remove all of the lines starting with &quot;from keras&quot;, I don't get the error. However, I never touched these lines before, so I don't know why they would suddenly cause an error now. Also, it is not the python version causing this error, because colab changed it to 3.7.10 in April and I had no problem.</p>
",14481226.0,,14481226.0,,2021-05-27 02:13:59,2022-10-05 07:39:40,module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__' google colab error,<python><tensorflow><google-colaboratory>,5,4,0.0,,,CC BY-SA 4.0
65623468,1,,,2021-01-08 04:29:48,,10,15097,"<p>I have a tensorflow 2.2 conda environment setup with python 3.8.2 on Ubuntu.</p>
<p>I ran <code>pip install tensorflow-io==0.14.0</code>.</p>
<p>When I try to</p>
<pre><code>import tensorflow-io as tfio
</code></pre>
<p>I get the erorr:</p>
<pre><code>File &quot;/home/somedir/miniconda3/envs/env_name/lib/python3.8/site-packages/tensorflow_io/core/python/ops/__init__.py&quot;, line 65, in _load_library
raise NotImplementedError(

NotImplementedError: unable to open file: libtensorflow_io.so, from paths: ['/home/somedir/miniconda3/envs/env_name/lib/python3.8/site-packages/tensorflow_io/core/python/ops/libtensorflow_io.so']

caused by: ['/home/somedir/miniconda3/envs/env_name/lib/python3.8/site-packages/tensorflow_io/core/python/ops/libtensorflow_io.so undefined symbol:
_ZN10tensorflow0pKernel11TraceStringEPNS_150pKernelContextEb']
</code></pre>
<p>What's the issue and how can I fix it?</p>
",3731622.0,,,,,2023-06-04 23:02:00,Unable to open file libtensorflow_io.so caused by undefined symbol,<python><tensorflow><pip><conda><undefined-symbol>,6,0,0.0,,,CC BY-SA 4.0
62767438,1,62901463.0,,2020-07-07 02:54:18,,10,1219,"<p>I need to non-linearly expand on each pixel value from 1 dim pixel vector with taylor series expansion of specific non-linear function (<code>e^x or log(x) or log(1+e^x)</code>), but my current implementation is not right to me at least based on taylor series concepts. The basic intuition behind is taking pixel array as input neurons for a CNN model where each pixel should be non-linearly expanded with taylor series expansion of non-linear function.</p>
<p><strong>new update 1</strong>:</p>
<p>From my understanding from taylor series,  taylor series is written for a function <code>F</code> of a variable <code>x</code> in terms of the value of the function <code>F</code> and it's derivatives in for another value of variable <code>x0</code>. In my problem, <code>F</code> is function of non-linear transformation of features (a.k.a, pixels), <code>x</code> is each pixel value, <code>x0</code> is maclaurin series approximation at 0.</p>
<p><strong>new update 2</strong></p>
<p>if we use taylor series of <code>log(1+e^x)</code> with approximation order of 2, each pixel value will yield two new pixel by taking first and second expansion terms of taylor series.</p>
<p><strong>graphic illustration</strong></p>
<p>Here is the graphical illustration of the above formulation:</p>
<p><a href=""https://i.stack.imgur.com/3kvcR.jpg"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/3kvcR.jpg"" alt=""enter image description here"" /></a></p>
<p>Where <code>X</code> is pixel array, <code>p</code> is approximation order of taylor series, and <code>α</code> is the taylor expansion coefficient.</p>
<p>I wanted to non-linearly expand pixel vectors with taylor series expansion of non-linear function like above illustration demonstrated.</p>
<p><strong>My current attempt</strong></p>
<p>This is my current attempt which is not working correctly for pixel arrays. I was thinking about how to make the same idea applicable to pixel arrays.</p>
<pre class=""lang-py prettyprint-override""><code>def taylor_func(x, approx_order=2):
    x_ = x[..., None] 
    x_ = tf.tile(x_, multiples=[1, 1, approx_order+ 1])  
    pows = tf.range(0, approx_order + 1, dtype=tf.float32) 
    x_p = tf.pow(x_, pows) 
    x_p_ = x_p[..., None]
    return x_p_

x = Input(shape=(4,4,3))
x_new = Lambda(lambda x: taylor_func(x, max_pow))(x)
</code></pre>
<p><strong>my new updated attempt</strong>:</p>
<pre><code>x_input= Input(shape=(32, 32,3))

def maclurin_exp(x, powers=2):
    out= 0
    for k in range(powers):
        out+= ((-1)**k) * (x ** (2*k)) / (math.factorial(2 * k))
    return res

x_input_new = Lambda(lambda x: maclurin_exp(x, max_pow))(x_input)
</code></pre>
<p>This attempt doesn't yield what the above mathematical formulation describes. I bet I missed something while doing the expansion. Can anyone point me on how to make this correct? Any better idea?</p>
<p><strong>goal</strong></p>
<p>I wanted to take pixel vector and make non-linearly distributed or expanded with taylor series expansion of certain non-linear function. Is there any possible way to do this? any thoughts? thanks</p>
",7302169.0,,7302169.0,,2020-07-18 17:05:24,2020-09-25 18:45:01,expand 1 dim vector by using taylor series of log(1+e^x) in python,<python><tensorflow><conv-neural-network><nonlinear-functions><taylor-series>,2,10,0.0,,,CC BY-SA 4.0
70670205,1,,,2022-01-11 16:31:14,,10,4795,"<p>I am trying to install AlphaFold in a python virtual env. While trying to install dependencies, I get this error:</p>
<pre><code>    ERROR: Could not find a version that satisfies the requirement tensorflow==1.14 (from versions: none)
ERROR: No matching distribution found for tensorflow==1.14
</code></pre>
<p>To fix this, I run the command: &quot; pip3 install --upgrade <a href=""https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.14.0-py3-none-any.whl"" rel=""noreferrer"">https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.14.0-py3-none-any.whl</a> &quot;</p>
<p>but on doing this, I get a long error message concluded with:</p>
<pre><code> In file included from /private/var/folders/r_/bgrrl1md0s5gx95p0q8qjj8h0000gn/T/pip-install-kh7jhw3_/h5py_4b8d652f3fda4ea0a05597c3cbb46b5b/h5py/defs.c:734:
  ./h5py/api_compat.h:27:10: fatal error: 'hdf5.h' file not found
  #include &quot;hdf5.h&quot;
           ^~~~~~~~
  1 error generated.
  error: command '/usr/bin/clang' failed with exit code 1
  ----------------------------------------
  ERROR: Failed building wheel for h5py
Failed to build h5py
ERROR: Could not build wheels for h5py, which is required to install pyproject.toml-based projects
</code></pre>
<p>I tried the following commands with no success and the same error:</p>
<p>&quot;brew install hdf5&quot; (it gets installed, but the h5py installation is still unable to locate hdf5.h file)
&quot;sudo -H pip3 install h5py&quot;
&quot;pip install h5py&quot;</p>
",13690553.0,,,,,2022-04-18 13:21:45,Failed to build h5py on mac M1,<python><tensorflow><hdf5><apple-m1><h5py>,3,0,,,,CC BY-SA 4.0
66827371,1,,,2021-03-27 02:02:30,,10,6213,"<p>I have a keras model and want to save it as a tensorflow graph. Is there a difference between <code>tf.saved_model.save(model, path_to_dir)</code> and <code>tf.keras.model.save</code>.</p>
<p>In both of these, I want to save in a <code>tensorflow</code> saved format and will not be using <code>h5</code> format. I understand <code>tf.saved_model.save</code> is more generic but if I am using a <code>keras</code> model are these two different in anyways.</p>
",10708659.0,,9215780.0,,2021-03-27 02:46:23,2023-03-08 03:55:02,"Difference between tf.saved_model.save(model, path_to_dir) and tf.keras.model.save",<tensorflow><tensorflow2.0>,2,1,,,,CC BY-SA 4.0
75907635,1,75988538.0,,2023-04-01 16:59:25,,10,5112,"<p>I have tried to install TensorFlow v2.12 in Anaconda with <strong>Python 3.9.16</strong> and <strong>Windows 10</strong> OS with <strong>pip v23.0.1</strong>. I need Tensforflow v2.12 for my GPU (RTX4080), and only this version works with my GPU, because support Cuda Toolkit v11.8, which is the oldest to support Ada Lovelace GPUs.</p>
<p>The official tensorflow.org for Windows OS provides the following installation instructions.</p>
<ol>
<li><code>conda install -c conda-forge cudatoolkit=11.8.0</code></li>
<li><code>python3 -m pip install nvidia-cudnn-cu11==8.6.0.163 tensorflow==2.12.*</code></li>
<li><code>CUDNN_PATH=$(dirname $(python -c &quot;import nvidia.cudnn;print(nvidia.cudnn.__file__)&quot;))</code></li>
<li><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib</code></li>
</ol>
<p><strong>Verify install:</strong></p>
<ol start=""5"">
<li><code>python3 -c &quot;import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))&quot;</code></li>
</ol>
<p>In the above steps, <strong>Cuda Toolkit 11.8.0</strong> and <strong>Tensorflow 2.12</strong> are installed without any problems. However, if I try the <strong>cuDNN 8.6.0.163</strong> installation, not installed, and give the following result.</p>
<pre><code>Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com
ERROR: Could not find a version that satisfies the requirement nvidia-cudnn-cu11==8.6.0.163 (from versions: 0.0.1.dev5, 2021.10.26, 2021.11.18, 2021.12.8, 2022.1.13, 2022.4.2, 2022.5.19)
ERROR: No matching distribution found for nvidia-cudnn-cu11==8.6.0.163
</code></pre>
<p>What is going wrong? Is there any idea on how to install the cuDNN 8.6.0.163?</p>
<p>I tried to install <strong>cuDNN 8.6.0.163</strong> in the following ways and all gave the same result:</p>
<ul>
<li><code>python -m pip install nvidia-cudnn-cu11==8.6.0.163</code>   (python in my
conda environment is v3.9.16 so python3 command is not exist, it is
the same with python. I have check it with: <code>python --version</code>)</li>
<li><code>pip install nvidia-cudnn-cu11==8.6.0.163</code></li>
<li><code>python -m pip install nvidia-cudnn-cu11==8.6.0.163 --no-cache-dir</code></li>
<li><code>python -m pip install nvidia-cudnn-cu11==8.6.0.163 --proxy=&quot;xxx.xxx.xxx.xxx:xxxx&quot;</code>  (where xxx.xxx.xxx.xxx:xxxx some proxy IPs and corresponding port)</li>
</ul>
<p>Additionally, I have install 'nvidia-pyindex': <code>pip install nvidia-pyindex</code>, but this does not solve the problem.</p>
<p>After all this, I have tried to install the cuDNN 8.6.0.163 version in Windows environment, and I have set the proper environmental variables, but Tenrosflow in Anaconda does not see that, etc.:
<code>print(tf.config.list_physical_devices('GPU'))</code></p>
<p>return nothing:  [ ]</p>
<p>and from step 3, I have check this:</p>
<p><code>python -c &quot;import nvidia.cudnn;print(nvidia.cudnn.__file__)</code></p>
<p>where this return:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
ModuleNotFoundError: No module named 'nvidia'
</code></pre>
<h3>In PowerShell:</h3>
<p><code>nvidia-smi</code> return this:</p>
<blockquote>
<p>&quot;NVIDIA-SMI 531.41 | Driver Version: 531.41 | CUDA Version: 12.1&quot;</p>
</blockquote>
<p>and <code>nvcc -V</code> return this:</p>
<blockquote>
<p>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation
Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022
Cuda compilation tools, release 11.8, V11.8.89
Build cuda_11.8.r11.8/compiler.31833905_0</p>
</blockquote>
",5535341.0,,5535341.0,,2023-04-09 01:36:32,2023-04-11 17:17:52,No matching distribution found in the installation of the cuDNN for TensorFlow v2.12 in Anaconda,<python-3.x><tensorflow><anaconda><cudnn>,1,0,,,,CC BY-SA 4.0
68133846,1,68134030.0,,2021-06-25 15:53:45,,10,76684,"<p>i was trying to install tensorflow-gpu on my pycharm (<code>pip install tensorflow-gpu</code>), but unfortunately im getting a Error Message. How can i install this package on my pycharm? What is wrong here? Should i install it directly with cmd? How can I install them with pycharm? However, I was able to install the tenserflow Version 2.5.0 without any problems. Only the Tenserflow gpu I cannot install. Im using python Version 3.7.9</p>
",,user16282895,,user16282895,2021-07-04 12:48:17,2022-05-11 15:22:54,ERROR: Could not install packages due to an OSError: [WinError 5],<python><tensorflow><pip><pycharm>,1,1,,2021-06-25 16:51:25,,CC BY-SA 4.0
69401285,1,69537626.0,,2021-10-01 06:14:13,,10,16014,"<p>my computer has only 1 GPU.</p>
<p>Below is what I get the result by entering someone's code</p>
<pre><code>[name: &quot;/device:CPU:0&quot; device_type: &quot;CPU&quot; memory_limit: 268435456
locality {} incarnation: 16894043898758027805, name: &quot;/device:GPU:0&quot;
device_type: &quot;GPU&quot; memory_limit: 10088284160
locality {bus_id: 1 links {}}
incarnation: 17925533084010082620
physical_device_desc: &quot;device: 0, name: GeForce RTX 3060, pci bus id: 0000:17:00.0, compute 
capability: 8.6&quot;]
</code></pre>
<p>I use jupyter notebook and I run 2 kernels now.
(TensorFlow 2.6.0 and also installed CUDA and cuDNN as TensorFlow guide)</p>
<p>The first kernel is no problem to run my Sequential model from Keras.</p>
<p>But when I learn the same code in the second kernel, I got the error as below.</p>
<p>Attempting to perform BLAS operation using StreamExecutor without BLAS support
[[node sequential_3/dense_21/MatMul (defined at \AppData\Local\Temp/ipykernel_14764/3692363323.py:1) ]] [Op:__inference_train_function_7682]</p>
<p>Function call stack:
train_function</p>
<p>how can I learn multiple kernels without any problem and share them with only 1 GPU?</p>
<p>I am not familiar with TensorFlow 1.x.x version though.</p>
<hr />
<p>I just solved this problem as below. This problem is because when keras run with gpu. It uses almost all vram. So i needed to give memory_limit for each notebook. Here is my code how i could solve it. You can just change memory_limit value.</p>
<pre><code>gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    tf.config.experimental.set_virtual_device_configuration(
        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])
  except RuntimeError as e:
    print(e)
</code></pre>
",16723655.0,,16723655.0,,2021-10-02 07:07:18,2022-02-21 16:09:55,"""Attempting to perform BLAS operation using StreamExecutor without BLAS support"" error occurs",<tensorflow><jupyter-notebook><gpu><tensorflow2.0><blas>,2,0,0.0,,,CC BY-SA 4.0
69125173,1,69153742.0,,2021-09-09 22:05:14,,10,433,"<p>I am writing a small code to calculate the fourth derivative using the method of finite differences in tensorflow. This is as follows:</p>
<pre><code>def action(y,x):
    #spacing between points.
    h = (x[-1] - x[0]) / (int(x.shape[0]) - 1)
    
    #fourth derivative 
    dy4 = (y[4:] - 4*y[3:-1] + 6*y[2:-2] - 4*y[1:-3] + y[:-4])/(h*h*h*h)

    return dy4

x = tf.linspace(0.0, 30, 1000)
y = tf.tanh(x)
dy4 = action(y,x)

sess = tf.compat.v1.Session()
plt.plot(sess.run(dy4))
</code></pre>
<p>This results in the following graph:</p>
<p><a href=""https://i.stack.imgur.com/UT6Or.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/UT6Or.png"" alt=""Graph "" /></a></p>
<p>However if I use essentially the same code but just using numpy, the results are much cleaner:</p>
<pre><code>def fourth_deriv(y, x):
    h = (x[-1] - x[0]) / (int(x.shape[0]) - 1)
    dy = (y[4:] - 4*y[3:-1] + 6*y[2:-2] - 4*y[1:-3] + y[:-4])/(h*h*h*h)
    return dy

x = np.linspace(0.0, 30, 1000)
test = fourth_deriv(np.tanh(x), x)
plt.plot(test)
</code></pre>
<p>Which gives:</p>
<p><a href=""https://i.stack.imgur.com/Njxkv.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Njxkv.png"" alt=""enter image description here"" /></a></p>
<p>What is the issue here? I was thinking at first that the separation between points could be too small to give an accurate computation, but clearly, that's not the case if numpy can handle it fine.</p>
",13948301.0,,9215780.0,,2021-09-16 12:58:06,2021-09-16 13:01:39,Accuracy in Calculating Fourth Derivative using Finite Differences in Tensorflow,<python><numpy><tensorflow>,2,0,0.0,,,CC BY-SA 4.0
74556733,1,,,2022-11-24 06:58:49,,10,22106,"<p>I have Python3.11.0 on a Windows10 PC.
Trying to install tensorflow using:</p>
<pre><code>pip install tensorflow
</code></pre>
<p>gives error. Upon visiting the tensorflow site I realised that it supports only 3.7 - 3.10
Should I downgrade the python version or is any workaround available?</p>
",10430848.0,,,,,2023-03-23 06:08:13,Tensorflow support for Python3.11,<tensorflow><python-3.11>,4,2,,,,CC BY-SA 4.0
65874441,1,,,2021-01-24 18:32:51,,10,17431,"<p>When i run my program i see this error:
<a href=""https://i.stack.imgur.com/WyJqv.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/WyJqv.jpg"" alt=""enter image description here"" /></a></p>
<p>I have CUDA 11 and there is not such dll file in bin folder:</p>
<p><a href=""https://i.stack.imgur.com/wR5bx.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/wR5bx.jpg"" alt=""enter image description here"" /></a></p>
<p>I use:</p>
<p>Python 3.8.7</p>
<p>CUDA 11.0</p>
<p>tensorflow 2.4.1</p>
<p>On this page there is information <a href=""https://www.tensorflow.org/install/source#gpu"" rel=""noreferrer"">https://www.tensorflow.org/install/source#gpu</a>
tensorflow-2.4.0    Python 3.6-3.8  CUDA  11.0
that that versions are correct.
Does anyone know how I can fix this problem? What should I download or reinstall?</p>
",9800633.0,,681865.0,,2021-01-24 23:42:17,2021-01-24 23:42:17,tensorflow: Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found,<python><tensorflow>,0,4,,,,CC BY-SA 4.0
66141547,1,66158495.0,,2021-02-10 17:05:34,,10,14743,"<p>The code below used to work last year, but updates in keras/tensorflow/numpy broke it. It now outputs the exception below. Does anyone know how to make it work again?</p>
<p>I'm using:</p>
<ul>
<li>Tensorflow 2.4.1</li>
<li>Keras 2.4.3</li>
<li>Numpy 1.20.1</li>
<li>Python 3.9.1</li>
</ul>
<pre><code>import numpy as np
from keras.layers import LSTM, Embedding, Input, Bidirectional

dim = 30
max_seq_length = 40
vecs = np.random.rand(45,dim)

input_layer = Input(shape=(max_seq_length,))
embedding_layer = Embedding(len(vecs), dim, weights=[vecs], input_length=max_seq_length, trainable=False, name=&quot;layerA&quot;)(input_layer)
lstm_nobi = LSTM(max_seq_length, return_sequences=True, activation=&quot;linear&quot;, name=&quot;layerB&quot;)
lstm = Bidirectional(lstm_nobi, name=&quot;layerC&quot;)(embedding_layer)
</code></pre>
<p>Complete output of the script above: <a href=""https://pastebin.com/DsQNWVwz"" rel=""noreferrer"">https://pastebin.com/DsQNWVwz</a></p>
<p>Shortened output:</p>
<pre><code>2021-02-10 17:51:13.037468: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-02-10 17:51:13.037899: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-02-10 17:51:13.038418: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File &quot;/run/media/volker/DATA/configruns/load/./test.py&quot;, line 13, in &lt;module&gt;
    lstm = Bidirectional(lstm_nobi, name=&quot;layerC&quot;)(embedding_layer)
  ... omitted, see pastebin ...
  File &quot;/usr/lib/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 852, in __array__
    raise NotImplementedError(
NotImplementedError: Cannot convert a symbolic Tensor (layerC/forward_layerB/strided_slice:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
</code></pre>
",6779860.0,,,,,2021-09-07 06:57:27,NotImplementedError: Cannot convert a symbolic Tensor to a numpy array,<numpy><tensorflow>,2,1,0.0,,,CC BY-SA 4.0
64811841,1,64820487.0,,2020-11-12 21:15:02,,10,17200,"<p>I am trying to install tensorflow-gpu 1.15 using Conda for an easy install of CUDA and cuDNN. The problem is that checking the <a href=""https://www.tensorflow.org/install/source#configuration_options"" rel=""noreferrer"">compatibility chart</a> of the official web I need python 3.6, CUDA 10.0 and cuDNN 7.4.</p>
<p>Searching the Conda rep via <code>conda search cudnn</code> it says that there isn't cuDNN 7.4. Is there any other way to install the required packages? Or maybe tensorflow 1.15 also works with other combinations of versions?</p>
<p>As a side note, python 3.6, tensorflow-gpu 1.15 and CUDA 10 install correctly, but it seems I can't use the GPU correctly without cuDNN.
I just recently started using Conda, so maybe there is a straight forward way to do this that I don't realize. My Conda version is 4.9.1 (miniconda version).</p>
<p>---update---</p>
<p>Just in case I add the error while trying <code>conda create -n myenv -c conda-forge tensorflow-gpu=1.15</code>:</p>
<pre><code>Collecting package metadata (current_repodata.json): done
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: - 
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
failed                                                                                                                                                                                                                                        

UnsatisfiableError: The following specifications were found to be incompatible with each other:

Output in format: Requested package -&gt; Available versions

Package _tflow_select conflicts for:
_tflow_select==2.1.0=gpu
tensorflow==1.15.0 -&gt; _tflow_select[version='2.1.0|2.3.0|2.2.0',build='gpu|mkl|eigen']
Note that strict channel priority may have removed packages required for satisfiability.
</code></pre>
",9948248.0,,681865.0,,2020-12-29 15:11:26,2023-03-11 19:48:11,Tensorflow 1.15 + CUDA + cuDNN installation using Conda,<tensorflow><conda><miniconda>,3,0,0.0,,,CC BY-SA 4.0
69395671,1,,,2021-09-30 16:18:18,,9,12668,"<p>I get this error when I run the model.fit_generator code to train images using the CNN model. I don't understand the error, and what should I do? Can anyone help me?
this is the full error description</p>
<pre><code>Loaded runtime CuDNN library: 8.0.5, but the source was compiled with: 8.1.0.  
CuDNN library needs to have a matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.
If building from sources, ensure the library loaded at runtime is compatible with the version specified during compile configuration.
</code></pre>
",16950703.0,,8618242.0,,2023-06-14 12:56:27,2023-06-14 12:56:27,Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0,<tensorflow>,5,0,,,,CC BY-SA 4.0
67352841,1,67953428.0,,2021-05-02 03:51:54,,9,14916,"<p>I have installed tensorflow-macos and while training this is my CPU usage <img src=""https://i.stack.imgur.com/ta29D.png"" alt="""" />
and GPU usage <img src=""https://i.stack.imgur.com/qtp0k.png"" alt="""" />.</p>
<p>Can I make Tensorflow run on GPU anyway?</p>
",11987955.0,,7117003.0,,2021-05-14 06:21:10,2023-06-19 10:42:29,TensorFlow is not using my M1 MacBook GPU during training,<python><tensorflow><apple-m1>,6,3,0.0,,,CC BY-SA 4.0
68119561,1,68128436.0,,2021-06-24 16:32:47,,9,1063,"<p>I am using the pre-built deep learning VM instances offered by google cloud, with an Nvidia tesla K80 GPU attached. I choose to have Tensorflow 2.5 and CUDA 11.0 automatically installed. When I start the instance, everything works great - I can run:</p>
<pre><code>Import tensorflow as tf
tf.config.list_physical_devices()
</code></pre>
<p>And my function returns the CPU, accelerated CPU, and the GPU. Similarly, if I run <code>tf.test.is_gpu_available()</code>, the function returns True.</p>
<p>However, if I log out, stop the instance, and then restart the instance, running the same exact code only sees the CPU and <code>tf.test.is_gpu_available()</code> results in False. I get an error that looks like the driver initialization is failing:</p>
<pre><code> E tensorflow/stream_executor/cuda/cuda_driver.cc:355] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
</code></pre>
<p>Running nvidia-smi shows that the computer still sees the GPU, but my tensorflow can’t see it.</p>
<p>Does anyone know what could be causing this? I don’t want to have to reinstall everything when I’m restarting the instance.</p>
",16308144.0,,681865.0,,2021-07-18 15:05:02,2021-07-18 15:05:02,Stopping and starting a deep learning google cloud VM instance causes tensorflow to stop recognizing GPU,<tensorflow><google-cloud-platform><nvidia><google-dl-platform>,3,7,0.0,,,CC BY-SA 4.0
64121386,1,69376624.0,,2020-09-29 14:09:58,,9,1915,"<p>I was curious of the new &quot;turn on/off&quot; <a href=""http://jnack.com/blog/2020/09/17/google-meet-adds-background-blur-with-a-twist/"" rel=""noreferrer"">background blur functionality</a> of Google Meet (currently in test). I have investigated a bit and it seems it is using <strong>Tensorflow Lite</strong> models:</p>
<pre><code>segm_heavy.tflite
segm_lite.tflite
</code></pre>
<p>via <strong><a href=""https://webassembly.org/"" rel=""noreferrer"">WASM</a></strong></p>
<pre><code>mediapipe_wasm_simd.wasm
</code></pre>
<p>while the model graph should be</p>
<pre><code>background_blur_graph.binarypb
</code></pre>
<p>The model seems works at the level of the <code>HTMLCanvasElement</code> as far as I can see. Anyone aware of a similar model?</p>
<p><strong>[UPDATE]</strong></p>
<p>Thanks to <a href=""https://stackoverflow.com/users/5627842/jason-mayes"">Jason Mayes</a> and <a href=""https://stackoverflow.com/users/63069/physicaled"">Physical Ed</a> I was able to reproduce a very close background blur effect in the <a href=""https://github.com/tensorflow/tfjs-models/tree/master/body-pix"" rel=""noreferrer"">Google's BodyPix demo</a></p>
<p>The settings of the application are showed in the Controls box. There is a <code>backgroundBlurAmount</code> option that let you customize the blur percentage to apply as well.</p>
<p><a href=""https://i.stack.imgur.com/z0A4r.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/z0A4r.png"" alt=""enter image description here"" /></a></p>
<p>The result is almost close to the official Google Meet application.</p>
<p><a href=""https://i.stack.imgur.com/QySIY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QySIY.png"" alt=""enter image description here"" /></a></p>
",758836.0,,758836.0,,2020-10-05 07:58:16,2021-09-29 12:42:54,Google Meet background Blur,<tensorflow><tensorflow-lite><tensorflow.js><google-meet>,1,6,0.0,,,CC BY-SA 4.0
63231021,1,,,2020-08-03 14:10:33,,9,8337,"<p>I have a python virtual environment (conda) where I’ve installed CUDA toolkit 10.1.243 and tensorflow-gpu 2.3.0rc0. My CUDA driver is 11.0.</p>
<p>In order to test if tensorflow was installed to GPU correctly, I ran a series of commands from within the venv:</p>
<p><code>tf.test.is_built_with_cuda()</code></p>
<p>True</p>
<p><code>tf.config.list_physical_devices(‘GPU’)</code></p>
<p>Found device 0 with properties:
pciBusID: 0000:01:00.0 name: Quadro M2000M computeCapability: 5.0
[PhysicalDevice(name=’/physical_device:GPU:0’, device_type=‘GPU’)]</p>
<p><code>python -c &quot;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000,1000])))&quot;</code></p>
<p>tensorflow.python.framework.errors_impl.InternalError: CUDA runtime implicit initialization on GPU:0 failed. Status: device kernel image is invalid</p>
<p>I am not sure how to troubleshoot this. I have a feeling that it is related to modifying the compilation such that tensorflow supports the compute capability of my device (5.0), but I am not sure how to proceed. Thank you!!</p>
",13930709.0,,681865.0,,2020-08-03 14:39:16,2021-04-21 09:13:53,Tensorflow-gpu issue (CUDA runtime error: device kernel image is invalid),<python><tensorflow><gpu><nvidia>,3,0,0.0,,,CC BY-SA 4.0
70277737,1,,,2021-12-08 15:41:40,,9,5208,"<p>I cant able to install tensorflow-io on m1 mac under Environment. Though i Succeeded in installing tensorflow-macos and other libraries but Getting error in tensorflow_io</p>
<p>&quot;ERROR: Could not find a version that satisfies the requirement tensorflow-io (from versions: none)</p>
<p>ERROR: No matching distribution found for tensorflow-io&quot;</p>
<p>Machine: M1 Mac</p>
<p>OS: MacOS Monterey 12.0.1</p>
<p>Env: Miniforge</p>
<p>Python: 3.9</p>
<p>Tensorflow version: 2.5.0</p>
",14212577.0,,,,,2023-04-18 02:09:18,Can't install tensorflow-io on m1,<python><tensorflow><apple-m1>,3,0,,,,CC BY-SA 4.0
62943048,1,,,2020-07-16 20:33:16,,9,30134,"<p>Unable to solve this error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError:  logits and labels must have the same first dimension, got logits shape [4,1] and labels shape [12]
     [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /Users/astro/pythonprojects/covid_chest_xray_image_classification/main.py:86) ]] [Op:__inference_train_function_978]

Function call stack:
train_function

2020-07-17 01:50:11.552216: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
     [[{{node PyFunc}}]]

Process finished with exit code 1
</code></pre>
<p>I am trying to classify Images based on three categories,</p>
<p>Here is my code if it helps:</p>
<pre><code># Classification of cases from Chest-xray images
import os
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import imread
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten
from tensorflow.keras.callbacks import EarlyStopping

data_dir = 'Covid19-dataset'

# Creating paths
train_path = 'Covid19-dataset/train'
test_path = 'Covid19-dataset/test'

# Displaying one image
normal_xray_path = train_path + '/Normal/01.jpeg'
normal_xray_path_arr = imread(normal_xray_path)

# print(len(os.listdir(os.path.join(train_path, 'Viral Pneumonia'))))
# number of images for Normal, Covid, Viral Pneumonia = 70,111,70

# Dimensionality study
dim1 = []
dim2 = []
test_covid_path = test_path + '/Covid'
for image_name in os.listdir(os.path.join(test_path, 'Covid')):
    img = imread(os.path.join(test_covid_path, image_name))
    # print(img.shape)
    '''debug for tuple length 2'''
    # if len(img.shape)==2:
    #     print(image_name)
    d1, d2, colors = img.shape
    dim1.append(d1)
    dim2.append(d2)
# print(np.mean(dim1), np.mean(dim2)) = 728.2 782.6

# Keeping dimensions of images same
image_shape = (540, 583, 3)

img_gen = ImageDataGenerator(rotation_range=5, width_shift_range=0.1, height_shift_range=0.1,
                             rescale=1 / 255, shear_range=0.1, zoom_range=0.1)

img_gen.flow_from_directory(test_path)
# plt.imshow(normal_xray_path_arr)
# plt.show()
# plt.imshow(img_gen.random_transform(normal_xray_path_arr))
# plt.show()

model = Sequential()
model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=image_shape, activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=image_shape, activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=image_shape, activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Conv2D(filters=128, kernel_size=(3, 3), input_shape=image_shape, activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(1, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=2)

# Settings
batch_size = 4

train_image_gen = img_gen.flow_from_directory(train_path, target_size=image_shape[:2],
                                              color_mode='rgb',
                                              batch_size=batch_size,
                                              class_mode='categorical')
test_image_gen = img_gen.flow_from_directory(test_path, target_size=image_shape[:2],
                                             color_mode='rgb',
                                             batch_size=batch_size,
                                             class_mode='categorical',
                                             shuffle=False)
model.summary()

# Result index
# print(train_image_gen.class_indices) ={'Covid': 0, 'Normal': 1, 'Viral Pneumonia': 2}

results = model.fit_generator(train_image_gen, epochs=30, validation_data=test_image_gen,
                              callbacks=[early_stop])
</code></pre>
<p>The output:</p>
<pre><code>C:\Users\astro\AppData\Local\Programs\Python\Python38\python.exe C:/Users/astro/pythonprojects/covid_chest_xray_image_classification/main.py
2020-07-17 01:50:05.579136: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Found 57 images belonging to 3 classes.
2020-07-17 01:50:07.487490: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-07-17 01:50:07.513725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.515GHz coreCount: 14 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s
2020-07-17 01:50:07.514010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-17 01:50:07.517418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-17 01:50:07.520744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-17 01:50:07.521789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-17 01:50:07.525789: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-17 01:50:07.527902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-17 01:50:07.535988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-17 01:50:07.536194: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-07-17 01:50:07.536719: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-07-17 01:50:07.543508: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1195281a180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-17 01:50:07.543717: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-17 01:50:07.543983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1650 computeCapability: 7.5
coreClock: 1.515GHz coreCount: 14 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 178.84GiB/s
2020-07-17 01:50:07.544272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-07-17 01:50:07.544416: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-17 01:50:07.544560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-07-17 01:50:07.544703: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-07-17 01:50:07.544846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-07-17 01:50:07.544997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-07-17 01:50:07.545141: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-17 01:50:07.545307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-07-17 01:50:08.078548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-07-17 01:50:08.078710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-07-17 01:50:08.078803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-07-17 01:50:08.079063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2917 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-07-17 01:50:08.081913: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1197b02d2c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-07-17 01:50:08.082107: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1650, Compute Capability 7.5
Found 209 images belonging to 3 classes.
WARNING:tensorflow:From C:/Users/astro/pythonprojects/covid_chest_xray_image_classification/main.py:86: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
Please use Model.fit, which supports generators.
Found 57 images belonging to 3 classes.
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 538, 581, 32)      896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 269, 290, 32)      0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 267, 288, 64)      18496     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 133, 144, 64)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 131, 142, 64)      36928     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 65, 71, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 63, 69, 128)       73856     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 31, 34, 128)       0         
_________________________________________________________________
flatten (Flatten)            (None, 134912)            0         
_________________________________________________________________
dense (Dense)                (None, 128)               17268864  
_________________________________________________________________
dropout (Dropout)            (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 129       
=================================================================
Total params: 17,399,169
Trainable params: 17,399,169
Non-trainable params: 0
_________________________________________________________________
Epoch 1/30
2020-07-17 01:50:09.168943: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-07-17 01:50:09.706730: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-07-17 01:50:10.841108: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
Traceback (most recent call last):
  File &quot;C:/Users/astro/pythonprojects/covid_chest_xray_image_classification/main.py&quot;, line 86, in &lt;module&gt;
    results = model.fit_generator(train_image_gen, epochs=30, validation_data=test_image_gen,
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\util\deprecation.py&quot;, line 324, in new_func
    return func(*args, **kwargs)
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\engine\training.py&quot;, line 1465, in fit_generator
    return self.fit(
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\engine\training.py&quot;, line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\keras\engine\training.py&quot;, line 848, in fit
    tmp_logs = train_function(iterator)
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 580, in __call__
    result = self._call(*args, **kwds)
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 644, in _call
    return self._stateless_fn(*args, **kwds)
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 2420, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 1661, in _filtered_call
    return self._call_flat(
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 1745, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 593, in call
    outputs = execute.execute(
  File &quot;C:\Users\astro\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\python\eager\execute.py&quot;, line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError:  logits and labels must have the same first dimension, got logits shape [4,1] and labels shape [12]
     [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits (defined at /Users/astro/pythonprojects/covid_chest_xray_image_classification/main.py:86) ]] [Op:__inference_train_function_978]

Function call stack:
train_function

2020-07-17 01:50:11.552216: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
     [[{{node PyFunc}}]]

Process finished with exit code 1
</code></pre>
<p>The error also includes:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError:  logits and labels must have the same first dimension, got logits shape [4,1] and labels shape [12]
</code></pre>
<p>I don't know how to solve this issue</p>
",13140685.0,,,,,2022-12-30 10:48:46,"Unable to solve, Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized",<python><python-3.x><tensorflow><conv-neural-network><tensorflow2.0>,6,4,,,,CC BY-SA 4.0
69641708,1,,,2021-10-20 07:09:01,,9,17829,"<p>Hi I am a beginner in DL and tensorflow,</p>
<p>I created a CNN (you can see the model below)</p>
<pre><code>model = tf.keras.Sequential()

model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=7, activation=&quot;relu&quot;, input_shape=[512, 640, 3]))
model.add(tf.keras.layers.MaxPooling2D(2))
model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;))
model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation=&quot;relu&quot;))
model.add(tf.keras.layers.MaxPooling2D(2))
model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=&quot;relu&quot;))
model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation=&quot;relu&quot;))
model.add(tf.keras.layers.MaxPooling2D(2))

model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(2, activation='softmax'))

optimizer = tf.keras.optimizers.SGD(learning_rate=0.2) #, momentum=0.9, decay=0.1)
model.compile(optimizer=optimizer, loss='mse', metrics=['accuracy'])
</code></pre>
<p>I tried building and training it with the cpu and it was completed successfully (but very slowly) so I decided to install tensorflow-gpu.
Installed everything as instructed in <a href=""https://www.tensorflow.org/install/gpu"" rel=""noreferrer"">https://www.tensorflow.org/install/gpu</a>).</p>
<p>But now when I am trying to build the model this error comes up:</p>
<pre><code>&gt; Traceback (most recent call last):   File
&gt; &quot;C:/Users/thano/Documents/Py_workspace/AI_tensorflow/fire_detection/main.py&quot;,
&gt; line 63, in &lt;module&gt;
&gt;     model = create_models.model1()   File &quot;C:\Users\thano\Documents\Py_workspace\AI_tensorflow\fire_detection\create_models.py&quot;,
&gt; line 20, in model1
&gt;     model.add(tf.keras.layers.Dense(128, activation='relu'))   File &quot;C:\Python37\lib\site-packages\tensorflow\python\training\tracking\base.py&quot;,
&gt; line 530, in _method_wrapper
&gt;     result = method(self, *args, **kwargs)   File &quot;C:\Python37\lib\site-packages\keras\engine\sequential.py&quot;, line 217,
&gt; in add
&gt;     output_tensor = layer(self.outputs[0])   File &quot;C:\Python37\lib\site-packages\keras\engine\base_layer.py&quot;, line 977,
&gt; in __call__
&gt;     input_list)   File &quot;C:\Python37\lib\site-packages\keras\engine\base_layer.py&quot;, line 1115,
&gt; in _functional_construction_call
&gt;     inputs, input_masks, args, kwargs)   File &quot;C:\Python37\lib\site-packages\keras\engine\base_layer.py&quot;, line 848,
&gt; in _keras_tensor_symbolic_call
&gt;     return self._infer_output_signature(inputs, args, kwargs, input_masks)   File
&gt; &quot;C:\Python37\lib\site-packages\keras\engine\base_layer.py&quot;, line 886,
&gt; in _infer_output_signature
&gt;     self._maybe_build(inputs)   File &quot;C:\Python37\lib\site-packages\keras\engine\base_layer.py&quot;, line 2659,
&gt; in _maybe_build
&gt;     self.build(input_shapes)  # pylint:disable=not-callable   File &quot;C:\Python37\lib\site-packages\keras\layers\core.py&quot;, line 1185, in
&gt; build
&gt;     trainable=True)   File &quot;C:\Python37\lib\site-packages\keras\engine\base_layer.py&quot;, line 663,
&gt; in add_weight
&gt;     caching_device=caching_device)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\training\tracking\base.py&quot;,
&gt; line 818, in _add_variable_with_custom_getter
&gt;     **kwargs_for_getter)   File &quot;C:\Python37\lib\site-packages\keras\engine\base_layer_utils.py&quot;, line
&gt; 129, in make_variable
&gt;     shape=variable_shape if variable_shape else None)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\variables.py&quot;,
&gt; line 266, in __call__
&gt;     return cls._variable_v1_call(*args, **kwargs)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\variables.py&quot;,
&gt; line 227, in _variable_v1_call
&gt;     shape=shape)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\variables.py&quot;,
&gt; line 205, in &lt;lambda&gt;
&gt;     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\variable_scope.py&quot;,
&gt; line 2626, in default_variable_creator
&gt;     shape=shape)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\variables.py&quot;,
&gt; line 270, in __call__
&gt;     return super(VariableMetaclass, cls).__call__(*args, **kwargs)   File
&gt; &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py&quot;,
&gt; line 1613, in __init__
&gt;     distribute_strategy=distribute_strategy)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py&quot;,
&gt; line 1740, in _init_from_args
&gt;     initial_value = initial_value()   File &quot;C:\Python37\lib\site-packages\keras\initializers\initializers_v2.py&quot;,
&gt; line 517, in __call__
&gt;     return self._random_generator.random_uniform(shape, -limit, limit, dtype)   File
&gt; &quot;C:\Python37\lib\site-packages\keras\initializers\initializers_v2.py&quot;,
&gt; line 973, in random_uniform
&gt;     shape=shape, minval=minval, maxval=maxval, dtype=dtype, seed=self.seed)   File
&gt; &quot;C:\Python37\lib\site-packages\tensorflow\python\util\dispatch.py&quot;,
&gt; line 206, in wrapper
&gt;     return target(*args, **kwargs)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\random_ops.py&quot;,
&gt; line 315, in random_uniform
&gt;     result = math_ops.add(result * (maxval - minval), minval, name=name)   File
&gt; &quot;C:\Python37\lib\site-packages\tensorflow\python\util\dispatch.py&quot;,
&gt; line 206, in wrapper
&gt;     return target(*args, **kwargs)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\math_ops.py&quot;,
&gt; line 3943, in add
&gt;     return gen_math_ops.add_v2(x, y, name=name)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\ops\gen_math_ops.py&quot;,
&gt; line 454, in add_v2
&gt;     _ops.raise_from_not_ok_status(e, name)   File &quot;C:\Python37\lib\site-packages\tensorflow\python\framework\ops.py&quot;,
&gt; line 6941, in raise_from_not_ok_status
&gt;     six.raise_from(core._status_to_exception(e.code, message), None)   File &quot;&lt;string&gt;&quot;, line 3, in raise_from
&gt; tensorflow.python.framework.errors_impl.ResourceExhaustedError: failed
&gt; to allocate memory [Op:AddV2]
</code></pre>
<p>Any ideas what might be the problem?</p>
",12429245.0,,,,,2023-05-01 13:25:51,tensorflow.python.framework.errors_impl.ResourceExhaustedError: failed to allocate memory [Op:AddV2],<python><tensorflow><deep-learning><conv-neural-network><gpu>,5,2,,,,CC BY-SA 4.0
70254127,1,,,2021-12-07 02:08:26,,9,467,"<p>I need to build a custom loss method based on BLEU. I'm passing my LabelEncoder in the constructor to reverse labels and predictions and calculate the bleu distance.</p>
<p>Here is my Loss class</p>
<pre class=""lang-py prettyprint-override""><code>class CIMCodeSuccessiveLoss(Loss):

    def __init__(self, labelEncoder: LabelEncoder):
        super().__init__()
        self.le = labelEncoder

    def bleu_score(self, true_label, pred_label):
        cim_true_label = self.le.inverse_transform(true_label.numpy())
        cim_pred_label = self.le.inverse_transform(pred_label.numpy())
        bleu_scores = [sentence_bleu(list(one_true_label),
                                     list(one_pred_label),
                                     weights=(0.5, 0.25, 0.125, 0.125)) for one_true_label, one_pred_label in
                       zip(cim_true_label, cim_pred_label)]
        return np.float32(bleu_scores)

    def call(self, y_true, y_pred):
        labeled_y_pred = tf.cast(tf.argmax(y_pred, axis=-1), tf.int32)
        bleu = tf.py_function(self.bleu_score, (tf.reshape(y_true, [-1]), labeled_y_pred), tf.float32)
        return tf.reduce_sum(tf.square(1 - bleu))
</code></pre>
<p>The bleu_score method is calculating the correct scores and returns a NumPy array.
when I try to return the squared sum, I get this error</p>
<pre><code>raise ValueError(f&quot;No gradients provided for any variable: {variable}.
</code></pre>
<p>I'm also providing the model:</p>
<pre><code>inputs = tf.keras.Input(shape=(1,), dtype=tf.string)
x = vectorize_layer(inputs)
x = Embedding(vocab_size, embedding_dim, name=&quot;embedding&quot;)(x)
x = LSTM(units=32, name=&quot;lstm&quot;)(x)
outputs = Dense(classes_number, name=&quot;classification&quot;)(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs, name=&quot;first_cim_classifier&quot;)

model.summary()


# we add early stopping for our model.
early_stopping = EarlyStopping(monitor='loss', patience=2)

model.compile(
    loss=CIMCodeSuccessiveLoss(le),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=[&quot;accuracy&quot;, &quot;crossentropy&quot;],
    run_eagerly=True)

trained_model = model.fit(np.array(x_train), np.array(y_train), batch_size=64, epochs=10,
                          validation_data=(np.array(x_val), np.array(y_val)),
                          callbacks=[early_stopping])
</code></pre>
<p>Any help is appreciated. Thanks in advance.</p>
",8576878.0,,8576878.0,,2021-12-09 16:02:21,2021-12-14 18:24:56,"Creating BLEU loss method on tensorflow gives ""No gradient provided""",<python><tensorflow><deep-learning>,2,3,0.0,,,CC BY-SA 4.0
66984809,1,,,2021-04-07 11:05:51,,9,16447,"<p>I remotely access High-performance computing nodes. I am not sure about NVIDIA Collective Communications Library (NCCL) is installed in my directory or not. Is there any way to check whether the NCCL is installed or not?</p>
",14808637.0,,14808637.0,,2023-01-01 05:58:01,2023-01-01 18:02:49,How to check the version of NCCL,<python><tensorflow><nvidia><horovod>,2,2,,,,CC BY-SA 4.0
66962099,1,66962230.0,,2021-04-06 02:47:16,,9,11387,"<p>I am using AUTOTUNE for audio processing.</p>
<pre><code>import tensorflow as tf
AUTOTUNE = tf.data.AUTOTUNE
</code></pre>
<p>But i am getting attribute error which said &quot;module 'tensorflow._api.v2.data' has no attribute 'AUTOTUNE'&quot;. Tensorflow version is 2.3.0. How to solve this?</p>
",11383872.0,,,,,2022-04-08 17:49:07,Getting attribute error when using AUTOTUNE in Tensorflow?,<python><tensorflow>,4,1,,,,CC BY-SA 4.0
72129213,1,,,2022-05-05 14:49:04,,9,7650,"<p>I want to use a GPU inside a Visual Studio Code docker container to train model with TensorFlow. To build an image for my container I use next Dockerfile:</p>
<pre><code>FROM mcr.microsoft.com/vscode/devcontainers/anaconda:0-3


ARG PROJECT_NAME=fire_rec

ARG NODE_VERSION=&quot;none&quot;
RUN if [ &quot;${NODE_VERSION}&quot; != &quot;none&quot; ]; then su vscode -c &quot;umask 0002 &amp;&amp; . /usr/local/share/nvm/nvm.sh &amp;&amp; nvm install ${NODE_VERSION} 2&gt;&amp;1&quot;; fi


COPY environment.yml* .devcontainer/noop.txt /tmp/conda-tmp/
RUN if [ -f &quot;/tmp/conda-tmp/environment.yml&quot; ]; then umask 0002 &amp;&amp; /opt/conda/bin/conda env update -n base -f /tmp/conda-tmp/environment.yml; fi \
    &amp;&amp; rm -rf /tmp/conda-tmp


WORKDIR /srv/${PROJECT_NAME}

COPY requirements.txt /srv/${PROJECT_NAME}

RUN apt-get update &amp;&amp; apt-get install -y python3-opencv
RUN apt-get update &amp;&amp; apt-get install -y pip
RUN python3 -m pip install --no-cache -r requirements.txt
RUN apt-get update &amp;&amp; apt-get install -y nvidia-cuda-toolkit
</code></pre>
<p>&quot;requirements.txt&quot; consists of:</p>
<pre><code>opencv-python
tensorflow-gpu
numpy
matplotlib
albumentations
tensorflow_addons
</code></pre>
<p>Also I have .devcontainer.json file:</p>
<pre><code>{
    &quot;name&quot;: &quot;Anaconda (Python 3)&quot;,
    &quot;build&quot;: { 
        &quot;context&quot;: &quot;..&quot;,
        &quot;dockerfile&quot;: &quot;Dockerfile&quot;,
        &quot;args&quot;: {
            &quot;NODE_VERSION&quot;: &quot;none&quot;
        }
    },

    &quot;settings&quot;: { 
        &quot;python.defaultInterpreterPath&quot;: &quot;/opt/conda/bin/python&quot;,
        &quot;python.linting.enabled&quot;: true,
        &quot;python.linting.pylintEnabled&quot;: true,
        &quot;python.formatting.autopep8Path&quot;: &quot;/opt/conda/bin/autopep8&quot;,
        &quot;python.formatting.yapfPath&quot;: &quot;/opt/conda/bin/yapf&quot;,
        &quot;python.linting.flake8Path&quot;: &quot;/opt/conda/bin/flake8&quot;,
        &quot;python.linting.pycodestylePath&quot;: &quot;/opt/conda/bin/pycodestyle&quot;,
        &quot;python.linting.pydocstylePath&quot;: &quot;/opt/conda/bin/pydocstyle&quot;,
        &quot;python.linting.pylintPath&quot;: &quot;/opt/conda/bin/pylint&quot;
    },

    &quot;extensions&quot;: [
        &quot;ms-python.python&quot;,
        &quot;ms-python.vscode-pylance&quot;
    ],

    &quot;remoteUser&quot;: &quot;vscode&quot;,
}
</code></pre>
<p>I successfully built the image and launched the container. But when I try to launch this code in jupyter-notebook inside the container:</p>
<pre><code>import tensorflow as tf

tf.config.list_physical_devices('GPU')
</code></pre>
<p>I get next messages:</p>
<pre><code>2022-05-05 14:42:02.712454: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2022-05-05 14:42:02.712483: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
</code></pre>
<p>So this code fails to use GPU. How can I fix this problem?</p>
",16207818.0,,681865.0,,2022-07-16 07:28:37,2022-08-03 00:03:44,Using GPU in VS code container,<python><docker><tensorflow><visual-studio-code>,4,1,0.0,,,CC BY-SA 4.0
64721602,1,,,2020-11-06 21:06:38,,9,895,"<p>I've been looking into the loss function of the training and the validation dataset, and I keep seeing the validation loss being smaller than the training loss, even when they are the same data set. I'm trying to get some insight as to why this would be the case.</p>
<p>I am training a model in tensorflow to predict some time series data.
Consequently, the model creation and preprocessing is as follows:</p>
<pre><code>window_size = 40
batch_size  = 32
forecast_period = 6
model_name = &quot;LSTM&quot;
tf.keras.backend.clear_session()

_seed = 42
tf.random.set_seed(_seed)

def _sub_to_batch(sub):
    return sub.batch(window_size, drop_remainder=True)

def _return_input_output(tensor):
    _input  = tensor[:, :-forecast_period, :]
    _output = tensor[:, forecast_period:, :]
    return _input, _output

def _reshape_tensor(tensor):
    tensor = tf.expand_dims(tensor, axis=-1)
    tensor = tf.transpose(tensor, [1, 0, 2])
    return tensor


# total elements after unbatch(): 3813
train_ts_dataset = tf.data.Dataset.from_tensor_slices(train_ts)\
                            .window(window_size, shift=1)\
                            .flat_map(_sub_to_batch)\
                            .map(_reshape_tensor)\
                            .map(_return_input_output)
#                             .unbatch().shuffle(buffer_size=500, seed=_seed).batch(batch_size)\
#                             .map(_return_input_output)

valid_ts_dataset = tf.data.Dataset.from_tensor_slices(valid_ts)\
                            .window(window_size, shift=1)\
                            .flat_map(_sub_to_batch)\
                            .map(_reshape_tensor)\
                            .unbatch().shuffle(buffer_size=500, seed=_seed).batch(batch_size)\
                            .map(_return_input_output)

def _forecast_mae(y_pred, y_true):
    _y_pred = y_pred[:, -forecast_period:, :]
    _y_true = y_true[:, -forecast_period:, :]
    mae = tf.losses.MAE(_y_true, _y_pred)
    return mae

def _accuracy(y_pred, y_true):
    # print(y_true) =&gt; Tensor(&quot;sequential/time_distributed/Reshape_1:0&quot;, shape=(None, 34, 1), dtype=float32)
    # y_true[-forecast_period:, :]  =&gt;   Tensor(&quot;strided_slice_4:0&quot;, shape=(None, 34, 1), dtype=float32)
    # y_true[:, -forecast_period:, :] =&gt; Tensor(&quot;strided_slice_4:0&quot;, shape=(None, 6, 1), dtype=float32)

    _y_pred = y_pred[:, -forecast_period:, :]
    _y_pred = tf.reshape(_y_pred, shape=[-1, forecast_period])
    _y_true = y_true[:, -forecast_period:, :]
    _y_true = tf.reshape(_y_true, shape=[-1, forecast_period])

    # MAPE: Tensor(&quot;Mean_1:0&quot;, shape=(None, 1), dtype=float32)
    MAPE = tf.math.reduce_mean(tf.math.abs((_y_pred - _y_true) / _y_true), axis=1, keepdims=True)

    accuracy = 1 - MAPE
    accuracy = tf.where(accuracy &lt; 0, tf.zeros_like(accuracy), accuracy)
    accuracy = tf.reduce_mean(accuracy)
    return accuracy

model = k.models.Sequential([
    k.layers.Bidirectional(k.layers.LSTM(units=100, return_sequences=True), input_shape=(None, 1)),
    k.layers.Bidirectional(k.layers.LSTM(units=100, return_sequences=True)),
    k.layers.TimeDistributed(k.layers.Dense(1))
])

model_name = []
model_name_symbols = {&quot;bidirectional&quot;: &quot;BILSTM_1&quot;, &quot;bidirectional_1&quot;: &quot;BILSTM_2&quot;, &quot;time_distributed&quot;: &quot;td&quot;}
for l in model.layers:
    model_name.append(model_name_symbols.get(l.name, l.name))

model_name = &quot;_&quot;.join(model_name)
print(model_name)

for i, (x, y) in enumerate(train_ts_dataset):
    print(i, x.numpy().shape, y.numpy().shape)
</code></pre>
<p>The output of the shapes of the datasets is as follows:</p>
<pre><code>BILSTM_1_BILSTM_2_td
0 (123, 34, 1) (123, 34, 1)
1 (123, 34, 1) (123, 34, 1)
2 (123, 34, 1) (123, 34, 1)
3 (123, 34, 1) (123, 34, 1)
4 (123, 34, 1) (123, 34, 1)
5 (123, 34, 1) (123, 34, 1)
6 (123, 34, 1) (123, 34, 1)
7 (123, 34, 1) (123, 34, 1)
8 (123, 34, 1) (123, 34, 1)
</code></pre>
<p>then:</p>
<pre><code>_datetime = datetime.datetime.now().strftime(&quot;%Y%m%d-%H-%M-%S&quot;)
_log_dir = os.path.join(&quot;.&quot;, &quot;logs&quot;, &quot;fit7&quot;, model_name, _datetime)

tensorboard_cb = k.callbacks.TensorBoard(log_dir=_log_dir)

model.compile(loss=&quot;mae&quot;, optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=[_forecast_mae, _accuracy])

history = model.fit(train_ts_dataset, epochs=100, validation_data=train_ts_dataset, callbacks=[tensorboard_cb])
</code></pre>
<p>I've been looking into the loss function of the training and the validation dataset, and <em>I keep seeing the validation loss being smaller than the training loss.</em> I could be underfitting. However, I replaced the validation set with the training set as a simple test to monitor the loss and accuracy while training and testing. But I'm still getting validation accuracy being greater than the training one. Below is the accuracy across the training and the validation datasets:</p>
<p><a href=""https://i.stack.imgur.com/zZAAf.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/zZAAf.png"" alt=""enter image description here"" /></a></p>
<p>To me its very weird I'm getting a validation accuracy that is greater than the training accuracy though I'm using the same dataset for training and testing. And there is no dropout, no batchNormalization layer etc...</p>
<p>Any hint on what could be the reason for this behavior? That would be much appreciated!!</p>
<p>===================================================================</p>
<p>Here are made some modification to the code to check if the batch size has any effect or not. Additionally, in order to remove any doubts in the <code>tf.data.Dataset</code>, I used numpy arrays as input. Hence the new code is as follows:</p>
<pre><code>custom_train_ts   = train_ts.transpose(1, 0)[..., np.newaxis]
custom_train_ts_x = custom_train_ts[:, :window_size, :] # size: 123, window_size, 1
custom_train_ts_y = custom_train_ts[:, -window_size:, :] # size: 123, window_size, 1

custom_valid_ts   = valid_ts.transpose(1, 0)[..., np.newaxis]
custom_valid_ts_x = custom_valid_ts[:, :window_size, :]
custom_valid_ts_y = custom_valid_ts[:, -window_size:, :]
custom_valid_ts   = (custom_valid_ts_x, custom_valid_ts_y)
</code></pre>
<p>Secondly, in order to make sure that the accuracy is calculated over the entire dataset, and not dependent on the batch size, I fed in the dataset as is, without batching. Additionally, I implemented a custom metric as follows:</p>
<pre><code>def _accuracy(y_true, y_pred):
    # print(y_true) =&gt; Tensor(&quot;sequential/time_distributed/Reshape_1:0&quot;, shape=(None, 34, 1), dtype=float32)
    # y_true[-forecast_period:, :]  =&gt;   Tensor(&quot;strided_slice_4:0&quot;, shape=(None, 34, 1), dtype=float32)
    # y_true[:, -forecast_period:, :] =&gt; Tensor(&quot;strided_slice_4:0&quot;, shape=(None, 6, 1), dtype=float32)

    _y_pred = y_pred[:, -forecast_period:, :]
    _y_pred = tf.reshape(_y_pred, shape=[-1, forecast_period])
    _y_true = y_true[:, -forecast_period:, :]
    _y_true = tf.reshape(_y_true, shape=[-1, forecast_period])

    # MAPE: Tensor(&quot;Mean_1:0&quot;, shape=(None, 1), dtype=float32)
    MAPE = tf.math.reduce_mean(tf.math.abs((_y_pred - _y_true) / _y_true), axis=1, keepdims=True)

    accuracy = 1 - MAPE
    accuracy = tf.where(accuracy &lt; 0, tf.zeros_like(accuracy), accuracy)        
    accuracy = tf.reduce_mean(accuracy)
    return accuracy


class MyAccuracy(tf.keras.metrics.Metric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.accuracy_function = _accuracy
        self.y_true_lst = []
        self.y_pred_lst = []

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.y_true_lst.append(y_true)
        self.y_pred_lst.append(y_pred)

    def result(self):
        y_true_concat = tf.concat(self.y_true_lst, axis=0)
        y_pred_concat = tf.concat(self.y_pred_lst, axis=0)
        accuracy = self.accuracy_function(y_true_concat, y_pred_concat)
        self.y_true_lst = []
        self.y_pred_lst = []
        return accuracy
    def get_config(self):
        base_config = super().get_config()
        return {**base_config}
</code></pre>
<p>Finally, model compile and fit as:</p>
<pre><code>model.compile(loss=&quot;mae&quot;, optimizer=tf.optimizers.Adam(hparams[&quot;learning_rate&quot;]), 
              metrics=[tf.metrics.MAE, MyAccuracy()])

history = model.fit(custom_train_ts_x, custom_train_ts_y, epochs=120, batch_size=123, validation_data=custom_valid_ts, 
                    callbacks=[tensorboard_cb])
</code></pre>
<p>And when I looked into the training and the validation accuracy in tensorboard I got the following:</p>
<p><a href=""https://i.stack.imgur.com/Og8NL.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Og8NL.png"" alt=""enter image description here"" /></a></p>
<p>Therefore, clearly, this doesn't make any sense. Additionally, in this case, I make sure that calculate the accuracy only one, at the end of the epoch after calling the <code>result()</code>. However, when looking into the validation loss, I found that the training loss is lower than the validation loss:</p>
<p><a href=""https://i.stack.imgur.com/Jb8Uh.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Jb8Uh.png"" alt=""enter image description here"" /></a></p>
",7886651.0,,7886651.0,,2020-11-11 02:35:28,2020-11-14 13:34:41,Why would using the same dataset for training and testing gives different accuracies?,<python><tensorflow>,1,11,0.0,,,CC BY-SA 4.0
62764336,1,,,2020-07-06 20:52:35,,9,488,"<p>As far as I understand, TensorFlow uses <a href=""https://www.tensorflow.org/tfx/guide/mlmd"" rel=""noreferrer"">MLMD</a> to record and retrieve metadata associated with workflows. This may include:</p>
<ol>
<li>results of pipeline components</li>
<li>metadata about artifacts generated through the components of the pipelines</li>
<li>metadata about executions of these components</li>
<li>metadata about the pipeline and associated lineage information</li>
</ol>
<p><strong>Features:</strong></p>
<p>Does the above (e.g. #1 aka &quot;results of components&quot;) imply that MLMD stores actual <strong>data</strong>? (e.g. input <strong>features</strong> for ML training?). If not, what does it mean by results of pipeline components?</p>
<p><strong>Orchestration and pipeline history:</strong></p>
<p>Also, when using TFX with e.g. AirFlow, which uses its own metastore (e.g. metadata about DAGs, their runs, and other Airflow configurations like users, roles, and connections) does MLMD store redundant information? Does it supersede it?</p>
",1732769.0,,,user11530462,2020-07-10 07:06:27,2021-11-05 15:05:53,Data stored in MLMD in TensorFlow TFX,<tensorflow><deep-learning><tfx><mlmd>,2,0,0.0,,,CC BY-SA 4.0
69315586,1,69315741.0,,2021-09-24 13:08:30,,9,3044,"<p>I am going through this tutorial on how to customize the training loop</p>
<p><a href=""https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/customizing_what_happens_in_fit.ipynb#scrollTo=46832f2077ac"" rel=""noreferrer"">https://colab.research.google.com/github/tensorflow/docs/blob/snapshot-keras/site/en/guide/keras/customizing_what_happens_in_fit.ipynb#scrollTo=46832f2077ac</a></p>
<p>The last example shows a GAN implemented with a custom training, where only <code>__init__</code>, <code>train_step</code>, and <code>compile</code> methods are defined</p>
<pre><code>class GAN(keras.Model):
    def __init__(self, discriminator, generator, latent_dim):
        super(GAN, self).__init__()
        self.discriminator = discriminator
        self.generator = generator
        self.latent_dim = latent_dim

    def compile(self, d_optimizer, g_optimizer, loss_fn):
        super(GAN, self).compile()
        self.d_optimizer = d_optimizer
        self.g_optimizer = g_optimizer
        self.loss_fn = loss_fn

    def train_step(self, real_images):
        if isinstance(real_images, tuple):
            real_images = real_images[0]
        ...
</code></pre>
<p>What happens if my model also has a <code>call()</code> custom function? Does <code>train_step()</code> overrides <code>call()</code>?
Aren't <code>call()</code> and <code>train_step()</code> both called by <code>fit()</code> and what is the difference between both ?</p>
<p>Below another piece of code &quot;I&quot; wrote where I wonder what is called into <code>fit()</code>, <code>call()</code> or <code>train_step()</code>:</p>
<pre><code>class MyModel(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, rnn_units):
    super().__init__(self)
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(rnn_units,
                                   return_sequences=True,
                                   return_state=True,
                                   reset_after=True
                                   )
    self.dense = tf.keras.layers.Dense(vocab_size)

  def call(self, inputs, states=None, return_state=False, training=False):
    x = inputs
    x = self.embedding(x, training=training)
    if states is None:
      states = self.gru.get_initial_state(x)
    x, states = self.gru(x, initial_state=states, training=training)
    x = self.dense(x, training=training)

    if return_state:
      return x, states
    else:
      return x

  @tf.function
  def train_step(self, inputs):
    # unpack the data
    inputs, labels = inputs
  
    with tf.GradientTape() as tape:
      predictions = self(inputs, training=True) # forward pass
      # Compute the loss value
      # (the loss function is configured in `compile()`)
      loss=self.compiled_loss(labels, predictions, regularization_losses=self.losses)

    # compute the gradients
    grads=tape.gradient(loss, model.trainable_variables)
    # Update weights
    self.optimizer.apply_gradients(zip(grads, model.trainable_variables))
    # Update metrics (includes the metric that tracks the loss)
    self.compiled_metrics.update_state(labels, predictions)

    # Return a dict mapping metric names to current value
    return {m.name: m.result() for m in self.metrics}
</code></pre>
",1141493.0,,4685471.0,,2021-09-25 01:01:03,2021-09-25 13:17:54,When are Model call() and train_step() called?,<python><tensorflow><machine-learning>,1,0,0.0,,,CC BY-SA 4.0
63258022,1,63428708.0,,2020-08-05 03:33:36,,9,7320,"<p><strong>I run my code on tensorflow 2.1.0 Anaconda with CUDA Toolkit 10.1 CUDNN 7.6.0 (Windows 10) and it returns a issue</strong></p>
<pre><code>F .\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch&lt;Distribution&gt;, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device
</code></pre>
<p><strong>My GPU : GT940MX Compute Capability 5.0</strong></p>
<p><strong>I already run the nvcc -V and it returns :</strong></p>
<pre><code>nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Fri_Feb__8_19:08:26_Pacific_Standard_Time_2019
Cuda compilation tools, release 10.1, V10.1.105
</code></pre>
<p><strong>This is the full result :</strong></p>
<pre><code>2020-08-05 10:05:48.368012: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-05 10:06:00.488544: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-08-05 10:06:48.153611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0
coreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-08-05 10:06:48.164731: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-05 10:06:48.245826: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-05 10:06:48.296245: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-08-05 10:06:48.338860: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-08-05 10:06:48.439393: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-08-05 10:06:48.489830: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-08-05 10:06:48.941872: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-08-05 10:06:48.946651: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-05 10:06:48.951881: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-05 10:06:48.979077: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d29b660d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-05 10:06:48.985680: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-05 10:06:48.990616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce 940MX computeCapability: 5.0
coreClock: 0.8605GHz coreCount: 4 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 37.33GiB/s
2020-08-05 10:06:49.003356: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-05 10:06:49.009869: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-05 10:06:49.014858: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-08-05 10:06:49.020699: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-08-05 10:06:49.028876: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-08-05 10:06:49.033607: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-08-05 10:06:49.039192: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-08-05 10:06:49.045288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-05 10:06:49.218497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-05 10:06:49.223536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2020-08-05 10:06:49.226857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2020-08-05 10:06:49.230413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1460 MB memory) -&gt; physical GPU (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-08-05 10:06:49.244107: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x23d301b8fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-05 10:06:49.250377: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce 940MX, Compute Capability 5.0
2020-08-05 10:06:49.446601: F .\tensorflow/core/kernels/random_op_gpu.h:232] Non-OK-status: GpuLaunchKernel(FillPhiloxRandomKernelLaunch&lt;Distribution&gt;, num_blocks, block_size, 0, d.stream(), gen, data, size, dist) status: Internal: no kernel image is available for execution on the device
</code></pre>
<p><strong>What are the issues and how to fix it?</strong></p>
",14051461.0,,681865.0,,2020-08-05 03:40:09,2020-09-22 08:43:41,Non-OK-status: GpuLaunchKernel(...) status: Internal: no kernel image is available for execution on the device,<tensorflow><nvidia><cudnn>,4,8,0.0,,,CC BY-SA 4.0
64662085,1,64663089.0,,2020-11-03 11:35:32,,8,44871,"<p>I want to use my GPU for Tensorflow.</p>
<p>I tried this <a href=""https://stackoverflow.com/questions/59823283/could-not-load-dynamic-library-cudart64-101-dll-on-tensorflow-cpu-only-install"">Could not load dynamic library &#39;cudart64_101.dll&#39; on tensorflow CPU-only installation</a></p>
<p>Unfortunately, I keep getting an error <code>Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found</code>. How can I fix this?
Python-version: 3.8.3, CUDA 10.1</p>
<pre><code>2020-11-03 12:30:28.832014: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2020-11-03 12:30:28.832688: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found
2020-11-03 12:30:28.833342: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found
2020-11-03 12:30:28.833994: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found
2020-11-03 12:30:28.834645: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found
2020-11-03 12:30:28.835297: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusolver64_10.dll'; dlerror: cusolver64_10.dll not found
2020-11-03 12:30:28.835948: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found
2020-11-03 12:30:28.836594: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found
2020-11-03 12:30:28.836789: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1761] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-11-03 12:30:28.837575: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-03 12:30:28.838495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1265] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-03 12:30:28.838708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1271]      
2020-11-03 12:30:28.838831: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
</code></pre>
",,user13614329,,user13614329,2020-11-03 11:38:26,2023-02-04 09:20:22,Fix not load dynamic library for Tensorflow GPU,<python><tensorflow>,5,4,0.0,,,CC BY-SA 4.0
65402617,1,65415788.0,,2020-12-22 02:42:31,,8,685,"<p>I'm trying to move a tensorflow model from its original html into a react app (built with create-react-app).</p>
<p>My App.js looks like this:</p>
<pre><code>import logo from './logo.svg';
import * as tf from &quot;@tensorflow/tfjs&quot;;
// import { loadImageclassification } from &quot;@tensorflow/tfjs&quot;;
import './App.css';
import * as automl from &quot;@tensorflow/tfjs-automl&quot;;
import * as modelJSON from './model.json';

function App() {

var loadFile = function(event) {
    var image = document.getElementById('output');
    image.src = URL.createObjectURL(event.target.files[0]);
  run();
};

async function run() {
  console.log(modelJSON);
        // const model = await tf.loadImageclassification('model.json');
        const model = await automl.loadImageClassification(modelJSON);
        const image = document.getElementById('output');
        const predictions = await model.classify(image);
        console.log(predictions);

        const pre = document.getElementById('result');
        pre.textContent = JSON.stringify(predictions, null, 2);
}

  return (
  &lt;div className=&quot;App&quot;&gt;
    &lt;div className=&quot;hero-text&quot;&gt;
      &lt;h1&gt;classifier&lt;/h1&gt;
      &lt;h3&gt;Upload a picture to see what type it is! &lt;/h3&gt;
      &lt;p&gt;
        &lt;input type=&quot;file&quot;  accept=&quot;image/*&quot; name=&quot;image&quot; id=&quot;file&quot;  onChange={loadFile} /&gt;
      &lt;/p&gt;
      &lt;div id=&quot;demobox&quot;&gt;
        &lt;p&gt;
          &lt;label htmlFor=&quot;file&quot;&gt;Upload your image&lt;/label&gt;
        &lt;/p&gt;
      &lt;/div&gt; 
      &lt;p&gt;&lt;img id=&quot;output&quot; width=&quot;200&quot; alt=&quot;output&quot; /&gt;&lt;/p&gt;
      &lt;div className=&quot;result&quot; id=&quot;result&quot;&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/div&gt;
  );
}

export default App;
</code></pre>
<p>My index.html looks like this:</p>
<pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en&quot;&gt;
  &lt;head&gt;
    &lt;meta charset=&quot;utf-8&quot; /&gt;
    &lt;link rel=&quot;icon&quot; href=&quot;%PUBLIC_URL%/favicon.ico&quot; /&gt;
    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt;
    &lt;meta name=&quot;theme-color&quot; content=&quot;#000000&quot; /&gt;
    &lt;meta
      name=&quot;description&quot;
      content=&quot;Web site created using create-react-app&quot;
    /&gt;
    &lt;link rel=&quot;apple-touch-icon&quot; href=&quot;%PUBLIC_URL%/logo192.png&quot; /&gt;
    &lt;link rel=&quot;manifest&quot; href=&quot;%PUBLIC_URL%/manifest.json&quot; /&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;noscript&gt;You need to enable JavaScript to run this app.&lt;/noscript&gt;
    &lt;div id=&quot;root&quot;&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>I am getting the following error, which seems to be issuing from somewhere in the <code>loadImageClassification</code> method:</p>
<pre><code>Unhandled Rejection (TypeError): modelUrl.lastIndexOf is not a function
</code></pre>
<h1>Edit:</h1>
<p>Apparently loadImageClassification uses a fetch request under the hood and so requires a remote file (which is strange, because it seemed to work fine in the static index.html original version of this same project).</p>
<p>So I am now trying it just with a localhost express server, which at present looks like this:</p>
<pre><code>const modelJSON = require('./model.json');

const express = require(&quot;express&quot;);
const bodyParser = require(&quot;body-parser&quot;);
const CORS = require(&quot;cors&quot;);

const app = express();

app.use(bodyParser.json());
app.use(CORS());

let modelObj = modelJSON;

app.get(&quot;/&quot;, (req, res) =&gt; {
  // console.log(modelObj);
  res.send(modelObj);
});

app.listen(5000, () =&gt; {
  console.log(&quot;Server listening on port 5000&quot;);
});

</code></pre>
<p>I can see the correct data when I navigate to localhost5000, but when I change</p>
<pre><code>async function run() {
  const model = await automl.loadImageClassification(modelJSON);
</code></pre>
<p>to</p>
<pre><code>async function run() {
  const modelUrl = &quot;http://localhost:5000/&quot;;
  const model = await automl.loadImageClassification(modelUrl);
</code></pre>
<p>I get these errors:</p>
<p><a href=""https://i.stack.imgur.com/KGL59.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/KGL59.png"" alt=""enter image description here"" /></a></p>
<h1>Edit 2:</h1>
<p>My server.js file now looks like this:</p>
<p><a href=""https://i.stack.imgur.com/9iq0i.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/9iq0i.png"" alt=""enter image description here"" /></a></p>
<p>This produces the same errors as in the previous screenshot. (I am leaving in the comments that mess of an attempt to include all the shard files in this server.js file screenshot just because it may illustrate that I don't understand how to pass those ancillary model files to <code>loadImageClassification</code> when it makes its fetch request.)</p>
<p>So presumably the problem now has to do with the fact that <code>loadImageClassification</code> assumes that the <code>...shard__of6.bin</code> and <code>dict</code> files are in the same directory as the <code>model.json</code> file.</p>
<p>So the question may (?) be: how to simulate the file structure that it (i.e., <code>loadImageClassification</code>) is expecting within a remote node server.</p>
<h1>Fundamental confusion:</h1>
<p>I'm don't understand why, when loadImageClassification is in the original static html, it does not seem to require a remote url from which to fetch model.json — but then when I put it in my react app, it suddenly gives me this error: &quot;Fetch API cannot load file:///Users///client/src/model.json. URL scheme must be 'http' or 'https' for CORS request.&quot;</p>
",9044421.0,,9044421.0,,2020-12-24 05:37:30,2020-12-24 05:37:30,Tensorflow automl model in react,<javascript><reactjs><tensorflow><tensorflow.js><automl>,1,0,,,,CC BY-SA 4.0
66324458,1,,,2021-02-22 22:37:03,,8,568,"<p>Does anyone have a working example implementing Each State Network with  <code>Tfa.layers.ESN.</code></p>
<p>Currently, I have the following model</p>
<pre><code>Model: &quot;model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
_________________________________________________________________
digits (InputLayer)          [(None, 41, 4)]           0         
_________________________________________________________________
ESN                          (None, 41, 242)           30492     
_________________________________________________________________
flatten (Flatten)            (None, 9922)              0         
_________________________________________________________________
predictions (Dense)          (None, 2)                 19846  
_________________________________________________________________   
Total params: 50,338
Trainable params: 19,846
Non-trainable params: 30,492
_________________________________________________________________
</code></pre>
<p>The problem that I am having is that once I increase the number of hidden units the accuracy of the model drops instead of increasing, both in training and testing dataset. Also, the accuracy that I got is very low.</p>
",15190955.0,,13273054.0,,2021-02-22 22:53:04,2021-02-22 22:53:04,Tfa.layers.ESN Example,<python><tensorflow>,0,1,0.0,,,CC BY-SA 4.0
70562033,1,,,2022-01-03 05:36:03,,8,7993,"<p>I'm using my new M1 Pro with the latest Mac OS 12.1 and I'm experiencing issues with installing tensorflow.</p>
<p>I installed Miniforge3 using:</p>
<pre><code>chmod +x ~/Downloads/Miniforge3-MacOSX-arm64.sh

sh ~/Downloads/Miniforge3-MacOSX-arm64.sh

source ~/miniforge3/bin/activate
</code></pre>
<p>I've created an environment and have it activated.</p>
<p>Then I tried</p>
<pre><code>conda install -c apple tensorflow-deps
</code></pre>
<p>And here is what's returned:</p>
<pre><code>Collecting package metadata (current_repodata.json): done

Solving environment: failed with initial frozen solve. Retrying with flexible solve.

Collecting package metadata (repodata.json): done

Solving environment: failed with initial frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:
  - tensorflow-deps

Current channels:

  - https://conda.anaconda.org/apple/osx-64
  - https://conda.anaconda.org/apple/noarch
  - https://repo.anaconda.com/pkgs/main/osx-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/osx-64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you`re
looking for, navigate to

https://anaconda.org

and use the search bar at the top of the page.

Note: you may need to restart the kernel to use updated packages.

</code></pre>
<p><code>conda info</code></p>
<pre><code>    active env location : /Users/andrewli/tensorflow-test/env
            shell level : 2
       user config file : /Users/andrewli/.condarc
 populated config files : /Users/andrewli/.condarc
          conda version : 4.10.3
    conda-build version : 3.21.5
         python version : 3.9.7.final.0
       virtual packages : __osx=10.16=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /Users/andrewli/opt/anaconda3  (writable)
      conda av data dir : /Users/andrewli/opt/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/osx-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/osx-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /Users/andrewli/opt/anaconda3/pkgs
                          /Users/andrewli/.conda/pkgs
       envs directories : /Users/andrewli/opt/anaconda3/envs
                          /Users/andrewli/.conda/envs
               platform : osx-64
             user-agent : conda/4.10.3 requests/2.26.0 CPython/3.9.7 Darwin/21.2.0 OSX/10.16
                UID:GID : 501:20
             netrc file : None
           offline mode : False
</code></pre>
<p>Did anyone have the same issue and any advice to address this?</p>
",17819655.0,,17819655.0,,2022-01-03 18:54:21,2023-02-23 15:53:27,tensorflow-deps - PackagesNotFoundError,<macos><tensorflow><conda><apple-m1>,3,4,0.0,,,CC BY-SA 4.0
63728800,1,64223512.0,,2020-09-03 17:27:14,,8,2148,"<p>I'm working in <strong>A2C</strong> reinforcement learning where my environment has an increasing and decreasing in the number of agents. As a result of the increasing and decreasing the number of agents, the state space will also change.  I have tried to solve the problem of changing the state space this way:</p>
<ul>
<li><p>If the state space exceeds the maximum state space that selected
as <code>n_input</code>, the excess state space will be selected by
<code>np.random.choice</code> where random choice provides a way of creating random samples from the state space after converting the state space into probabilities.</p>
</li>
<li><p>If the state space is less than the maximum state I padded the state
space with zeros.</p>
<pre><code>def get_state_new(state):
 n_features =  n_input-len(get_state(env))
 # print(&quot;state&quot;,len(get_state(env)))
 p = np.array(state)
 p = np.exp(p)
 if p.sum() != 1.0:
     p = p * (1. / p.sum())
 if len(get_state(env)) &gt; n_input:
     statappend = np.random.choice(state, size=n_input, p=p)
     # print(statappend)
 else:
     statappend = np.zeros(n_input)
     statappend[:state.shape[0]] = state
 return statappend
</code></pre>
</li>
</ul>
<p>It works but the results are not as expected and I don't know if this correct or not.</p>
<p><strong>My question</strong></p>
<p>Are there any reference papers that deal with such a problem and how to deal with the changing of state space?</p>
",4694757.0,,4694757.0,,2020-09-05 17:49:38,2021-02-06 12:22:21,How to deal with different state space size in reinforcement learning?,<python><tensorflow><reinforcement-learning>,2,0,0.0,,,CC BY-SA 4.0
64987253,1,,,2020-11-24 13:11:52,,8,811,"<p>I want to apply various filters like <a href=""https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.greycomatrix"" rel=""nofollow noreferrer"">GLCM</a> or <a href=""https://scikit-image.org/docs/dev/api/skimage.filters.html#skimage.filters.gabor_kernel"" rel=""nofollow noreferrer"">Gabor filter bank</a> as a custom layer in Tensorflow, but I could not find enough custom layer samples. How can I apply these type of filters as a layer?</p>
<p>The process of generating GLCM is defined in the scikit-image library as follows:</p>
<pre><code>from skimage.feature import greycomatrix, greycoprops
from skimage import data
#load image
img = data.brick()
#result glcm
glcm = greycomatrix(img, distances=[5], angles=[0], levels=256, symmetric=True, normed=True)
</code></pre>
<p>The use of Gabor filter bank is as follows:</p>
<pre><code>import matplotlib.pyplot as plt
import numpy as np
from scipy import ndimage as ndi
from skimage import data
from skimage.util import img_as_float
from skimage.filters import gabor_kernel

shrink = (slice(0, None, 3), slice(0, None, 3))
brick = img_as_float(data.brick())[shrink]
grass = img_as_float(data.grass())[shrink]
gravel = img_as_float(data.gravel())[shrink]
image_names = ('brick', 'grass', 'gravel')
images = (brick, grass, gravel)

def power(image, kernel):
    # Normalize images for better comparison.
    image = (image - image.mean()) / image.std()
    return np.sqrt(ndi.convolve(image, np.real(kernel), mode='wrap')**2 +
                   ndi.convolve(image, np.imag(kernel), mode='wrap')**2)

# Plot a selection of the filter bank kernels and their responses.
results = []
kernel_params = []
for theta in (0, 1):
    theta = theta / 4. * np.pi
    for sigmax in (1, 3):
        for sigmay in (1, 3):
            for frequency in (0.1, 0.4):
                kernel = gabor_kernel(frequency, theta=theta,sigma_x=sigmax, sigma_y=sigmay)
                params = 'theta=%d,f=%.2f\nsx=%.2f sy=%.2f' % (theta * 180 / np.pi, frequency,sigmax, sigmay)
                kernel_params.append(params)
                # Save kernel and the power image for each image
                results.append((kernel, [power(img, kernel) for img in images]))

fig, axes = plt.subplots(nrows=6, ncols=4, figsize=(5, 6))
plt.gray()
fig.suptitle('Image responses for Gabor filter kernels', fontsize=12)
axes[0][0].axis('off')
# Plot original images
for label, img, ax in zip(image_names, images, axes[0][1:]):
    ax.imshow(img)
    ax.set_title(label, fontsize=9)
    ax.axis('off')
for label, (kernel, powers), ax_row in zip(kernel_params, results, axes[1:]):
    # Plot Gabor kernel
    ax = ax_row[0]
    ax.imshow(np.real(kernel))
    ax.set_ylabel(label, fontsize=7)
    ax.set_xticks([])
    ax.set_yticks([])
    # Plot Gabor responses with the contrast normalized for each filter
    vmin = np.min(powers)
    vmax = np.max(powers)
    for patch, ax in zip(powers, ax_row[1:]):
        ax.imshow(patch, vmin=vmin, vmax=vmax)
        ax.axis('off')
plt.show()
</code></pre>
<p>How do I define these and similar filters in tensorflow.</p>
<p>I tried above code but it didnt gave the same results like : <a href=""https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_gabor.html"" rel=""nofollow noreferrer"">https://scikit-image.org/docs/dev/auto_examples/features_detection/plot_gabor.html</a></p>
<p><a href=""https://i.stack.imgur.com/xPb7w.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xPb7w.png"" alt=""enter image description here"" /></a></p>
<p>I got this: <a href=""https://i.stack.imgur.com/NODRV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/NODRV.png"" alt=""enter image description here"" /></a></p>
<pre><code>import numpy as np
import matplotlib.pyplot as plt
import tensorflow.keras.backend as K
from tensorflow.keras import Input, layers
from tensorflow.keras.models import Model
from scipy import ndimage as ndi

from skimage import data
from skimage.util import img_as_float
from skimage.filters import gabor_kernel

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


def gfb_filter(shape,size=3, tlist=[1,2,3], slist=[2,5],flist=[0.01,0.25],dtype=None):
    print(shape)
    fsize=np.ones([size,size])
    kernels = []
    for theta in tlist:
        theta = theta / 4. * np.pi
        for sigma in slist:
            for frequency in flist:
                kernel = np.real(gabor_kernel(frequency, theta=theta,sigma_x=sigma, sigma_y=sigma))
                kernels.append(kernel)
    gfblist = []
    for k, kernel in enumerate(kernels):
        ck=ndi.convolve(fsize, kernel, mode='wrap')
        gfblist.append(ck)
    
    gfblist=np.asarray(gfblist).reshape(size,size,1,len(gfblist))
    print(gfblist.shape)
    return K.variable(gfblist, dtype='float32')


dimg=img_as_float(data.brick())
input_mat = dimg.reshape((1, 512, 512, 1))

def build_model():
    input_tensor = Input(shape=(512,512,1))
    x = layers.Conv2D(filters=12, 
                      kernel_size = 3,
                      kernel_initializer=gfb_filter,
                      strides=1, 
                      padding='valid') (input_tensor)

    model = Model(inputs=input_tensor, outputs=x)
    return model

model = build_model()
out = model.predict(input_mat)
print(out)

o1=out.reshape(12,510,510)
plt.subplot(2,2,1)
plt.imshow(dimg)

plt.subplot(2,2,2)
plt.imshow(o1[0,:,:])

plt.subplot(2,2,3)
plt.imshow(o1[6,:,:])

plt.subplot(2,2,4)
plt.imshow(o1[10,:,:])
</code></pre>
",2590509.0,,2590509.0,,2020-12-03 10:00:04,2020-12-03 10:00:04,Tensorflow custom filter layer definition like glcm or gabor,<python><tensorflow><deep-learning><glcm><gabor-filter>,1,6,,,,CC BY-SA 4.0
64794378,1,64946805.0,,2020-11-11 21:28:06,,8,2533,"<p>I have a TensorFlow model that I built (a 1D CNN) that I would now like to implement into .NET.<br />
In order to do so I need to know the Input and Output nodes.<br />
When I uploaded the model on <a href=""https://netron.app/"" rel=""nofollow noreferrer"">Netron</a> I get a different graph depending on my save method and the only one that looks correct comes from an h5 upload.  Here is the <code>model.summary()</code>:</p>
<p><a href=""https://i.stack.imgur.com/wLUfb.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/wLUfb.png"" alt=""enter image description here"" /></a></p>
<p>If I save the model as an h5 <code>model.save(&quot;Mn_pb_model.h5&quot;)</code> and load that into the Netron to graph it, everything looks correct:</p>
<p><a href=""https://i.stack.imgur.com/gV1CD.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/gV1CD.png"" alt=""enter image description here"" /></a></p>
<p>However, ML.NET will not accept h5 format so it needs to be saved as a pb.</p>
<p>In looking through samples of adopting TensorFlow in ML.NET, this <a href=""https://github.com/dotnet/samples/tree/master/machine-learning/tutorials/TextClassificationTF/sentiment_model"" rel=""nofollow noreferrer"">sample</a> shows a TensorFlow model that is saved in a similar format to the <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">SavedModel</a> format - recommended by TensorFlow (and also recommended by ML.NET <a href=""https://github.com/dotnet/machinelearning/blob/master/docs/samples/Microsoft.ML.Samples/Dynamic/TensorFlow/TextClassification.cs"" rel=""nofollow noreferrer"">here</a> &quot;Download an unfrozen [SavedModel format] ...&quot;).  However when saving and loading the pb file into Netron I get this:</p>
<p><a href=""https://i.stack.imgur.com/1tk96.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/1tk96.png"" alt=""enter image description here"" /></a></p>
<p>And zoomed in a little further (on the far right side),</p>
<p><a href=""https://i.stack.imgur.com/LE0Ru.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/LE0Ru.png"" alt=""enter image description here"" /></a></p>
<p>As you can see, it looks nothing like it should.<br />
Additionally the input nodes and output nodes are not correct so it will not work for ML.NET (and I think something is wrong).<br />
I am using the <a href=""https://www.tensorflow.org/lite/guide/faq"" rel=""nofollow noreferrer"">recommended way</a> from TensorFlow to determine the Input / Output nodes:</p>
<p><a href=""https://i.stack.imgur.com/CyWxR.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CyWxR.png"" alt=""enter image description here"" /></a></p>
<p>When I try to obtain a <a href=""https://stackoverflow.com/questions/60974077/how-to-save-keras-model-as-frozen-graph"">frozen</a> graph and load it into Netron, at first it looks correct, but I don't think that it is:</p>
<p><a href=""https://i.stack.imgur.com/jlPU6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/jlPU6.png"" alt=""enter image description here"" /></a></p>
<p>There are four reasons I do not think this is correct.</p>
<ul>
<li>it is very different from the graph when it was uploaded as an h5 (which looks correct to me).</li>
<li>as you can see from earlier, I am using 1D convolutions throughout and this is showing that it goes to 2D (and remains that way).</li>
<li>this file size is 128MB whereas the one in the TensorFlow to ML.NET example is only 252KB.  Even the <a href=""https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/getting-started/DeepLearning_ImageClassification_TensorFlow/ImageClassification/assets/inputs/inception"" rel=""nofollow noreferrer"">Inception</a> model is only 56MB.</li>
<li>if I load the Inception model in TensorFlow and save it as an h5, it looks the same as from the ML.NET resource, yet when I save it as a frozen graph it looks different.  <strong>If I take the same model and save it in the recommended <code>SavedModel</code> format, it shows up all messed up in Netron.</strong>  Take any model you want and save it in the recommended <code>SavedModel</code> format and you will see for yourself (I've tried it on a lot of different models).</li>
</ul>
<p>Additionally in looking at the <code>model.summary()</code> of Inception with it's graph, it is similar to its graph in the same way my <code>model.summary()</code> is to the h5 graph.</p>
<p>It seems like there should be an easier way (and a correct way) to save a TensorFlow model so it can be used in ML.NET.</p>
<p><strong>Please show that your suggested solution works:</strong>  In the answer that you provide, please check that it works (load the <code>pb</code> model [this should also have a <code>Variables</code> folder in order to work for ML.NET] into Netron and show that it is the same as the <code>h5</code> model, e.g., screenshot it).  So that we are all trying the same thing, here is a <a href=""https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/multi-class_classification_with_MNIST.ipynb?utm_source=mlcc&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=multiclass_tf2-colab&amp;hl=en"" rel=""nofollow noreferrer"">link</a> to a MNIST ML crash course example.  It takes less than 30s to run the program and produces a model called <code>my_model</code>.  From here you can save it according to your method and upload it to see the graph on Netron.  Here is the <code>h5</code> model upload:</p>
<p><a href=""https://i.stack.imgur.com/xTUOB.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/xTUOB.png"" alt=""enter image description here"" /></a></p>
",8922090.0,,8922090.0,,2020-11-23 18:43:22,2020-11-24 10:57:36,Correct pb file to move Tensorflow model into ML.NET,<python><.net><tensorflow><nodes><ml.net>,1,2,0.0,,,CC BY-SA 4.0
67896966,1,67897145.0,,2021-06-09 03:03:49,,8,2323,"<p>Tensoflow Embedding Layer (<a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding</a>) is easy to use,
and there are massive articles talking about
&quot;how to use&quot; Embedding (<a href=""https://machinelearningmastery.com/what-are-word-embeddings/"" rel=""noreferrer"">https://machinelearningmastery.com/what-are-word-embeddings/</a>, <a href=""https://www.sciencedirect.com/topics/computer-science/embedding-method"" rel=""noreferrer"">https://www.sciencedirect.com/topics/computer-science/embedding-method</a>)
.
However, I want to know the Implemention of the very &quot;Embedding Layer&quot; in Tensorflow or Pytorch.
Is it a word2vec?
Is it a Cbow?
Is it a special Dense Layer?</p>
",13027238.0,,13027238.0,,2021-06-09 03:41:45,2021-06-09 09:22:40,What is the network structure inside a Tensorflow Embedding Layer?,<tensorflow><word2vec><embedding>,1,0,0.0,,,CC BY-SA 4.0
66968102,1,,,2021-04-06 11:50:18,,8,3838,"<p>Is it possible to use the Tensorflow data types <a href=""https://www.tensorflow.org/api_docs/python/tf/dtypes/DType"" rel=""noreferrer"">tf.dtypes.DType</a> such as tf.int32 in Python type hint?</p>
<pre><code>from typing import (
    Union,
)
import tensorflow as tf
import numpy as np


def f(
    a: Union[tf.int32, tf.float32]  # &lt;----
): 
    return a * 2


def g(a: Union[np.int32, np.float32]):
    return a * 2


def test_a():
    f(tf.cast(1.0, dtype=tf.float32))  # &lt;----
    g(np.float32(1.0))                 # Numpy type has no issue

</code></pre>
<p>It causes the error below and wonder if this is possible.</p>
<pre><code>python3.8/typing.py:149: in _type_check
    raise TypeError(f&quot;{msg} Got {arg!r:.100}.&quot;)
E   TypeError: Union[arg, ...]: each arg must be a type. Got tf.int32.
</code></pre>
",4281353.0,,,,,2021-04-06 14:54:27,python type hint - can tensorflow data type be used?,<python><tensorflow><type-hinting>,1,0,0.0,,,CC BY-SA 4.0
70928279,1,,,2022-01-31 15:13:16,,8,2061,"<p>Hi I get this message when I run my job in slurm what does it mean?
tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory</p>
",11852838.0,,,,,2022-06-05 04:44:45,Start cannot spawn child process: No such file or directory,<python><tensorflow><slurm>,0,3,,,,CC BY-SA 4.0
66605679,1,66615100.0,,2021-03-12 18:55:56,,8,1139,"<p>I have a ragged tensor of dimensions <code>[BATCH_SIZE, TIME_STEPS, EMBEDDING_DIM]</code>.  I want to augment the last axis with data from another tensor of shape <code>[BATCH_SIZE, AUG_DIM]</code>. Each time step of a given example gets augmented with the same value.</p>
<p>If the tensor wasn't ragged with varying <code>TIME_STEPS</code> for each example, I could simply reshape the second tensor with <code>tf.repeat</code> and then use <code>tf.concat</code>:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf


# create data
# shape: [BATCH_SIZE, TIME_STEPS, EMBEDDING_DIM]
emb = tf.constant([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [0, 0, 0]]])
# shape: [BATCH_SIZE, 1, AUG_DIM]
aug = tf.constant([[[8]], [[9]]])

# concat
aug = tf.repeat(aug, emb.shape[1], axis=1)
emb_aug = tf.concat([emb, aug], axis=-1)
</code></pre>
<p>This doesn't approach work when <code>emb</code> is ragged since <code>emb.shape[1]</code> is unknown and varies across examples:</p>
<pre><code># rag and remove padding
emb = tf.RaggedTensor.from_tensor(emb, padding=(0, 0, 0))

# reshape for augmentation - this doesn't work
aug = tf.repeat(aug, emb.shape[1], axis=1)
</code></pre>
<blockquote>
<p>ValueError: Attempt to convert a value (None) with an unsupported type (&lt;class 'NoneType'&gt;) to a Tensor.</p>
</blockquote>
<p>The goal is to create a ragged tensor <code>emb_aug</code> which looks like this:</p>
<pre><code>&lt;tf.RaggedTensor [[[1, 2, 3, 8], [4, 5, 6, 8]], [[1, 2, 3 ,9]]]&gt;
</code></pre>
<p>Any ideas?</p>
",5904059.0,,3222797.0,,2021-03-13 15:29:11,2022-03-20 23:46:52,Broadcast and concatenate ragged tensors,<python><tensorflow><concatenation><ragged><ragged-tensors>,2,0,0.0,,,CC BY-SA 4.0
75501048,1,,,2023-02-19 15:03:08,,8,8205,"<p>when i import TensorFlow GPU I get this error</p>
<pre><code>AttributeError: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)
</code></pre>
<p>after i type</p>
<pre><code>import tensorflow as tf
</code></pre>
<p>like below:</p>
<pre><code>AttributeError                            Traceback (most recent call last)
Cell In[22], line 1
----&gt; 1 import tensorflow as tf

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\__init__.py:51
     49 from ._api.v2 import autograph
     50 from ._api.v2 import bitwise
---&gt; 51 from ._api.v2 import compat
     52 from ._api.v2 import config
     53 from ._api.v2 import data

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\__init__.py:37
      3 &quot;&quot;&quot;Compatibility functions.
      4 
      5 The `tf.compat` module contains two sets of compatibility functions.
   (...)
     32 
     33 &quot;&quot;&quot;
     35 import sys as _sys
---&gt; 37 from . import v1
     38 from . import v2
     39 from tensorflow.python.compat.compat import forward_compatibility_horizon

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v1\__init__.py:30
     28 from . import autograph
     29 from . import bitwise
---&gt; 30 from . import compat
     31 from . import config
     32 from . import data

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v1\compat\__init__.py:38
     35 import sys as _sys
     37 from . import v1
---&gt; 38 from . import v2
     39 from tensorflow.python.compat.compat import forward_compatibility_horizon
     40 from tensorflow.python.compat.compat import forward_compatible

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v1\compat\v2\__init__.py:28
     25 # pylint: disable=g-bad-import-order
     27 from . import compat
---&gt; 28 from tensorflow._api.v2.compat.v2 import __internal__
     29 from tensorflow._api.v2.compat.v2 import __operators__
     30 from tensorflow._api.v2.compat.v2 import audio

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v2\__init__.py:33
     31 from . import autograph
     32 from . import bitwise
---&gt; 33 from . import compat
     34 from . import config
     35 from . import data

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v2\compat\__init__.py:38
     35 import sys as _sys
     37 from . import v1
---&gt; 38 from . import v2
     39 from tensorflow.python.compat.compat import forward_compatibility_horizon
     40 from tensorflow.python.compat.compat import forward_compatible

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v2\compat\v2\__init__.py:37
     35 from tensorflow._api.v2.compat.v2 import data
     36 from tensorflow._api.v2.compat.v2 import debugging
---&gt; 37 from tensorflow._api.v2.compat.v2 import distribute
     38 from tensorflow._api.v2.compat.v2 import dtypes
     39 from tensorflow._api.v2.compat.v2 import errors

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v2\distribute\__init__.py:182
    180 from . import cluster_resolver
    181 from . import coordinator
--&gt; 182 from . import experimental
    183 from tensorflow.python.distribute.collective_all_reduce_strategy import CollectiveAllReduceStrategy as MultiWorkerMirroredStrategy
    184 from tensorflow.python.distribute.cross_device_ops import CrossDeviceOps

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v2\distribute\experimental\__init__.py:10
      8 from . import coordinator
      9 from . import partitioners
---&gt; 10 from . import rpc
     11 from tensorflow.python.distribute.central_storage_strategy import CentralStorageStrategy
     12 from tensorflow.python.distribute.collective_all_reduce_strategy import _CollectiveAllReduceStrategyExperimental as MultiWorkerMirroredStrategy

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\_api\v2\compat\v2\distribute\experimental\rpc\__init__.py:8
      3 &quot;&quot;&quot;Public API for tf.distribute.experimental.rpc namespace.
      4 &quot;&quot;&quot;
      6 import sys as _sys
----&gt; 8 from tensorflow.python.distribute.experimental.rpc.rpc_ops import Client
      9 from tensorflow.python.distribute.experimental.rpc.rpc_ops import Server

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\distribute\experimental\__init__.py:22
     20 from tensorflow.python.distribute import parameter_server_strategy
     21 from tensorflow.python.distribute import tpu_strategy
---&gt; 22 from tensorflow.python.distribute.failure_handling import failure_handling

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\distribute\failure_handling\failure_handling.py:33
     31 from tensorflow.python.checkpoint import checkpoint_management
     32 from tensorflow.python.distribute import multi_worker_util
---&gt; 33 from tensorflow.python.distribute.failure_handling import gce_util
     34 from tensorflow.python.eager import context
     35 from tensorflow.python.framework import constant_op

File ~\anaconda3\envs\tf_gpu\lib\site-packages\tensorflow\python\distribute\failure_handling\gce_util.py:20
     17 import os
     18 import sys
---&gt; 20 import requests
     22 from six.moves.urllib import request
     23 from tensorflow.python.eager import context

File ~\anaconda3\envs\tf_gpu\lib\site-packages\requests\__init__.py:48
     45 from .exceptions import RequestsDependencyWarning
     47 try:
---&gt; 48     from charset_normalizer import __version__ as charset_normalizer_version
     49 except ImportError:
     50     charset_normalizer_version = None

File ~\anaconda3\envs\tf_gpu\lib\site-packages\charset_normalizer\__init__.py:23
      1 &quot;&quot;&quot;
      2 Charset-Normalizer
      3 ~~~~~~~~~~~~~~
   (...)
     21 :license: MIT, see LICENSE for more details.
     22 &quot;&quot;&quot;
---&gt; 23 from charset_normalizer.api import from_fp, from_path, from_bytes, normalize
     24 from charset_normalizer.legacy import detect
     25 from charset_normalizer.version import __version__, VERSION

File ~\anaconda3\envs\tf_gpu\lib\site-packages\charset_normalizer\api.py:10
      7     PathLike = Union[str, 'os.PathLike[str]']  # type: ignore
      9 from charset_normalizer.constant import TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED
---&gt; 10 from charset_normalizer.md import mess_ratio
     11 from charset_normalizer.models import CharsetMatches, CharsetMatch
     12 from warnings import warn

AttributeError: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)
</code></pre>
<p>I install &quot;requests&quot; , &quot;chardet&quot; ,&quot;openpyxl&quot; but nothing change .</p>
",7188929.0,,4685471.0,,2023-06-07 21:53:55,2023-06-07 21:53:55,How to fix AttributeError: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import),<tensorflow><deep-learning>,1,2,,,,CC BY-SA 4.0
63702841,1,64690450.0,,2020-09-02 09:40:50,,8,1029,"<p>I successfully trained a model using Object Detection APIs for TF 2 on TPUs which is saved as a .pb (SavedModel format). I then load it back using <code>tf.saved_model.load</code> and it works fine when predicting boxes using a single image converted to a tensor with shape <code>(1, w, h, 3)</code>.</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf
import numpy as np

# Load Object Detection APIs model
detect_fn = tf.saved_model.load('/path/to/saved_model/')

image = tf.io.read_file(image_path)
image_np = tf.image.decode_jpeg(image, channels=3).numpy()
input_tensor = np.expand_dims(image_np, 0)
detections = detect_fn(input_tensor) # This works fine
</code></pre>
<p>Problem is I need to make this a batch prediction to scale it to half a million images, but the input signature of this model seems to be limited to handling only data with shape <code>(1, w, h, 3)</code>.
This also means that I can't use batch processing with Tensorflow Serving.
How can I solve this problem? Can I merely change the model Signature to handle batches of data?</p>
<p>All work (loading model + predictions) was performed inside the official container released with the Object Detection APIs (from <a href=""https://github.com/tensorflow/models/tree/master/research/object_detection/dockerfiles/tf2"" rel=""noreferrer"">here</a>)</p>
",1958843.0,,6738015.0,,2020-09-02 11:37:13,2020-11-05 02:37:08,Batch prediction using a trained Object Detection APIs model and TF 2,<tensorflow><object><batch-processing><prediction><detection>,1,0,0.0,,,CC BY-SA 4.0
63100699,1,63117725.0,,2020-07-26 13:19:59,,7,1149,"<p>According to the Keras Tuner examples <a href=""https://keras-team.github.io/keras-tuner/examples/helloworld/"" rel=""noreferrer"">here</a> and <a href=""https://keras-team.github.io/keras-tuner/#the-search-space-may-contain-conditional-hyperparameters"" rel=""noreferrer"">here</a>, if you want to define the number of layers and each layer's units in a deep learning model using hyper parameters you do something like this:</p>
<pre><code>for i in range(hp.Int('num_layers', 1, 10)):
    model.add(layers.Dense(units=hp.Int('unit_' + str(i), 32, 512, 32)))
</code></pre>
<p>However, as others have noted <a href=""https://stackoverflow.com/questions/61375125/keras-tuner-mismatch-between-number-of-layers-used-and-number-of-layers-reporte"">here</a> and <a href=""https://github.com/keras-team/keras-tuner/issues/66"" rel=""noreferrer"">here</a> after the oracle has seen a model with <code>num_layers = 10</code> it will always assign a value to <code>unit_0</code> through <code>unit_9</code>, even when <code>num_layers</code> is less than 10.</p>
<p>In the case that <code>num_layers = 1</code> for example, only <code>unit_0</code> will be used to build the model. But, <code>unit_1</code> through <code>unit_9</code> will be defined and <em><strong>active</strong></em> in the hyper parameters.</p>
<p>Does the oracle &quot;know&quot; that <code>unit_1</code> through <code>unit_9</code> weren't actually used to build the model (and therefore disregard their relevance for impacting the results of that trial)?</p>
<p>Or, does it assume <code>unit_1</code> through <code>unit_9</code> are being used because they have been defined (and calling <code>hp.get('unit_9')</code> for example will return a value)?</p>
<p>In the latter case <strong>the oracle is using misinformation to drive the tuning process</strong>. As a result <strong>it will take longer to converge</strong> (at best) and <strong>incorrectly converge</strong> to a solution as a result of assigning relevance to the unused hyper parameters (at worst).</p>
<p>Should the model actually be defined using <a href=""https://keras-team.github.io/keras-tuner/documentation/hyperparameters/#conditional_scope-method"" rel=""noreferrer"">conditional scopes</a>, like this?</p>
<pre><code>num_layers = hp.Int('num_layers', 1, 10)
for i in range(num_layers):
    with hp.conditional_scope('num_layers', list(range(i + 1, 10 + 1))):
        model.add(layers.Dense(units=hp.Int('unit_' + str(i), 32, 512, 32)))
</code></pre>
<p>When defining the model like this, if <code>num_layers &lt; 10</code>, calling <code>hp.get('unit_9')</code> will return a <code>ValueError: Conditional parameter unit_10 is not currently active</code>, as expected.</p>
",5306011.0,,5306011.0,,2020-07-26 13:44:28,2020-07-27 14:42:17,Do the number of units in a layer need to be defined within a conditional scope when using keras tuner to setup a model?,<tensorflow><tf.keras><keras-tuner>,1,0,,,,CC BY-SA 4.0
67084512,1,67088947.0,,2021-04-14 01:58:05,,7,7490,"<h1>Question</h1>
<p>Please confirm that to use both CPU and GPU with TensorFlow <strong>after 1.15</strong>, install <strong>tensorflow</strong> package is enough and <strong>tensorflow-gpu</strong> is no more required.</p>
<h1>Background</h1>
<p>Still see articles stating to install <strong>tensorflow-gpu</strong> e.g. <code>pip install tensorflow-gpu==2.2.0</code> and the <a href=""https://pypi.org/project/tensorflow-gpu/"" rel=""noreferrer"">PyPi repository for tensorflow-gpu package</a> is active with the latest <strong>tensorflow-gpu 2.4.1</strong>.</p>
<p>The Annaconda document also refers to tensorflow-gpu package still.</p>
<ul>
<li><a href=""https://docs.anaconda.com/anaconda/user-guide/tasks/gpu-packages/#tensorflow"" rel=""noreferrer"">Working with GPU packages - Available packages - TensorFlow</a></li>
</ul>
<blockquote>
<p>TensorFlow is a general machine learning library, but most popular for deep learning applications. There are three supported variants of the tensorflow package in Anaconda, one of which is the NVIDIA GPU version. This is selected by installing the meta-package tensorflow-gpu:</p>
</blockquote>
<p>However, according to the TensorFlow v2.4.1 (as of Apr 2021) Core document <a href=""https://www.tensorflow.org/install/gpu"" rel=""noreferrer"">GPU support - Older versions of TensorFlow</a></p>
<blockquote>
<p>For releases 1.15 and older, CPU and GPU packages are separate:</p>
</blockquote>
<pre><code>pip install tensorflow==1.15      # CPU
pip install tensorflow-gpu==1.15  # GPU
</code></pre>
<p>According to the TensorFlow Core Guide <a href=""https://www.tensorflow.org/guide/gpu"" rel=""noreferrer"">Use a GPU</a>.</p>
<blockquote>
<p>TensorFlow code, and tf.keras models will transparently run on a single GPU with no code changes required.</p>
</blockquote>
<p>According to <a href=""https://stackoverflow.com/a/52627105/4281353"">Difference between installation libraries of TensorFlow GPU vs CPU</a>.</p>
<blockquote>
<p>Just a quick (unnecessary?) note... from TensorFlow 2.0 onwards these are not separated, and you simply install tensorflow (as this includes GPU support if you have an appropriate card/CUDA installed).</p>
</blockquote>
<p>Hence would like to have a definite confirmation that the tensorflow-gpu package would be for convenience (legacy script which has specified tensorflow-gpu, etc) only and no more required. There is no difference between tensorflow and tensorflow-gpu packages now.</p>
",4281353.0,,,,,2022-05-28 00:28:26,Tensorflow after 1.15 - No need to install tensorflow-gpu package,<tensorflow>,1,0,0.0,,,CC BY-SA 4.0
64201778,1,,,2020-10-05 02:32:19,,7,544,"<p>I trained the model as:
<a href=""https://www.google.com.au/amp/s/blog.roboflow.com/training-a-tensorflow-object-detection-model-with-a-custom-dataset/amp/"" rel=""noreferrer"">https://www.google.com.au/amp/s/blog.roboflow.com/training-a-tensorflow-object-detection-model-with-a-custom-dataset/amp/</a>
And converted it to tflite. Then I try to put the AI model in an android APP. I followed:
<a href=""https://developers.google.com/ml-kit/vision/object-detection/custom-models/android?fbclid=IwAR07uNgzQ2c5PTp13TiPVeKGQsXaJnJR9jzyvtviXCRegFFJlM-_G799TlY"" rel=""noreferrer"">https://developers.google.com/ml-kit/vision/object-detection/custom-models/android?fbclid=IwAR07uNgzQ2c5PTp13TiPVeKGQsXaJnJR9jzyvtviXCRegFFJlM-_G799TlY</a>
converted the bitmap to InputImage Object. And do all the configurations.
I converted the image and then load model try to print results:</p>
<pre><code>    // getting bitmap of the image
    Bitmap photo = (Bitmap) data.getExtras().get(&quot;data&quot;);
    //convert image
    InputImage image = InputImage.fromBitmap(photo,0);
//load local model
LocalModel localModel =
                    new LocalModel.Builder()
                            .setAssetFilePath(&quot;mobilenet_v1_1.0_224_quant.tflite&quot;)
                            // or .setAbsoluteFilePath(absolute file path to tflite model)
                            .build();

            // Multiple object detection in static images
            CustomObjectDetectorOptions customObjectDetectorOptions =
                    new CustomObjectDetectorOptions.Builder(localModel)
                            .setDetectorMode(CustomObjectDetectorOptions.SINGLE_IMAGE_MODE)
                            .enableMultipleObjects()
                            .enableClassification()
                            .setClassificationConfidenceThreshold(0.5f)
                            .setMaxPerObjectLabelCount(3)
                            .build();

            ObjectDetector objectDetector =
                    ObjectDetection.getClient(customObjectDetectorOptions);

            objectDetector
                    .process(image)
                    .addOnFailureListener(e -&gt; {System.out.println(e.getMessage());})
                    .addOnSuccessListener(results -&gt; {
                        for (DetectedObject detectedObject : results) {
                            Rect boundingBox = detectedObject.getBoundingBox();
                            Integer trackingId = detectedObject.getTrackingId();
                            for (DetectedObject.Label label : detectedObject.getLabels()) {
                                String text = label.getText();
                                int index = label.getIndex();
                                float confidence = label.getConfidence();
                                System.out.println(text);
                                System.out.println(index);
                                System.out.println(confidence);
                            }

                            System.out.println(boundingBox);
                            System.out.println(trackingId);
                        }
                    });
</code></pre>
<p>But I got errors:</p>
<blockquote>
<p>“Failed to initialize detector. Unexpected number of dimensions for
output index 0: got 3D, expected either 2D.&quot;</p>
</blockquote>
<p>Do you have any idea
about this issue? Thank you so much if you can give me some solutions.</p>
<p>The full error:</p>
<pre><code>E/native: calculator_graph.cc:776 INVALID_ARGUMENT: CalculatorGraph::Run() failed in Run:
    Calculator::Open() for node &quot;[BoxClassifierCalculator, BoxClassifierCalculator with output stream: detection_results0]&quot; failed: #vk Unexpected number of dimensions for output index 0: got 3D, expected either 2D (BxN with B=1) or 4D (BxHxWxN with B=1, W=1, H=1).
    pipeline_jni.cc:62 CalculatorGraph::Run() failed in Run:
    Calculator::Open() for node &quot;[BoxClassifierCalculator, BoxClassifierCalculator with output stream: detection_results0]&quot; failed: #vk Unexpected number of dimensions for output index 0: got 3D, expected either 2D (BxN with B=1) or 4D (BxHxWxN with B=1, W=1, H=1).
E/native: pipeline_jni.cc:209 Graph has errors:
    Calculator::Open() for node &quot;[BoxClassifierCalculator, BoxClassifierCalculator with output stream: detection_results0]&quot; failed: #vk Unexpected number of dimensions for output index 0: got 3D, expected either 2D (BxN with B=1) or 4D (BxHxWxN with B=1, W=1, H=1).
E/MobileVisionBase: Error preloading model resource
    com.google.mlkit.common.MlKitException: Failed to initialize detector. Unexpected number of dimensions for output index 0: got 3D, expected either 2D (BxN with B=1) or 4D (BxHxWxN with B=1, W=1, H=1).
        at com.google.mlkit.vision.vkp.PipelineManager.start(com.google.mlkit:vision-internal-vkp@@16.0.0:68)
        at com.google.mlkit.vision.objects.custom.internal.zzd.load(com.google.mlkit:object-detection-custom@@16.0.0:79)
        at com.google.mlkit.common.sdkinternal.ModelResource.zza(com.google.mlkit:common@@16.0.0:22)
        at com.google.mlkit.common.sdkinternal.zzn.call(com.google.mlkit:common@@16.0.0)
        at com.google.mlkit.common.sdkinternal.zzm.run(com.google.mlkit:common@@16.0.0:5)
        at com.google.mlkit.common.sdkinternal.zzq.run(com.google.mlkit:common@@16.0.0:3)
        at android.os.Handler.handleCallback(Handler.java:751)
        at android.os.Handler.dispatchMessage(Handler.java:95)
        at com.google.android.gms.internal.mlkit_common.zzb.dispatchMessage(com.google.mlkit:common@@16.0.0:6)
        at android.os.Looper.loop(Looper.java:154)
        at android.os.HandlerThread.run(HandlerThread.java:61)
</code></pre>
",14391655.0,,899365.0,,2020-10-05 14:35:48,2020-12-05 07:10:17,How to Put TensorFlow AI model(Object recognition) in Android APP?,<android><tensorflow><model><artificial-intelligence><roboflow>,0,2,,,,CC BY-SA 4.0
62771868,1,62771972.0,,2020-07-07 09:14:20,,7,66131,"<p>I try to predict 10 classes using this code</p>
<pre><code>#Predicting the Test set rules
y_pred = model.predict(traindata)
y_pred = np.argmax(y_pred, axis=1) 
y_true = np.argmax(testdata, axis=1) 

target_names = [&quot;akLembut&quot;,&quot;akMundur&quot;,&quot;akTajam&quot;,&quot;caMenaik&quot;, &quot;caMenurun&quot;, &quot;coretanTengah&quot;, &quot;garisAtas&quot;, &quot;garisBawah&quot;, &quot;garisBawahBanyak&quot;, &quot;ttdCangkang&quot;]
print(&quot;\n&quot;+ classification_report(y_true, y_pred, target_names=target_names))
</code></pre>
<p>But then I got an error message like this</p>
<pre><code>AxisError                                 Traceback (most recent call last)
&lt;ipython-input-13-a2b02b251547&gt; in &lt;module&gt;()
      2 y_pred = model.predict(traindata)
      3 y_pred = np.argmax(y_pred, axis=1)
----&gt; 4 y_true = np.argmax(testdata, axis=1)
      5 
      6 target_names = [&quot;akLembut&quot;,&quot;akMundur&quot;,&quot;akTajam&quot;,&quot;caMenaik&quot;, &quot;caMenurun&quot;, &quot;coretanTengah&quot;, &quot;garisAtas&quot;, &quot;garisBawah&quot;, &quot;garisBawahBanyak&quot;, &quot;ttdCangkang&quot;]

&lt;__array_function__ internals&gt; in argmax(*args, **kwargs)

2 frames
/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py in _wrapit(obj, method, *args, **kwds)
     45     except AttributeError:
     46         wrap = None
---&gt; 47     result = getattr(asarray(obj), method)(*args, **kwds)
     48     if wrap:
     49         if not isinstance(result, mu.ndarray):

AxisError: axis 1 is out of bounds for array of dimension 1
</code></pre>
<p>I already train the data and I need to know each accuracy.</p>
",13651375.0,,,,,2022-04-14 00:46:04,AxisError: axis 1 is out of bounds for array of dimension 1 when calculating accuracy of classes,<python><python-3.x><tensorflow>,1,0,0.0,,,CC BY-SA 4.0
64218503,1,,,2020-10-06 02:24:59,,7,3928,"<p><strong>Update: See my own answer to this question. This is a bug of tensorflow Efficientnet</strong></p>
<p><strong>WHAT I WANT TO</strong><br />
I want to finetune efficientnet. First, I successfully finished training and saved a model. It consists of a frozen efficientnet, and fully connected layer. I used <code>SavedModel</code> format to save it(See train.py). Then, at the finetuning stage (See finetune.py), I tried to load <code>SavedModel</code> , but failed to load.</p>
<p><strong>PROBLEM</strong><br />
I couldn't load and retrain <code>SavedModel</code> containing Efficientnet successfully.</p>
<p><strong>WHAT I HAVE TRIED</strong><br />
I tried to <code>load_model</code> and <code>load_weights</code>, but either didn't help. Does anyone know how to do it?  GradientTape doesn't go with SavedMmodel?. Should I use something else than <code>load_model</code> or <code>load_weights</code>?</p>
<p><strong>ENVIRONMENT</strong><br />
macOS: 10.15.6<br />
Tensorflow==2.3.1</p>
<p><strong>LOG OUTPUT</strong></p>
<pre><code>
... (a very long line of something like this below)

WARNING:tensorflow:Importing a function (__inference_my_model_layer_call_and_return_conditional_losses_3683150) with ops with custom gradients. Will likely fail if a gradient is requested.
ail if a gradient is requested.
WARNING:tensorflow:Importing a function (__inference_my_model_layer_call_and_return_conditional_losses_3683150) with ops with custom gradients. Will likely fail if a gradient is requested.

...

File &quot;finetune.py&quot;, line 90, in &lt;module&gt;
    _train_loss = train_step(train_images, train_labels).numpy()
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 780, in __call__
    result = self._call(*args, **kwds)
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 823, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 697, in _initialize
    *args, **kwds))
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 2855, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py&quot;, line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py&quot;, line 600, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File &quot;/Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 973, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.autograph.impl.api.StagingError: in user code:

    finetune.py:54 train_step  *
        gradients = tape.gradient(loss, model.trainable_variables)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:1073 gradient  **
        unconnected_gradients=unconnected_gradients)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py:77 imperative_grad
        compat.as_str(unconnected_gradients.value))
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:797 _backward_function
        return self._rewrite_forward_and_call_backward(call_op, *args)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:712 _rewrite_forward_and_call_backward
        forward_function, backwards_function = self.forward_backward(len(doutputs))
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:621 forward_backward
        forward, backward = self._construct_forward_backward(num_doutputs)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:669 _construct_forward_backward
        func_graph=backwards_graph)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:659 _backprop_function
        src_graph=self._func_graph)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:669 _GradientsHelper
        lambda: grad_fn(op, *out_grads))
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:336 _MaybeCompile
        return grad_fn()  # Exit early
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:669 &lt;lambda&gt;
        lambda: grad_fn(op, *out_grads))
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:712 _rewrite_forward_and_call_backward
        forward_function, backwards_function = self.forward_backward(len(doutputs))
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:621 forward_backward
        forward, backward = self._construct_forward_backward(num_doutputs)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:669 _construct_forward_backward
        func_graph=backwards_graph)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/eager/function.py:659 _backprop_function
        src_graph=self._func_graph)
    /Users/a/my_awesome_project/.venv/lib/python3.7/site-packages/tensorflow/python/ops/gradients_util.py:623 _GradientsHelper
        (op.name, op.type))

    LookupError: No gradient defined for operation 'efficientnetb0/top_activation/IdentityN' (op type: IdentityN)
</code></pre>
<p><strong>SOURCE CODE</strong></p>
<p>train.py</p>
<pre class=""lang-py prettyprint-override""><code>import datetime
import os

import tensorflow as tf

from myutils import decode_jpg # defined in another module

class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.base_model = tf.keras.applications.EfficientNetB0(
        input_shape=(256, 256, 3),
        include_top=False,
        weights='imagenet')
    self.base_model.trainable = False  # unfreeze at finetuning stage later
    self.global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
    self.prediction_layer = tf.keras.layers.Dense(200)

  def call(self, x):
    x = self.base_model(x)
    x = self.global_average_layer(x)
    x = self.prediction_layer(x)
    return x

model = MyModel()

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()

@tf.function
def train_step(images, labels):
  with tf.GradientTape() as tape:
    predictions = model(images, training=True)
    loss = loss_object(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

data = tf.data.Dataset.list_files('./data/*/*.jpg').batch(128).map(decode_jpg)

for epoch in range(100):
  for images, labels in data:
    train_step(images, labels).
  model.save('saved_models/{}'.format(epoch + 1))
</code></pre>
<p>finetune.py (I refactored for minimized reproduction, so the line number in error log doesn't match)</p>
<pre class=""lang-py prettyprint-override""><code>import datetime
import os

import tensorflow as tf


class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.base_model = tf.keras.applications.EfficientNetB0(
        input_shape=(256, 256, 3),
        include_top=False,
        weights='imagenet'
    )
    self.base_model.trainable = True
    self.global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
    self.prediction_layer = tf.keras.layers.Dense(200)

  def call(self, x):
    x = self.base_model(x)
    x = self.global_average_layer(x)
    x = self.prediction_layer(x)
    return x
  
# model = MyModel()
# model.load_weights('./saved_models/65') ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.
model = tf.keras.models.load_model('./saved_models/65') # This way ends up error message above
model.get_layer('efficientnetb0').trainable = True


loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam(lr=1e-5)

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

@tf.function
def train_step(images, labels):
  with tf.GradientTape() as tape:
    # training=True is only needed if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    predictions = model(images, training=True)
    loss = loss_object(labels, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

EPOCHS = 100
data = tf.data.Dataset.list_files('./data/*/*.jpg').batch(128).map(decode_jpg)
for epoch in range(EPOCHS):
  for images, labels in data:
    train_step(train_images, train_labels)
  model.save('finetuned/{}'.format(epoch + 1))
</code></pre>
<p>I tried to reproduce on Colab but saw a different error message
<a href=""https://colab.research.google.com/drive/1gzOwSWJ1Kvwzo01SEpjqGq6Lb-OsI-ob?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1gzOwSWJ1Kvwzo01SEpjqGq6Lb-OsI-ob?usp=sharing</a></p>
<p>Now I made an issue on tensorflow/tensorflow repository.
<a href=""https://github.com/tensorflow/tensorflow/issues/43806"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensorflow/issues/43806</a></p>
",8281276.0,,8281276.0,,2021-04-24 15:37:42,2021-04-24 15:37:42,Finetune SavedModel Failure due to No Gradient loaded,<python><tensorflow>,1,1,,,,CC BY-SA 4.0
62794219,1,62809994.0,,2020-07-08 11:53:40,,7,5720,"<h2>In terminal</h2>
<ul>
<li>windows 10</li>
<li>using cuda 10.1</li>
<li>python 3.7.7</li>
<li>GPU GeForce GTX 1050 4GB</li>
</ul>
<pre class=""lang-sh prettyprint-override""><code>&gt;&gt;&gt; import tensorflow as tf
2020-07-08 17:10:50.005569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
&gt;&gt;&gt; tf.config.list_physical_devices('GPU')
2020-07-08 17:10:55.657489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1
coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s
2020-07-08 17:10:55.701387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
</code></pre>
<h2>In jupyter notebook</h2>
<pre class=""lang-py prettyprint-override""><code>in [1]: import tensorflow as tf
        tf.config.list_physical_devices('GPU')
out[1]: []

in [1]: tf.__version__
out[1]: '2.2.0'
</code></pre>
",12635140.0,,681865.0,,2020-07-08 12:00:36,2021-04-22 13:56:57,tensorflow GPU not showing in jupyter notebook,<python><tensorflow><jupyter-notebook>,1,5,,,,CC BY-SA 4.0
72199498,1,,,2022-05-11 10:33:12,,7,2657,"<p>While trying to download the &quot;Cats_vs_Dogs&quot; TensorFlow dataset using the <code>tfds</code> module, I get the following error 👇</p>
<pre><code>DownloadError                             Traceback (most recent call last)
&lt;ipython-input-2-244305a07c33&gt; in &lt;module&gt;()
      7     split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
      8     with_info=True,
----&gt; 9     as_supervised=True,
     10 )

21 frames
/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/download/downloader.py in _assert_status(response)
    257   if response.status_code != 200:
    258     raise DownloadError('Failed to get url {}. HTTP code: {}.'.format(
--&gt; 259         response.url, response.status_code))

DownloadError: Failed to get url https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip. HTTP code: 404.
</code></pre>
<p>The code I have used is</p>
<pre><code>import tensorflow_datasets as tfds
tfds.disable_progress_bar()

# split the data manually into 80% training, 10% testing, 10% validation
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],
    with_info=True,
    as_supervised=True,
)
</code></pre>
<p>It worked yesterday but suddenly gave an error today.......</p>
",18893370.0,,,,,2022-06-07 11:19:25,Error in importing Cats-vs-Dogs dataset in Google Colab,<python><tensorflow><dataset><google-colaboratory><kaggle>,4,0,,,,CC BY-SA 4.0
70818269,1,70832127.0,,2022-01-23 00:30:10,,7,25729,"<p>I'm trying to make a face detection model with CNN. I used codes that I made for number detection. When I use number images, program work. But, when I use my face images, I get an error that is:</p>
<p>Unexpected result of <code>train_function</code> (Empty logs). Please use <code>Model.compile(..., run_eagerly=True)</code>, or <code>tf.config.run_functions_eagerly(True)</code> for more information of where went wrong, or file a issue/bug to <code>tf.keras</code>.</p>
<p>Notebook link: <a href=""https://github.com/AkifCanSonmez/ImageProccessingCourse/blob/main/CNN/Number%20Classification%20Project/Building%20Model/Building%20Number%20Classification%20Model%20with%20Keras.ipynb"" rel=""noreferrer"">https://github.com/AkifCanSonmez/ImageProccessingCourse/blob/main/CNN/Number%20Classification%20Project/Building%20Model/Building%20Number%20Classification%20Model%20with%20Keras.ipynb</a></p>
<p>Number image:<a href=""https://i.stack.imgur.com/tUQmH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/tUQmH.png"" alt=""a number image"" /></a></p>
<p>Face image:<a href=""https://i.stack.imgur.com/shbsN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/shbsN.png"" alt=""a face image"" /></a></p>
",15710847.0,,,,,2023-03-20 17:05:57,"Tensorflow ValueError: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)",<python><tensorflow><conv-neural-network>,2,0,,,,CC BY-SA 4.0
71676507,1,,,2022-03-30 11:37:28,,7,9402,"<p>I am running into this error , i can't unpickle a file on my jupyter notebook:</p>
<pre><code>import os 
import pickle
import joblib
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
filename = open(&quot;loan_model3.pkl&quot;, &quot;rb&quot;)
mdl = pickle.load(filename)
mdl.close()
</code></pre>
<p>and it always shows the below error message , even tho i'vce upgraded all my libraries</p>
<p>Error Message:</p>
<blockquote>
<p>FileNotFoundError: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram://89506590-ec42-44a9-b67c-3ee4cc8e884e/variables/variables  You may be trying to load on a different device from the computational device. Consider setting the <code>experimental_io_device</code>option in<code>tf.saved_model.LoadOptions</code> to the io_device such as '/job:localhost'.</p>
</blockquote>
<p>I tried to upgrade my libraries but still didn't work.</p>
",7188374.0,,4685471.0,,2022-03-30 12:09:36,2022-11-04 15:57:28,"Error , Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ram to unpickle a file",<python><tensorflow><scikit-learn><pickle><joblib>,4,9,0.0,,,CC BY-SA 4.0
66623541,1,,,2021-03-14 10:42:30,,7,9911,"<p>I try to run Tensorflow with GPU support (GTX 1660 SUPER).</p>
<p>I created an enviroment using anaconda, than installed <code>cudatoolkit</code> (version <code>11.0.221</code>) and <code>tensorflow-gpu</code> (version <code>2.4.1</code>). Afterwards, I downloaded <code>cuDNN</code> (version <code>8.0.4</code>), and copied all files from cuDNN's <code>bin</code> folder to my environment's bin folder at <code>anaconda3\envs\&lt;env name&gt;\Library\bin</code>.
In my script, I've set the memory limit to my GPU's memory using <code>tf.config.experimental.set_memory_growth</code>.</p>
<p>When I run the script (which uses convolutional algorithms), I get a warning that says <code>Couldn't invoke ptxas.exe --version</code> which comes after an <code>Call to CreateProcess failed. Error code: 2</code> error.
After the launch failure, I get: <code>Relying on driver to perform ptx compilation.  Modify $PATH to customize ptxas location.</code></p>
<p>I've already tried switching to cuDNN version <code>8.1.1</code>.</p>
<p>How I fix this?</p>
",9572236.0,,681865.0,,2021-03-14 11:19:09,2023-03-01 16:54:09,Tensorflow 2.4.1 - Couldn't invoke ptxas.exe,<python><tensorflow><anaconda>,3,7,,,,CC BY-SA 4.0
70339753,1,70354210.0,,2021-12-13 19:11:02,,7,271,"<p>What's the XLA class <code>XlaBuilder</code> for? <a href=""https://www.tensorflow.org/xla/operation_semantics"" rel=""nofollow noreferrer"">The docs</a> describe its interface but don't provide a motivation.</p>
<p>The presentation in the docs, and indeed the comment above <code>XlaBuilder</code> in the source code</p>
<pre><code>// A convenient interface for building up computations.
</code></pre>
<p>suggests it's no more than a utility. However, this doesn't appear to explain its behaviour in other places. For example, we can construct an <code>XlaOp</code> with an <code>XlaBuilder</code> via e.g.</p>
<pre><code>XlaOp ConstantLiteral(XlaBuilder* builder, const LiteralSlice&amp; literal);
</code></pre>
<p>Here, it's not clear to me what role <code>builder</code> plays (note functions for constructing <code>XlaOp</code>s aren't documented on the published docs). Further, when I add two <code>XlaOp</code>s (with <code>+</code> or <code>Add</code>) it appears the ops must be constructed with the same builder, else I see</p>
<pre class=""lang-none prettyprint-override""><code>F tensorflow/core/platform/statusor.cc:33] Attempting to fetch value instead of handling error Invalid argument: No XlaOp with handle -1
</code></pre>
<p>Indeed, <code>XlaOp</code> retains a handle for an <code>XlaBuilder</code>. This suggests to me that the <code>XlaBuilder</code> has a more fundamental significance.</p>
<p>Beyond the title question, is there a use case for using multiple <code>XlaBuilder</code>s, or would you typically use one global instance for everything?</p>
",5986907.0,,5986907.0,,2022-03-20 18:41:01,2022-03-20 18:41:01,What is XlaBuilder for?,<c++><tensorflow><tensorflow-xla>,1,0,,,,CC BY-SA 4.0
67265525,1,,,2021-04-26 11:07:22,,7,1157,"<p>I am trying to feed a very large image into Triton server. I need to divide the input image into patches and feed the patches one by one into a tensorflow model. The image has a variable size, so the number of patches N is variable for each call.</p>
<p>I think a Triton ensemble model that calls the following steps would do the job:</p>
<ol>
<li>A python model (pre-process) to create the patches</li>
<li>The segmentation model</li>
<li>Finally another python model (post-process) to merge the output patches into a big output mask</li>
</ol>
<p>However, for this, I would have to write a <code>config. pbtxt</code> file with <code>1:N</code> and <code>N:1</code> relation, meaning the ensemble scheduler needs to call the 2nd step multiple times and the 3rd once with the aggregated output.</p>
<p>Is this possible, or do I need to use some other technique?</p>
",184925.0,,9215780.0,,2021-05-04 17:36:21,2021-12-26 16:31:52,"How to use Triton server ""ensemble model"" with 1:N input/output to create patches from large image?",<python><tensorflow><tensorrt><tritonserver>,1,4,0.0,,,CC BY-SA 4.0
68134618,1,,,2021-06-25 16:52:54,,7,359,"<p>I am trying to read a TFRecord file directly from an Amazon S3 bucket using file path and tf.data.TFRecordDataset().</p>
<p>It seems tf.data.TFRecordDataset() only accepts filename in tf.string or tf.data.dataset format.</p>
<p>I tried following and it did not work.</p>
<pre><code>filenames = [&quot;s3://path_to_TFRecord&quot;]
dataset = tf.data.TFRecordDataset(filenames)
</code></pre>
<p>I also tried using s3fs and a file handle to solve the issue but it fails.</p>
<p>Any solution is appreciated.</p>
",4505223.0,,1435543.0,,2021-06-25 17:52:28,2021-06-25 17:52:28,Reading a TFRecordfile from an Amazon S3 path,<amazon-web-services><tensorflow><amazon-s3>,0,1,,,,CC BY-SA 4.0
64864097,1,,,2020-11-16 19:01:21,,7,451,"<p>Everything is in the title.</p>
<p>I have a RaggedTensor of shape <code>(nsamples, None, M1)</code>, and I would like to transpose the last two axes to get a RaggedTensor of shape <code>(nsamples, M1, None)</code>.</p>
<p><code>tf.transpose</code> does not work on ragged tensors, nor <code>tf.linalg.matrix_transpose</code> (as they are the same function).</p>
<p>Is there a way to do it purely in Tensorflow or do I have to do a (slow) <code>for</code> loop manually ?</p>
",8141617.0,,,,,2020-11-16 19:01:21,How to transpose a ragged tensor,<python><tensorflow>,0,0,0.0,,,CC BY-SA 4.0
65388357,1,,,2020-12-21 06:23:44,,7,18043,"<p>I am new to VGG19 and image processing in python. I am trying to test my trained VGG19 model for predicting an image. I am getting this error:-</p>
<pre><code>ValueError: Input 0 is incompatible with layer functional_3: expected shape=(None, 224, 224, 3), found shape=(None, 240, 240, 3)
</code></pre>
<p>My tensorflow code for predicting is:-</p>
<pre><code>import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
import numpy as np
import cv2
import tensorflow as tf
from tensorflow.keras.models import load_model
model = load_model('VGG19.h5')
CATEGORIES = [&quot;Pneumonia&quot;, &quot;Non-Pneumonia&quot;]
img = cv2.imread('person1_bacteria_1.jpeg')
img = cv2.resize(img,(240,240))     # resize image to match model's expected sizing
img = np.reshape(img,[1,240,240,3]) # return the image with shaping that TF wants.
prediction = model.predict(img)
prediction
</code></pre>
<p>But in the case of .ipynb file I simply get a warning regarding this:-</p>
<p><a href=""https://i.stack.imgur.com/6aZ4Y.png"" rel=""noreferrer"">This the image</a></p>
",,user14863281,,user14863281,2020-12-21 06:58:57,2021-11-10 21:59:19,"Input 0 is incompatible with layer functional_3: expected shape=(None, 224, 224, 3), found shape=(None, 240, 240, 3)",<python><tensorflow><deep-learning><conv-neural-network><vgg-net>,2,0,,,,CC BY-SA 4.0
68478456,1,,,2021-07-22 02:32:23,,7,12658,"<p>I got this error file while following this tutorial: <a href=""https://www.youtube.com/watch?v=yqkISICHH-U"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=yqkISICHH-U</a></p>
<p>So far I have created a training dataset to feed into Tensorflow. I am using transfer learning on one of the pre-trained TensorFlow object recognition models. I get the error when I attempt to train the model with my new dataset. <a href=""https://i.stack.imgur.com/Age9z.png"" rel=""nofollow noreferrer"">This is a picture of the full error message that shows up</a>.</p>
<p>I attempted to solve the problem by trying the following:</p>
<ul>
<li>Reinstalling Numpy</li>
<li>Installing Numpy 1.20.x</li>
<li>Installing Numpy 1.18.x</li>
<li>Reinstalling pycocotools</li>
</ul>
<p>When I installed Numpy 1.20.x the error went away and was replaced with a separate error: notimplementederror: cannot convert a symbolic Tensor (cond_2/strided:0) to a numpy array. This seemed to be a compatibility issue between Tensorflow and Numpy 1.20.x. When debugging this error, I found that everyone solved it by downgrading their Numpy to 1.19.x. However, in my case I am still left with the first error.</p>
<p>I am using python 3.8.9 with anaconda.
Here are the packages I have installed: <a href=""https://pastebin.com/BNW8tU2A"" rel=""nofollow noreferrer"">https://pastebin.com/BNW8tU2A</a></p>
<pre><code>(tfod) (base) C:\Users\piper\python\Tensorflow\TFODCourse&gt;pip list
Package                 Version             Location
----------------------- ------------------- ---------------------------------------------------------------------------
absl-py                 0.13.0
astunparse              1.6.3
backcall                0.2.0
cachetools              4.2.2
certifi                 2021.5.30
charset-normalizer      2.0.3
colorama                0.4.4
cycler                  0.10.0
Cython                  0.29.24
debugpy                 1.3.0
decorator               5.0.9
flatbuffers             1.12
gast                    0.4.0
gin-config              0.4.0
google-auth             1.33.1
google-auth-oauthlib    0.4.4
google-cloud-bigquery   1.21.0
google-pasta            0.2.0
grpcio                  1.34.1
h5py                    3.1.0
idna                    3.2
ipykernel               6.0.3
ipython                 7.25.0
ipython-genutils        0.2.0
jedi                    0.18.0
jupyter-client          6.1.12
jupyter-core            4.7.1
keras-nightly           2.5.0.dev2021032900
Keras-Preprocessing     1.1.2
kiwisolver              1.3.1
lvis                    0.5.3
lxml                    4.6.3
Markdown                3.3.4
matplotlib              3.2.0
matplotlib-inline       0.1.2
numpy                   1.19.5
oauthlib                3.1.1
object-detection        0.1
opencv-python           4.5.3.56
opt-einsum              3.3.0
pandas                  1.3.0
parso                   0.8.2
pickleshare             0.7.5
Pillow                  8.3.1
pip                     21.1.3
prompt-toolkit          3.0.19
protobuf                3.17.3
pyasn1                  0.4.8
pyasn1-modules          0.2.8
pycocotools             2.0.2
Pygments                2.9.0
pyparsing               2.4.7
PyQt5                   5.15.4
PyQt5-Qt5               5.15.2
PyQt5-sip               12.9.0
python-dateutil         2.8.2
pytz                    2021.1
pywin32                 225
PyYAML                  5.4.1
pyzmq                   22.1.0
requests                2.26.0
requests-oauthlib       1.3.0
rsa                     4.7.2
scipy                   1.7.0
setuptools              49.2.1
six                     1.15.0
slim                    0.1                 c:\users\piper\python\tensorflow\tfodcourse\tensorflow\models\research\slim
tensorboard             2.5.0
tensorboard-data-server 0.6.1
tensorboard-plugin-wit  1.8.0
tensorflow              2.5.0
tensorflow-addons       0.13.0
tensorflow-estimator    2.5.0
tensorflow-gpu          2.5.0
termcolor               1.1.0
tf-models-official      2.5.0
tf-slim                 1.1.0
tornado                 6.1
traitlets               5.0.5
typeguard               2.12.1
typing-extensions       3.7.4.3
urllib3                 1.26.6
wcwidth                 0.2.5
Werkzeug                2.0.1
wget                    3.2
wheel                   0.36.2
wrapt                   1.12.1
</code></pre>
<p>Any help would be appreciated!</p>
<p>So I solved the issue by reinstalling pycocotools with the --no-cache-dir flag. So I did:</p>
<pre><code>pip uninstall pycocotools
pip install --no-cache-dir pycocotools
</code></pre>
<p>This completely solved the issue.</p>
",6253767.0,,4294399.0,,2022-04-28 19:35:52,2023-01-05 20:03:30,"Error ""Numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject.""",<python><numpy><tensorflow>,1,1,0.0,,,CC BY-SA 4.0
65037119,1,,,2020-11-27 12:08:14,,7,1427,"<p>Is it possible to delete the in-memory cache that's built after calling <code>tf.data.Dataset.cache()</code>?</p>
<p>Here's what I'd like to do. The augmentation for the dataset is very costly, so the current code is more or less:</p>
<pre class=""lang-py prettyprint-override""><code>data = tf.data.Dataset(...) \
       .map(&lt;expensive_augmentation&gt;) \
       .cache() \
       # .shuffle().batch() etc. 
</code></pre>
<p>However, this means that every iteration over <code>data</code> will see the same augmented versions of the data samples. What I'd like to do instead is to use the cache for a couple of epochs and then start over, or equivalently do something like <code>Dataset.map(&lt;augmentation&gt;).fleeting_cache().repeat(8)</code>. Is this possible to achieve?</p>
",3019480.0,,,,,2022-02-08 01:49:23,tf.data.Dataset - delete cache?,<python><tensorflow>,1,2,0.0,,,CC BY-SA 4.0
69088577,1,,,2021-09-07 12:52:43,,7,2335,"<p>this is my code</p>
<pre><code>random = tf.random.Generator.from_seed(42)
random = random.normal(shape=(2,2))
</code></pre>
<p>but i got this error:</p>
<pre><code>tensorflow.python.framework.errors_impl.NotFoundError: No registered 'RngReadAndSkip' 
OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}. Registered: device='CPU'
</code></pre>
",14627656.0,,4685471.0,,2021-09-07 15:06:39,2023-02-25 18:19:15,[Apple M1]: I got No registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}} . Registered: device='CPU',<tensorflow><machine-learning><deep-learning><tensorflow2.0><apple-m1>,2,0,0.0,,,CC BY-SA 4.0
63122486,1,63157923.0,,2020-07-27 19:46:59,,7,7557,"<p>Running the latest docker with:</p>
<pre><code>docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu-jupyter jupyter notebook --notebook-dir=/tf --ip 0.0.0.0 --no-browser --allow-root --NotebookApp.allow_origin='https://colab.research.google.com'
</code></pre>
<p>code:</p>
<pre><code>import tensorflow as tf
print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU')))
</code></pre>
<p>gives me:</p>
<pre><code>2020-07-27 19:44:03.826149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-07-27 19:44:03.826179: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: UNKNOWN ERROR (-1)
2020-07-27 19:44:03.826201: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist
</code></pre>
<p>I'm on Pop_OS 20.04, have tried installing the CUDA drivers from the Pop repository as well as from NVidia. No dice. Any help appreciated.</p>
<p>Running</p>
<pre><code>docker run --gpus all nvidia/cuda:10.0-base nvidia-smi
</code></pre>
<p>gives me:</p>
<pre><code>+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce RTX 2080    On   | 00000000:09:00.0  On |                  N/A |
|  0%   52C    P5    15W / 225W |    513MiB /  7959MiB |     17%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
</code></pre>
",1135125.0,,1135125.0,,2020-07-27 19:52:31,2020-07-29 16:12:32,the tensorflow docker gpu image doesn't detect my GPU,<linux><docker><tensorflow><nvidia>,1,0,0.0,,,CC BY-SA 4.0
62931568,1,62932897.0,,2020-07-16 09:26:31,,7,3213,"<p>Could you explain how it works during training?</p>
<pre><code>learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: 8e-2
          total_steps: 300000
          warmup_learning_rate: .0001
          warmup_steps: 400
        }
      }```
</code></pre>
",11758585.0,,,,,2020-12-03 17:51:52,How should I understand warmup learning rate in tensorflow object detection api?,<tensorflow><tensorflow2.0><object-detection>,2,0,0.0,,,CC BY-SA 4.0
68191448,1,68192520.0,,2021-06-30 08:49:20,,7,10053,"<p>I built a simple CNN model and it raised below errors:</p>
<pre><code>Epoch 1/10
235/235 [==============================] - ETA: 0s - loss: 540.2643 - accuracy: 0.4358
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
&lt;ipython-input-14-ab88232c98aa&gt; in &lt;module&gt;()
     15     train_ds,
     16     validation_data=val_ds,
---&gt; 17     epochs=epochs
     18 )

7 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.
     [[{{node decode_image/DecodeImage}}]]
     [[IteratorGetNext]] [Op:__inference_test_function_2924]

Function call stack:
test_function
</code></pre>
<p>The code I wrote is quite simple and standard. Most of them are just directly copied from the official website. It raised this error before the first epoch finish. I am pretty sure that the images are all png files. The train folder does not contain anything like text, code, except imgages. I am using Colab. The version of <code>tensorlfow</code> is 2.5.0. Appreciate for any help.</p>
<pre class=""lang-py prettyprint-override""><code>data_dir = './train'

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir, 
    subset='training',
    validation_split=0.2,
    batch_size=batch_size,
    seed=42
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir, 
    subset='validation',
    validation_split=0.2,
    batch_size=batch_size,
    seed=42
)

model = Sequential([
    layers.InputLayer(input_shape=(image_size, image_size, 3)),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
    ])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(
    optimizer=optimizer,
    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'])

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)
</code></pre>
",12125395.0,,7370153.0,,2021-06-30 09:56:08,2023-05-23 15:44:45,"Unknown image file format. One of JPEG, PNG, GIF, BMP required",<python><tensorflow>,5,4,,,,CC BY-SA 4.0
65074687,1,,,2020-11-30 13:41:51,,7,5343,"<p>I am try to classfication with object detection at the colab.I am using &quot;ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.config&quot;When I start to training I get error.
Training=</p>
<pre><code>!python model_main_tf2.py \
    --pipeline_config_path=training/ssd_resnet101_v1_fpn_640x640_coco17_tpu-8.config \
    --model_dir=training \
    --alsologtostderr
</code></pre>
<pre><code>WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
W1130 13:39:27.991891 140559633127296 util.py:158] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
</code></pre>
",14522895.0,,,,,2022-12-03 23:24:47,Object detection Classfication /A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights),<tensorflow><computer-vision><object-detection>,3,0,,,,CC BY-SA 4.0
74504568,1,,,2022-11-19 23:34:59,,7,5425,"<p>I keep on getting the following error while making imports:</p>
<pre><code>ImportError: cannot import name 'experimental_functions_run_eagerly' from 'tensorflow.python.eager.def_function'
</code></pre>
<p>I have alread tried using <code>pip</code> to uninstall and reinstall tensorflow.</p>
<p>The errors occur while I'm trying to import tensorflow but here are all of them:</p>
<pre><code>import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import tensorflow as tf
from tf import keras
</code></pre>
<p>I'm running <code>Python 3.7.9</code> on Windows 10. I can't import Tensorflow due to the error, but according to <code>pip list</code> these are my install versions:</p>
<pre><code>jupyter_client               7.4.7
jupyter_core                 4.11.2
jupyter-server               1.23.2
jupyterlab                   3.5.0
jupyterlab-pygments          0.2.2
jupyterlab_server            2.16.3
jupyterthemes                0.20.0
keras                        2.11.0
tensorboard                  2.11.0
tensorboard-data-server      0.6.1
tensorboard-plugin-wit       1.8.1
tensorflow                   2.11.0
tensorflow-cpu               2.11.0
tensorflow-estimator         2.11.0
tensorflow-intel             2.11.0
tensorflow-io-gcs-filesystem 0.27.0
termcolor                    2.1.0
</code></pre>
<p>How can I solve this error?</p>
",11247589.0,,,,,2023-02-21 21:28:40,Tensorflow ImportError: cannot import name 'experimental_functions_run_eagerly' from 'tensorflow.python.eager.def_function',<python><tensorflow>,3,3,,,,CC BY-SA 4.0
74792286,1,,,2022-12-14 00:12:24,,7,4197,"<p>I've been trying to get into ML and I wanted to follow a course on it but it requires Tensorflow and I've been trying to get that working on my system. I have the 2021 14&quot; 16GB Macbook Pro with the M1 Pro Chip and I am running Ventura 13.1. I have been following <a href=""https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706"" rel=""noreferrer"">this article</a> as well as digging around about getting Tensorflow working on M1 but to no avail. I managed to get tensorflow-macos installed in my environment as well as tensorflow-metal but when I try to run some sample code in Juyter, I'm getting an error that I do not understand. In Jupyter, when I run:</p>
<pre><code>import tensorflow as tf print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU')))
</code></pre>
<p>I get</p>
<p><code>Num GPUs Available: 1</code></p>
<p>So it does seem like I have tensorflow and metal installed, but when I try to run the rest of the code, I get:</p>
<pre><code>TensorFlow version: 2.11.0
Num GPUs Available:  1
Metal device set to: Apple M1 Pro
WARNING:tensorflow:AutoGraph could not transform &lt;function normalize_img at 0x14a4cec10&gt; and will run it as-is.
Cause: Unable to locate the source code of &lt;function normalize_img at 0x14a4cec10&gt;. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
2022-12-13 13:54:33.658225: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2022-12-13 13:54:33.658309: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)
WARNING:tensorflow:AutoGraph could not transform &lt;function normalize_img at 0x14a4cec10&gt; and will run it as-is.
Cause: Unable to locate the source code of &lt;function normalize_img at 0x14a4cec10&gt;. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform &lt;function normalize_img at 0x14a4cec10&gt; and will run it as-is.
Cause: Unable to locate the source code of &lt;function normalize_img at 0x14a4cec10&gt;. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
Epoch 1/12
2022-12-13 13:54:34.162300: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
2022-12-13 13:54:34.163015: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.
2022-12-13 13:54:35.383325: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.383350: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.389028: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.389049: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.401250: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.401274: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.405004: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
2022-12-13 13:54:35.405025: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x14a345660
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
File &lt;timed exec&gt;:45

File ~/conda/envs/mlp3/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---&gt; 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/conda/envs/mlp3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---&gt; 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

NotFoundError: Graph execution error:

Detected at node 'StatefulPartitionedCall_6' defined at (most recent call last):
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/runpy.py&quot;, line 87, in _run_code
      exec(code, run_globals)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel_launcher.py&quot;, line 17, in &lt;module&gt;
      app.launch_new_instance()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/traitlets/config/application.py&quot;, line 992, in launch_instance
      app.start()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelapp.py&quot;, line 711, in start
      self.io_loop.start()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/tornado/platform/asyncio.py&quot;, line 215, in start
      self.asyncio_loop.run_forever()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/asyncio/base_events.py&quot;, line 570, in run_forever
      self._run_once()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/asyncio/base_events.py&quot;, line 1859, in _run_once
      handle._run()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/asyncio/events.py&quot;, line 81, in _run
      self._context.run(self._callback, *self._args)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 510, in dispatch_queue
      await self.process_one()
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 499, in process_one
      await dispatch(*args)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 406, in dispatch_shell
      await result
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/kernelbase.py&quot;, line 729, in execute_request
      reply_content = await reply_content
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/ipkernel.py&quot;, line 411, in do_execute
      res = shell.run_cell(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/ipykernel/zmqshell.py&quot;, line 531, in run_cell
      return super().run_cell(*args, **kwargs)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 2940, in run_cell
      result = self._run_cell(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 2995, in _run_cell
      return runner(coro)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/async_helpers.py&quot;, line 129, in _pseudo_sync_runner
      coro.send(None)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 3194, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 3373, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 3433, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File &quot;/var/folders/k4/vgd34_w913ndkfkmvgssqgjr0000gn/T/ipykernel_16072/1016625245.py&quot;, line 1, in &lt;module&gt;
      get_ipython().run_cell_magic('time', '', 'import tensorflow as tf\nimport tensorflow_datasets as tfds\nprint(&quot;TensorFlow version:&quot;, tf.__version__)\nprint(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(\'GPU\')))\ntf.config.list_physical_devices(\'GPU\')\n(ds_train, ds_test), ds_info = tfds.load(\n    \'mnist\',\n    split=[\'train\', \'test\'],\n    shuffle_files=True,\n    as_supervised=True,\n    with_info=True,\n)\ndef normalize_img(image, label):\n  &quot;&quot;&quot;Normalizes images: `uint8` -&gt; `float32`.&quot;&quot;&quot;\n  return tf.cast(image, tf.float32) / 255., label\nbatch_size = 128\nds_train = ds_train.map(\n    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nds_train = ds_train.cache()\nds_train = ds_train.shuffle(ds_info.splits[\'train\'].num_examples)\nds_train = ds_train.batch(batch_size)\nds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\nds_test = ds_test.map(\n    normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nds_test = ds_test.batch(batch_size)\nds_test = ds_test.cache()\nds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n                 activation=\'relu\'),\n  tf.keras.layers.Conv2D(64, kernel_size=(3, 3),\n                 activation=\'relu\'),\n  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n#   tf.keras.layers.Dropout(0.25),\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(128, activation=\'relu\'),\n#   tf.keras.layers.Dropout(0.5),\n  tf.keras.layers.Dense(10, activation=\'softmax\')\n])\nmodel.compile(\n    loss=\'sparse_categorical_crossentropy\',\n    optimizer=tf.keras.optimizers.Adam(0.001),\n    metrics=[\'accuracy\'],\n)\nmodel.fit(\n    ds_train,\n    epochs=12,\n    validation_data=ds_test,\n)\n')
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/interactiveshell.py&quot;, line 2417, in run_cell_magic
      result = fn(*args, **kwargs)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/IPython/core/magics/execution.py&quot;, line 1321, in time
      out = eval(code_2, glob, local_ns)
    File &quot;&lt;timed exec&gt;&quot;, line 45, in &lt;module&gt;
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/utils/traceback_utils.py&quot;, line 65, in error_handler
      return fn(*args, **kwargs)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1249, in train_function
      return step_function(self, iterator)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1222, in run_step
      outputs = model.train_step(data)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/engine/training.py&quot;, line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 527, in minimize
      self.apply_gradients(grads_and_vars)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1140, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 634, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1166, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1216, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File &quot;/Users/imigh/conda/envs/mlp3/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1211, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_6'
could not find registered platform with id: 0x14a345660
     [[{{node StatefulPartitionedCall_6}}]] [Op:__inference_train_function_1261]
</code></pre>
<p>Sorry for just dumping the entire error code but as you can see something's gone awry. It only seems to run the first Epoch and I'm not sure what's going wrong. I've followed everything in that guide as well as the instructions from <a href=""https://developer.apple.com/metal/tensorflow-plugin/"" rel=""noreferrer"">tensor flow-metal</a>. I've looked around seeminly everywhere but this is as far as I've gotten after hours of battling. I just updated my Mac today so the Xcode command line tools should be up to date. Any and all advice or helping me decipher the error code would be greatly appreciated. I just want to learn Machine Learning but I can't even follow my course without this working.</p>
<p>I've uninstalled and reinstalled Conda Miniforge for M1 several times. I've created and tried the steps in a blank environment. I've followed the steps listed in the guides I've linked above and went through them multiple times. I was originally getting some issues with numpy, h5py, grcio, and protobuf but after tinkering with the versions I no longer get error codes for them, so I'm not sure if that's all good but I don't see any explicit mentions. I've also ran</p>
<pre><code>conda install -c conda-forge openblas
</code></pre>
<p>after looking at <a href=""https://stackoverflow.com/questions/68996176/tensorflow-2-5-mac-m1-installing-problem-compatibility-with-numpy-library-co"">this page from StackOverflow</a> from someone with a similar issue, but I'm still getting this error.</p>
",20770952.0,,,,,2023-03-07 08:58:51,Can't get Tensorflow working on macOS M1 Pro Chip,<python><tensorflow><apple-m1><metal><tensorflow-datasets>,2,0,,,,CC BY-SA 4.0
69834335,1,69881065.0,,2021-11-04 03:47:03,,7,8930,"<p>Getting an error for IndexError: invalid index to scalar variable on the yolo_layers line.</p>
<pre><code>network = cv2.dnn.readNetFromDarknet('yolov3.cfg', 'yolov3.weights')
layers = network.getLayerNames()
yolo_layers = [layers[i[0] - 1] for i in network.getUnconnectedOutLayers()]
</code></pre>
<p>This code won't work on my Jupyter notebook but will run fine on google collab. No idea why. Could be my python version?</p>
",12881334.0,,6885902.0,,2022-06-18 14:24:59,2022-06-18 14:24:59,Loading YOLO: invalid index to scalar variable,<python><tensorflow><opencv><machine-learning><yolo>,2,1,0.0,,,CC BY-SA 4.0
70081599,1,,,2021-11-23 13:16:07,,7,1262,"<p>I've tested CPU to GPU data transfer throughput with TensorFlow and it seems to be significantly lower than in PyTorch. For large tensors between 2x and 5x slower. In TF, I reach maximum speed for 25MB tensors (~4 GB/s) and it drops down to 2 GB/s with increasing tensor size. PyTorch data transfer speed grows with tensor size and saturates at 9 GB/s (25MB tensors). The behavior is consistent on RTX 2080ti and GTX 1080ti, and with TF 2.4 and 2.6.</p>
<p>Am I doing something wrong? Is there some way how to match the data throughput of PyTorch? I'm not just looking to hide the latency e.g. using async queues, but I'd like to get the full data bandwidth.</p>
<p>Results on batches of 256x256x3 images in TF (avarageg over 100 transfers):</p>
<pre><code>code: tf.cast(x, dtype=tf.float32)[0, 0]
Batch size 1; Batch time 0.0005; BPS 1851.8; FPS 1851.8; MB/S 364.1
Batch size 2; Batch time 0.0004; BPS 2223.5; FPS 4447.1; MB/S 874.3
Batch size 4; Batch time 0.0006; BPS 1555.2; FPS 6220.6; MB/S 1223.0
Batch size 8; Batch time 0.0006; BPS 1784.8; FPS 14278.7; MB/S 2807.3
Batch size 16; Batch time 0.0013; BPS 755.3; FPS 12084.7; MB/S 2376.0
Batch size 32; Batch time 0.0023; BPS 443.8; FPS 14201.3; MB/S 2792.1
Batch size 64; Batch time 0.0035; BPS 282.5; FPS 18079.5; MB/S 3554.6
Batch size 128; Batch time 0.0061; BPS 163.4; FPS 20916.4; MB/S 4112.3
Batch size 256; Batch time 0.0241; BPS 41.5; FPS 10623.0; MB/S 2088.6
Batch size 512; Batch time 0.0460; BPS 21.7; FPS 11135.8; MB/S 2189.4
</code></pre>
<p>Same results with PyTorch:</p>
<pre><code>Code: torch.from_numpy(x).to(self.device).type(torch.float32)[0, 0].cpu()
Batch size 1; Batch time 0.0001; BPS 10756.6; FPS 10756.6; MB/S 2114.8
Batch size 1; Batch time 0.0001; BPS 12914.7; FPS 12914.7; MB/S 2539.1
Batch size 2; Batch time 0.0001; BPS 10204.4; FPS 20408.7; MB/S 4012.5
Batch size 4; Batch time 0.0002; BPS 5841.1; FPS 23364.3; MB/S 4593.6
Batch size 8; Batch time 0.0003; BPS 3994.4; FPS 31955.4; MB/S 6282.7
Batch size 16; Batch time 0.0004; BPS 2713.8; FPS 43421.3; MB/S 8537.0
Batch size 32; Batch time 0.0007; BPS 1486.3; FPS 47562.7; MB/S 9351.2
Batch size 64; Batch time 0.0015; BPS 679.3; FPS 43475.9; MB/S 8547.7
Batch size 128; Batch time 0.0028; BPS 359.5; FPS 46017.7; MB/S 9047.5
Batch size 256; Batch time 0.0054; BPS 185.2; FPS 47404.1; MB/S 9320.0
Batch size 512; Batch time 0.0108; BPS 92.9; FPS 47564.5; MB/S 9351.6
</code></pre>
<p>The full code to reproduce the measurements is:</p>
<pre><code>import time
import numpy as np
import tensorflow as tf
import torch
import argparse


def parseargs():
    parser = argparse.ArgumentParser(usage='Test GPU transfer speed in TensorFlow(default) and Pytorch.')
    parser.add_argument('--pytorch', action='store_true', help='Use PyTorch instead of TensorFlow')
    args = parser.parse_args()
    return args


class TimingModelTF(tf.keras.Model):
    def __init__(self, ):
        super(TimingModelTF, self).__init__()

    @tf.function
    def call(self, x):
        return tf.cast(x, dtype=tf.float32)[0, 0]


class TimingModelTorch(torch.nn.Module):
    def __init__(self, ):
        super(TimingModelTorch, self).__init__()
        self.device = torch.device('cuda')

    def forward(self, x):
        with torch.no_grad():
            return torch.from_numpy(x).to(self.device).type(torch.float32)[0, 0].cpu()


if __name__ == '__main__':
    args = parseargs()
    width = 256
    height = 256
    channels = 3
    iterations = 100
    model = TimingModelTorch() if args.pytorch else TimingModelTF()

    for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]:
        img = np.random.randint(5, size=(batch_size, height, width, channels), dtype=np.uint8)

        result = model(img)
        result.numpy()

        start = time.time()
        for i in range(iterations):
            result = model(img)
            result.numpy()
        batch_time = (time.time() - start) / iterations
        print(f'Batch size {batch_size}; Batch time {batch_time:.4f}; BPS {1 / batch_time:.1f}; FPS {(1 / batch_time) * batch_size:.1f}; MB/S {(((1 / batch_time) * batch_size) * 256 * 256 * 3) / 1000000:.1f}')

</code></pre>
",5170044.0,,,,,2021-12-02 00:48:19,Is CPU to GPU data transfer slow in TensorFlow?,<python><performance><tensorflow><gpu><pci-e>,1,4,,,,CC BY-SA 4.0
65081913,1,,,2020-11-30 22:13:09,,7,954,"<p>I read the data and processed it using the following code :</p>
<pre><code>data = pd.read_csv('Step1_output.csv')
data = data.sample(frac=1).reset_index(drop=True)
data1 = pd.DataFrame(data, columns=['Res_pair'])

# creating instance of labelencoder
labelencoder = LabelEncoder()
# Assigning numerical values and storing in another column
data1['Res_pair_ID'] = labelencoder.fit_transform(data1['Res_pair'])
data['Res_pair'] = data1['Res_pair_ID']
data = data.to_numpy()
train_X = data[0:data.shape[0],0:566]
train_y = data[0:data.shape[0],566:data.shape[1]]
train_X = train_X.reshape((train_X.shape[0], train_X.shape[1], 1))
</code></pre>
<p>I build the model using following code where I have tried to distribute the dataset using mirrored strategy of Tensorflow :</p>
<pre><code>print(&quot;Hyper-parameter values:\n&quot;)
print('Momentum Rate =',momentum_rate,'\n')
print('learning rate =',learning_rate,'\n')
print('Number of neurons =',neurons,'\n')

  

strategy = tensorflow.distribute.MirroredStrategy()
with strategy.scope():
        model = tf.keras.Sequential([ 
          tf.keras.layers.Conv1D(64,kernel_size = 3,activation='relu',input_shape=train_X.shape[1:]),
          tf.keras.layers.Flatten(),
          tf.keras.layers.Dense(neurons,activation='relu'),
          tf.keras.layers.Dense(neurons,activation='relu'),
          tf.keras.layers.Dense(neurons,activation='relu'),
          tf.keras.layers.Dense(neurons,activation='relu'),
          tf.keras.layers.Dense(10, activation='softmax'),])
        sgd = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=momentum_rate, nesterov=True)
        model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy',tensorflow.keras.metrics.Precision()])
        results = model.fit(train_X,train_y,validation_split = 0.2,epochs=10,batch_size = 100)
        print(results)
       
    path = 'saved_model/'
    
    model.save(path, save_format='tf')

    for k in range(100):
        momentum_rate = random.random()
        learning_rate = random.uniform(0,0.2)
        neurons = random.randint(10,50)
</code></pre>
<p>I tried to run the code on GPU but it runs for some time and then throws this error :</p>
<pre><code>Hyper-parameter values:

Momentum Rate = 0.6477407029392913

learning rate = 0.03988890117492503

Number of neurons = 35

Epoch 1/10
     1/270110 [..............................] - ETA: 28s - loss: nan - accuracy: 0.0100 - precision: 0.0100Traceback (most recent call last):
  File &quot;parallelised_script_realdata2.py&quot;, line 56, in &lt;module&gt;
    results = model.fit(train_X,train_y,validation_split = 0.2,epochs=10,batch_size = 100)
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1098, in fit
    tmp_logs = train_function(iterator) 
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py&quot;, line 780, in __call__
    result = self._call(*args, **kwds)
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py&quot;, line 807, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/eager/function.py&quot;, line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/eager/function.py&quot;, line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/eager/function.py&quot;, line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/eager/function.py&quot;, line 550, in call
    ctx=ctx) 
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/eager/execute.py&quot;, line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  assertion failed: [predictions must be &gt;= 0] [Condition x &gt;= y did not hold element-wise:] [x (sequential/dense_4/Softmax:0) = ] [[nan nan nan...]...] [y (Cast_6/x:0) = ] [0]
         [[{{node assert_greater_equal/Assert/AssertGuard/else/_21/assert_greater_equal/Assert/AssertGuard/Assert}}]] [Op:__inference_train_function_1270]

Function call stack:
train_function
</code></pre>
<p>Update: The code works well if I don't use <code>strategy = tensorflow.distribute.MirroredStrategy()</code>. Like the code below (but will fail for larger datasets for memory shortage):</p>
<pre><code>def convolutional_neural_network(x, y):
    print(&quot;Hyper-parameter values:\n&quot;)
    print('Momentum Rate =',momentum_rate,'\n')
    print('learning rate =',learning_rate,'\n')
    print('Number of neurons =',neurons,'\n')

    model = Sequential()
    model.add(Conv1D(filters=64,input_shape=train_X.shape[1:],activation='relu',kernel_size = 3))
    model.add(Flatten())
    model.add(Dense(neurons,activation='relu')) # first hidden layer
    model.add(Dense(neurons, activation='relu')) # second hidden layer
    model.add(Dense(neurons, activation='relu'))
    model.add(Dense(neurons, activation='relu'))
    model.add(Dense(10, activation='softmax'))
    sgd = optimizers.SGD(lr=learning_rate, decay=1e-6, momentum=momentum_rate, nesterov=True)
    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy',tensorflow.keras.metrics.Precision()])

    history = model.fit(train_X, train_y, validation_split=0.2, epochs=10, batch_size=100)



momentum_rate = 0.09
learning_rate = 0.01
neurons = 40
print(convolutional_neural_network(train_X, train_y))
</code></pre>
<p>Update 2: Still facing a similar issue with smaller dataset</p>
<pre><code>_________________________________________________________________
Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv1d (Conv1D)              (None, 564, 64)           256
_________________________________________________________________
flatten (Flatten)            (None, 36096)             0
_________________________________________________________________
dense (Dense)                (None, 50)                1804850
_________________________________________________________________
dense_1 (Dense)              (None, 50)                2550
_________________________________________________________________
dense_2 (Dense)              (None, 50)                2550
_________________________________________________________________
dense_3 (Dense)              (None, 50)                2550
_________________________________________________________________
dense_4 (Dense)              (None, 10)                510
=================================================================
Total params: 1,813,266
Trainable params: 1,813,266
Non-trainable params: 0
</code></pre>
",5653423.0,,4685471.0,,2020-12-07 22:34:55,2020-12-11 02:41:07,error when using Mirrored strategy in Tensorflow,<python><tensorflow><machine-learning>,1,13,,,,CC BY-SA 4.0
70268140,1,70268141.0,,2021-12-07 23:20:55,,7,21908,"<p>I just updated my graphics cards drives with</p>
<pre><code>sudo apt install nvidia-driver-470
sudo apt install cuda-drivers-470
</code></pre>
<p>I decided to install them in this manner because they were being held back when trying to <code>sudo apt upgrade</code>. I mistakenly then did <code>sudo apt autoremove</code> to cleanup old packages. After restarting my computer for new drivers to get setup properly, I could no longer use GPU acceleration with tensorflow.</p>
<pre><code>import tensorflow as tf
tf.test.is_gpu_available()
WARNING:tensorflow:From &lt;stdin&gt;:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-12-07 16:52:01.771391: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-12-07 16:52:01.807283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-12-07 16:52:01.807973: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-12-07 16:52:01.808017: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory
2021-12-07 16:52:01.808048: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory
2021-12-07 16:52:01.856391: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory
2021-12-07 16:52:01.856466: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory
2021-12-07 16:52:01.857601: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
False

</code></pre>
",11575257.0,,,,,2022-06-02 20:47:16,Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory,<python><tensorflow><cuda><driver>,2,0,,,,CC BY-SA 4.0
65704588,1,65707151.0,,2021-01-13 15:07:24,,7,1937,"<p>I am trying to tune a basic neural network as practice. (Based on an example from a coursera course: Neural Networks and Deep Learning - DeepLearning.AI)
I face the issue of the random weight initialization. Lets say I try to tune the number of layers in the network.
I have two options:</p>
<ul>
<li>1.: set the random seed to a fixed value</li>
<li>2.: run my experiments more times without setting the seed</li>
</ul>
<p>Both version has pros and cons.
My biggest concern is that if I use a random seed (e.g.: <code>tf.random.set_seed(1)</code>) then the determined values can be &quot;over-fitted&quot; to the seed and may not work well without the seed or if the value is changed (e.g.: <code>tf.random.set_seed(1)</code> -&gt; <code>tf.random.set_seed(2)</code>. On the other hand, if I run my experiments more times without random seed then I can inspect less option (due to limited computing capacity) and still only inspect a subset of possible random weight initialization.
In both cases I feel that luck is a strong factor in the process.</p>
<p>Is there a best practice how to handle this topic?</p>
<p>Has TensorFlow built in tools for this purpose? I appreciate any source of descriptions or tutorials. Thanks in advance!</p>
",14421092.0,,11341120.0,,2021-01-13 16:38:01,2021-01-13 17:52:26,Neural network hyperparameter tuning - is setting random seed a good idea?,<tensorflow><machine-learning><deep-learning><neural-network><hyperparameters>,2,1,0.0,2021-01-14 02:00:42,,CC BY-SA 4.0
68162820,1,,,2021-06-28 11:47:02,,7,732,"<p>I am interested in multi class segmentation of skin tissues, I have 3000 skin tissue labels classified into 4 classes,  I have created a CNN classification algorithm to train my classification model. I would like to use the classification model for segmentation task of new skin tissue image and perform feature extraction of the skin tissue belonging to each of the class</p>
<p>Following is the code that is written to train my classification model</p>
<pre><code>from tensorflow.keras.layers import Input, Concatenate, Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D
from tensorflow.keras import backend as K
#from tensorflow.keras.utils import np_utils
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import optimizers
from tensorflow.keras.metrics import top_k_categorical_accuracy
from tensorflow.keras.models import Sequential, Model, load_model
import tensorflow as tf
from tensorflow.keras.initializers import he_uniform
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, CSVLogger, ReduceLROnPlateau
#from tensorflow.compat.keras.backend import KTF
#import keras.backend.tensorflow_backend as KTF
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.inception_v3 import InceptionV3

import os
import matplotlib.pylab as plt
import numpy as np
import pandas as pd
#import numpy as np, Pillow, skimage, imageio, matplotlib
#from scipy.misc import imresize
from skimage.transform import resize
from tqdm import tqdm
from tensorflow.keras import metrics

#### PREPROCESS STAGE ####

# Path to superpixels class files
classes_file = &quot;/home/DEV/SKIN_3000_CLASSES.csv&quot;
concatenated_data= pd.read_csv(classes_file, header=None)

# Instances with targets
targets = concatenated_data[1].tolist()

# Split data according to their classes
class_0 = concatenated_data[concatenated_data[1] == 0]
class_1 = concatenated_data[concatenated_data[1] == 1]
class_2 = concatenated_data[concatenated_data[1] == 2]
class_3 = concatenated_data[concatenated_data[1] == 3]

# Holdout split train/test set (Other options are k-folds or leave-one-out)
split_proportion = 0.8

split_size_0 = int(len(class_0)*split_proportion)
split_size_1 = int(len(class_1)*split_proportion)
split_size_2 = int(len(class_2)*split_proportion)
split_size_3 = int(len(class_3)*split_proportion)

new_class_0_train = np.random.choice(len(class_0), split_size_0, replace=False)
new_class_0_train = class_0.iloc[new_class_0_train]
new_class_0_test = ~class_0.iloc[:][0].isin(new_class_0_train.iloc[:][0])
new_class_0_test = class_0[new_class_0_test]

new_class_1_train = np.random.choice(len(class_1), split_size_1, replace=False)
new_class_1_train = class_1.iloc[new_class_1_train]
new_class_1_test = ~class_1.iloc[:][0].isin(new_class_1_train.iloc[:][0])
new_class_1_test = class_1[new_class_1_test]

new_class_2_train = np.random.choice(len(class_2), split_size_2, replace=False)
new_class_2_train = class_2.iloc[new_class_2_train]
new_class_2_test = ~class_2.iloc[:][0].isin(new_class_2_train.iloc[:][0])
new_class_2_test = class_2[new_class_2_test]

new_class_3_train = np.random.choice(len(class_3), split_size_3, replace=False)
new_class_3_train = class_3.iloc[new_class_3_train]
new_class_3_test = ~class_3.iloc[:][0].isin(new_class_3_train.iloc[:][0])
new_class_3_test = class_3[new_class_3_test]

x_train_list = pd.concat(
    [new_class_0_train, new_class_1_train, new_class_2_train, new_class_3_train])
x_test_list = pd.concat(
    [new_class_0_test, new_class_1_test, new_class_2_test, new_class_3_test])

# Load superpixels files
imagePath = &quot;/home/DEV/SKIN_SET_3000/&quot;

x_train = []
y_train = []
for index, row in tqdm(x_train_list.iterrows(), total=x_train_list.shape[0]):
    try:
        loadedImage = plt.imread(imagePath + str(row[0]) + &quot;.jpg&quot;)
        x_train.append(loadedImage)
        y_train.append(row[1])
    except:
        # Try with .png file format if images are not properly loaded
        try:
            loadedImage = plt.imread(imagePath + str(row[0]) + &quot;.png&quot;)
            x_train.append(loadedImage)
            y_train.append(row[1])
        except:
            # Print file names whenever it is impossible to load image files
            print(imagePath + str(row[0]))

x_test = []
y_test = []
for index, row in tqdm(x_test_list.iterrows(), total=x_test_list.shape[0]):
    try:
        loadedImage = plt.imread(imagePath + str(row[0]) + &quot;.jpg&quot;)
        x_test.append(loadedImage)
        y_test.append(row[1])
    except:
        # Try with .png file format if images are not properly loaded
        try:
            loadedImage = plt.imread(imagePath + str(row[0]) + &quot;.png&quot;)
            x_test.append(loadedImage)
            y_test.append(row[1])
        except:
            # Print file names whenever it is impossible to load image files
            print(imagePath + str(row[0]))


# Reescaling of images 
img_width, img_height = 139, 139

index = 0
for image in tqdm(x_train):
    #aux = resize(image, (img_width, img_height, 3), &quot;bilinear&quot;)
    aux = resize(image, (img_width, img_height))
    x_train[index] = aux / 255.0  # Normalization
    index += 1

index = 0
for image in tqdm(x_test):
    #aux = resize(image, (img_width, img_height, 3), &quot;bilinear&quot;)
    aux = resize(image, (img_width, img_height))
    x_test[index] = aux / 255.0  # Normalization
    index += 1




#### TRAINING STAGE ####

os.environ[&quot;KERAS_BACKEND&quot;] = &quot;tensorflow&quot;
RANDOM_STATE = 42

def get_session(gpu_fraction=0.8):

    num_threads = os.environ.get('OMP_NUM_THREADS')
    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_fraction)

    if num_threads:
        return tf.Session(config=tf.ConfigProto(
            gpu_options=gpu_options, intra_op_parallelism_threads=num_threads))
    else:
        return tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))


#KTF.set_session(get_session())


def precision(y_true, y_pred):

    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision


def recall(y_true, y_pred):

    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall


def fbeta_score(y_true, y_pred, beta=1):

    if beta &lt; 0:
        raise ValueError('The lowest choosable beta is zero (only precision).')

    # Set F-score as 0 if there are no true positives (sklearn-like).
    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:
        return 0.0

    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    bb = beta ** 2
    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())
    return fbeta_score


nb_classes = 4
final_model = []


# Option = InceptionV3
model = InceptionV3(weights=&quot;imagenet&quot;, include_top=False,
                    input_shape=(img_width, img_height, 3))
# Option = ResNet
# model = ResNet50(weights=&quot;imagenet&quot;, include_top=False, input_shape=(3,img_width, img_height))

# Creating new outputs for the model
x = model.output
x = Flatten()(x)
x = Dense(512, activation=&quot;relu&quot;)(x)
x = Dropout(0.5)(x)
x = Dense(512, activation=&quot;relu&quot;)(x)
x = Dropout(0.5)(x)
predictions = Dense(nb_classes, activation='softmax')(x)
#predictions = Dense(nb_classes, activation='sigmoid')(x)
final_model = Model(inputs=model.input, outputs=predictions)


# Metrics
learningRate = 0.001
optimizer = optimizers.SGD(learning_rate=learningRate, momentum=0.88, nesterov=True)

# Compiling the model...
final_model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=optimizer,
                    metrics=[&quot;accuracy&quot;, fbeta_score])
final_model.summary()

#final_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

#model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])

#x_train = np.array(x_train)
#x_test = np.array(x_test)

x_train = np.asarray(x_train).astype(np.float32)
#x_test = np.array(x_test)
x_test = np.asarray(x_test).astype(np.float32)

# Defining targets...
y_train = np.concatenate([np.full((new_class_0_train.shape[0]), 0), np.full((new_class_1_train.shape[0]), 1),
                          np.full((new_class_2_train.shape[0]), 2), np.full((new_class_3_train.shape[0]), 3)])

y_test = np.concatenate([np.full((new_class_0_test.shape[0]), 0), np.full((new_class_1_test.shape[0]), 1),
                         np.full((new_class_2_test.shape[0]), 2), np.full((new_class_3_test.shape[0]), 3)])

y_train = to_categorical(y_train)
y_test =  to_categorical(y_test)

modelFilename = &quot;/home/DEV/SKIN_SET_3000/model_inception.h5&quot;

trainingFilename = &quot;/home/DEV/SKIN_SET_3000/training.csv&quot;
nb_train_samples = y_train.shape[0]
nb_test_samples = y_test.shape[0]
#epochs = 10000
epochs = 100
batch_size = 24
trainingPatience = 200
decayPatience = trainingPatience / 4

# Setting the data generator...
train_datagen = ImageDataGenerator(
    horizontal_flip=True,
    fill_mode=&quot;reflect&quot;,
    zoom_range=0.2
)

train_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)

# Saving the model
checkpoint = ModelCheckpoint(modelFilename,
                             monitor='val_accuracy',
                             verbose=1,
                             save_best_only=True,
                             save_weights_only=False,
                             mode='auto',
                             save_freq=1)

adaptativeLearningRate = ReduceLROnPlateau(monitor='val_accuracy',
                                           factor=0.5,
                                           patience=decayPatience,
                                           verbose=1,
                                           mode='auto',
                                           min_delta=0.0001,
                                           cooldown=0,
                                           min_lr=1e-8)

early = EarlyStopping(monitor='val_accuracy',
                      min_delta=0,
                      patience=trainingPatience,
                      verbose=1,
                      mode='auto')

csv_logger = CSVLogger(trainingFilename, separator=&quot;,&quot;, append=False)

# Callbacks
callbacks = [checkpoint, early, csv_logger, adaptativeLearningRate]

# Training of the model
final_model.fit(train_generator,
                          steps_per_epoch=nb_train_samples / batch_size,
                          epochs=epochs,
                          shuffle=True,
                          validation_data=(x_test, y_test),
                          validation_steps=nb_test_samples / batch_size,
                          callbacks=callbacks)

final_model.save('/home/DEV/SKIN_SET_3000/model_inception.h5')
#compile metrics
</code></pre>
<p>In order to  segment my image, first i have transformed my input image to super pixel  using SLIC</p>
<pre><code>from skimage.segmentation import slic
from skimage.segmentation import mark_boundaries
from skimage.util import img_as_float
from skimage import io; io.use_plugin('matplotlib')
import cv2 as cv
from skimage.color import label2rgb

img_width, img_height = 139, 139

# load the model we saved
model = load_model('/home/DEV/SKIN_SET_3000/model_inception.h5', compile=False)

# Get test image ready
 img = skimage.img_as_float(skimage.io.imread('/home/DEV/SKIN_ULCER.jpg')).astype(np.float32)
plt.imshow(img)
test_image_slic = slic(img, n_segments=500, compactness=10.0)
test_image_slic_out = mark_boundaries(img,test_image_slic)
plt.imshow(test_image_slic_out)
#test_image=test_image/255
test_image_array = np.array(test_image_slic_out)
test_image_resize = cv2.resize(test_image_array,(img_width,img_height))
test_image_reshape = test_image_resize.reshape(1,img_width, img_height,3)
</code></pre>
<p>I would like to check if each superpixel of my input is labeled as one of my target class among 4 tissue classes, and extract the features belonging to each class as a mask and quantify the total surface area of mask .
any suggestions of how to implement this approach would be appreciated.</p>
",7320705.0,,11891228.0,,2021-07-02 09:10:45,2021-07-09 03:37:46,How to perform super pixel image segmentation and feature extraction,<python><tensorflow><image-segmentation><multilabel-classification><superpixels>,0,1,0.0,,,CC BY-SA 4.0
68267785,1,,,2021-07-06 09:17:40,,7,153,"<p>I have been running into performance issues with numpy. I tested different ways to compute a dot product, but similar performance issues have been seen with other numpy functions.</p>
<pre><code>import timeit
import numpy as np
import tensorflow as tf

def np_dot(U, X):
    tic = timeit.default_timer()
    X_bar = np.dot(U.T, X)
    toc = timeit.default_timer()
    print(&quot;np.dot took {:.4f} s&quot;.format(toc-tic))
    return X_bar

def np_at(U, X):
    tic = timeit.default_timer()
    X_bar = U.T @ X
    toc = timeit.default_timer()
    print(&quot;np @ took {:.4f} s&quot;.format(toc-tic))
    return X_bar

def np_matmul(U, X):
    tic = timeit.default_timer()
    X_bar = np.matmul(U.T, X)
    toc = timeit.default_timer()
    print(&quot;np.matmul took {:.4f} s&quot;.format(toc-tic))
    return X_bar

def np_einsum(U, X):
    tic = timeit.default_timer()
    X_bar = np.einsum('ij,jk', U.T, X)
    toc = timeit.default_timer()
    print(&quot;np.einsum took {:.4f} s&quot;.format(toc-tic))
    return X_bar

def tf_matmul(U, X):
    tic = timeit.default_timer()
    X_bar = tf.matmul(U.T, X)
    toc = timeit.default_timer()
    print(&quot;tf.matmul took {:.4f} s&quot;.format(toc-tic))
    return X_bar


if __name__ == &quot;__main__&quot;:
    print(&quot;is_gpu_available?&quot;, tf.test.is_gpu_available(), tf.test.is_built_with_cuda())
    print(&quot;numpy version:&quot;, np.__version__)
    print(&quot;tensorflow version:&quot;, tf.__version__)
    
    M = N = 1000
    X = np.random.rand(M, N)
    U = np.random.rand(M, N)
    X_bar = np_dot(U, X)
    X_bar0 = np_at(U, X)
    X_bar1 = np_matmul(U, X)
    X_bar2 = np_einsum(U, X)
    X_bar3 = tf_matmul(U, X)
    print(np.allclose(X_bar, X_bar0))
    print(np.allclose(X_bar, X_bar1))
    print(np.allclose(X_bar, X_bar2))
    print(np.allclose(X_bar, X_bar3))
</code></pre>
<p>The output on my laptop (with Windows 10 and python 3.8) is:</p>
<pre><code>is_gpu_available? False False
numpy version: 1.19.2
tensorflow version: 2.2.0
np.dot took 12.5368 s
np @ took 14.3303 s
np.matmul took 14.0634 s
np.einsum took 0.5937 s
tf.matmul took 0.0263 s
True
True
True
True
</code></pre>
<p>meaning <strong>the standard <code>@</code> is 545 times slower than than it could be!</strong> On a friends laptop the result look more like expected (with Ubuntu and python 3.6.9):</p>
<pre><code>is_gpu_available? False False
numpy version: 1.19.5
tensorflow version: 1.14.0
np.dot took 0.0192 s
np @ took 0.0196 s
np.matmul took 0.0208 s
np.einsum took 0.3893 s
tf.matmul took 0.0280 s
True
True
True
True
</code></pre>
<p>Interestingly, numpys <code>einsum</code> does not seem to be affected by whatever is going wrong on my laptop and neither is tensorflow.</p>
<p>What is going on here?
What are the potential reasons for such a huge performance difference?</p>
<p><strong>edit:</strong></p>
<p>Always make sure numpy is using some BLAS library. Read more about it here: <a href=""https://markus-beuckelmann.de/blog/boosting-numpy-blas.html"" rel=""nofollow noreferrer"">Boosting numpy: Why BLAS Matters</a>.</p>
<p>In my case <code>numpy.show_config()</code> returns the following:</p>
<pre><code>blas_mkl_info:
  NOT AVAILABLE
blis_info:
  NOT AVAILABLE
openblas_info:
  NOT AVAILABLE
atlas_3_10_blas_threads_info:
  NOT AVAILABLE
atlas_3_10_blas_info:
  NOT AVAILABLE
atlas_blas_threads_info:
  NOT AVAILABLE
atlas_blas_info:
  NOT AVAILABLE
accelerate_info:
  NOT AVAILABLE
blas_info:
  NOT AVAILABLE
blas_src_info:
  NOT AVAILABLE
blas_opt_info:
  NOT AVAILABLE
lapack_mkl_info:
  NOT AVAILABLE
openblas_lapack_info:
  NOT AVAILABLE
openblas_clapack_info:
  NOT AVAILABLE
flame_info:
  NOT AVAILABLE
atlas_3_10_threads_info:
  NOT AVAILABLE
atlas_3_10_info:
  NOT AVAILABLE
atlas_threads_info:
  NOT AVAILABLE
atlas_info:
  NOT AVAILABLE
lapack_info:
  NOT AVAILABLE
lapack_src_info:
  NOT AVAILABLE
lapack_opt_info:
  NOT AVAILABLE
numpy_linalg_lapack_lite:
    language = c
    define_macros = [('HAVE_BLAS_ILP64', None), ('BLAS_SYMBOL_SUFFIX', '64_')]
</code></pre>
",6747238.0,,6747238.0,,2021-07-07 09:26:42,2021-07-07 09:26:42,Resons for potentially bad performance of numpy?,<python><arrays><performance><numpy><tensorflow>,0,5,0.0,,,CC BY-SA 4.0
71828861,1,71838022.0,,2022-04-11 13:39:19,,7,701,"<p>I am building an audio-based deep learning model. As part of the preporcessing I want to augment the audio in my datasets. One augmentation that I want to do is to apply RIR (room impulse response) function. I am working with <code>Python 3.9.5</code> and <code>TensorFlow 2.8</code>.</p>
<p>In Python the standard way to do it is, if the RIR is given as a finite impulse response (FIR) of <em>n</em> taps, is using <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html"" rel=""nofollow noreferrer"">SciPy lfilter</a></p>
<pre><code>import numpy as np
from scipy import signal
import soundfile as sf

h = np.load(&quot;rir.npy&quot;)
x, fs = sf.read(&quot;audio.wav&quot;)

y = signal.lfilter(h, 1, x)
</code></pre>
<p>Running in loop on all the files may take a long time. Doing it with TensorFlow <code>map</code> utility on TensorFlow datasets:</p>
<pre><code># define filter function
def h_filt(audio, label):
    h = np.load(&quot;rir.npy&quot;)
    x = audio.numpy()
    y = signal.lfilter(h, 1, x)
    return tf.convert_to_tensor(y, dtype=tf.float32), label

# apply it via TF map on dataset
aug_ds = ds.map(h_filt)
</code></pre>
<p>Using <code>tf.numpy_function</code>:</p>
<pre><code>tf_h_filt = tf.numpy_function(h_filt, [audio, label], [tf.float32, tf.string])

# apply it via TF map on dataset
aug_ds = ds.map(tf_h_filt)
</code></pre>
<p>I have two questions:</p>
<ol>
<li>Is this way correct and fast enough (less than a minute for 50,000 files)?</li>
<li>Is there a faster way to do it? E.g. replace the SciPy function with a built-in TensforFlow function. I didn't find the equivalent of <code>lfilter</code> or <a href=""https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve.html"" rel=""nofollow noreferrer"">SciPy's convolve</a>.</li>
</ol>
",13440165.0,,13440165.0,,2022-04-11 15:49:29,2022-04-12 06:27:25,Filtering audio signal in TensorFlow,<python><tensorflow><scipy><dataset><signal-processing>,1,3,,,,CC BY-SA 4.0
64818511,1,,,2020-11-13 09:33:26,,7,528,"<p>The shape of the TFLite model is [1, 2535, 85]. You can find the TFLite model <a href=""https://drive.google.com/file/d/17wyP3Vy9RG3EC6gZ5LfaCOQFCZaenwBs/view?usp=sharing"" rel=""noreferrer"">here</a> and label text <a href=""https://drive.google.com/file/d/1L_Tdb-aggzXEJG1qyjb0e9tbcSlCRnPT/view?usp=sharing"" rel=""noreferrer"">here</a>.</p>
<p>This is how the bug looks.</p>
<p><a href=""https://i.stack.imgur.com/QK1IS.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QK1IS.jpg"" alt=""enter image description here"" /></a></p>
<p>This is the project I used <a href=""https://github.com/hunglc007/tensorflow-yolov4-tflite/tree/master/android"" rel=""noreferrer"">https://github.com/hunglc007/tensorflow-yolov4-tflite/tree/master/android</a> with some few changes. The changes are as following:</p>
<ol>
<li><p>Added the TFLite model and the text in the assets folder (the label text is already present in the project, its the same).</p>
</li>
<li><p>Line 57 DetectorActivity.java.</p>
</li>
</ol>
<pre><code>private static final String TF_OD_API_MODEL_FILE = &quot;yolov3-tiny.tflite&quot;;
private static final String TF_OD_API_LABELS_FILE = &quot;file:///android_asset/coco.txt&quot;;
</code></pre>
<ol start=""3"">
<li>line 181 tflite/YoloV4Classifier.java.</li>
</ol>
<pre><code>private static boolean isTiny = true;
</code></pre>
<ol start=""4"">
<li>line 426 tflite/YoloV4Classifier.java, (Replace the function to the down below).</li>
</ol>
<p>This is the code:</p>
<pre><code>private ArrayList&lt;Recognition&gt; getDetectionsForTiny(ByteBuffer byteBuffer, Bitmap bitmap) {
    ArrayList&lt;Recognition&gt; detections = new ArrayList&lt;Recognition&gt;();
    Map&lt;Integer, Object&gt; outputMap = new HashMap&lt;&gt;();
    //  outputMap.put(0, new float[1][OUTPUT_WIDTH_TINY[0]][4]);
    outputMap.put(0, new float[1][OUTPUT_WIDTH_TINY[1]][labels.size() + 5]);
    Object[] inputArray = {byteBuffer};
    tfLite.runForMultipleInputsOutputs(inputArray, outputMap);
    int gridWidth = OUTPUT_WIDTH_TINY[0];
    float[][][] bboxes = (float [][][]) outputMap.get(0);
    // float[][][] out_score = (float[][][]) outputMap.get(1);
    int count = 0;
    for (int i = 0; i &lt; gridWidth; i++) {
        float maxClass = 0;
        int detectedClass = -1;
        final float[] classes = new float[labels.size()];
        for (int c = 0; c &lt; labels.size(); c++) {
            classes [c] = bboxes[0][i][c + 5];
        }
        for (int c = 0; c &lt; labels.size(); ++c) {
            if (classes[c] &gt; maxClass) {
                detectedClass = c;
                maxClass = classes[c];
            }
        }
        final float score = maxClass;
        if (score &gt; getObjThresh()) {
            final float xPos = bboxes[0][i][0];
            final float yPos = bboxes[0][i][1];
            final float w = bboxes[0][i][2];
            final float h = bboxes[0][i][3];
            final RectF rectF = new RectF(
                Math.max(0, xPos - w / 2),
                Math.max(0, yPos - h / 2),
                Math.min(bitmap.getWidth() - 1, xPos + w / 2),
                Math.min(bitmap.getHeight() - 1, yPos + h / 2));
            detections.add(new Recognition(&quot;&quot; + i, labels.get(detectedClass), score, rectF, detectedClass));
            count++;
        }
    }
    Log.d(&quot;Count&quot;, &quot; &quot; + count);
    return detections;
}
</code></pre>
<p>Please I don't know where I'm going wrong! Struggling with it since days! Thanks for helping.</p>
",5829906.0,,11652943.0,,2020-11-15 15:22:32,2020-11-15 15:22:32,YoloV3 object detection with tflite model returns around 160 bounding boxes randomly all tagged with first class from the label text,<java><android><tensorflow><object-detection><yolo>,0,3,0.0,,,CC BY-SA 4.0
72479044,1,,,2022-06-02 15:56:55,,7,66840,"<p>Why am i getting this issue? I can import image module from kera.preprocessing. But cannot import image_dataset_from_directory. TF version: 2.9.1</p>
<pre><code># make a prediction for a new image.
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras.models import load_model

# load and prepare the image
def load_image(filename):
    # load the image
    img = load_img(filename, target_size=(224, 224))
    # convert to array
    img = img_to_array(img)
    # reshape into a single sample with 3 channels
    img = img.reshape(1, 224, 224, 3)
    # center pixel data
    img = img.astype('float32')
    img = img - [123.68, 116.779, 103.939]
    return img

# load an image and predict the class
def run_example():
    # load the image
    img = load_image('test.jpg')
    # load model
    model = load_model('final_model.h5')
    # predict the class
    result = model.predict(img)
    print(result[0])

# entry point, run the example
run_example()
</code></pre>
<p>error</p>
<pre><code>from keras.preprocessing.image import load_img
ImportError: cannot import name 'load_img' from 'keras.preprocessing.image'
</code></pre>
",15558193.0,,,,,2023-03-14 18:13:00,cannot import name 'load_img' from 'keras.preprocessing.image',<python><tensorflow>,3,2,,,,CC BY-SA 4.0
71688065,1,71866908.0,,2022-03-31 06:49:28,,6,1177,"<p>I have a new MacBook with the Apple M1 chipset. To install tensorflow, I follow the instructions <a href=""https://developer.apple.com/metal/tensorflow-plugin/"" rel=""noreferrer"">here</a>, i.e., installing <code>tensorflow-metal</code> and <code>tensorflow-macos</code> instead of the normal <code>tensorflow</code> package.</p>
<p>While this works fine, it means that I can't run the typical <code>pip install -r requirements.txt</code> as long as we have <code>tensorflow</code> in the <code>requirements.txt</code>. If we instead include <code>tensorflow-macos</code>, it'll lead to problems for non-M1 or even non-macOS users.</p>
<p>Our library must work on all platforms. Is there a generic install command that installs the correct TensorFlow version depending on whether the computer is a M1 Mac or not? So that we can use a single <code>requirements.txt</code> for everyone?</p>
<p>Or if that's not possible, can we pass some flag/option, e.g., <code>pip install -r requirements.txt --m1</code> to install some variation?
What's the simplest and most elegant solution here?</p>
",2745116.0,,,,,2022-05-05 11:15:37,Generic requirements.txt for TensorFlow on both Apple M1 and other devices,<python><macos><tensorflow><apple-m1><requirements.txt>,1,0,0.0,,,CC BY-SA 4.0
64114707,1,64119127.0,,2020-09-29 07:21:53,,6,1772,"<p>This is what I have</p>
<pre><code># Ratings data.
ratings = tfds.load('movie_lens/100k-ratings', split=&quot;train&quot;)
# Features of all the available movies.
movies = tfds.load('movie_lens/100k-movies', split=&quot;train&quot;)

# Select the basic features.
ratings = ratings.map(lambda x: {
    &quot;movie_title&quot;: x[&quot;movie_title&quot;],
    &quot;user_id&quot;: x[&quot;user_id&quot;]
})
movies = movies.map(lambda x: x[&quot;movie_title&quot;])
</code></pre>
<p>Since I don't want to download <code>MovieLens</code>, but my own data set. I tried reading it through <code>pandas</code>. Unfortunately, a <code>data frame</code> does not have a <code>map(...)</code> method. Is there an option that reads my .csv file and transfers it like <code>tfds.load(...)</code></p>
<p>This is what I tried</p>
<pre><code># Ratings data.
ratings = pd.read_csv('/content/drive/My Drive/Dataset/Test/ratings.csv')
movies = pd.read_csv('/content/drive/My Drive/Dataset/Test/movies.csv')
</code></pre>
<p>The Error</p>
<pre><code>AttributeError: 'DataFrame' object has no attribute 'map'
</code></pre>
",,user7597016,,,,2020-09-29 12:26:30,Create your own tfds.load,<python><tensorflow>,2,2,,,,CC BY-SA 4.0
66030439,1,,,2021-02-03 15:19:57,,6,1041,"<p>I'm unable to collect trace data using <code>tf.profiler.experimental.client.trace</code> Please can someone help? I'm following the (CPU/GPU) example usage here <a href=""https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/profiler/experimental/client/trace</a> which looks simple enough.</p>
<p>I have a very simple model, and I'm able to collect trace data from it using <code>tf.profiler.experimental.start</code> and <code>tf.profiler.experimental.stop</code>.</p>
<p>But <code>tf.profiler.experimental.client.trace</code> gives me empty trace data.</p>
<p>My code is as follows:</p>
<pre><code>import tensorflow as tf
import numpy as np
                                                                                                    
def mnist_dataset(batch_size):
    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()                                              
    x_train = x_train / np.float32(255)
    y_train = y_train.astype(np.int64)
    train_dataset = tf.data.Dataset.from_tensor_slices(
        (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)
    return train_dataset

batch_size = 64
dataset = mnist_dataset(batch_size)

model = tf.keras.Sequential([
    tf.keras.Input(shape=(28, 28)),
    tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
    tf.keras.layers.Conv2D(32, 3, activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
    metrics=['accuracy'])
                                                                                           
#tf.profiler.experimental.start('./logs/tb_log')                                                                        
tf.profiler.experimental.server.start(6009)

model.fit(dataset, epochs=10, steps_per_epoch=70)

tf.profiler.experimental.client.trace('grpc://localhost:6009', './logs/tbc_log', 20000)
#tf.profiler.experimental.stop()         
</code></pre>
<p>The code runs through the epochs, and then outputs</p>
<pre><code>2021-02-02 17:49:44.943933: I tensorflow/core/profiler/rpc/client/capture_profile.cc:198] Profiler delay_ms was 0, start_timestamp_ns set to 1612288184943887718 [2021-02-02T17:49:44.943887718+00:00]
Starting to trace for 20000 ms. Remaining attempt(s): 2
2021-02-02 17:49:44.944037: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:75] Deadline set to 2021-02-02T17:50:44.890124419+00:00 because max_session_duration_ms was 60000 and session_creation_timestamp_ns was 1612288184890124419 [2021-02-02T17:49:44.890124419+00:00]
2021-02-02 17:49:44.944197: I tensorflow/core/profiler/rpc/client/profiler_client.cc:113] Asynchronous gRPC Profile() to localhost:6009
2021-02-02 17:49:44.944316: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:96] Issued Profile gRPC to 1 clients
2021-02-02 17:49:44.944340: I tensorflow/core/profiler/rpc/client/profiler_client.cc:131] Waiting for completion.
2021-02-02 17:49:44.946274: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-02-02 17:49:44.947547: W tensorflow/core/profiler/lib/profiler_session.cc:144] Profiling is late (2021-02-02T17:49:44.946338176+00:00) for the scheduled start (2021-02-02T17:49:44.943887718+00:00) and will start immediately.
2021-02-02 17:49:44.947582: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-02-02 17:49:44.947660: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 2 GPUs
2021-02-02 17:49:44.949656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so.11.0
2021-02-02 17:50:08.435260: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-02-02 17:50:08.435591: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-02-02 17:50:08.635192: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. 
2021-02-02 17:50:08.648616: I tensorflow/core/profiler/rpc/profiler_service_impl.cc:67] Collecting XSpace to repository: ./logs/tbc_log/plugins/profile/2021_02_02_17_49_44/localhost_6009.xplane.pb
2021-02-02 17:50:08.650309: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-02-02 17:50:08.650676: W tensorflow/core/profiler/rpc/client/capture_profile.cc:133] No trace event is collected from localhost:6009
No trace event is collected. Automatically retrying.

2021-02-02 17:50:08.651046: I tensorflow/core/profiler/rpc/client/capture_profile.cc:198] Profiler delay_ms was 0, start_timestamp_ns set to 1612288208651017638 [2021-02-02T17:50:08.651017638+00:00]
Starting to trace for 20000 ms. Remaining attempt(s): 1
2021-02-02 17:50:08.651123: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:75] Deadline set to 2021-02-02T17:50:44.890124419+00:00 because max_session_duration_ms was 60000 and session_creation_timestamp_ns was 1612288184890124419 [2021-02-02T17:49:44.890124419+00:00]
2021-02-02 17:50:08.651274: I tensorflow/core/profiler/rpc/client/profiler_client.cc:113] Asynchronous gRPC Profile() to localhost:6009
2021-02-02 17:50:08.651391: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:96] Issued Profile gRPC to 1 clients
2021-02-02 17:50:08.651420: I tensorflow/core/profiler/rpc/client/profiler_client.cc:131] Waiting for completion.
2021-02-02 17:50:08.652492: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-02-02 17:50:08.652570: W tensorflow/core/profiler/lib/profiler_session.cc:144] Profiling is late (2021-02-02T17:50:08.652539729+00:00) for the scheduled start (2021-02-02T17:50:08.651017638+00:00) and will start immediately.
2021-02-02 17:50:08.652591: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-02-02 17:50:31.280828: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.
2021-02-02 17:50:31.281134: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
2021-02-02 17:50:31.510697: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. 
2021-02-02 17:50:31.515475: I tensorflow/core/profiler/rpc/profiler_service_impl.cc:67] Collecting XSpace to repository: ./logs/tbc_log/plugins/profile/2021_02_02_17_49_44/localhost_6009.xplane.pb
2021-02-02 17:50:31.518037: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
2021-02-02 17:50:31.518440: W tensorflow/core/profiler/rpc/client/capture_profile.cc:133] No trace event is collected from localhost:6009
No trace event is collected. Automatically retrying.

2021-02-02 17:50:31.518819: I tensorflow/core/profiler/rpc/client/capture_profile.cc:198] Profiler delay_ms was 0, start_timestamp_ns set to 1612288231518793164 [2021-02-02T17:50:31.518793164+00:00]
Starting to trace for 20000 ms. Remaining attempt(s): 0
2021-02-02 17:50:31.518889: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:75] Deadline set to 2021-02-02T17:50:44.890124419+00:00 because max_session_duration_ms was 60000 and session_creation_timestamp_ns was 1612288184890124419 [2021-02-02T17:49:44.890124419+00:00]
2021-02-02 17:50:31.519021: I tensorflow/core/profiler/rpc/client/profiler_client.cc:113] Asynchronous gRPC Profile() to localhost:6009
2021-02-02 17:50:31.519124: I tensorflow/core/profiler/rpc/client/remote_profiler_session_manager.cc:96] Issued Profile gRPC to 1 clients
2021-02-02 17:50:31.519147: I tensorflow/core/profiler/rpc/client/profiler_client.cc:131] Waiting for completion.
2021-02-02 17:50:31.520067: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.
2021-02-02 17:50:31.520136: W tensorflow/core/profiler/lib/profiler_session.cc:144] Profiling is late (2021-02-02T17:50:31.520095781+00:00) for the scheduled start (2021-02-02T17:50:31.518793164+00:00) and will start immediately.
2021-02-02 17:50:31.520152: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.
2021-02-02 17:50:44.891412: W tensorflow/core/profiler/rpc/client/profiler_client.cc:152] Deadline exceeded: Deadline Exceeded
2021-02-02 17:50:44.891501: W tensorflow/core/profiler/rpc/client/capture_profile.cc:133] No trace event is collected from localhost:6009
2021-02-02 17:50:44.891526: W tensorflow/core/profiler/rpc/client/capture_profile.cc:145] localhost:6009 returned Deadline exceeded: Deadline Exceeded
No trace event is collected after 3 attempt(s). Perhaps, you want to try again (with more attempts?).
Tip: increase number of attempts with --num_tracing_attempts.
2021-02-02 17:50:44.891848: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.
Traceback (most recent call last):
  File &quot;keras_singleworker_2.py&quot;, line 37, in &lt;module&gt;
2021-02-02 17:50:44.893228: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
    tf.profiler.experimental.client.trace('grpc://localhost:6009', './logs/tbc_log', 20000)
  File &quot;/fserver/jonathanb/miniconda3/envs/tf2.4/lib/python3.8/site-packages/tensorflow/python/profiler/profiler_client.py&quot;, line 131, in trace
    _pywrap_profiler.trace(
tensorflow.python.framework.errors_impl.UnavailableError: No trace event was collected because there were no responses from clients or the responses did not have trace data.
</code></pre>
<p>I've tried locating tf.profiler.experimental.server.start and tf.profiler.experimental.client.trace in other locations in the code, but with no success.</p>
",15131641.0,,,,,2021-02-03 15:19:57,TensorFlow profiler using tf.profiler.experimental.client.trace gives empty trace data,<python><tensorflow><profiler>,0,0,,,,CC BY-SA 4.0
63462087,1,,,2020-08-18 04:54:50,,6,3435,"<pre><code>tf.Tensor([3,2])
</code></pre>
<p>when I do this I got the error with <br/></p>
<pre><code>TypeError: __init__() missing 2 required positional arguments: 'value_index' and 'dtype'
</code></pre>
",14001694.0,,,,,2023-02-23 05:11:39,TypeError: __init__() missing 2 required positional arguments: 'value_index' and 'dtype',<tensorflow><tensorflow2.0>,1,0,,,,CC BY-SA 4.0
72790517,1,,,2022-06-28 16:59:08,,6,2552,"<p>I am trying to set up a object detection api in Google Colab.
I am trying to follow this tutorial and downloaded the models from github</p>
<p><a href=""https://medium.com/swlh/tensorflow-2-object-detection-api-with-google-colab-b2af171e81cc"" rel=""noreferrer"">Object Detection Tuturial</a></p>
<p><a href=""https://github.com/tensorflow/models"" rel=""noreferrer"">Github Model Garden</a></p>
<p>I downladed the recent centernet_resnet50_v1_fpn_512x512_coco17_tpu-8 from <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"" rel=""noreferrer"">Model Zoo</a></p>
<p>When I tried to start the training (step 15 from the tutorial)I got a error like this</p>
<p><code> File &quot;model_main_tf2.py&quot;, line 31, in &lt;module&gt; from object_detection import model_lib_v2 File &quot;/usr/local/lib/python3.7/dist-packages/object_detection-0.1-py3.7.egg/object_detection/model_lib_v2.py&quot;, line 31, in &lt;module&gt; from object_detection import model_lib File &quot;/usr/local/lib/python3.7/dist-packages/object_detection-0.1-py3.7.egg/object_detection/model_lib.py&quot;, line 35, in &lt;module&gt; from object_detection.builders import optimizer_builder File &quot;/usr/local/lib/python3.7/dist-packages/object_detection-0.1-py3.7.egg/object_detection/builders/optimizer_builder.py&quot;, line 25, in &lt;module&gt; from official.modeling.optimization import ema_optimizer File &quot;/content/gdrive/My Drive/TensorFlow/models/official/modeling/optimization/__init__.py&quot;, line 23, in &lt;module&gt; from official.modeling.optimization.optimizer_factory import OptimizerFactory File &quot;/content/gdrive/My Drive/TensorFlow/models/official/modeling/optimization/optimizer_factory.py&quot;, line 36, in &lt;module&gt; 'adamw_experimental': tf.keras.optimizers.experimental.AdamW, AttributeError: module 'keras.api._v2.keras.optimizers' has no attribute 'experimental'</code></p>
<p>When I read AttributeError: module 'keras.api._v2.keras.optimizers' has no attribute 'experimental, I fed it to google found two possible solution of using tf-nightly and solution from this <a href=""https://github.com/tensorflow/models/issues/10525"" rel=""noreferrer"">link</a> but still failed.</p>
<p>What could cause the error? If the tutorial is outdated, what adjustment shd I make to make it work?</p>
<p>Thank you so so much.</p>
",15674487.0,,,,,2022-12-02 09:52:11,AttributeError: module 'keras.api._v2.keras.optimizers' has no attribute 'experimental' problem,<python><tensorflow><google-colaboratory><object-detection><object-detection-api>,0,2,,,,CC BY-SA 4.0
63687113,1,63691577.0,,2020-09-01 11:51:36,,6,7512,"<p>I'm trying to install Tensorflow Object Detection API, following the steps at <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md"" rel=""noreferrer"">this link</a>, which is the official installation's documentation for Tensorflow 2.</p>
<pre><code>git clone https://github.com/tensorflow/models.git
&gt; everything is ok
cd models/research/
&gt; everything is ok
protoc object_detection/protos/*.proto --python_out=.
&gt; everything is ok
cp object_detection/packages/tf2/setup.py .
&gt; everything is ok
python -m pip install --use-feature=2020-resolver .

&gt; Usage:   
&gt;   /opt/anaconda3/envs/ml/bin/python -m pip install [options] &lt;requirement specifier&gt; [package-&gt; index-options] ...
&gt;   /opt/anaconda3/envs/ml/bin/python -m pip install [options] -r &lt;requirements file&gt; [package-index-options] ...
&gt;   /opt/anaconda3/envs/ml/bin/python -m pip install [options] [-e] &lt;vcs project url&gt; ...
&gt;   /opt/anaconda3/envs/ml/bin/python -m pip install [options] [-e] &lt;local project path&gt; ...
&gt;   /opt/anaconda3/envs/ml/bin/python -m pip install [options] &lt;archive url/path&gt; ...

&gt; no such option: --use-feature
</code></pre>
<p>Can someone help me understand why the installation stops as it does? I'm using macOS Mojave, Python 3.6 (on a conda virtual env), and Tensorflow 2.3.0.</p>
",13140508.0,,11680578.0,,2020-09-03 12:55:02,2022-11-25 09:34:02,no such option: --use-feature while installing tensorflow object detection api,<python><tensorflow><tensorflow2.0><object-detection>,4,8,,,,CC BY-SA 4.0
63563120,1,,,2020-08-24 14:33:00,,6,867,"<p><strong>System information</strong></p>
<ul>
<li>Windows10 64bit</li>
<li>Python version 3.8</li>
<li>Tryed to install in a virtualenv using conda and pip</li>
<li>CUDA/cuDNN version: 9.2</li>
<li>GPU model and memory: NVIDIA GeForce GTX 950M</li>
</ul>
<p><strong>Problem</strong>
I created a virtual environment and there I started to install necessary packages. When I tried to run a script that used tensorflow, it could not imported it.
After searching for this, I tried to import it in jupyter notebook, but it didn't work.
Then, I tried to add this to my code but it didn't work either:
tf.compat.v1.enable_eager_execution(
config=None, device_policy=None, execution_mode=None
)
I tried to install tensorflow in that environment using first conda and then pip.</p>
<p><strong>Sequence of commands / steps executed</strong></p>
<p><em><strong>-Running the script:</strong></em></p>
<pre><code>d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\utils\json.py:64: ResourceWarning: unclosed file &lt;_io.TextIOWrapper name='D:\\Software\\Anaconda_envs\\sweaver\\avgn_paper-vizmerge\\data\\processed\\sociable_weaver_damelio\\2020-08-21_09-35-13\\JSON\\2018-10-19_09-00-00-000001.JSON' mode='r' encoding='cp1252'&gt;
  return json.load(open(json_loc), object_pairs_hook=OrderedDict)
ResourceWarning: Enable tracemalloc to get the object allocation traceback
HBox(children=(FloatProgress(value=0.0, description='loading json', max=1.0, style=ProgressStyle(description_w…
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.

[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.0s finished
HBox(children=(FloatProgress(value=0.0, description='getting unique individuals', max=1.0, style=ProgressStyle…
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-43-e50f361ab3a6&gt; in &lt;module&gt;
      1 # create a dataset object
----&gt; 2 dataset = DataSet(DATASET_ID, hparams = hparams)

d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\dataset.py in __init__(self, DATASET_ID, hparams, default_rate, build_mel_matrix)
     43 
     44         if build_mel_matrix:
---&gt; 45             self.build_mel_matrix()
     46 
     47     def _get_wav_json_files(self):

d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\dataset.py in build_mel_matrix(self, rate)
     66         if rate is None:
     67             rate = self.sample_json[&quot;samplerate_hz&quot;]
---&gt; 68         self.mel_matrix = prepare_mel_matrix(self.hparams, rate)
     69 
     70     def _get_unique_individuals(self):

d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\signalprocessing\filtering.py in prepare_mel_matrix(hparams, rate, return_numpy, GPU_backend)
     71             os.environ[&quot;CUDA_DEVICE_ORDER&quot;] = &quot;PCI_BUS_ID&quot;  # see issue #152
     72             os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;&quot;
---&gt; 73         import tensorflow as tf
     74 
     75     tf.compat.v1.enable_eager_execution(

ModuleNotFoundError: No module named 'tensorflow'
</code></pre>
<p>-Import it in jupyter notebook</p>
<pre><code>import sys
!conda install --yes --prefix {sys.prefix} tensorflow

UnsatisfiableError: The following specifications were found to be incompatible with the existing python installation in your environment:
Specifications:
- tensorflow -&gt; python[version'3.5.*|3.6.*|3.7.*'
Your python: python=3.8

The following specifications were found to be incompatible with your CUDA driver:
- feature:/win-64::__cuda==9.2=0
Your installed CUDA driver is: 9.2
</code></pre>
<p><em><strong>-When I tried to install it through conda:</strong></em></p>
<pre><code>(D:\Software\Anaconda_envs\sweaver) D:\Software\Anaconda_envs\sweaver&gt;conda install tensorflow
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: -
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Examining @/win-64::__cuda==9.2=0:  67%|████████████████████████████████▋                | 2/3 [00:00&lt;00:00, 18.17it/s]|Examining conflict for __cuda:  67%|███████████████████████████████████▎                 | 2/3 [00:00&lt;00:00,  3.77it/s]|failed

UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:

  - tensorflow -&gt; python[version='3.5.*|3.6.*|3.7.*']

Your python: python=3.8

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

The following specifications were found to be incompatible with your system:

  - feature:/win-64::__cuda==9.2=0
  - feature:|@/win-64::__cuda==9.2=0

Your installed version is: 9.2
</code></pre>
<p><em><strong>-When I tried to install it using pip:</strong></em></p>
<pre><code>(D:\Software\Anaconda_envs\sweaver) D:\Software\Anaconda_envs\sweaver&gt;pip install tensorflow
Collecting tensorflow
  Using cached tensorflow-2.3.0-cp38-cp38-win_amd64.whl (342.5 MB)
Requirement already satisfied: wrapt&gt;=1.11.1 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (1.11.2)
Collecting keras-preprocessing&lt;1.2,&gt;=1.1.1
  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
Collecting tensorboard&lt;3,&gt;=2.3.0
  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)
Requirement already satisfied: grpcio&gt;=1.8.6 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (1.31.0)
Collecting numpy&lt;1.19.0,&gt;=1.16.0
  Using cached numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)
Collecting opt-einsum&gt;=2.3.2
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Requirement already satisfied: absl-py&gt;=0.7.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (0.10.0)
Requirement already satisfied: wheel&gt;=0.26 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (0.34.2)
Collecting scipy==1.4.1
  Using cached scipy-1.4.1-cp38-cp38-win_amd64.whl (31.0 MB)
Collecting astunparse==1.6.3
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Requirement already satisfied: h5py&lt;2.11.0,&gt;=2.10.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (2.10.0)
Collecting protobuf&gt;=3.9.2
  Using cached protobuf-3.13.0-py2.py3-none-any.whl (438 kB)
Collecting google-pasta&gt;=0.1.8
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Requirement already satisfied: tensorflow-estimator&lt;2.4.0,&gt;=2.3.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (2.3.0)
Requirement already satisfied: gast==0.3.3 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (0.3.3)
Requirement already satisfied: six&gt;=1.12.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (1.15.0)
Processing c:\users\bf\appdata\local\pip\cache\wheels\a0\16\9c\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\termcolor-1.1.0-py3-none-any.whl
Requirement already satisfied: werkzeug&gt;=0.11.15 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (1.0.1)
Collecting markdown&gt;=2.6.8
  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)
Collecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1
  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)
Requirement already satisfied: setuptools&gt;=41.0.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (49.6.0.post20200814)
Collecting tensorboard-plugin-wit&gt;=1.6.0
  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)
Requirement already satisfied: requests&lt;3,&gt;=2.21.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (2.24.0)
Requirement already satisfied: google-auth&lt;2,&gt;=1.6.3 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (1.20.1)
Collecting requests-oauthlib&gt;=0.7.0
  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (1.25.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (2020.6.20)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (3.0.4)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (2.10)
Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in d:\software\anaconda_envs\sweaver\lib\site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (0.2.8)
Requirement already satisfied: rsa&lt;5,&gt;=3.1.4; python_version &gt;= &quot;3.5&quot; in d:\software\anaconda_envs\sweaver\lib\site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (4.5)
Requirement already satisfied: cachetools&lt;5.0,&gt;=2.0.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (4.1.1)
Collecting oauthlib&gt;=3.0.0
  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)
Requirement already satisfied: pyasn1&lt;0.5.0,&gt;=0.4.6 in d:\software\anaconda_envs\sweaver\lib\site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&lt;2,&gt;=1.6.3-&gt;tensorboard&lt;3,&gt;=2.3.0-&gt;tensorflow) (0.4.8)
ERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new
Traceback (most recent call last):
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 3021, in _dep_map
    return self.__dep_map
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 2815, in __getattr__
    raise AttributeError(attr)
AttributeError: _DistInfoDistribution__dep_map

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 3012, in _parsed_pkg_info
    return self._pkg_info
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 2815, in __getattr__
    raise AttributeError(attr)
AttributeError: _pkg_info

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_internal\commands\install.py&quot;, line 535, in _determine_conflicts
    return check_install_conflicts(to_install)
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_internal\operations\check.py&quot;, line 108, in check_install_conflicts
    package_set, _ = create_package_set_from_installed()
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_internal\operations\check.py&quot;, line 50, in create_package_set_from_installed
    package_set[name] = PackageDetails(dist.version, dist.requires())
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 2736, in requires
    dm = self._dep_map
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 3023, in _dep_map
    self.__dep_map = self._compute_dependencies()
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 3032, in _compute_dependencies
    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 3014, in _parsed_pkg_info
    metadata = self.get_metadata(self.PKG_INFO)
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 1420, in get_metadata
    value = self._get(path)
  File &quot;D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py&quot;, line 1616, in _get
    with open(path, 'rb') as stream:
FileNotFoundError: [Errno 2] No such file or directory: 'd:\\software\\anaconda_envs\\sweaver\\lib\\site-packages\\numpy-1.19.1.dist-info\\METADATA'
Installing collected packages: numpy, keras-preprocessing, protobuf, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, opt-einsum, scipy, astunparse, google-pasta, termcolor, tensorflow
  Attempting uninstall: numpy
    Found existing installation: numpy 1.19.1
ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'd:\\software\\anaconda_envs\\sweaver\\lib\\site-packages\\numpy-1.19.1.dist-info\\RECORD'
</code></pre>
<p>Maybe it's a problem of compatibility between tensorflow and numpy? But the file 'd:\software\anaconda_envs\sweaver\lib\site-packages\numpy-1.19.1.dist-info\RECORD' actually does not exist.
I searched in my packages and there is a folder of numpy, numpy-1.18.5.dist-info and numpy-1.19.1.dist-info, but inside the last one, there are only two files: LICENSES_bundled.txt and REQUESTED.</p>
<p>Then, I deleted the folder numpy-1.19.1dist-info and intalled tensorflow through the command line using --user to allow the installation, but it got installed in other place.
So, I use:</p>
<pre><code>import sys
sys.path.append()
</code></pre>
<p>in every py file of tensorflow but still didn't work. Also one of the files (pywrap_tensorflow_internal.py) was empty, showing that there was some problem in installation.</p>
<p>Hope you can help me.</p>
",14157770.0,,,,,2020-08-24 14:44:18,Can't install tensorflow properly,<python><tensorflow><installation><pip><virtualenv>,1,0,,,,CC BY-SA 4.0
63145532,1,63148748.0,,2020-07-29 02:39:38,,6,9594,"<p>I have a ragged tensor, and upon trying to create a model, and use model.fit(), I get an error:
<code>TypeError: Failed to convert object of type &lt;class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'&gt; to Tensor. Contents: tf.RaggedTensor(values=Tensor(&quot;Cast_1:0&quot;, shape=(None,), dtype=float32), row_splits=Tensor(&quot;RaggedFromVariant_1/RaggedTensorFromVariant:0&quot;, shape=(None,), dtype=int64)). Consider casting elements to a supported type.</code></p>
<p>Is this an issue with the shape of my data? Perhaps the data altogether? Maybe I need to use a sparse or dense tensor instead? Here is my full traceback error:</p>
<p><a href=""https://i.stack.imgur.com/nBvjp.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nBvjp.png"" alt=""Full Traceback"" /></a></p>
<p>As well as my data: <a href=""https://i.stack.imgur.com/1kqIa.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1kqIa.png"" alt=""XTrain dataset"" /></a></p>
",13521785.0,,,,,2020-07-29 07:45:44,How to Convert Ragged Tensor to Tensor in Python?,<python><arrays><tensorflow><tensor>,1,1,,,,CC BY-SA 4.0
73691371,1,,,2022-09-12 15:07:38,,6,186,"<p>tl-dr version: why do the first 2 action/observations i take not line up with my first two objects in my replay buffer?</p>
<p>Do tf-agent replay buffers automatically shuffle data around?</p>
<p>by adding these prints im able to see what my first 2 steps look like</p>
<pre><code>print(&quot;just addding this as traj num = &quot;+str(num))
print(&quot; next time step  = &quot;+str(next_time_step))
replay_buffer.add_batch(traj)
</code></pre>
<p>this produces</p>
<pre><code>just addding this as traj num = 0
 next time step  = TimeStep(
{'discount': &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)&gt;,
 'observation': &lt;tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 1., 0.]]]], dtype=float32)&gt;,
 'reward': &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.05], dtype=float32)&gt;,
 'step_type': &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])&gt;})
just addding this as traj num = 1
 next time step  = TimeStep(
{'discount': &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)&gt;,
 'observation': &lt;tf.Tensor: shape=(1, 1, 5, 5), dtype=float32, numpy=
array([[[[0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 0., 0., 0.],
         [0., 0., 1., 1., 0.]]]], dtype=float32)&gt;,
 'reward': &lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([-0.05], dtype=float32)&gt;,
 'step_type': &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([2])&gt;})
</code></pre>
<p>a few lines later when i have my data set as an iterator, i once again explicitly print the first data points. (i have set my batch size to 3 so we should get the first 3 results, we seem to get 3 copies  of the first result)</p>
<pre><code>Trajectory(
{'action': &lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=
array([[3],
       [0],
       [0]])&gt;,
 'discount': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.],
       [0.],
       [0.]], dtype=float32)&gt;,
 'next_step_type': &lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=
array([[2],
       [2],
       [2]])&gt;,
 'observation': &lt;tf.Tensor: shape=(3, 1, 1, 5, 5), dtype=float32, numpy=
array([[[[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]]],



       [[[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]]],



       [[[[0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.],
          [0., 0., 0., 0., 0.]]]]], dtype=float32)&gt;,
 'policy_info': (),
 'reward': &lt;tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[-1.  ],
       [-0.05],
       [ 1.  ]], dtype=float32)&gt;,
 'step_type': &lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=
array([[0],
       [0],
       [0]])&gt;})
### experience 1 above
### experience 1 above
### experience 1 above
</code></pre>
<p>the experiences are blank. if we continue to iterate through we continue to get the same results.</p>
<p>what is wrong with this? How can i keep the data in the replay buffer in the same order i collected it?</p>
<p>########################</p>
<p>reproducable example below (much of the game has been gutted to make it smaller)</p>
<pre><code>import tensorflow as tf
from tf_agents.networks import q_network
from tf_agents.agents.dqn import dqn_agent
import tf_agents
import tf_agents.environments.py_environment as PyEnvironment 
from tf_agents.trajectories import time_step as ts
import numpy as np
import keras 
import tf_agents.policies.random_tf_policy as random_tf_policy
import tf_agents.environments as tf_py_environment
import numpy as np
import random
import copy

class simple_slots():
    def __init__(self, x, y):
        self.x_rows = x
        self.y_rows = y
        self.slots = []
        for i in range(x):
            ys=[]
            for j in range(y):
                ys.append(0)
            self.slots.append(ys)
    def new_game(self):
        for xs in self.slots:
            for slot in xs:
                slot = 0
        
    def find_lowest_slot(self , x):
        lowest_y = 0
        best_slot = &quot;none&quot;
        for slot, y_ind in zip(reversed(self.slots[x]), reversed(range(len(self.slots[x])))):
            if slot == 0:
                if y_ind &gt; lowest_y:
                    lowest_y = y_ind
                    best_slot = slot
        if best_slot != &quot;none&quot;:
            return lowest_y
        return False
    
    def ml_plays_turn(self, action):
        y = self.find_lowest_slot(action)
        self.slots[action][y] = 1
    
    def script_plays_turn(self, action = 5):
        y = self.find_lowest_slot(action)
        self.slots[action][y] = 2
        
    def arbirtrarily_decide_if_game_over(self):
        if random.random() &lt; 0.2:
            reward = 1
        elif  random.random() &lt; 0.5:
            reward = -1
        else:
            reward = 0
        return reward

class Con4Env(PyEnvironment.PyEnvironment):
    
    def __init__(self, game):
        self.game = game
        self._action_spec = tf_agents.specs.BoundedArraySpec(
            shape=(), dtype=np.int32, minimum=0, maximum=game.x_rows-1 , name='action')
        self._observation_spec = tf_agents.specs.BoundedArraySpec(
            shape=(1, game.x_rows,game.y_rows), dtype=np.float32, minimum=0, name='observation')
        self._state = np.zeros((game.x_rows,game.y_rows) , dtype=np.float32)
        self._time_step_spec = ts.time_step_spec(self._observation_spec)
        self._episode_ended = False
        
    def action_spec(self):
        return self._action_spec

    def observation_spec(self):
        return self._observation_spec

    def _reset(self):
        self._state = np.zeros((game.x_rows,game.y_rows) , dtype=np.float32)
        self._episode_ended = False
        return ts.restart(np.array([self._state], dtype=np.float32))
    
    def copy_gameboard_to_state(self):
        for ys, yind in zip(self.game.slots, range(len(self.game.slots))):
            for x , xind in zip(ys, range(len(ys))):
                self._state[xind][yind] = x

    def _step(self, action):
        if self._episode_ended:
            return self.reset()
                               
        reward = self.game.arbirtrarily_decide_if_game_over()
        if reward != 0:
            self._episode_ended = True
            
            
        elif self.game.ml_plays_turn(action):
            self.game.script_plays_turn()
            self.copy_gameboard_to_state()
        else:
            reward = -0.05 #### column full,     call it draw 
            self._episode_ended = True
                               
        if self._episode_ended: #### if game was ended last round the reward then we go in here 1 last time                             
            self.game.new_game()
            self.copy_gameboard_to_state()
            return ts.termination(np.array([self._state], dtype=np.float32), reward)
        else:
            self.copy_gameboard_to_state()
            return ts.transition(np.array([self._state], dtype=np.float32), reward=0.0, discount=0.0)
            
game = simple_slots(5,5)
the_env = Con4Env(game)
eval_env = Con4Env(game)
the_env = tf_py_environment.TFPyEnvironment(the_env)
eval_env = tf_py_environment.TFPyEnvironment(eval_env)

#    create time_step_spec
from tf_agents.utils import common
step_type_spec = tf.TensorSpec(shape=(), dtype=tf.dtypes.int32, name='step_type') # just declare a time step spec 
reward_spec= tf.TensorSpec(shape=(), dtype=tf.dtypes.float32, name='reward_spec')
discount_spec= tf.TensorSpec(shape=(), dtype=tf.dtypes.float32, name='discount_spec')
time_step_spec = tf_agents.trajectories.TimeStep( step_type_spec ,reward_spec, discount_spec, the_env.observation_spec() )

#####################################################################

q_net = tf_agents.networks.q_network.QNetwork(
    input_tensor_spec = the_env.observation_spec(),
    action_spec = the_env.action_spec(),
    preprocessing_layers=None,
    preprocessing_combiner=None,
    conv_layer_params=None,
    fc_layer_params=(75, 40),
    dropout_layer_params=None,
    activation_fn=tf.keras.activations.relu,
    kernel_initializer=None,
    batch_squash=True,
    dtype=tf.float32,
    q_layer_activation_fn=None,
    name='QNetwork'
)

train_step_counter = tf.Variable(0)
gamma = 0.99
min_q_value = -20 
max_q_value = 20  
n_step_update = 2 
agent = dqn_agent.DqnAgent( 
    time_step_spec , 
    the_env.action_spec() , 
    q_net, 
    optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.000001),
    n_step_update=n_step_update,
    td_errors_loss_fn=common.element_wise_squared_loss,
    gamma=gamma,
    train_step_counter=train_step_counter
)

random_policy = random_tf_policy.RandomTFPolicy(time_step_spec, the_env.action_spec())


# data collector 

data_spec=agent.collect_data_spec
print(data_spec)
from tf_agents.utils import common
import copy
replay_buffer_capacity = 999
initial_collect_steps = 50
batch_size = 3
n_step_update = 1
num_parallel_calls = 2
replay_buffer = tf_agents.replay_buffers.TFUniformReplayBuffer(
    data_spec=agent.collect_data_spec,
    batch_size=the_env.batch_size,
    max_length=replay_buffer_capacity
)

def collect_step(environment, policy, num):
    if environment.current_time_step().is_last():
        time_step = environment.reset()
    else:
        time_step = environment.current_time_step()
    action_step = policy.action(time_step)
    next_time_step = environment.step(action_step.action)
    traj = tf_agents.trajectories.from_transition(time_step, action_step, next_time_step)
    print(&quot;just addding this as traj num = &quot;+str(num))
    print(&quot; next time step  = &quot;+str(next_time_step))
    replay_buffer.add_batch(traj)

nom = 0 
for _ in range(initial_collect_steps):
    collect_step(the_env, random_policy , nom)
    nom+=1

#    after each step check to see if data is in order 

dataset = replay_buffer.as_dataset(
num_parallel_calls=num_parallel_calls, 
sample_batch_size=batch_size,
num_steps=n_step_update).prefetch(9)

iterator = iter(dataset)
experience, unused_info = next(iterator)
print(experience)#### why is this thing out of order
for i in range(3):
    print(&quot;### experience 1 above&quot;)
experience, unused_info = next(iterator)
print(experience)#### why is this thing out of order
for i in range(3):
    print(&quot;### experience 2 above&quot;)
</code></pre>
",17560920.0,,17560920.0,,2022-09-14 16:00:19,2023-03-31 20:38:36,why is data from tf-agents buffer in random order,<python><tensorflow><buffer><tf-agent>,1,0,0.0,,,CC BY-SA 4.0
73691788,1,,,2022-09-12 15:39:09,,6,168,"<p>I want to use the tensorflow dataset saving and loading functions but I am not sure to understand the sharding method.</p>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#save"" rel=""noreferrer"">documentation</a> indicates :</p>
<blockquote>
<p>The saved dataset is saved in multiple file &quot;shards&quot;. By default, the dataset output is divided to shards in a round-robin fashion but custom sharding can be specified via the shard_func function.</p>
</blockquote>
<p>But when I save a dataset through the save function, it seems that only one huge shard is generated.</p>
<pre><code>import tempfile
import tensorflow as tf

path = os.path.join(tempfile.gettempdir(), &quot;saved_data&quot;)
dataset = tf.data.Dataset.range(10**8)

dataset.save(path)
</code></pre>
<p><a href=""https://i.stack.imgur.com/K2Qkh.png"" rel=""noreferrer"">generated dataset screenshot</a></p>
<p>Am I missing something ?</p>
<p>I use Tensorflow 2.10.0 and Python 3.9.7</p>
",5130199.0,,286934.0,,2022-09-12 20:04:26,2022-09-12 20:04:26,Tensorflow dataset not saved in multiple shards,<python><tensorflow><tensorflow-datasets>,0,2,,,,CC BY-SA 4.0
63388030,1,64062465.0,,2020-08-13 04:13:41,,6,2132,"<p>I am using the following lines of codes to visualise the gradients of an ANN model using tensorboard</p>
<pre><code>  tensorboard_callback = tf.compat.v1.keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=1, write_graph = True, write_grads =True, write_images = False)

tensorboard_callback .set_model(model)


%tensorboard --logdir ./Graph
</code></pre>
<p>I received a warning message saying &quot;WARNING:tensorflow:<code>write_grads</code> will be ignored in TensorFlow 2.0 for the <code>TensorBoard</code> Callback.&quot;</p>
<p>I get the tensorboard output, but without gradients.</p>
<p><a href=""https://i.stack.imgur.com/1SpS6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/1SpS6.png"" alt=""enter image description here"" /></a></p>
<p>What could be the possible reason?</p>
<p>(Note: I use 2.3.0 tensorflow version)</p>
<p>Thank you.</p>
",12508883.0,,12508883.0,,2020-08-13 04:27:06,2020-09-25 10:37:31,WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback,<python><tensorflow><neural-network><tensorboard><tf.keras>,1,0,,,,CC BY-SA 4.0
73569804,1,73583522.0,,2022-09-01 13:02:04,,6,165,"<p>I have a dataset like this:</p>
<pre><code>a = tf.data.Dataset.range(1, 16)
b = tf.data.Dataset.range(16, 32)
zipped = tf.data.Dataset.zip((a, b))
list(zipped.as_numpy_iterator())

# output: 
[(0, 16),
 (1, 17),
 (2, 18),
 (3, 19),
 (4, 20),
 (5, 21),
 (6, 22),
 (7, 23),
 (8, 24),
 (9, 25),
 (10, 26),
 (11, 27),
 (12, 28),
 (13, 29),
 (14, 30),
 (15, 31)]
</code></pre>
<p>When I apply <code>batch(4)</code> to it, the expected result is an array of batches, where each batch contains four tuples:</p>
<pre><code>[[(0, 16), (1, 17), (2, 18), (3, 19)],
 [(4, 20), (5, 21), (6, 22), (7, 23)],
 [(9, 24), (10, 25), (10, 26), (11, 27)],
 [(12, 28), (13, 29), (14, 30), (15, 31)]]
</code></pre>
<p>But this is what I receive instead:</p>
<pre><code>batched = zipped.batch(4)
list(batched.as_numpy_iterator())

# Output:
[(array([0, 1, 2, 3]), array([16, 17, 18, 19])), 
 (array([4, 5, 6, 7]), array([20, 21, 22, 23])), 
 (array([ 8,  9, 10, 11]), array([24, 25, 26, 27])), 
 (array([12, 13, 14, 15]), array([28, 29, 30, 31]))]
</code></pre>
<p>I'm following this <a href=""https://www.youtube.com/watch?v=N_W4EYtsa10&amp;t=5591s"" rel=""nofollow noreferrer"">tutorial</a>, he does the same steps but gets the correct output somehow.</p>
<hr />
<p>Update: according to the documentation this is the intended behavior:</p>
<blockquote>
<p>The components of the resulting element will have an additional <strong>outer</strong> dimension, which will be batch_size</p>
</blockquote>
<p>But it doesn't make any sense. To my understanding, dataset is a list of pieces of data. It doesn't matter the shape of those pieces of data, when we are batching it we are combining the elements [whatever their shape is] into batches, therefore it should always insert the new dimention to the second position (<code>(length, a, b, c)</code> -&gt; <code>(length', batch_size, a, b, c)</code>).</p>
<p>So my questions are: I wonder what is the purpose of <code>batch()</code> being implemented this way? And what is the alternative that does what I described?</p>
",12694438.0,,9657861.0,,2022-09-02 13:58:35,2022-09-02 14:53:10,Dataset.batch doesn't work as expected with a zipped dataset,<python><tensorflow><tensorflow-datasets>,2,0,0.0,,,CC BY-SA 4.0
63336300,1,,,2020-08-10 08:02:18,,6,10366,"<p>My Tensorflow model makes heavy use of data preprocessing that should be done on the CPU to leave the GPU open for training.</p>
<pre><code>top - 09:57:54 up 16:23,  1 user,  load average: 3,67, 1,57, 0,67
Tasks: 400 total,   1 running, 399 sleeping,   0 stopped,   0 zombie
%Cpu(s): 19,1 us,  2,8 sy,  0,0 ni, 78,1 id,  0,0 wa,  0,0 hi,  0,0 si,  0,0 st
MiB Mem :  32049,7 total,    314,6 free,   5162,9 used,  26572,2 buff/cache
MiB Swap:   6779,0 total,   6556,0 free,    223,0 used.  25716,1 avail Mem 

    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                                                                
  17604 joro      20   0   22,1g   2,3g 704896 S 331,2   7,2   4:39.33 python  
</code></pre>
<p>This is what top shows me. I would like to make this python process use at least 90% of available CPU across all cores. How can this be achieved?</p>
<p>GPU utilization is better, around 90%. Even though I don't know why it is not at 100%</p>
<pre><code>Mon Aug 10 10:00:13 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  Off  | 00000000:01:00.0  On |                  N/A |
| 35%   41C    P2    90W / 260W |  10515MiB / 11016MiB |     11%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1128      G   /usr/lib/xorg/Xorg                           102MiB |
|    0      1648      G   /usr/lib/xorg/Xorg                           380MiB |
|    0      1848      G   /usr/bin/gnome-shell                         279MiB |
|    0     10633      G   ...uest-channel-token=1206236727             266MiB |
|    0     13794      G   /usr/lib/firefox/firefox                       6MiB |
|    0     17604      C   python                                      9457MiB |
+-----------------------------------------------------------------------------+

</code></pre>
<p>All i found was a solution for tensorflow 1.0:</p>
<pre><code>sess = tf.Session(config=tf.ConfigProto(
  intra_op_parallelism_threads=NUM_THREADS))
</code></pre>
<p>I have an Intel 9900k and a RTX 2080 Ti and use Ubuntu 20.04</p>
<p>E: When I add the following code on top, it uses 1 core 100%</p>
<pre><code>tf.config.threading.set_intra_op_parallelism_threads(1)
tf.config.threading.set_inter_op_parallelism_threads(1)
</code></pre>
<p>But increasing this number to 16 again only utilizes all cores ~30%</p>
",9280994.0,,9280994.0,,2020-08-10 08:22:42,2021-08-28 19:13:46,Tensorflow 2.0 utilize all CPU cores 100%,<tensorflow><cpu><preprocessor>,2,5,0.0,,,CC BY-SA 4.0
68545187,1,,,2021-07-27 12:51:36,,6,494,"<p>Is there any way to detect the channels first or last format for TF saved model loaded as <code>model=tf.saved_model.load(path)</code>?</p>
<p>In Keras and can go over <code>model.layers</code> and check for a layer l <code>l.data_format == 'channels_last'</code></p>
<p>Is there something like this for TF saved model? I can't find any suitable documentation of TF model details - everything goes back to Keras.</p>
",66522.0,,,,,2023-01-17 21:01:19,Detect channels first/last of tensorflow saved model?,<tensorflow><inference>,3,7,,,,CC BY-SA 4.0
64735907,1,,,2020-11-08 07:52:15,,6,410,"<p>I'm running a job on AI Platform and it's running for over an hour with no progress, no results, no logs(only few logs showing it's running)</p>
<p>Here is the region, machine type, gpus I was using:</p>
<pre><code>  &quot;region&quot;: &quot;us-central1&quot;,
  &quot;runtimeVersion&quot;: &quot;2.2&quot;,
  &quot;pythonVersion&quot;: &quot;3.7&quot;,
  &quot;masterConfig&quot;: {
    &quot;acceleratorConfig&quot;: {
      &quot;count&quot;: &quot;8&quot;,
      &quot;type&quot;: &quot;NVIDIA_TESLA_K80&quot;
    }
  }
</code></pre>
<p><strong>the AI Platform job</strong>
<img src=""https://i.stack.imgur.com/ZcVxg.png"" alt=""the AI Platform job"" /></p>
<p><strong>only few logs for this job</strong>
<img src=""https://i.stack.imgur.com/Udxfd.png"" alt=""only few logs for this job"" /></p>
<p>The model I'm training is big and uses a lot of memory. The job is just hanging there without any progress, logs or errors. But I notice that it's consumed 12.81 ML units on GCP. Normally, if the GPU is running out of memory, it would throw an &quot;OOM/resourceExhausted error&quot;. Without logs, I have no idea what's wrong there.</p>
<p>I ran a different job with smaller dimension of the input and it completed successfully in 12 mins:</p>
<p><strong>successed job</strong>
<img src=""https://i.stack.imgur.com/9DAl4.png"" alt=""successed job"" /></p>
<p>Also, I use tf.MirroredStrategy for the training process so that it can distribute across GPUs.</p>
<p>Any thoughts on this?</p>
",14599176.0,,12537691.0,,2020-11-10 10:10:54,2020-11-10 10:10:54,GCP AI Platform job is stuck,<tensorflow><google-cloud-platform><tensorflow2.0><gcp-ai-platform-training><google-ai-platform>,0,5,0.0,,,CC BY-SA 4.0
64754955,1,,,2020-11-09 15:57:22,,6,1353,"<p>I am in the process of creating a recommender system that suggests 20 most suitable songs to a user. I've trained my model, I'm ready to recommend songs for a given playlist! However, one issue that I encountered is that I need the embedding of that new playlist in order to find the closest relevant playlists in that embedding space using kmeans.</p>
<p>To recommend songs, I first cluster the learned embeddings for all of the training playlists, and then select &quot;neighbor&quot; playlists for my given test playlist as all of the other playlists in that same cluster. I then take all of the tracks from these playlists and feed the test playlist embedding and these &quot;neighboring&quot; tracks into my model for prediction. This ranks the &quot;neighboring&quot; tracks by how likely they are (under my model) to occur next in the given test playlist.</p>
<pre><code>desired_user_id = 123
model_path = Path(PATH, 'model.h5')
print('using model: %s' % model_path)
model =keras.models.load_model(model_path)
print('Loaded model!')

mlp_user_embedding_weights = (next(iter(filter(lambda x: x.name == 'mlp_user_embedding', model.layers))).get_weights())

# get the latent embedding for your desired user
user_latent_matrix = mlp_user_embedding_weights[0]
one_user_vector = user_latent_matrix[desired_user_id,:]
one_user_vector = np.reshape(one_user_vector, (1,32))

print('\nPerforming kmeans to find the nearest users/playlists...')
# get 100 similar users
kmeans = KMeans(n_clusters=100, random_state=0, verbose=0).fit(user_latent_matrix)
desired_user_label = kmeans.predict(one_user_vector)
user_label = kmeans.labels_
neighbors = []
for user_id, user_label in enumerate(user_label):
    if user_label == desired_user_label:
        neighbors.append(user_id)
print('Found {0} neighbor users/playlists.'.format(len(neighbors)))

tracks = []
for user_id in neighbors:
    tracks += list(df[df['pid'] == int(user_id)]['trackindex'])
print('Found {0} neighbor tracks from these users.'.format(len(tracks))) 

users = np.full(len(tracks), desired_user_id, dtype='int32')
items = np.array(tracks, dtype='int32')

# and predict tracks for my user
results = model.predict([users,items],batch_size=100, verbose=0) 
results = results.tolist()
print('Ranked the tracks!')

results_df = pd.DataFrame(np.nan, index=range(len(results)), columns=['probability','track_name', 'track artist'])
print(results_df.shape)

# loop through and get the probability (of being in the playlist according to my model), the track, and the track's artist 
for i, prob in enumerate(results):
    results_df.loc[i] = [prob[0], df[df['trackindex'] == i].iloc[0]['track_name'], df[df['trackindex'] == i].iloc[0]['artist_name']]
results_df = results_df.sort_values(by=['probability'], ascending=False)

results_df.head(20)
</code></pre>
<p>Instead of this code above, I would like to use this <a href=""https://www.tensorflow.org/recommenders/examples/basic_retrieval#building_a_candidate_ann_index"" rel=""nofollow noreferrer"">https://www.tensorflow.org/recommenders/examples/basic_retrieval#building_a_candidate_ann_index</a> or the official GitHub repository from Spotify <a href=""https://github.com/spotify/annoy"" rel=""nofollow noreferrer"">https://github.com/spotify/annoy</a>.
Unfortunately I don't know exactly how to use this so that the new program gives me the 20 most popular tracks for a user.
How do I have to change this?</p>
<hr />
<p><strong>Edit</strong>:</p>
<p>What I tried:</p>
<pre><code>from annoy import AnnoyIndex
import random
desired_user_id = 123
model_path = Path(PATH, 'model.h5')
print('using model: %s' % model_path)
model =keras.models.load_model(model_path)
print('Loaded model!')
    
mlp_user_embedding_weights = (next(iter(filter(lambda x: x.name == 'mlp_user_embedding', model.layers))).get_weights())
    
# get the latent embedding for your desired user
user_latent_matrix = mlp_user_embedding_weights[0]
one_user_vector = user_latent_matrix[desired_user_id,:]
one_user_vector = np.reshape(one_user_vector, (1,32))

t = AnnoyIndex(desired_user_id , one_user_vector)  #Length of item vector that will be indexed
for i in range(1000):
    v = [random.gauss(0, 1) for z in range(f)]
    t.add_item(i, v)

t.build(10) # 10 trees
t.save('test.ann')

u = AnnoyIndex(desired_user_id , one_user_vector)
u.load('test.ann') # super fast, will just mmap the file
print(u.get_nns_by_item(0, 1000)) # will find the 1000 nearest neighbors
# Now how to I get the probability and the values? 
</code></pre>
",,user14606893,,user14606893,2020-11-13 13:27:47,2020-11-18 11:18:21,Find nearest neighbors change algorithm,<python><algorithm><tensorflow><k-means>,2,5,0.0,,,CC BY-SA 4.0
67439611,1,67557243.0,,2021-05-07 17:49:49,,6,560,"<p>I have an unbatched <code>tensorflow</code> dataset that looks like this:</p>
<pre class=""lang-py prettyprint-override""><code>ds = ...
for record in ds.take(3):
    print('data shape={}'.format(record['data'].shape))

-&gt; data shape=(512, 512, 87)
-&gt; data shape=(512, 512, 277)
-&gt; data shape=(512, 512, 133)
</code></pre>
<p>I want to feed the data to my network in chunks of depth 5. In the example above, the tensor of shape (512, 512, 87) would be divided into 17 tensors of shape (512, 512, 5). The final 2 rows of the matrix (<code>tensor[:,:, 85:87]</code>) should be discarded.</p>
<p>For example:</p>
<pre class=""lang-py prettyprint-override""><code>chunked_ds = ...
for record in chunked_ds.take(1):
    print('chunked data shape={}'.format(record['data'].shape))

-&gt; chunked data shape=(512, 512, 5)
</code></pre>
<p>How can I get from <code>ds</code> to <code>chunked_ds</code>? <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"" rel=""noreferrer""><code>tf.data.Dataset.window()</code></a> looks like what I need but I cannot get this working.</p>
",6835665.0,,9215780.0,,2021-05-15 12:30:35,2021-05-16 13:27:31,Chunk tensorflow dataset records into multiple records,<tensorflow><conv-neural-network><tensorflow-datasets><tfrecord>,2,1,0.0,,,CC BY-SA 4.0
67081266,1,,,2021-04-13 19:29:44,,6,3246,"<p>I'm trying to install additional libraries for ML.</p>
<p>I'm using a file called tools.yml. Which is built like this</p>
<pre><code> # conda create --name tensorflow-cpu python=3.8
 # conda install jupyter
 # conda install nb_conda
 # conda install -c anaconda tensorflow
 # conda env update --file tools.yml
dependencies:
    - jupyter
    - scikit-learn
    - scipy
    - pandas
    - pandas-datareader
    - matplotlib
    - pillow
    - tqdm
    - requests
    - h5py
    - pyyaml
    - flask
    - boto3
    - pip:
        - bayesian-optimization
        - gym
        - kaggle
        
</code></pre>
<p>But when I run the conda environment update I get this warning</p>
<blockquote>
<p><strong>Warning</strong>: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies. Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency. I'm adding one for you, but still nagging you.</p>
</blockquote>
<p>Which fills the cmd screen. But as far as I'm aware pip is listed a dependency already in the file. Sorry if this seems obvious I'm new.</p>
",14173660.0,,13302.0,,2021-04-16 19:42:56,2022-11-03 15:14:02,"Conda YML file prompts ""Please add an explicit pip dependency""",<tensorflow><anaconda><conda>,2,0,0.0,,,CC BY-SA 4.0
67505710,1,67506643.0,,2021-05-12 14:33:16,,6,19912,"<p>I am trying to train my model (Image classification) using Tensorflow. I keep getting an error  when I try to run the following cell:</p>
<pre><code>    hist = model.fit(
        train_generator, 
        epochs=100,
        verbose=1,
        steps_per_epoch=steps_per_epoch,
        validation_data=valid_generator,
        validation_steps=val_steps_per_epoch).history
</code></pre>
<p>Error is:</p>
<pre><code>Epoch 1/100
27/31 [=========================&gt;....] - ETA: 1s - loss: 0.7309 - acc: 0.6181
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
&lt;ipython-input-36-b1c104100211&gt; in &lt;module&gt;
      2 val_steps_per_epoch = np.ceil(valid_generator.samples/valid_generator.batch_size)
      3 
----&gt; 4 hist = model.fit(
      5     train_generator,
      6     epochs=100,

/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-&gt; 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--&gt; 828       result = self._call(*args, **kwds)
    829       compiler = &quot;xla&quot; if self._experimental_compile else &quot;nonXla&quot;
    830       new_tracing_count = self.experimental_get_tracing_count()

/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    853       # In this case we have created variables on the first call, so we run the
    854       # defunned version which is guaranteed to never create variables.
--&gt; 855       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    856     elif self._stateful_fn is not None:
    857       # Release the lock early so that multiple threads can perform the call

/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2940       (graph_function,
   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
-&gt; 2942     return graph_function._call_flat(
   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2944 

/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1916         and executing_eagerly):
   1917       # No tape is watching; skip to running the function.
-&gt; 1918       return self._build_call_outputs(self._inference_function.call(
   1919           ctx, args, cancellation_manager=cancellation_manager))
   1920     forward_backward = self._select_forward_and_backward_functions(

/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    553       with _InterpolateFunctionError(self):
    554         if cancellation_manager is None:
--&gt; 555           outputs = execute.execute(
    556               str(self.signature.name),
    557               num_outputs=self._num_outputs,

/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---&gt; 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

UnknownError:  UnidentifiedImageError: cannot identify image file &lt;_io.BytesIO object at 0x7fc88d55c9a0&gt;
Traceback (most recent call last):

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py&quot;, line 249, in __call__
    ret = func(*args)

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py&quot;, line 620, in wrapper
    return func(*args, **kwargs)

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py&quot;, line 891, in generator_py_func
    values = next(generator_state.get_iterator(iterator_id))

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py&quot;, line 807, in wrapped_generator
    for data in generator_fn():

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py&quot;, line 933, in generator_fn
    yield x[i]

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py&quot;, line 65, in __getitem__
    return self._get_batches_of_transformed_samples(index_array)

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py&quot;, line 227, in _get_batches_of_transformed_samples
    img = load_img(filepaths[j],

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/keras_preprocessing/image/utils.py&quot;, line 114, in load_img
    img = pil_image.open(io.BytesIO(f.read()))

  File &quot;/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py&quot;, line 2943, in open
    raise UnidentifiedImageError(

PIL.UnidentifiedImageError: cannot identify image file &lt;_io.BytesIO object at 0x7fc88d55c9a0&gt;


     [[{{node PyFunc}}]]
     [[IteratorGetNext]] [Op:__inference_train_function_24233]

Function call stack:
train_function
</code></pre>
<p>I tried changing from loss='categorical_crossentropy' to loss='binary_crossentropy' but still the issue persists. I wish to train the model but the Epoch keeps getting stuck.</p>
<p>Edit:</p>
<p>The train generator function and where it is used is as follows:</p>
<pre><code>IMAGE_SHAPE = (224, 224)
TRAINING_DATA_DIR = str(data_root)


datagen_kwargs = dict(rescale=1./255, validation_split=.20)
valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)
valid_generator = valid_datagen.flow_from_directory(
    TRAINING_DATA_DIR, 
    subset=&quot;validation&quot;, 
    shuffle=True,
    target_size=IMAGE_SHAPE
)

train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)
train_generator = train_datagen.flow_from_directory(
    TRAINING_DATA_DIR, 
    subset=&quot;training&quot;,
    shuffle=True,
    target_size=IMAGE_SHAPE)


for image_batch, label_batch in train_generator:
  break
image_batch.shape, label_batch.shape
</code></pre>
<p>Output: ((32, 224, 224, 3), (32, 2))</p>
<pre><code>print (train_generator.class_indices)

labels = '\n'.join(sorted(train_generator.class_indices.keys()))

with open('labels.txt', 'w') as f:
  f.write(labels)
</code></pre>
<p>Output: {'off': 0, 'on': 1}</p>
",12540328.0,,12540328.0,,2021-05-12 14:40:36,2022-04-22 20:59:31,PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object,<python><tensorflow><image-classification>,4,6,0.0,,,CC BY-SA 4.0
71768363,1,,,2022-04-06 14:12:18,,6,13829,"<p>The kernel is <strong>dead</strong> while executing <code>model.fit(train_generator,epochs=20)</code>, but the same code works on another pc.</p>
<p>This is a juputer log:</p>
<p>Warn 17:03:05: Error in waiting for cell to complete [Error: Canceled future for execute_request message before replies were done</p>
<pre><code>at t.KernelShellFutureHandler.dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1204175)
at c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1223227
at Map.forEach (&lt;anonymous&gt;)
at v._clearKernelState (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1223212)
at v.dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1216694)
at c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:533674
at t.swallowExceptions (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:913059)
at dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:533652)
at t.RawSession.dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:537330)
at processTicksAndRejections (node:internal/process/task_queues:96:5)]
</code></pre>
<p>Warn 17:03:05: Cell completed with errors [Error: Canceled future for execute_request message before replies were done</p>
<pre><code>at t.KernelShellFutureHandler.dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1204175)
at c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1223227
at Map.forEach (&lt;anonymous&gt;)
at v._clearKernelState (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1223212)
at v.dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:1216694)
at c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:533674
at t.swallowExceptions (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:913059)
at dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:533652)
at t.RawSession.dispose (c:\Users\ivanf\.vscode\extensions\ms-toolsai.jupyter-2022.3.1000901801\out\extension.js:2:537330)
at processTicksAndRejections (node:internal/process/task_queues:96:5)]
</code></pre>
",17984379.0,,,,,2022-04-06 14:18:44,How to fix dead kernel while executing tensorflow .fit,<tensorflow><jupyter><miniconda>,1,0,,,,CC BY-SA 4.0
63515767,1,63516168.0,,2020-08-21 02:43:26,,6,8935,"<p>The tensorflow does not detect the GPU card. I have following the procedures suggest at Nvidia website and tensorflow/install/gpu.</p>
<p>How can I fix it?</p>
<p>I am using the following packages and drives:</p>
<p><strong>NVIDIA</strong></p>
<pre><code>[nvcc: NVIDIA (R) Cuda compiler driver

Copyright (c) 2005-2019 NVIDIA Corporation

Built on Sun_Jul_28_19:12:52_Pacific_Daylight_Time_2019

Cuda compilation tools, release 10.1, V10.1.243][1]
</code></pre>
<p><strong>Cudnn</strong>
Version 8.0.2</p>
<p><strong>Tensor Flow</strong></p>
<pre><code>Name                      Version                   Build  Channel
tensorflow                2.3.0                    pypi_0    pypi
tensorflow-addons         0.11.1                   pypi_0    pypi
tensorflow-estimator      2.3.0                    pypi_0    pypi
</code></pre>
<p>I use the following code to check it;</p>
<pre><code>Python 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)]
Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.

IPython 7.17.0 -- An enhanced Interactive Python.

from tensorflow.python.client import device_lib
device_lib.list_local_devices()
</code></pre>
<p><strong>Result</strong></p>
<pre><code>2020-08-20 22:58:38.419555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 
Out[1]:  [name: &quot;/device:CPU:0&quot;  device_type: &quot;CPU&quot;  memory_limit: 268435456  locality {  }  incarnation: 12639439165040732604,  name: &quot;/device:XLA_CPU:0&quot;  device_type: &quot;XLA_CPU&quot;  memory_limit: 17179869184  locality {  }  incarnation: 2249215130251849864  physical_device_desc: &quot;device: XLA_CPU device&quot;,  name: &quot;/device:XLA_GPU:0&quot;  device_type: &quot;XLA_GPU&quot;  memory_limit: 17179869184  locality {  }  incarnation: 7640064762024919839  physical_device_desc: &quot;device: XLA_GPU device&quot;]
2020-08-20 22:58:38.419555: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-20 22:58:40.332579: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-20 22:58:40.340307: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22481a47710 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-20 22:58:40.341741: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-20 22:58:40.342711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-08-20 22:58:40.362324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties:  pciBusID: 0000:01:00.0 name: GeForce GTX 1050 computeCapability: 6.1 coreClock: 1.493GHz coreCount: 5 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.43GiB/s 
2020-08-20 22:58:40.362354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll 
2020-08-20 22:58:40.366447: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll 
2020-08-20 22:58:40.369790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll 
2020-08-20 22:58:40.370968: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll 
2020-08-20 22:58:40.374957: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll 
2020-08-20 22:58:40.377382: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll 
2020-08-20 22:58:40.378955: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2020-08-20 22:58:40.378977: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform. Skipping registering GPU devices...
2020-08-20 22:58:40.455688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-20 22:58:40.455717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2020-08-20 22:58:40.455728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2020-08-20 22:58:40.458391: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x22490b5c830 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-20 22:58:40.458412: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1050, Compute Capability 6.1
</code></pre>
",14140755.0,,5015356.0,,2020-08-21 09:50:29,2020-08-21 09:50:29,Tensorflow 2.3.0 does not detect GPU,<python><tensorflow><gpu>,1,0,,,,CC BY-SA 4.0
64227384,1,64240796.0,,2020-10-06 13:59:56,,6,6962,"<p>How can I know whether tensorflow tensor is in cuda or cpu? Take this very simple example:</p>
<pre><code>import tensorflow as tf
tf.debugging.set_log_device_placement(True)

# Place tensors on the CPU
with tf.device('/device:GPU:0'):
   a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
   b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

# print tensor a
print(a)

# Run on the GPU
c = tf.matmul(a, b)
print(c)
</code></pre>
<p>The code runs fine. Here, I am physically placing tensor 'a' and 'b' on the GPU. While printing 'a', I get:</p>
<pre><code>tf.Tensor(
  [[1. 2. 3.]
  [4. 5. 6.]], shape=(2, 3), dtype=float32)
</code></pre>
<p>It does not give any info whether 'a' in CPU or GPU. Now, suppose that there is an intermediate tensor like tensor 'c' which gets created during some operation. How can I know that tensor 'c' is a CPU or a GPU tensor?
Also, suppose the tensor is placed on GPU. How can I move it to CPU?</p>
",8177024.0,,681865.0,,2020-10-06 18:55:35,2020-10-07 09:06:25,How can I know whether a tensorflow tensor is in cuda or cpu?,<python><tensorflow><gpu>,2,0,0.0,,,CC BY-SA 4.0
64081214,1,64159833.0,,2020-09-26 18:30:52,,6,4398,"<p>Many people have also faced this issue, but it alway seems to have happened because of some mistake in the command line argument</p>
<p>This is the command I'm running</p>
<pre><code>!python &quot;/content/drive/My Drive/Tensorflow/models/research/object_detection/model_main_tf2.py&quot; --model_dir=&quot;/content/drive/My Drive/Tensorflow/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8&quot; --pipeline_config_path=&quot;/content/drive/My Drive/Tensorflow/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config&quot;
</code></pre>
<p>There doesn't seem to be any mistake in it.</p>
<p>This is the stack trace</p>
<pre><code>    Traceback (most recent call last):
  File &quot;/content/drive/My Drive/Tensorflow/models/research/object_detection/model_main_tf2.py&quot;, line 113, in &lt;module&gt;
    tf.compat.v1.app.run()
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py&quot;, line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File &quot;/usr/local/lib/python3.6/dist-packages/absl/app.py&quot;, line 300, in run
    _run_main(main, args)
  File &quot;/usr/local/lib/python3.6/dist-packages/absl/app.py&quot;, line 251, in _run_main
    sys.exit(main(argv))
  File &quot;/content/drive/My Drive/Tensorflow/models/research/object_detection/model_main_tf2.py&quot;, line 110, in main
    record_summaries=FLAGS.record_summaries)
  File &quot;/usr/local/lib/python3.6/dist-packages/object_detection/model_lib_v2.py&quot;, line 630, in train_loop
    manager.save()
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py&quot;, line 819, in save
    self._record_state()
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py&quot;, line 728, in _record_state
    save_relative_paths=True)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpoint_management.py&quot;, line 248, in update_checkpoint_state_internal
    text_format.MessageToString(ckpt))
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py&quot;, line 570, in atomic_write_string_to_file
    rename(temp_pathname, filename, overwrite)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py&quot;, line 529, in rename
    rename_v2(oldname, newname, overwrite)
  File &quot;/usr/local/lib/python3.6/dist-packages/tensorflow/python/lib/io/file_io.py&quot;, line 546, in rename_v2
    compat.as_bytes(src), compat.as_bytes(dst), overwrite)
</code></pre>
<p>Error message:</p>
<pre><code>tensorflow.python.framework.errors_impl.FailedPreconditionError: /content/drive/My Drive/Tensorflow/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint.tmp91048f3bf67645619be6603094546de1; Is a directory
</code></pre>
<p>The error is raised from <code>_pywrap_file_io.RenameFile()</code>, where <code>_pywrap_file_io</code> is imported from <code>tensorflow.python</code>. I tried to look into the source code to find the problem, but I couldn't find it anywhere.</p>
<p>Could the problem have arraised because I'm running this on <strong>colab</strong> ?</p>
<p>Tensorflow version: 2.3
Python version: 3.6</p>
<p>Can someone please help me with this.</p>
",10510862.0,,10510862.0,,2020-09-26 18:49:07,2020-10-02 04:18:24,"""tensorflow.python.framework.errors_impl.FailedPreconditionError"" while running ""model_main_tf2.py"" for training object detection model in tensorflow",<python-3.x><tensorflow><tensorflow2.0><object-detection-api>,1,3,,,,CC BY-SA 4.0
71831415,1,,,2022-04-11 16:41:20,,6,18683,"<p>I am always getting the same error regarding TensorFlow:
<code>ModuleNotFoundError: No module named 'tensorflow.contrib'</code>.</p>
<p>I am actually using Python version <code>3.9</code> but, reading online, it seems that version <code>3.7</code> is the last stable one that can work with TensorFlow version <code>&gt;2.0</code>.</p>
<p>Unfortunately I have started my project in a <code>venv</code> with the wrong version of Python and I would like to downgrade it, how can I do that?</p>
",18203140.0,,11138259.0,,2023-01-08 13:07:09,2023-03-12 00:52:24,Downgrade Python version in virtual environment,<python><tensorflow><python-venv>,3,4,,,,CC BY-SA 4.0
72188890,1,,,2022-05-10 15:13:27,,6,314,"<p>Error when merging two application projects.</p>
<pre><code>Duplicate class org.tensorflow.lite.DataType found in modules jetified-tensorflow-lite-1.13.1-runtime (org.tensorflow:tensorflow-lite:1.13.1) and jetified-tensorflow-lite-api-2.7.0-runtime (org.tensorflow:tensorflow-lite-api:2.7.0)
Duplicate class org.tensorflow.lite.DataType$1 found in modules jetified-tensorflow-lite-1.13.1-runtime (org.tensorflow:tensorflow-lite:1.13.1) and jetified-tensorflow-lite-api-2.7.0-runtime (org.tensorflow:tensorflow-lite-api:2.7.0)
Duplicate class org.tensorflow.lite.Delegate found in modules jetified-tensorflow-lite-1.13.1-runtime (org.tensorflow:tensorflow-lite:1.13.1) and jetified-tensorflow-lite-api-2.7.0-runtime (org.tensorflow:tensorflow-lite-api:2.7.0)
Duplicate class org.tensorflow.lite.Tensor found in modules jetified-tensorflow-lite-1.13.1-runtime (org.tensorflow:tensorflow-lite:1.13.1) and jetified-tensorflow-lite-api-2.7.0-runtime (org.tensorflow:tensorflow-lite-api:2.7.0)

Go to the documentation to learn how to Fix dependency resolution errors.

</code></pre>
",17469648.0,,,,,2022-05-10 15:13:27,Getting duplicate class found error when merging two applications projects,<android><tensorflow>,0,0,,,,CC BY-SA 4.0
67638345,1,67639990.0,,2021-05-21 13:59:49,,6,5695,"<p>I am trying to implement a model with the ArcFace Layer:
<a href=""https://github.com/4uiiurz1/keras-arcface"" rel=""noreferrer"">https://github.com/4uiiurz1/keras-arcface</a></p>
<p>to this extend I created a tf.data.dataset like so:</p>
<pre><code>images= tf.data.Dataset.from_tensor_slices(train.A_image.to_numpy())
target = tf.keras.utils.to_categorical(
    train.Label.to_numpy(), num_classes=n_class, dtype='float32'
)
target = tf.data.Dataset.from_tensor_slices(target)

images= images.map(transform_img)

dataset = tf.data.Dataset.zip((images, target, target))
</code></pre>
<p>when I call <code>model.fit(dataset)</code></p>
<p>I get the following error:</p>
<pre><code>ValueError: Layer model expects 2 input(s), but it received 1 input tensors. Inputs received: [&lt;tf.Tensor 'IteratorGetNext:0' shape=&lt;unknown&gt; dtype=float32&gt;]
</code></pre>
<p>But this should work according:</p>
<p><a href=""https://stackoverflow.com/questions/52582275/tf-data-with-multiple-inputs-outputs-in-keras"">tf.data with multiple inputs / outputs in Keras</a></p>
<p>Can someone point out my folly?</p>
<p>Thanks!</p>
<p>Edit:
this solves some problems:</p>
<pre><code>#reads in filepaths to images from dataframe train
images = tf.data.Dataset.from_tensor_slices(train.image.to_numpy())
#converts labels to one hot encoding vector
target = tf.keras.utils.to_categorical(train.Label.to_numpy(), num_classes=n_class, dtype='float32')
#reads in the image and resizes it
images= images.map(transform_img)
input_1 = tf.data.Dataset.zip((anchors, target))
dataset = tf.data.Dataset.zip((input_1, target))
</code></pre>
<p>And I think it's what we are trying. But I get a shape error for targets, it's (n_class, 1) instead of just (n_class,)</p>
<p>I.e. the fit methods throws this error</p>
<pre><code>ValueError: Shapes (n_class, 1) and (n_class, n_class) are incompatible
</code></pre>
<p>and this warning</p>
<pre><code>input expected is (None, n_class) but received an input of (n_class, 1)
</code></pre>
",11106507.0,,11106507.0,,2021-05-21 15:36:54,2021-05-21 16:12:03,Tensorflow dataset with multiple inputs and target,<python><tensorflow><tensorflow-datasets><tf.data.dataset>,2,1,0.0,,,CC BY-SA 4.0
68776283,1,,,2021-08-13 17:32:04,,6,2874,"<p>I'm trying to use both TA-lib version 0.4.21 and Tensorflow 2.6.0 in the same project. Both require different numpy versions:
TF ~= 1.19.2
TA-lib &gt;= 1.19.4</p>
<p>Given those dependencies, numpy 1.19.4 or 1.19.5 should work just fine, but I get the following exception:</p>
<p><code>numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject</code></p>
<p>I tried uninstalling and installing numpy 1.19.4 and 1.19.5 several times, on python 3.8 and 3.9 but the result is the same.</p>
<p>Removing TF and using latest version on numpy resolves the issue - but this isn't a solution for me.</p>
<p>Didn't find anything online about this collision between the two libraries.</p>
<p>Will be glad for help, thanks!</p>
",3767514.0,,,,,2022-12-20 17:51:46,Tensorflow 2.6.0 and TA-lib 0.4.21 numpy version collision,<python-3.x><numpy><tensorflow><ta-lib>,4,2,0.0,,,CC BY-SA 4.0
65541946,1,,,2021-01-02 17:21:12,,6,2212,"<p>This is the code to get embeddings using EMLo .</p>
<pre><code>import tensorflow_hub as hub

import tensorflow as tf

elmo = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;)

x = [&quot;Roasted ants are a popular snack in Columbia&quot;]


embeddings = elmo(x, signature=&quot;default&quot;, as_dict=True)[&quot;elmo&quot;] # To Extract ELMo features 

embeddings.shape
</code></pre>
<p>I'm getting this type error , <code>type error : pruned(text): expected argument #0(zero-based) to be a Tensor; got list (['Roasted ants are a popular snack in Columbia'])</code>.</p>
",13544258.0,,4755954.0,,2021-01-02 18:16:36,2022-12-23 14:33:22,pruned(text): expected argument #0(zero-based) to be a Tensor; got list (['Roasted ants are a popular snack in Columbia']),<python><tensorflow>,2,4,,,,CC BY-SA 4.0
70206026,1,,,2021-12-02 20:24:04,,6,3376,"<p>I'm trying to run some projects using the new Mac M1. Those projects already work on Intel processor and are used by other developers that use Intel.</p>
<p>I am not able to build this simple Dockerfile:</p>
<pre><code>FROM python:3.9

RUN python -m pip install --upgrade pip

RUN pip install tensorflow==2.6.2
</code></pre>
<p>I get this message:</p>
<pre><code> &gt; [3/3] RUN pip install tensorflow==2.6.2:                                                                                                            
#6 0.583 ERROR: Could not find a version that satisfies the requirement tensorflow==2.6.2 (from versions: none)                                        
#6 0.583 ERROR: No matching distribution found for tensorflow==2.6.2  
</code></pre>
<p>I am able to install tensorflow locally, outside of the Dockerfile. Also, friends are able to build this image from their intel Mac.</p>
<p>I even tried to run docker build com different console architectures: i386 and arm64, but none work.</p>
<p>Any suggestions?</p>
",5227615.0,,,,,2022-03-20 07:37:24,Tensorflow not found on pip install inside Docker Container using Mac M1,<python><docker><tensorflow><apple-m1>,3,0,0.0,,,CC BY-SA 4.0
64853113,1,64853189.0,,2020-11-16 05:32:57,,6,24264,"<p>I have written a <strong>python</strong> code that can <strong>mark attendance using face recognition</strong>. It basically writes the (Name, Registration no. and entry time) in <strong>.csv</strong> file of those whose faces match with the data(images of my friends and mine) that i've provided.</p>
<p>And now I want to make an app(<strong>using flutter</strong>) that uses this python code.</p>
<p>What should be my approach to this?</p>
",13050987.0,,,,,2022-06-23 17:34:05,How to integrate Flutter app with Python code,<python><flutter><tensorflow><dart>,3,1,0.0,2022-06-02 16:18:26,,CC BY-SA 4.0
64845160,1,64845532.0,,2020-11-15 13:50:46,,6,13297,"<p>I have Ubuntu 18.04LTS install inside WSL2 and I was able to use GPU. I can run
$nvidia-smi from window run terminal.</p>
<p>However, I can not find any result when I run $nvidia-smi on WSL2</p>
",1138921.0,,1138921.0,,2020-11-24 06:31:12,2023-02-16 07:53:30,WSL2- $nvidia-smi command not running,<tensorflow><gpu><windows-subsystem-for-linux>,3,1,,,,CC BY-SA 4.0
64856654,1,,,2020-11-16 10:52:27,,6,1783,"<p>i'm using tensorflow_io_bigquery_client in order to read data from bq.
each record has 4 integer elements.</p>
<p>but i need to transform it into tensor (4 integer each).</p>
<p>so i have something like:</p>
<pre><code>tensorflow_io_bigquery_client = BigQueryClient()
read_session = tensorflow_io_bigquery_client.read_session(..)

dataset = read_session.parallel_read_rows()
dataset = dataset.map (transofrom_row)
#todo convert it into tensor (but i guess it should be an iterator or something like this because the dataset is hugh?

k_means_estimator = tf.compat.v1.estimator.experimental.KMeans(num_clusters = num_clusters, use_mini_batch = False, relative_tolerance = 1)

fit = k_means_estimator.train(input_fn=lambda: to_tensor(dataset), steps=1000) 
# todo convert dataset into tensor
</code></pre>
",14584019.0,,,,,2021-01-30 15:56:32,how to convert a MapDataset to tensor,<tensorflow><tensorflow-datasets>,0,2,,,,CC BY-SA 4.0
65525944,1,65979056.0,,2020-12-31 22:18:57,,6,886,"<p>I'm very new to TFX, but have an apparently-working ML Pipeline which is to be used via <a href=""https://www.tensorflow.org/tfx/api_docs/python/tfx/components/BulkInferrer"" rel=""noreferrer"">BulkInferrer</a>. That seems to produce output exclusively in Protobuf format, but since I'm running bulk inference I want to pipe the results to a database instead. (DB output seems like it should be the default for bulk inference, since both Bulk Inference &amp; DB access take advantage of parallelization... but Protobuf is a per-record, serialized format.)</p>
<p>I assume I could use something like <a href=""https://github.com/rdblue/parquet-avro-protobuf/blob/master/src/main/java/com/example/ProtobufToParquet.java"" rel=""noreferrer"">Parquet-Avro-Protobuf</a> to do the conversion (though that's in Java and the rest of the pipeline's in Python), or I could write something myself to consume all the protobuf messages one-by-one, convert them into JSON, deserialize the JSON into a list of dicts, and load the dict into a Pandas DataFrame, or store it as a bunch of key-value pairs which I treat like a single-use DB... but that sounds like a lot of work and pain involving parallelization and optimization for a very common use case. The top-level Protobuf message definition is Tensorflow's <a href=""https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_log.proto"" rel=""noreferrer"">PredictionLog</a>.</p>
<p>This <em>must</em> be a common use case, because TensorFlowModelAnalytics functions like <a href=""https://tensorflow.google.cn/tfx/model_analysis/api_docs/python/tfma/analyze_raw_data?hl=en"" rel=""noreferrer"">this one</a> consume Pandas DataFrames. I'd rather be able to write directly to a DB (preferably Google BigQuery), or a Parquet file (since Parquet / Spark seems to parallelize better than Pandas), and again, those seem like they should be common use cases, but I haven't found any examples.  Maybe I'm using the wrong search terms?</p>
<p>I also looked at the <a href=""https://tensorflow.google.cn/tfx/model_analysis/api_docs/python/tfma/extractors/PredictExtractor?hl=en"" rel=""noreferrer"">PredictExtractor</a>, since &quot;extracting predictions&quot; sounds close to what I want... but the official documentation appears silent on how that class is supposed to be used. I thought <a href=""https://tensorflow.google.cn/tfx/transform/api_docs/python/tft/TFTransformOutput?hl=en"" rel=""noreferrer"">TFTransformOutput</a> sounded like a promising verb, but instead it's a noun.</p>
<p>I'm clearly missing something fundamental here. Is there a reason no one wants to store BulkInferrer results in a database? Is there a configuration option that allows me to write the results to a DB? Maybe I want to add a <a href=""https://beam.apache.org/releases/pydoc/2.26.0/apache_beam.io.parquetio.html"" rel=""noreferrer"">ParquetIO</a> or <a href=""https://beam.apache.org/releases/pydoc/2.26.0/apache_beam.io.gcp.bigquery.html"" rel=""noreferrer"">BigQueryIO</a> instance to the TFX pipeline? (TFX docs say it uses Beam &quot;<a href=""https://www.tensorflow.org/tfx/guide/transform"" rel=""noreferrer"">under the hood</a>&quot; but that doesn't say much about how I should use them together.) But the syntax in those documents looks sufficiently different from my TFX code that I'm not sure if they're compatible?</p>
<p>Help?</p>
",2112722.0,,2112722.0,,2021-01-05 23:08:36,2022-02-04 22:16:53,How do I get a dataframe or database write from TFX BulkInferrer?,<database><tensorflow><output><tfx>,3,2,0.0,,,CC BY-SA 4.0
67167886,1,67215178.0,,2021-04-19 19:00:31,,6,2808,"<p>I have installed <code>TensorFlow</code> on an M1 (<strong>ARM</strong>) Mac according to <a href=""https://github.com/apple/tensorflow_macos/issues/153"" rel=""nofollow noreferrer"">these instructions</a>. Everything works fine.</p>
<p>However, model training is happening on the <code>CPU</code>. How do I switch training to the <code>GPU</code>?</p>
<pre><code>In: tensorflow.config.list_physical_devices()
Out: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]
</code></pre>
<p>In the documentation of <a href=""https://github.com/apple/tensorflow_macos"" rel=""nofollow noreferrer"">Apple's TensorFlow distribution</a> I found the following slightly confusing <a href=""https://github.com/apple/tensorflow_macos#additional-information"" rel=""nofollow noreferrer"">paragraph</a>:</p>
<blockquote>
<p>It is not necessary to make any changes to your existing TensorFlow scripts to use ML Compute as a backend for TensorFlow and TensorFlow Addons. There is an optional <code>mlcompute.set_mlc_device(device_name='any')</code> API for ML Compute device selection. The default value for device_name is 'any', which means ML Compute will select the best available device on your system, including multiple GPUs on multi-GPU configurations. Other available options are <code>CPU</code> and <code>GPU</code>. Please note that in eager mode, ML Compute will use the CPU. For example, to choose the CPU device, you may do the following:</p>
</blockquote>
<pre><code># Import mlcompute module to use the optional set_mlc_device API for device selection with ML Compute.
from tensorflow.python.compiler.mlcompute import mlcompute

# Select CPU device.
mlcompute.set_mlc_device(device_name='cpu') # Available options are 'cpu', 'gpu', and 'any'.
</code></pre>
<p>So I try to run:</p>
<pre><code>from tensorflow.python.compiler.mlcompute import mlcompute
mlcompute.set_mlc_device(device_name='gpu')
</code></pre>
<p>and get:</p>
<pre><code>WARNING:tensorflow: Eager mode uses the CPU. Switching to the CPU.
</code></pre>
<p>At this point I am stuck. How can I train <code>keras</code> models on the GPU to my MacBook Air?</p>
<p>TensorFlow version: <code>2.4.0-rc0</code></p>
",626537.0,,9215780.0,,2021-04-28 04:55:24,2021-07-03 22:45:19,Make TensorFlow use the GPU on an ARM Mac,<python><macos><tensorflow><deep-learning><arm>,2,0,0.0,,,CC BY-SA 4.0
73692131,1,,,2022-09-12 16:04:54,,6,706,"<p>I am getting started with tensor flow and I have the following messages I am trying to understand:</p>
<p><code>2022-09-12 09:58:48.355646: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz 2022-09-12 09:58:48.548392: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.</code></p>
<p>I am running on M1 and installed tensorflow following these steps:</p>
<p><a href=""https://betterdatascience.com/install-tensorflow-2-7-on-macbook-pro-m1-pro/"" rel=""noreferrer"">https://betterdatascience.com/install-tensorflow-2-7-on-macbook-pro-m1-pro/</a></p>
<p>I am trying to understand if these messages are normal or if I have to fix something in my setup. Especially the frequency 0Hz message. Can somebody help me understand why I am getting these messages and tell me if I need to make changes in my setup? Is this normal?</p>
<p><a href=""https://i.stack.imgur.com/Vp880.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Vp880.png"" alt=""enter image description here"" /></a></p>
",728246.0,,15416.0,,2022-09-12 16:16:01,2022-09-12 16:16:01,"What does ""Tensorflow: Failed to get CPU frequency: 0Hz"" mean?",<python><tensorflow><conda>,0,1,,,,CC BY-SA 4.0
63073711,1,63073939.0,,2020-07-24 12:36:50,,6,27827,"<p>I would like to test some function in the new <a href=""https://pypi.org/project/tensorflow-cpu/2.3.0rc2/"" rel=""noreferrer"">tensorflow2.3</a> However, I am struggling with installation process.</p>
<p>I saw: <a href=""https://stackoverflow.com/questions/61357038/how-do-i-install-tensorflow-2-2-in-windows"">How do I install the most recent Tensorflow (here: 2.2) on Windows when conda does not yet support it?</a></p>
<p>I executed: <code>pip install --upgrade pip</code></p>
<p>I got:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement tensorflow-cpu==2.3.0rc2 (from versions: 1.15.0rc0, 1.15.0rc1, 1.15.0rc2, 1.15.0rc3, 1.15.0, 2.1.0rc0, 2.1.0rc1, 2.1.0rc2, 2.1.0)
ERROR: No matching distribution found for tensorflow-cpu==2.3.0rc2
</code></pre>
",6691265.0,,1234909.0,,2020-11-15 02:38:44,2022-07-14 09:25:41,How to install tensorflow==2.3.0,<tensorflow><pip><tensorflow2>,3,0,,,,CC BY-SA 4.0
67227577,1,67227816.0,,2021-04-23 09:38:24,,6,1399,"<p>So I'm trying to install Perceptilabs so I can have a GUI for TensorFlow. I'm following their install instructions, which is <code>pip install perceptilabs</code>. However, I keep having the error:</p>
<blockquote>
<p>ERROR: Could not find a version that satisfies the requirement perceptilabs
ERROR: No matching distribution found for perceptilabs</p>
</blockquote>
<p>Here's a screen shot:</p>
<p><a href=""https://i.stack.imgur.com/6GvPB.png"" rel=""noreferrer"">Can't embed images yet, here's the link</a></p>
<p>Even when I do <code>python -m pip install perceptilabs</code>, <code>py -m pip install perceptilabs</code>, <code>pip3 install perceptilabs</code>, <code>python -m pip3 install perceptilabs</code> or <code>py pip3 -m install perceptilabs</code>, it still doesn't work. Even in a virtual env.</p>
<p>I have Python 3.8.3 and 3.9.4 installed, pip and pip3 21.0.1, Tensorflow 2.5.0-rc1, Windows 10 Pro version 20H2 x64, with an Intel Core i5-3470 CPU, 8 GB RAM.</p>
",15746556.0,,,,,2021-04-23 09:55:33,"Pip error ""No matching distribution found for perceptilabs""",<python><windows><tensorflow><pip>,1,0,,,,CC BY-SA 4.0
70797818,1,,,2022-01-21 07:33:12,,6,1896,"<p>I want to use tensorflow on aws g5.xlarge. For AMI, I used AWS Deep Learning AMI (Ubuntu 18.04) ver 50.0. But when I start the instance and try nvidia-smi, the following error exists. Why am I getting the following error even though I used deep learning ami?</p>
<blockquote>
<p>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</p>
</blockquote>
",14894805.0,,,,,2022-01-21 07:33:12,Why is nvidia-smi not working on deep learning ami + aws g5.xlarge,<amazon-web-services><tensorflow><amazon-ec2><gpu>,0,1,0.0,,,CC BY-SA 4.0
63940188,1,65737458.0,,2020-09-17 14:24:40,,6,117,"<p>I have a problem with building TF r2.3 when I would like to build a version with CUDA support. When I configure the build for CPU without CUDA everything build fine. With the CUDA support turned on I got Bad address (Exit 126) error for bash commands like this. I use windows 10 with MSVC 2019 v16.6.5. I also use python 3.6.8 and Bazel 3.3.1. I have encountered with this problem with both CUDA v10.2, cudnn-10.2-windows10-x64-v7.6.5.32 and CUDA 10.1 and cuDNN 7.4. I have tried to build with the following command after I have configured the project.</p>
<pre><code>bazel build --config=opt --define=no_tensorflow_py_deps=true //tensorflow:libtensorflow_cc.so
</code></pre>
<p>I put the content of the command.log file to this <a href=""https://pastebin.com/CK0VxEcF"" rel=""noreferrer"">pastebin link</a>.</p>
<p>I have opened a <a href=""https://github.com/tensorflow/tensorflow/issues/41850"" rel=""noreferrer"">github issue</a> for this error on the tensorflow repo but so far they haven't provided any solution for this issue. I hope here someone point me in the right direction.</p>
",2604336.0,,,,,2021-01-15 14:01:56,TF r2.3 Bad Address build issue on windows,<windows><tensorflow><build><nvidia>,1,1,,,,CC BY-SA 4.0
72323238,1,,,2022-05-20 18:15:29,,6,649,"<p>I was looking at different ways that one can do custom Tensorflow datasets, and I was used to looking at <a href=""https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"" rel=""noreferrer"">PyTorch's datasets</a>, but when I went to look at <a href=""https://www.tensorflow.org/guide/data_performance"" rel=""noreferrer"">Tensorflow's datasets</a>, I saw this example:</p>
<pre class=""lang-py prettyprint-override""><code>class ArtificialDataset(tf.data.Dataset):
  def _generator(num_samples):
    # Opening the file
    time.sleep(0.03)

    for sample_idx in range(num_samples):
      # Reading data (line, record) from the file
      time.sleep(0.015)

      yield (sample_idx,)

  def __new__(cls, num_samples=3):
    return tf.data.Dataset.from_generator(
        cls._generator,
        output_signature = tf.TensorSpec(shape = (1,), dtype = tf.int64),
        args=(num_samples,)
        )
</code></pre>
<p>But two questions came up:</p>
<ol>
<li>This looks like all it does is that when the object is instantiated, the <code>__new__</code> method just calls the <code>tf.data.Dataset.from_generator</code> static method. So why not just call it? Why is there a point of even subclassing <code>tf.data.Dataset</code>? Are there any methods that are even used from <code>tf.data.Dataset</code>?</li>
<li>Would there be a way to do it like a data generator, where one fills out an <code>__iter__</code> method while inheriting from <code>tf.data.Dataset</code>? Idk, something like</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>class MyDataLoader(tf.data.Dataset):
  def __init__(self, path, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.data = pd.read_csv(path)

  def __iter__(self):
    for datum in self.data.iterrows():
      yield datum
</code></pre>
<p>Thank you all very much!</p>
",4962905.0,,,,,2022-11-15 20:56:29,Is there a proper way to subclass Tensorflow's Dataset?,<python><tensorflow>,1,1,0.0,,,CC BY-SA 4.0
72029857,1,74006021.0,,2022-04-27 13:41:29,,6,32576,"<p>I'm trying to use the code from the <a href=""https://teachablemachine.withgoogle.com"" rel=""noreferrer"">Teachable Machine website</a>:</p>
<pre class=""lang-py prettyprint-override""><code>from keras.models import load_model
from PIL import Image, ImageOps
import numpy as np

# Load the model
model = load_model('keras_model.h5')

# Create the array of the right shape to feed into the keras model
# The 'length' or number of images you can put into the array is
# determined by the first position in the shape tuple, in this case 1.
data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)
# Replace this with the path to your image
image = Image.open('&lt;IMAGE_PATH&gt;')
#resize the image to a 224x224 with the same strategy as in TM2:
#resizing the image to be at least 224x224 and then cropping from the center
size = (224, 224)
image = ImageOps.fit(image, size, Image.ANTIALIAS)

#turn the image into a numpy array
image_array = np.asarray(image)
# Normalize the image
normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1
# Load the image into the array
data[0] = normalized_image_array

# run the inference
prediction = model.predict(data)
print(prediction)

</code></pre>
<p>but when running the code, I get the following error:
<code>ModuleNotFoundError: No module named 'tensorflow.compat'</code></p>
<p>I tried running the code on two separate machines, uninstalling and re-installing tensorflow, pip, keras, nothing seemed to help.</p>
<p>I'm using Python 3.9 and tensorflow 2.8.0</p>
",12538277.0,,12538277.0,,2022-04-27 13:50:33,2022-12-16 13:19:20,No module named 'tensorflow.compat',<python><tensorflow><pip><teachable-machine>,2,4,,,,CC BY-SA 4.0
64993130,1,75551041.0,,2020-11-24 18:59:26,,6,604,"<p>I'm working with a dataset that contains data from IoT devices and I have found that Hidden Markov Models work pretty well for my use case. As such, I'm trying to alter some code from a Tensorflow tutorial I've found <a href=""https://www.tensorflow.org/probability/examples/Multiple_changepoint_detection_and_Bayesian_model_selection"" rel=""nofollow noreferrer"">here</a>. The dataset contains real-values for the observed variable compared to the count data shown in the tutorial.</p>
<p>In particular, I believe the following needs to be changed so that the HMM has Normally distributed emissions. Unfortunately, I can't find any code on how to alter the model to have a different emission other than Poisson.</p>
<p>How should I change the code to emit normally distributed values?</p>
<pre><code># Define variable to represent the unknown log rates.
trainable_log_rates = tf.Variable(
  np.log(np.mean(observed_counts)) + tf.random.normal([num_states]),
  name='log_rates')

hmm = tfd.HiddenMarkovModel(
  initial_distribution=tfd.Categorical(
      logits=initial_state_logits),
  transition_distribution=tfd.Categorical(probs=transition_probs),
  observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),
  num_steps=len(observed_counts))

rate_prior = tfd.LogNormal(5, 5)

def log_prob():
 return (tf.reduce_sum(rate_prior.log_prob(tf.math.exp(trainable_log_rates))) +
         hmm.log_prob(observed_counts))

optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)

@tf.function(autograph=False)
def train_op():
  with tf.GradientTape() as tape:
    neg_log_prob = -log_prob()
  grads = tape.gradient(neg_log_prob, [trainable_log_rates])[0]
  optimizer.apply_gradients([(grads, trainable_log_rates)])
  return neg_log_prob, tf.math.exp(trainable_log_rates)
</code></pre>
",2771315.0,,4685471.0,,2020-12-07 22:36:36,2023-02-23 22:38:05,How to get HMM working with real-valued data in Tensorflow,<python><tensorflow><machine-learning><tensorflow2.0><hidden-markov-models>,2,3,,,,CC BY-SA 4.0
65052400,1,,,2020-11-28 17:16:31,,6,8332,"<p>I ran this code:</p>
<pre><code>import tensorflow_hub as hub
</code></pre>
<p>I got this error:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-3-5c017171c13e&gt; in &lt;module&gt;
----&gt; 1 import tensorflow_hub as hub

~\anaconda3\envs\Python 3-7\lib\site-packages\tensorflow_hub\__init__.py in &lt;module&gt;
     86 
     87 
---&gt; 88 from tensorflow_hub.estimator import LatestModuleExporter
     89 from tensorflow_hub.estimator import register_module_for_export
     90 from tensorflow_hub.feature_column import image_embedding_column

~\anaconda3\envs\Python 3-7\lib\site-packages\tensorflow_hub\estimator.py in &lt;module&gt;
     60 
     61 
---&gt; 62 class LatestModuleExporter(tf.compat.v1.estimator.Exporter):
     63   &quot;&quot;&quot;Regularly exports registered modules into timestamped directories.
     64 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\lazy_loader.py in __getattr__(self, item)
     60 
     61   def __getattr__(self, item):
---&gt; 62     module = self._load()
     63     return getattr(module, item)
     64 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\util\lazy_loader.py in _load(self)
     43     &quot;&quot;&quot;Load the module and insert it into the parent's globals.&quot;&quot;&quot;
     44     # Import the target module and insert it into the parent's namespace
---&gt; 45     module = importlib.import_module(self.__name__)
     46     self._parent_module_globals[self._local_name] = module
     47 

~\anaconda3\envs\Python 3-7\lib\importlib\__init__.py in import_module(name, package)
    125                 break
    126             level += 1
--&gt; 127     return _bootstrap._gcd_import(name[level:], package, level)
    128 
    129 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_estimator\python\estimator\api\__init__.py in &lt;module&gt;
      8 import sys as _sys
      9 
---&gt; 10 from tensorflow_estimator.python.estimator.api._v1 import estimator
     11 
     12 del _print_function

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_estimator\python\estimator\api\_v1\estimator\__init__.py in &lt;module&gt;
      8 import sys as _sys
      9 
---&gt; 10 from tensorflow_estimator.python.estimator.api._v1.estimator import experimental
     11 from tensorflow_estimator.python.estimator.api._v1.estimator import export
     12 from tensorflow_estimator.python.estimator.api._v1.estimator import inputs

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_estimator\python\estimator\api\_v1\estimator\experimental\__init__.py in &lt;module&gt;
      8 import sys as _sys
      9 
---&gt; 10 from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder
     11 from tensorflow_estimator.python.estimator.canned.kmeans import KMeansClustering as KMeans
     12 from tensorflow_estimator.python.estimator.canned.linear import LinearSDCA

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_estimator\python\estimator\canned\dnn.py in &lt;module&gt;
     29 from tensorflow.python.keras.utils import losses_utils
     30 from tensorflow.python.util.tf_export import estimator_export
---&gt; 31 from tensorflow_estimator.python.estimator import estimator
     32 from tensorflow_estimator.python.estimator.canned import head as head_lib
     33 from tensorflow_estimator.python.estimator.canned import optimizers

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_estimator\python\estimator\estimator.py in &lt;module&gt;
     50 from tensorflow.python.util.tf_export import estimator_export
     51 from tensorflow_estimator.python.estimator import model_fn as model_fn_lib
---&gt; 52 from tensorflow_estimator.python.estimator import run_config
     53 from tensorflow_estimator.python.estimator import util as estimator_util
     54 from tensorflow_estimator.python.estimator.export import export_lib

~\AppData\Roaming\Python\Python37\site-packages\tensorflow_estimator\python\estimator\run_config.py in &lt;module&gt;
     28 from tensorflow.core.protobuf import rewriter_config_pb2
     29 from tensorflow.python.distribute import estimator_training as distribute_coordinator_training
---&gt; 30 from tensorflow.python.distribute import parameter_server_strategy_v2
     31 from tensorflow.python.util import compat_internal
     32 from tensorflow.python.util import function_utils

ImportError: cannot import name 'parameter_server_strategy_v2' from 'tensorflow.python.distribute' (C:\Users\33651\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\distribute\__init__.py)
</code></pre>
<p>Any Idea on how to fix it ?</p>
<p>I'm on windows 10, with Anaconda, Python 3.7, tensorflow version is 2.3.1,<br />
numpy version is 1.19.2
tensorflow_hub version is 0.10.0
The same error was with tensorflow_hub version 0.8.0</p>
",14256059.0,,,,,2022-02-23 18:16:57,I have this error when trying to import tensorflow_hub: cannot import name 'parameter_server_strategy_v2' from 'tensorflow.python.distribute',<numpy><tensorflow><tensorflow-hub>,3,1,0.0,,,CC BY-SA 4.0
65361547,1,,,2020-12-18 17:40:53,,6,644,"<p>I'm trying to do a bit of manual memory management and am unsure how to move a tensor onto the CPU once an operation is complete. For my use case, I'd like to run an STFT on the GPU and then immediately move the tensor onto the CPU because it takes up a huge amount of memory. Something like.</p>
<pre><code>with tf.device(&quot;GPU:0&quot;):
    foo = tf.signal.stft(bar, ...)

with tf.device(&quot;CPU:0&quot;):
    baz = foo
del foo
</code></pre>
",2985049.0,,,,,2022-02-28 18:33:26,Tensorflow: Move tensor from GPU to CPU to free up memory,<python><tensorflow>,0,0,,,,CC BY-SA 4.0
70645074,1,70798023.0,,2022-01-09 20:08:14,,6,835,"<p>For the last 5 days, I am trying to make Keras/Tensorflow packages work in R. I am using RStudio for installation and have used <code>conda</code>, <code>miniconda</code>, <code>virtualenv</code> but it crashes each time in the end. Installing a library should not be a nightmare especially when we are talking about R (<em>one of the best statistical languages</em>) and TensorFlow (<em>one of the best deep learning libraries</em>). Can someone share a reliable way to install Keras/Tensorflow on CentOS 7?</p>
<p>Following are the steps I am using to install <code>tensorflow</code> in RStudio.</p>
<p>Since RStudio simply crashes each time I run <code>tensorflow::tf_config()</code> I have no way to check what is going wrong.</p>
<p><a href=""https://i.stack.imgur.com/kXZK8.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/kXZK8.png"" alt=""enter image description here"" /></a></p>
<pre><code>devtools::install_github(&quot;rstudio/reticulate&quot;)
devtools::install_github(&quot;rstudio/keras&quot;) # This package also installs tensorflow
library(reticulate)
reticulate::install_miniconda()
reticulate::use_miniconda(&quot;r-reticulate&quot;)
library(tensorflow)
tensorflow::tf_config() **# Crashes at this point**

sessionInfo()


R version 3.6.0 (2019-04-26)
Platform: x86_64-redhat-linux-gnu (64-bit)
Running under: CentOS Linux 7 (Core)

Matrix products: default
BLAS/LAPACK: /usr/lib64/R/lib/libRblas.so

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] tensorflow_2.7.0.9000 keras_2.7.0.9000      reticulate_1.22-9000 

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.7      lattice_0.20-45 png_0.1-7       zeallot_0.1.0  
 [5] rappdirs_0.3.3  grid_3.6.0      R6_2.5.1        jsonlite_1.7.2 
 [9] magrittr_2.0.1  tfruns_1.5.0    rlang_0.4.12    whisker_0.4    
[13] Matrix_1.3-4    generics_0.1.1  tools_3.6.0     compiler_3.6.0 
[17] base64enc_0.1-3


</code></pre>
<p><strong>Update 1</strong>
The only way RStudio does not crash while installing tensorflow is by executing following steps -</p>
<p>First, I created a new virtual environment using conda</p>
<pre><code>conda create --name py38 python=3.8.0
conda activate py38
conda install tensorflow=2.4
</code></pre>
<p>Then from within RStudio, I installed reticulate and activated the virtual environment which I earlier created using conda</p>
<pre><code>devtools::install_github(&quot;rstudio/reticulate&quot;)
library(reticulate)
reticulate::use_condaenv(&quot;/root/.conda/envs/py38&quot;, required = TRUE)
reticulate::use_python(&quot;/root/.conda/envs/py38/bin/python3.8&quot;, required = TRUE)
reticulate::py_available(initialize = TRUE)
ts &lt;- reticulate::import(&quot;tensorflow&quot;)
</code></pre>
<p>As soon as I try to import <code>tensorflow</code> in RStudio, it loads the library <code>/lib64/libstdc++.so.6</code> instead of <code>/root/.conda/envs/py38/lib/libstdc++.so.6</code> and I get the following error -</p>
<pre><code>Error in py_module_import(module, convert = convert) : 
  ImportError: Traceback (most recent call last):
  File &quot;/root/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/pywrap_tensorflow.py&quot;, line 64, in &lt;module&gt;
    from tensorflow.python._pywrap_tensorflow_internal import *
  File &quot;/home/R/x86_64-redhat-linux-gnu-library/3.6/reticulate/python/rpytools/loader.py&quot;, line 39, in _import_hook
    module = _import(
ImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20' not found (required by /root/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
</code></pre>
<p>Here is what inside <code>/lib64/libstdc++.so.6</code></p>
<pre><code>&gt; strings /lib64/libstdc++.so.6 | grep GLIBC

GLIBCXX_3.4
GLIBCXX_3.4.1
GLIBCXX_3.4.2
GLIBCXX_3.4.3
GLIBCXX_3.4.4
GLIBCXX_3.4.5
GLIBCXX_3.4.6
GLIBCXX_3.4.7
GLIBCXX_3.4.8
GLIBCXX_3.4.9
GLIBCXX_3.4.10
GLIBCXX_3.4.11
GLIBCXX_3.4.12
GLIBCXX_3.4.13
GLIBCXX_3.4.14
GLIBCXX_3.4.15
GLIBCXX_3.4.16
GLIBCXX_3.4.17
GLIBCXX_3.4.18
GLIBCXX_3.4.19
GLIBC_2.3
GLIBC_2.2.5
GLIBC_2.14
GLIBC_2.4
GLIBC_2.3.2
GLIBCXX_DEBUG_MESSAGE_LENGTH
</code></pre>
<p>To resolve the library issue, I added the path of the correct <code>libstdc++.so.6</code> library having <code>GLIBCXX_3.4.20</code> in RStudio.</p>
<pre><code>system('export LD_LIBRARY_PATH=/root/.conda/envs/py38/lib/:$LD_LIBRARY_PATH')
</code></pre>
<p>and, also</p>
<pre><code>Sys.setenv(&quot;LD_LIBRARY_PATH&quot; = &quot;/root/.conda/envs/py38/lib&quot;)
</code></pre>
<p>But still I get the same error <code>ImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.20'</code>. Somehow RStudio still loads <code>/lib64/libstdc++.so.6</code> first instead of <code>/root/.conda/envs/py38/lib/libstdc++.so.6</code></p>
<p>Instead of <code>RStudio</code>, if I execute the above steps in the <code>R</code> console, then also I get the exact same error.</p>
<p><strong>Update 2:</strong>
A solution is posted <a href=""https://stackoverflow.com/a/70798023/11939840"">here</a></p>
",11939840.0,,5221626.0,,2022-07-29 16:35:34,2022-07-29 16:35:34,Tensorflow setup on RStudio/ R | CentOS,<python><r><tensorflow><centos7>,2,11,0.0,,,CC BY-SA 4.0
64437184,1,,,2020-10-20 01:21:11,,6,859,"<p>For some reason the time used to extract results using .float_val is extremely high.</p>
<p>Scenario example along with its output:</p>
<pre class=""lang-py prettyprint-override""><code>t2 = time.time()
options = [('grpc.max_receive_message_length', 100 * 4000 * 4000)]
channel = grpc.insecure_channel('{host}:{port}'.format(host='localhost', port=str(self.serving_grpc_port)), options = options)
stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)
request = predict_pb2.PredictRequest()
request.model_spec.name = 'ivi-detector'
request.model_spec.signature_name = 'serving_default'

request.inputs['inputs'].CopyFrom(tf.make_tensor_proto(imgs_array, shape=imgs_array.shape))
res = stub.Predict(request, 100.0)

print(&quot;Time to detect:&quot;)
t3 = time.time(); print(&quot;t3:&quot;, t3 - t2)

t11 = time.time()
boxes_float_val = res.outputs['detection_boxes'].float_val
t12 = time.time(); print(&quot;t12:&quot;, t12 - t11)
classes_float_val = res.outputs['detection_classes'].float_val
t13 = time.time(); print(&quot;t13:&quot;, t13 - t12)
scores_float_val = res.outputs['detection_scores'].float_val
t14 = time.time(); print(&quot;t14:&quot;, t14 - t13)

boxes = np.reshape(boxes_float_val, [len(imgs_array), self.max_total_detections,4])
classes = np.reshape(classes_float_val, [len(imgs_array), self.max_total_detections])
scores = np.reshape(scores_float_val, [len(imgs_array), self.max_total_detections])
t15 = time.time(); print(&quot;t15:&quot;, t15 - t14)
</code></pre>
<pre><code>Time to detect:
t3: 1.4687104225158691
t12: 1.9140026569366455
t13: 3.719329833984375e-05
t14: 9.298324584960938e-06
t15: 0.0008063316345214844
</code></pre>
<p>Tensorflow Serving is running an object detection model from tensorflow's object detection api (faster_rncc_resnet101). As we can see, the extraction of the boxes found on detection is higher than the prediction itself.</p>
<p>The current shape of the detected boxes is [batch_size, 100, 4], with 100 being the number of max detections.
As a workaround I can low the number of max detection and decrease significantly the necessary time to extract these values, but it keeps staying unnecessary (on my point of view) high.</p>
<p>I'm using tensorflow-serving 2.3.0-gpu as a docker container along with tensorflow-serving-api==2.3.0</p>
<p>Also, it's important to inform that I tried to reproduce this behaviour on a public saved model (purely trained on imagenet) and the slow performance on .float_val didn't happen, pointing that the problem can be specifically with my custom trained model. I already tried to export the saved model from .ckpt files in different ways but the problem still occurs and, if I use any of the export methods for the downloaded model (the downloaded model comes with both .ckpt files and saved_model format files) the problem doesn't occur, so the export methods are safe.</p>
<p>Now I'm suspecting that something is wrong/different with the model I trained.... but.. why? Does it makes sense that it affects .float_val from tensorflow-serving-api?</p>
<p>The code I used (with fast results):
<a href=""https://github.com/denisb411/tfserving-od/blob/master/inference-using-tfserving-docker.ipynb"" rel=""nofollow noreferrer"">https://github.com/denisb411/tfserving-od/blob/master/inference-using-tfserving-docker.ipynb</a></p>
<p>I don't know how to proceed as my custom training follows almost the same pipeline.config as the original so, there's nothing different on the training process.</p>
<p>How can I manage to fix this? How is this related with .float_val, if there's any relation?</p>
<p>Assuming that this is a bug, a time ago I created a <a href=""https://github.com/tensorflow/serving/issues/1725"" rel=""nofollow noreferrer"">github issue</a> talking about this problem I got into but it didn't get enough attention.</p>
",5811508.0,,5811508.0,,2020-10-29 17:21:02,2020-10-29 17:21:02,Very slow performance on extracting tensorflow-serving grpc request results using .float_val,<python><tensorflow><grpc><tensorflow-serving>,0,7,0.0,,,CC BY-SA 4.0
72305688,1,72368721.0,,2022-05-19 13:44:13,,6,898,"<p>I have 2 tflite models hosted as s3 objects on aws. In my react-typescript app, I am trying to load those models if the browser is opened on mobile. Else, the web app will use other more efficient models.
<a href=""https://i.stack.imgur.com/oqFde.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/oqFde.png"" alt=""enter image description here"" /></a></p>
<p>The <code>Models</code> interface is as follows:
<a href=""https://i.stack.imgur.com/vse8D.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/vse8D.png"" alt=""enter image description here"" /></a></p>
<p>I have configured the s3 bucket so I can access it from this web app, by changing the CORS configuration. That works. If I go to the network tabs, I see the fetch for the model:</p>
<p><a href=""https://i.stack.imgur.com/UhZMf.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/UhZMf.png"" alt=""enter image description here"" /></a></p>
<p>Using Chrome, I can change from mobile to desktop display. The desktop display does not produce any errors. However, the mobile gives me errors that I do not understand.</p>
<p><a href=""https://i.stack.imgur.com/2TEuG.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/2TEuG.png"" alt=""enter image description here"" /></a></p>
<p>Ignore the <code>GET</code> error and the <code>date_created</code> console.log. They are from another part of my code that is not relevant to this.</p>
<p>I have searched various resources for deploying a tflite to a web app, but have not found anything useful.</p>
<p>------------------<strong>EDIT</strong>-------------------</p>
<p>I have tried using the method discussed in <a href=""https://github.com/tensorflow/tfjs/issues/6026"" rel=""nofollow noreferrer"">this github post</a>
<a href=""https://i.stack.imgur.com/Y3zrt.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Y3zrt.png"" alt=""enter image description here"" /></a></p>
<p>But only get the following error (you can ignore the GET error and isMobile console.log):</p>
<p><a href=""https://i.stack.imgur.com/W3IFw.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/W3IFw.png"" alt="""" /></a></p>
",14888491.0,,14888491.0,,2022-05-23 23:33:42,2022-05-24 19:50:40,Can't load tflite custom model into web using react,<reactjs><typescript><tensorflow><machine-learning><tensorflow-lite>,2,2,,,,CC BY-SA 4.0
62866577,1,62888294.0,,2020-07-12 21:20:18,,6,7332,"<p>I'm trying to run my own custom model for object detection. I created my dataset from  Google cloud - Vision (<a href=""https://console.cloud.google.com/vision/"" rel=""noreferrer"">https://console.cloud.google.com/vision/</a>) (I boxed and labeled the images) and it looks like this:</p>
<p><a href=""https://i.stack.imgur.com/yA23l.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/yA23l.png"" alt=""enter image description here"" /></a></p>
<p>After training the model, I downloaded the TFLite files  (labelmap.txt, model.tflite and a json file) from here:</p>
<p><a href=""https://i.stack.imgur.com/MynJ3.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MynJ3.png"" alt=""enter image description here"" /></a></p>
<p>Then, I added them to the Android Object Detection example ( <a href=""https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android"" rel=""noreferrer"">https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android</a> ) .</p>
<p><a href=""https://i.stack.imgur.com/bdKag.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/bdKag.png"" alt=""enter image description here"" /></a></p>
<p>But when I run the project it crashes:</p>
<pre><code>2020-07-12 18:03:05.160 14845-14883/? E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.detection, PID: 14845
    java.lang.IllegalArgumentException: Cannot copy to a TensorFlowLite tensor (normalized_input_image_tensor) with 307200 bytes from a Java Buffer with 4320000 bytes.
        at org.tensorflow.lite.Tensor.throwIfSrcShapeIsIncompatible(Tensor.java:423)
        at org.tensorflow.lite.Tensor.setTo(Tensor.java:189)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:343)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:197)
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:182)
        at android.os.Handler.handleCallback(Handler.java:883)
        at android.os.Handler.dispatchMessage(Handler.java:100)
        at android.os.Looper.loop(Looper.java:214)
        at android.os.HandlerThread.run(HandlerThread.java:67)
</code></pre>
<p>I tried changing the parameters <strong>TF_OD_API_IS_QUANTIZED</strong> to false and <strong>labelOffset</strong> to 0, and also I modified this line from the TFLiteObjectDetectionAPIModel.java to     <code>d.imgData = ByteBuffer.allocateDirect(_4_ * d.inputSize * d.inputSize * 3 * numBytesPerChannel);</code> (I replaced 1 for 4)</p>
<p>I am new to this, I would really appreciate if someone could help me understand and resolve the error. Thank you!</p>
<hr />
<p><strong>Update</strong>:
Here are the tflite files : <a href=""https://drive.google.com/drive/folders/11QT8CgaYF2EseORgGCceh4DT80_pMiFM?usp=sharing"" rel=""noreferrer"">https://drive.google.com/drive/folders/11QT8CgaYF2EseORgGCceh4DT80_pMiFM?usp=sharing</a>  (I don't care if the model recognize correctly the squares and circles, I just want to check if it compiles on the android app and then I will improve it)</p>
",1269404.0,,1269404.0,,2020-07-13 17:00:26,2021-12-11 13:45:18,Android - TFLite OD - Cannot copy to a TensorFlowLite tensor (normalized_input_image_tensor) with 307200 bytes from a Java Buffer with 4320000 bytes,<android><tensorflow><google-cloud-platform><tensorflow-lite>,2,12,0.0,,,CC BY-SA 4.0
68515561,1,,,2021-07-25 04:46:33,,6,2054,"<p>When attempting to install tensorflow on an ARM M1 MacBookPro, I am seeing the following issues:</p>
<pre><code>% pip install tensorflow
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
</code></pre>
<p>After searching around, I found a package called <a href=""https://pypi.org/project/tensorflow-macos/"" rel=""noreferrer"">tensorflow-macos</a> which produces the following issues:</p>
<pre><code>clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
    error: Command &quot;gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -fno-common -dynamic -DNDEBUG -g -fwrapv -O3 -Wall -arch arm64 -arch x86_64 -g -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-10.9-universal2-3.9/numpy/core/src/umath -Ibuild/src.macosx-10.9-universal2-3.9/numpy/core/src/npymath -Ibuild/src.macosx-10.9-universal2-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-10.9-universal2-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Users/thing/dev/SOTAMoon/venv/include -I/Library/Frameworks/Python.framework/Versions/3.9/include/python3.9 -Ibuild/src.macosx-10.9-universal2-3.9/numpy/core/src/common -Ibuild/src.macosx-10.9-universal2-3.9/numpy/core/src/npymath -c numpy/core/src/multiarray/descriptor.c -o build/temp.macosx-10.9-universal2-3.9/numpy/core/src/multiarray/descriptor.o -MMD -MF build/temp.macosx-10.9-universal2-3.9/numpy/core/src/multiarray/descriptor.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers -std=c99&quot; failed with exit status 1
    ----------------------------------------
    ERROR: Failed building wheel for numpy
  Failed to build numpy
  ERROR: Could not build wheels for numpy which use PEP 517 and cannot be installed directly
  ----------------------------------------
WARNING: Discarding https://files.pythonhosted.org/packages/a7/81/20d5d994c91ed8347efda90d32c396ea28254fd8eb9e071e28ee5700ffd5/h5py-3.1.0.tar.gz#sha256=1e2516f190652beedcb8c7acfa1c6fa92d99b42331cbef5e5c7ec2d65b0fc3c2 (from https://pypi.org/simple/h5py/) (requires-python:&gt;=3.6). Command errored out with exit status 1: /Users/thing/dev/SOTAMoon/venv/bin/python3 /private/var/folders/8k/z291bhgd5gs06tp9b4j6_bb40000gn/T/pip-standalone-pip-8qa87uy0/__env_pip__.zip/pip install --ignore-installed --no-user --prefix /private/var/folders/8k/z291bhgd5gs06tp9b4j6_bb40000gn/T/pip-build-env-bxy_nyoo/normal --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'numpy==1.12; python_version == &quot;3.6&quot;' 'numpy==1.19.3; python_version &gt;= &quot;3.9&quot;' 'numpy==1.14.5; python_version == &quot;3.7&quot;' pkgconfig 'Cython&gt;=0.29; python_version &lt; &quot;3.8&quot;' 'Cython&gt;=0.29.14; python_version &gt;= &quot;3.8&quot;' 'numpy==1.17.5; python_version == &quot;3.8&quot;' Check the logs for full command output.
ERROR: Could not find a version that satisfies the requirement h5py~=3.1.0 (from tensorflow-macos) (from versions: 2.2.1, 2.3.0b1, 2.3.0, 2.3.1, 2.4.0b1, 2.4.0, 2.5.0, 2.6.0, 2.7.0rc2, 2.7.0, 2.7.1, 2.8.0rc1, 2.8.0, 2.9.0rc1, 2.9.0, 2.10.0, 3.0.0rc1, 3.0.0, 3.1.0, 3.2.0, 3.2.1, 3.3.0)
ERROR: No matching distribution found for h5py~=3.1.0
</code></pre>
<p>Unfortunately I couldn't work out a solution to the 'faltivec' issue. I also found a <a href=""https://github.com/apple/tensorflow_macos"" rel=""noreferrer"">tensorflow package made by Apple</a> that seems specifically geared for M1's, however going by <a href=""https://towardsdatascience.com/installing-tensorflow-on-the-m1-mac-410bb36b776"" rel=""noreferrer"">this tutorial</a> produces the following issues:</p>
<pre><code>% pip install --upgrade --force --no-dependencies https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha3/tensorflow_addons_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl https://github.com/apple/tensorflow_macos/releases/download/v0.1alpha3/tensorflow_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl
ERROR: tensorflow_addons_macos-0.1a3-cp38-cp38-macosx_11_0_arm64.whl is not a supported wheel on this platform.
</code></pre>
<p>After this, I am a bit stuck. I have upgraded pip to <code>21.1.3</code>, and my python version is <code>Python 3.9.6</code>.</p>
",293895.0,,,,,2022-05-02 18:04:52,Installing Tensorflow on macOS on an Arm MBP,<tensorflow><apple-m1>,2,1,0.0,,,CC BY-SA 4.0
63909553,1,63915620.0,,2020-09-15 20:39:30,,6,4785,"<p>I ran into some code that gave data in the form of a <code>tensorflow.python.data.ops.dataset_ops.PrefetchDataset</code>. If I am being honest. I don't really understand how this breaks down past <code>data</code>. What does <code>ops</code>, <code>dataset_ops</code>, and <code>PrefetchDataset</code> mean generally, if anything at all</p>
",11945129.0,,,,,2020-09-16 09:06:25,What is tensorflow.python.data.ops.dataset_ops.PrefetchDataset?,<tensorflow>,1,0,,,,CC BY-SA 4.0
65081279,1,,,2020-11-30 21:15:42,,6,349,"<p>I have a <code>Tensorflow</code> model based on <code>BoostedTreesClassifier</code> and I want to deploy it on a mobile with the help of <code>Tensorflow Lite</code>.</p>
<p>However, when I try to convert my model to the <code>Tensorflow Lite</code> model I get an error saying that there are unsupported operations (as of <code>Tensorflow v2.3.1</code>):</p>
<pre><code>tf.BoostedTreesBucketize
tf.BoostedTreesEnsembleResourceHandleOp
tf.BoostedTreesPredict
tf.BoostedTreesQuantileStreamResourceGetBucketBoundaries
tf.BoostedTreesQuantileStreamResourceHandleOp
</code></pre>
<p>Adding <code>tf.lite.OpsSet.SELECT_TF_OPS</code> option helps a bit, but still some operations need a custom implementation:</p>
<pre><code>tf.BoostedTreesEnsembleResourceHandleOp
tf.BoostedTreesPredict
tf.BoostedTreesQuantileStreamResourceGetBucketBoundaries
tf.BoostedTreesQuantileStreamResourceHandleOp
</code></pre>
<p>I've also tried <code>Tensorflow v2.4.0-rc3</code>, which reduces the set to the following one:</p>
<pre><code>tf.BoostedTreesEnsembleResourceHandleOp
tf.BoostedTreesPredict
</code></pre>
<p>Conversion code is like the following:</p>
<pre><code>converter = tf.lite.TFLiteConverter.from_saved_model(model_path, signature_keys=['serving_default'])
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]

tflite_model = converter.convert()
</code></pre>
<p><code>signature_keys</code> is specified explicitly, because the model exported with <code>BoostedTreesClassifier#export_saved_model</code> has multiple signatures.</p>
<p>Is there a way to deploy this model on mobile other than writing custom implementation for non-supported ops?</p>
",3222695.0,,,,,2020-12-03 09:01:03,Missing some boosted trees operations in Tensorflow Lite,<tensorflow><machine-learning><tensorflow-lite>,0,5,0.0,,,CC BY-SA 4.0
64606267,1,65008315.0,,2020-10-30 10:05:34,,6,669,"<p>I am trying to use tensorflow/tfjs (TF) in a web-worker in an angular project.</p>
<p>Creating a web-worker using the <code>ng generate worker</code> command works just fine.</p>
<p>Importing TF in a component is fine too.</p>
<p>However importing TF in the worker i.e. :</p>
<pre><code>import * as tf from '@tensorflow/tfjs'
</code></pre>
<p>Generates a bunch of missing definition errors when building through the <code>ng build</code> command. Missing types are typically DOM-related types such as <code>error TS2304: Cannot find name ImageData | HTMLImageElement | HTMLCanvasElement | HTMLVideoElement</code>.
Those types are used in some definitions in TF and, as I understand, those types are not accessible with web-workers because <a href=""https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers"" rel=""nofollow noreferrer"">DOM manipulation cannot be done from workers</a>.
I am perfectly fine with that, my use of TF doesn't rely on those types. But I still need to find a way to build my worker.</p>
<p>I therefore tried to tinker with the <code>tsconfig.worker.json</code> file. My first attempt was to mimic the other tsconfig* files by adding &quot;dom&quot; in the <code>compilerOptions.lib</code> array :</p>
<pre><code>[&quot;es2018&quot;, &quot;webworker&quot;] 
</code></pre>
<p>replaced by</p>
<pre><code>[&quot;es2018&quot;, &quot;webworker&quot;, &quot;dom&quot;]
</code></pre>
<p>This results in conflicting types definitions</p>
<pre><code>error TS6200: Definitions of the following identifiers conflict with those in another file 
</code></pre>
<p>the <strong>webworker</strong> and <strong>dom</strong> libs have different definitions for the same types, but I can of course not remove the webworker lib reference.</p>
<p>My 2nd attempt was to add the <code>skipTypeCheck</code> compiler option in the <code>tsconfig.worker.json</code> file :
<strong>That works just fine</strong>, I got TF running in my web worker and outputting results.</p>
<p>BUT...</p>
<p>Skipping type checking feels like ripping of the whole idea of using typescript. So my question is :</p>
<p><strong>Is there a cleaner way to use TF in a webworker in angular while preserving type checking ?</strong></p>
<p>Thanks for your anwsers. Please let me know if I should provide more configuration details.</p>
",4315673.0,,4315673.0,,2020-11-16 17:41:05,2020-11-25 15:59:06,Angular typescript typecheck issues when importing tensorflow in web worker,<angular><typescript><tensorflow><web-worker><tsconfig>,1,0,0.0,,,CC BY-SA 4.0
64636038,1,,,2020-11-01 19:23:48,,6,7871,"<p>When I try to install <em>tensorflow</em> on <em>python 3.9</em> I get following error:</p>
<pre><code>ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow
</code></pre>
<p>Is not there any <em>tensorflow</em> for 3.9?
What do you guys recommend?
Can I install other version of python beside the existing version?</p>
",14559948.0,,521752.0,,2020-11-02 09:38:33,2022-11-21 03:14:13,Can't install tensorflow on python 3.9,<tensorflow><python-3.9>,3,1,,,,CC BY-SA 4.0
72783608,1,,,2022-06-28 09:03:14,,6,516,"<p>I have a multiple time series data that looks something like this:</p>
<pre><code>df = pd.DataFrame({'Time': np.tile(np.arange(5), 2),
                   'Object': np.concatenate([[i] * 5 for i in [1, 2]]),
                   'Feature1': np.random.randint(10, size=10),
                   'Feature2': np.random.randint(10, size=10)})

   Time  Object  Feature1  Feature2
0     0       1         3         3
1     1       1         9         2
2     2       1         6         6
3     3       1         4         0
4     4       1         7         7
5     0       2         4         8
6     1       2         3         7
7     2       2         1         1
8     3       2         7         5
9     4       2         1         7
</code></pre>
<p>where each object (1 and 2) has its own data (about 2000 objects in real data). I would like to feed this data chunkwise into RNN/LSTM using <code>tf.data.Dataset.window</code> in a way that different objects data don't come in one window like in this example:</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices(df)

for w in dataset.window(3, shift=1, drop_remainder=True):
  print(list(w.as_numpy_iterator()))
</code></pre>
<p>Output:</p>
<pre><code>[array([0, 1, 3, 3]), array([1, 1, 9, 2]), array([2, 1, 6, 6])]
[array([1, 1, 9, 2]), array([2, 1, 6, 6]), array([3, 1, 4, 0])]
[array([2, 1, 6, 6]), array([3, 1, 4, 0]), array([4, 1, 7, 7])]
[array([3, 1, 4, 0]), array([4, 1, 7, 7]), array([0, 2, 4, 8])] # Mixed data from both objects
[array([4, 1, 7, 7]), array([0, 2, 4, 8]), array([1, 2, 3, 7])] # Mixed data from both objects
[array([0, 2, 4, 8]), array([1, 2, 3, 7]), array([2, 2, 1, 1])]
[array([1, 2, 3, 7]), array([2, 2, 1, 1]), array([3, 2, 7, 5])]
[array([2, 2, 1, 1]), array([3, 2, 7, 5]), array([4, 2, 1, 7])]
</code></pre>
<p>Expected output:</p>
<pre><code>[array([0, 1, 3, 3]), array([1, 1, 9, 2]), array([2, 1, 6, 6])]
[array([1, 1, 9, 2]), array([2, 1, 6, 6]), array([3, 1, 4, 0])]
[array([2, 1, 6, 6]), array([3, 1, 4, 0]), array([4, 1, 7, 7])]
[array([0, 2, 4, 8]), array([1, 2, 3, 7]), array([2, 2, 1, 1])]
[array([1, 2, 3, 7]), array([2, 2, 1, 1]), array([3, 2, 7, 5])]
[array([2, 2, 1, 1]), array([3, 2, 7, 5]), array([4, 2, 1, 7])]
</code></pre>
<p>Maybe there is another way to do it. The main requirement that my model should see that non-mixed data chunks come from different objects (maybe via embedding).</p>
",8973620.0,,9657861.0,,2022-06-28 10:19:19,2022-06-30 18:42:25,Creating Tensorflow Dataset for mulitple time series,<python><tensorflow><deep-learning><time-series><tensorflow-datasets>,2,0,,,,CC BY-SA 4.0
63456427,1,65507427.0,,2020-08-17 18:12:48,,6,12597,"<p>I tried to run a model using TensorFlow Probability.<br>
But when I run it I got the error below:<br></p>
<pre><code>**ImportError: This version of TensorFlow Probability requires TensorFlow version &gt;= 2.3; Detected an installation of version 2.0.0-beta1. Please upgrade TensorFlow to proceed.**&lt;br&gt;
</code></pre>
<p>I cannot install TensorFlow 2.3 because it says there is no match.<br>
I have installed these libraries.<br></p>
<blockquote>
<p><strong>tensorflow (2.0.0b1)<br> tensorflow-estimator (1.14.0)<br> tensorflow-tensorboard (1.5.1)<br> tfp-nightly
(0.12.0.dev20200817)</strong><br></p>
</blockquote>
<p>Any suggestion how to fix it?</p>
<p>Thanks in advance</p>
",4593782.0,,9695098.0,,2020-08-18 03:27:28,2021-12-03 07:46:51,This version of TensorFlow Probability requires TensorFlow version >= 2.3,<python-3.x><tensorflow>,3,0,0.0,,,CC BY-SA 4.0
71969321,1,,,2022-04-22 13:10:29,,6,1450,"<p>I'm using tensorflow to open some .png images and every image it opens, an annoying message is printed.</p>
<pre><code>def open_img(path):
    img = tf.io.read_file(path)
    img = tf.io.decode_png(img)
    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])
</code></pre>
<p>Every time i try to open an image it says &quot;Cleanup called...&quot;, even while training:</p>
<p><a href=""https://i.stack.imgur.com/MxM7R.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/MxM7R.png"" alt=""enter image description here"" /></a></p>
<p>(This code is running on Kaggle)</p>
<p>tensorflow version: 2.6.3</p>
<p>How can i solve this annoying thing please?</p>
",11830394.0,,,,,2022-08-06 12:16:31,"Tensorflow ""decode_png"" keeps printing ""Cleanup called...""",<tensorflow><deep-learning><tensorflow2.0><kaggle>,3,0,,,,CC BY-SA 4.0
66087844,1,66183818.0,,2021-02-07 12:20:34,,6,2455,"<p>I am trying to speed up the segmentation model(unet-mobilenet-512x512). I converted my tensorflow model to tensorRT with FP16 precision mode. And the speed is lower than I expected.
Before the optimization i had 7FPS on inference with .pb frozen graph. After tensorRT oprimization I have 14FPS.</p>
<p>Here is benchmark results of Jetson NX from their site<br>
You can see, that unet 256x256 segmentation speed is 146 FPS. I thought, the speed of my unet512x512 should be 4 times slower in the worst case.</p>
<p><a href=""https://i.stack.imgur.com/Y7lmH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Y7lmH.png"" alt=""enter image description here"" /></a></p>
<p>Here is my code for optimizing tensorflow saved model using TensorRt:</p>
<pre><code>import numpy as np
from tensorflow.python.compiler.tensorrt import trt_convert as trt
import tensorflow as tf

params = trt.DEFAULT_TRT_CONVERSION_PARAMS
params = params._replace(
    max_workspace_size_bytes=(1&lt;&lt;32))
params = params._replace(precision_mode=&quot;FP16&quot;)
converter = tf.experimental.tensorrt.Converter(input_saved_model_dir='./model1', conversion_params=params)
converter.convert()

def my_input_fn():
  inp1 = np.random.normal(size=(1, 512, 512, 3)).astype(np.float32)
  yield [inp1]

converter.build(input_fn=my_input_fn)  # Generate corresponding TRT engines
output_saved_model_dir = &quot;trt_graph2&quot;
converter.save(output_saved_model_dir)  # Generated engines will be saved.


print(&quot;------------------------freezing the graph---------------------&quot;)


from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2

saved_model_loaded = tf.saved_model.load(
    output_saved_model_dir, tags=[tf.compat.v1.saved_model.SERVING])
graph_func = saved_model_loaded.signatures[
    tf.compat.v1.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
frozen_func = convert_variables_to_constants_v2(
    graph_func)
frozen_func.graph.as_graph_def()

tf.io.write_graph(graph_or_graph_def=frozen_func.graph,
                logdir=&quot;./&quot;,
                name=&quot;unet_frozen_graphTensorRt.pb&quot;,
                as_text=False)
</code></pre>
<p>I downloaded the repository, that was used for Jetson NX benchmarking ( <a href=""https://github.com/NVIDIA-AI-IOT/jetson_benchmarks"" rel=""noreferrer"">https://github.com/NVIDIA-AI-IOT/jetson_benchmarks</a> ), and the speed of unet256x256 really is ~146FPS. But there is no pipeline  to optimize the model.
How can I get the similar results? I am looking for the solutions to get speed of my model(unet-mobilenet-512x512) close to 30FPS<br>Maybe I should run inference in other way(without tensorflow) or change some converting parameters? <br>
Any suggestions, thanks</p>
",8885629.0,,,,,2021-03-18 07:19:04,Jetson NX optimize tensorflow model using TensorRT,<tensorflow><tensorrt><nvidia-jetson>,2,0,0.0,,,CC BY-SA 4.0
63823395,1,63833950.0,,2020-09-10 05:34:36,,6,5625,"<p>I would like to know how to obtain the total number of CUDA Cores in my GPU using Python, Numba and cudatoolkit.</p>
",2329045.0,,,,,2023-01-02 04:27:15,How can I get the number of CUDA cores in my GPU using Python and Numba?,<python><tensorflow><cuda><gpu><numba>,1,3,0.0,,,CC BY-SA 4.0
68988859,1,,,2021-08-30 18:35:37,,6,577,"<p>I have a Tensorflow multiclass classifier that is generating <code>nan</code> or <code>inf</code> while computing probabilities using <code>tf.nn.softmax</code>. See the following snippet (<code>logits</code> is of shape <code>batch_size x 6</code>, since I have 6 classes and the output is one-hot encoded). <code>batch_size</code> is 1024.</p>
<pre><code>logits = tf.debugging.check_numerics(logits, message='bad logits', name=None)
probabilities = tf.nn.softmax(logits=logits, name='Softmax')
probabilities = tf.debugging.check_numerics(probabilities, message='bad probabilities', name=None)
</code></pre>
<p>The classifier fails on the last statement as it finds <code>nan</code> or <code>inf</code> in <code>probabilities</code>. <code>logits</code> are clean, otherwise the first statement would have failed.</p>
<p>From what I read about <code>tf.nn.softmax</code>, it can handle very large and very small values in logits. I have verified this in interactive mode.</p>
<pre><code>&gt;&gt;&gt; with tf.Session() as s:
...   a = tf.constant([[1000, 10], [-100, -200], [3, 4.0]])
...   sm = tf.nn.softmax(logits=a, name='Softmax')
...   print(a.eval())
...   print(sm.eval())
...
[[1000.   10.]
 [-100. -200.]
 [   3.    4.]]
[[1.         0.        ]
 [1.         0.        ]
 [0.26894143 0.7310586 ]]
</code></pre>
<p>I then tried clipping the values in <code>logits</code> and the whole thing now works. See the modified snippet below.</p>
<pre><code>logits = tf.debugging.check_numerics(logits, message='logits', name=None)
safe_logits = tf.clip_by_value(logits, -15.0, 15.0)
probabilities = tf.nn.softmax(logits=safe_logits, name='Softmax')
probabilities = tf.debugging.check_numerics(probabilities, message='bad probabilities', name=None)
</code></pre>
<p>In second statement, I am clipping the values in <code>logits</code> to -15 and 15, and that somehow prevents <code>nan</code>/<code>inf</code> in softmax computation. So, I was able to fix the issue at hand.</p>
<p>However, I still don't understand why this clipping is working? (I should mention that clipping between -20 and 20 does not work and the model fails with <code>nan</code> or <code>inf</code> in <code>probabilities</code>).</p>
<p>Could someone help me understand why this is the case?</p>
<p>I am using tensorflow 1.15.0, running on a 64-bit instance.</p>
",1306819.0,,1306819.0,,2021-08-30 18:47:01,2021-09-03 09:09:46,Tensorflow issue with softmax,<python><tensorflow><softmax><numerical-stability>,1,4,,,,CC BY-SA 4.0
68398965,1,,,2021-07-15 18:18:06,,6,15855,"<p><a href=""https://i.stack.imgur.com/uXdSg.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uXdSg.jpg"" alt=""enter image description here"" /></a></p>
<p>how to convert a single COCO JSON annotation file into a YOLO darknet format?? like below
each individual image has separate filename.txt file
<a href=""https://i.stack.imgur.com/NeHLw.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/NeHLw.png"" alt=""enter image description here"" /></a></p>
",14408816.0,,,,,2022-11-14 07:28:26,COCO json annotation to YOLO txt format,<tensorflow><computer-vision><object-detection><yolo><coco>,4,0,,,,CC BY-SA 4.0
64867031,1,64914793.0,,2020-11-16 23:08:03,,6,8222,"<p>I am trying to train an object detection algorithm with samples that I have labeled using Label-img. My images have dimensions of 1100 x 1100 pixels. The algorithm I am using is the Faster R-CNN Inception ResNet V2 1024x1024, found on the <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"" rel=""noreferrer"">TensorFlow 2 Detection Model Zoo</a>. The specs of my operation are as follows:</p>
<ul>
<li>TensorFlow 2.3.1</li>
<li>Python 3.8.6</li>
<li>GPU: NVIDIA GEFORCE RTX 2060 (laptop has 16 GB RAM and 6 processing cores)</li>
<li>CUDA: 10.1</li>
<li>cuDNN: 7.6</li>
<li>Anaconda 3 command prompt</li>
</ul>
<p>The .config file is as follows:</p>
<pre><code># Faster R-CNN with Inception Resnet v2 (no atrous)
# Sync-trained on COCO (with 8 GPUs) with batch size 16 (800x1333 resolution)
# Initialized from Imagenet classification checkpoint
# TF2-Compatible, *Not* TPU-Compatible
#
# Achieves 39.6 mAP on COCO

model {
  faster_rcnn {
    num_classes: 1
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 800
        max_dimension: 1333
        pad_to_max_dimension: true
      }
    }
    feature_extractor {
      type: 'faster_rcnn_inception_resnet_v2_keras'
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 17
    maxpool_kernel_size: 1
    maxpool_stride: 1
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 1
  num_steps: 200000
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: 0.008
          total_steps: 200000
          warmup_learning_rate: 0.0
          warmup_steps: 5000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint_version: V2
  fine_tune_checkpoint: &quot;pre-trained-models/faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8/checkpoint/ckpt-0&quot;
  fine_tune_checkpoint_type: &quot;detection&quot;
  data_augmentation_options {
    random_horizontal_flip {
    }
  }

  data_augmentation_options {
    random_adjust_hue {
    }
  }

  data_augmentation_options {
    random_adjust_contrast {
    }
  }

  data_augmentation_options {
    random_adjust_saturation {
    }
  }

  data_augmentation_options {
     random_square_crop_by_scale {
      scale_min: 0.6
      scale_max: 1.3
    }
  }
}
train_input_reader: {
  label_map_path: &quot;annotations/label_map.pbtxt&quot;
  tf_record_input_reader {
    input_path: &quot;annotations/train.record&quot;
  }
}

eval_config: {
  metrics_set: &quot;coco_detection_metrics&quot;
  use_moving_averages: false
  batch_size: 1;
}

eval_input_reader: {
  label_map_path: &quot;annotations/label_map.pbtxt&quot;
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: &quot;annotations/test.record&quot;
  }
}
</code></pre>
<p>The following error is thrown after about 5 minutes of running:</p>
<pre><code>2020-11-16 16:52:14.415133: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:539 : Resource exhausted: OOM when allocating tensor with shape[64,288,9,9] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File &quot;model_main_tf2.py&quot;, line 113, in &lt;module&gt;
    tf.compat.v1.app.run()
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\platform\app.py&quot;, line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\absl\app.py&quot;, line 303, in run
    _run_main(main, args)
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\absl\app.py&quot;, line 251, in _run_main
    sys.exit(main(argv))
  File &quot;model_main_tf2.py&quot;, line 104, in main
    model_lib_v2.train_loop(
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\object_detection\model_lib_v2.py&quot;, line 639, in train_loop
    loss = _dist_train_step(train_input_iter)
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 780, in __call__
    result = self._call(*args, **kwds)
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\eager\def_function.py&quot;, line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 1843, in _filtered_call
    return self._call_flat(
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\eager\function.py&quot;, line 545, in call
    outputs = execute.execute(
  File &quot;C:\Users\user\anaconda3\envs\object_detection_api\lib\site-packages\tensorflow\python\eager\execute.py&quot;, line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted:  OOM when allocating tensor with shape[64,256,17,17] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node functional_3/conv2d_160/Conv2D (defined at \site-packages\object_detection\meta_architectures\faster_rcnn_meta_arch.py:1149) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

         [[Identity_1/_432]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted:  OOM when allocating tensor with shape[64,256,17,17] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
         [[node functional_3/conv2d_160/Conv2D (defined at \site-packages\object_detection\meta_architectures\faster_rcnn_meta_arch.py:1149) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored. [Op:__inference__dist_train_step_79248]

Errors may have originated from an input operation.
Input Source operations connected to node functional_3/conv2d_160/Conv2D:
 MaxPool2D/MaxPool (defined at \site-packages\object_detection\meta_architectures\faster_rcnn_meta_arch.py:1973)

Input Source operations connected to node functional_3/conv2d_160/Conv2D:
 MaxPool2D/MaxPool (defined at \site-packages\object_detection\meta_architectures\faster_rcnn_meta_arch.py:1973)

Function call stack:
_dist_train_step -&gt; _dist_train_step
</code></pre>
<p>A common solution to this problem is to reduce your batch size, but I have already reduced it to 1. <strong>Is the issue that I am out of memory for processing,</strong> or is there something else that could be done to fix this problem?</p>
<p>Note: Here is an output that was given right before the exception was thrown:</p>
<pre><code>2020-11-16 16:52:14.409101: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Stats:
Limit:                      4817616896
InUse:                      4809875456
MaxInUse:                   4817131776
NumAllocs:                       11104
MaxAllocSize:               4129325056
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2020-11-16 16:52:14.413310: W tensorflow/core/common_runtime/bfc_allocator.cc:439] ****************************************************************************************************
</code></pre>
",11059871.0,,11059871.0,,2020-11-17 14:57:21,2020-11-19 15:28:12,TensorFlow error: tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:539 : Resource exhausted,<python><tensorflow>,1,1,,,,CC BY-SA 4.0
65757008,1,,,2021-01-17 03:18:35,,6,2772,"<p>I'm using Tensorflow 2.4.0 on Macbook(arm64, M1 silicon), I get this output after I wanted to check if the GPU in M1 silicon can be used by Tensorflow:</p>
<p>My code:</p>
<pre><code>import tensorflow as tf
print(tf.test.gpu_device_name())
print(tf.config.list_physical_devices('GPU'))
</code></pre>
<p>The output:</p>
<pre><code>
[]
</code></pre>
<p>It looks like my GPU is unavailable. How should I use my GPU on M1 to accelerate trainning?</p>
",15021869.0,,15021869.0,,2021-01-17 03:33:42,2021-07-24 18:19:15,How can I check the GPU accessibility with Tensorflow on Mac of Apple M1 silicon?,<python><tensorflow><gpu><apple-silicon><apple-m1>,2,3,,,,CC BY-SA 4.0
65025263,1,,,2020-11-26 15:48:49,,6,340,"<p>I am currently trying to implement custom optimization for a custom tensorflow layer.
Without going in to much detail I have added a small code sample which illustrates how my current code works. The important part is that <code>calculate_gradients(variables, gradients, momentum)</code> is a function that requires the variable values and gradients of all the variables in the layer. Furthermore this calculation contains intermediate results which have to be stored during optimization. This explains the illustrative <code>momentum</code> variable. This behaviour to me makes using <code>@custom_gradient</code> not possible since this does not allow me to propagate this intermediate results to the optimizer which would then have to return it to the custom gradient function for use in the calculation of the next set of gradients. Unless someone knows how this would work (question one) i have not found a way around this.</p>
<pre><code>model = build_model()
for data, target in data:
    with tf.GradientTape() as tap:
        gradients = tape.gradient(loss(model(data), target), model.trainable_variables)
    for layer in model.layers:
        layer_gradients = gradients[indices] # actual indexing is not important
        new_gradients = calculate_gradients(layer.variables, layer_gradients, momentum)
        for variable, grad in zip(layer.variables, new_gradients):
            variable.assign(grad)
</code></pre>
<p>Trying to implement this in the tensorflow optimizer particularly by replacing <code>_resource_apply_dense</code> as shown in the documentation [1] i am running into some trouble with the layer-wise behaviour.
Particularly since <code>_resource_apply_dense</code> takes a variable and a gradient. The second code snippet illustrates what i am trying to to, but have currently not found a way to do the <code>get_other_variables_and_gradients(var)</code> behaviour. Furthermore this solution would calculate the gradients three times for each layer which is very suboptimal.</p>
<pre><code>def _resource_apply_dense(var, grad, apply_state):
    other_vars_and_grads = get_other_variables_and_gradients(var)
    calculate_gradients(zip((var, grad), other_vars_and_gards))
    var.assign(grad)

</code></pre>
<p>In short, my second question is: Does anyone have an idea how to implement this behaviour and maybe even better do it without redundant calculations or even a whole new better way. Currently the optimization works when i do everything in a training loop as shown in code snippet one. So this is merely a case of integration with the tensorflow optimizer paradigm and performance since doing everything very 'pythony' with lists in a large for loop is slow.</p>
<p>[1] <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer"" rel=""noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer</a></p>
",14691925.0,,,,,2021-06-09 07:39:05,Accessing gradient of multiple variables when applying resource [Tensorflow],<python><tensorflow><optimization><gradient>,1,1,0.0,,,CC BY-SA 4.0
63763809,1,63916790.0,,2020-09-06 11:47:57,,6,7560,"<p>I am following the TensorFlow 2 Object Detection API Tutorial on a Macbook</p>
<p>Here's what I got when running the given script for converting xmls to TFrecords</p>
<pre><code>Traceback (most recent call last):
  File &quot;generate_tfrecord.py&quot;, line 62, in &lt;module&gt;
    label_map_dict = label_map_util.get_label_map_dict(label_map)
  File &quot;/usr/local/lib/python3.8/site-packages/object_detection/utils/label_map_util.py&quot;, line 164, in get_label_map_dict
    label_map = load_labelmap(label_map_path)
  File &quot;/usr/local/lib/python3.8/site-packages/object_detection/utils/label_map_util.py&quot;, line 133, in load_labelmap
    label_map_string = fid.read()
  File &quot;/usr/local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py&quot;, line 116, in read
    self._preread_check()
  File &quot;/usr/local/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py&quot;, line 78, in _preread_check
    self._read_buf = _pywrap_file_io.BufferedInputStream(
TypeError: __init__(): incompatible constructor arguments. The following argument types are supported:
    1. tensorflow.python._pywrap_file_io.BufferedInputStream(arg0: str, arg1: int)

Invoked with: item {
  name: &quot;cat&quot;
  id: 1
}
, 524288
</code></pre>
<p>My label map file contains the following</p>
<pre><code>item {
    id: 1
    name: 'cat'
}
</code></pre>
",14229925.0,,14229925.0,,2020-09-06 14:17:18,2022-04-21 14:15:22,Error when converting xml files to tfrecord files,<python><tensorflow><object-detection>,4,2,0.0,,,CC BY-SA 4.0
69693757,1,,,2021-10-24 03:54:20,,6,515,"<p>I'm trying to load checkpoints and populate model weights using The Faster-RCNN architecture (<code>Faster R-CNN ResNet50 V1 640x640</code> to be precise, from <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"" rel=""noreferrer"">here</a>. I'm trying to load the weights for this network similar to how it's done in the <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb"" rel=""noreferrer"">example notebook for RetinaNet</a>, where they do the following:</p>
<pre class=""lang-py prettyprint-override""><code>fake_box_predictor = tf.compat.v2.train.Checkpoint(
    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,
    _box_prediction_head=detection_model._box_predictor._box_prediction_head,
)

fake_model = tf.compat.v2.train.Checkpoint(
          _feature_extractor=detection_model._feature_extractor,
          _box_predictor=fake_box_predictor
)

ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)
ckpt.restore(checkpoint_path).expect_partial()
</code></pre>
<p>I'm trying to get a similar checkpoint loading mechanism going for the Faster-RCNN network I want to use, but the properties like <code>_base_tower_layers_for_heads</code>, <code>_box_prediction_head</code> only exist for the architecture used in the example, and not for anything else.</p>
<p>I also couldn't find documentation on which parts of the model to populate using <code>Checkpoint</code> for my particular use case. Would greatly appreciate any help on how to approach this!</p>
",6274300.0,,,,,2021-11-01 08:07:00,Loading checkpoints while training a Faster-RCNN model on a custom dataset,<python><tensorflow><transfer-learning><tensorflow-model-garden>,1,0,,,,CC BY-SA 4.0
68923942,1,,,2021-08-25 13:34:38,,5,6185,"<p>I have three different .csv datasets that I typically read using pandas and train deep learning models with. Each data is a n by m matrix where n is the number of samples and m is the number of features. After reading the data, I do some reshaping and then feed them to my deep learning model using <code>feed_dict</code>:</p>
<pre><code>data1 = pd.DataFrame(np.random.uniform(low=0, high=1, size=(10,3)), columns=['A', 'B', 'C'])
data2 = pd.DataFrame(np.random.uniform(low=0, high=1, size=(10,3)), columns=['A', 'B', 'C'])
data3 = pd.DataFrame(np.random.uniform(low=0, high=1, size=(10,3)), columns=['A', 'B', 'C'])

data = pd.concat([data1, data2, data2], axis=1)

# Some deep learning model that work with data
# An optimizer

with tf.compat.v1.Session() as sess:
     sess.run(init)
     sess.run(optimizer, feed_dict={SOME VARIABLE: data})  
</code></pre>
<p>However my data is too big to fit in memory now and I am wondering how can I use tf.data to read the data instead of using pandas. Sorry if the script I've provided is a pseudo-code and not my actual code.</p>
",2056969.0,,,,,2021-08-25 17:17:15,How to use tf.data in tensorflow to read .csv files?,<python><tensorflow><deep-learning><tensorflow2.0><tf.data.dataset>,1,0,,,,CC BY-SA 4.0
63325216,1,63384123.0,,2020-08-09 10:33:34,,5,2725,"<p>I have a saved tensorflow model the same as all models in the <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md"" rel=""nofollow noreferrer"">model zoo</a>.</p>
<p>I want to convert it to tesorflow lite, I find the following way from tensorflow github (my tensorflw version is 2):</p>
<pre><code>!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz 
# extract the downloaded file
!tar -xzvf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz
    
</code></pre>
<blockquote>
</blockquote>
<pre><code>!pip install tf-nightly
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model('ssd_mobilenet_v2_320x320_coco17_tpu-8/saved_model')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()

open(&quot;m.tflite&quot;, &quot;wb&quot;).write(tflite_model)
</code></pre>
<p>But the output and input shape of the converted model don't match the original model, check the following:</p>
<ul>
<li><strong>Original Model Input &amp; Output shape</strong></li>
</ul>
<p><a href=""https://i.stack.imgur.com/yE8Pg.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/yE8Pg.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><strong>Converted Model Input &amp; Output shape</strong></li>
</ul>
<p><a href=""https://i.stack.imgur.com/AFiof.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/AFiof.png"" alt=""enter image description here"" /></a></p>
<p>So there is a problem here! the input / output shape should be matched the original model!
Any idea?</p>
",7032062.0,,7032062.0,,2020-08-09 14:34:19,2020-12-16 13:33:29,What is the right way to convert saved tensorflow model to tensorflow Lite,<tensorflow><tensorflow2.0><tensorflow-lite>,3,0,0.0,,,CC BY-SA 4.0
68418716,1,,,2021-07-17 08:21:38,,5,371,"<p>can you suggest some alternative of stable baselines that I can use to train my agent in reinforcement learning.</p>
<p>P.s. I'm using gym mini-grid environment so tell me those who work in this environment.</p>
",12892798.0,,,,,2021-07-17 08:21:38,Alternatives of Stable Baselines3,<python><tensorflow><openai-gym><stable-baselines>,0,0,,,,CC BY-SA 4.0
67789639,1,68719179.0,,2021-06-01 13:19:27,,5,384,"<p>I export some fairly large Pandas dataframes to Tensorflow's serialized format. And I do it often and it's really slow. Which is probably because I have to serialize the individual examples idk. Also, I compress the files with the &quot;GZIP&quot; option.</p>
<p>I have found some options for the TFRecordWriter in the documentation that look like they might help (buffers help, right?).
But there is no explanation of what <code>input_buffer_size</code> does or what range the values might take. Is it {0, 1, 2, 3} or a couple of million? Or do I want <code>output_buffer_size</code> or <code>mem_level</code> or something else?</p>
<p>From the <a href=""https://www.tensorflow.org/api_docs/python/tf/io/TFRecordOptions"" rel=""noreferrer"">Tensorflow 2.5 documentation</a>:</p>
<pre><code>Args
compression_type        &quot;GZIP&quot;, &quot;ZLIB&quot;, or &quot;&quot; (no compression).
flush_mode              flush mode or None, Default: Z_NO_FLUSH.
input_buffer_size       int or None.
output_buffer_size      int or None.
window_bits             int or None.
compression_level       0 to 9, or None.
compression_method      compression method or None.
mem_level               1 to 9, or None.
compression_strategy    strategy or None. Default: Z_DEFAULT_STRATEGY.
</code></pre>
",3406189.0,,,,,2021-08-09 22:22:56,What do the arguments for TFRecordOptions actually mean (wrt tf.io.TFRecordWriter)?,<python><tensorflow><tfrecord>,1,4,,,,CC BY-SA 4.0
74949556,1,,,2022-12-29 09:49:08,,5,1901,"<p>I've got a poetry project. My environment is Conda 22.9.0 on a windows machine with poetry version 1.2.2:</p>
<p>This is my pyproject.toml file:</p>
<pre><code>[tool.poetry]
name = &quot;myproject&quot;
version = &quot;0.1.0&quot;
description = &quot;&quot;

[tool.poetry.dependencies]
# REVIEW DEPENDENCIES
python = &quot;&gt;=3.7,&lt;3.11&quot;
numpy = &quot;*&quot;
tensorflow = &quot;^2.8&quot;

[build-system]
requires = [&quot;poetry&gt;=0.12&quot;]
build-backend = &quot;poetry.masonry.api&quot;

[tool.poetry.scripts]
start = &quot;myproject.main:start&quot;
</code></pre>
<p>The myproject\main.py module contains:</p>
<pre><code>import tensorflow as tf

def start():
    if tf.test.is_gpu_available():
        print(&quot;TensorFlow is using a GPU.&quot;)
    else:
        print(&quot;TensorFlow is NOT using a GPU.&quot;)
</code></pre>
<p>If I do <code>poetry install</code>, it seems to work fine:</p>
<pre><code>Creating virtualenv myproject in D:\Projects\myproject\dev\myproject-series-forecast\.venv
Updating dependencies
Resolving dependencies...

Writing lock file

Package operations: 41 installs, 0 updates, 0 removals

• Installing certifi (2022.12.7)
• Installing charset-normalizer (2.1.1)
• Installing idna (3.4)
• Installing pyasn1 (0.4.8)
• Installing urllib3 (1.26.13)
• Installing cachetools (5.2.0)
• Installing oauthlib (3.2.2)
• Installing rsa (4.9)
• Installing six (1.16.0)
• Installing zipp (3.11.0)
• Installing requests (2.28.1)
• Installing pyasn1-modules (0.2.8)
• Installing google-auth (2.15.0)
• Installing importlib-metadata (5.2.0)
• Installing requests-oauthlib (1.3.1)
• Installing markupsafe (2.1.1)
• Installing absl-py (1.3.0)
• Installing grpcio (1.51.1)
• Installing numpy (1.21.6)
• Installing tensorboard-data-server (0.6.1)
• Installing markdown (3.4.1)
• Installing tensorboard-plugin-wit (1.8.1)
• Installing protobuf (3.19.6)
• Installing werkzeug (2.2.2)
• Installing google-auth-oauthlib (0.4.6)
• Installing astunparse (1.6.3)
• Installing flatbuffers (22.12.6)
• Installing gast (0.4.0)
• Installing google-pasta (0.2.0)
• Installing h5py (3.7.0)
• Installing keras (2.11.0)
• Installing tensorflow-estimator (2.11.0)
• Installing packaging (22.0)
• Installing opt-einsum (3.3.0)
• Installing libclang (14.0.6)
• Installing tensorboard (2.11.0)
• Installing tensorflow-io-gcs-filesystem (0.29.0)
• Installing termcolor (2.1.1)
• Installing typing-extensions (4.4.0)
• Installing wrapt (1.14.1)
• Installing tensorflow (2.11.0)

Installing the current project: myproject (0.1.0)
</code></pre>
<p>But when executing <code>poetry run start</code> I got error in import</p>
<pre><code>Traceback (most recent call last):
File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
File &quot;D:\Python\Anaconda3\lib\importlib\__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1030, in _gcd_import
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1007, in _find_and_load
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 986, in _find_and_load_unlocked
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 680, in _load_unlocked
File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 850, in exec_module
File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 228, in _call_with_frames_removed
File &quot;D:\Projects\myproject\dev\myproject-series-forecast\myproject\main.py&quot;, line 3, in &lt;module&gt;
    import tensorflow as tf
ModuleNotFoundError: No module named 'tensorflow'
</code></pre>
",3375378.0,,,,,2023-06-14 21:15:21,Poetry fails to install tensorflow,<python><tensorflow><python-poetry>,2,1,,,,CC BY-SA 4.0
69295934,1,,,2021-09-23 07:48:17,,5,281,"<p>How do I subtract a tensor from ragged tensor?</p>
<p>Example:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow as tf    # TensorFlow 2.6

X = tf.ragged.constant([[[3, 1], [3]],
                        [[2], [3, 4]]], ragged_rank=2)
y = tf.constant([[1], [2]])
X-y
</code></pre>
<p>Expected result:</p>
<pre class=""lang-py prettyprint-override""><code>[[[2, 0], [1]],
 [[1], [1, 2]]]
</code></pre>
<p>However, it returns an error:</p>
<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'Unable to broadcast: dimension size mismatch in dimension'
1
b'lengths='
2
b'dim_size='
2, 2
</code></pre>
<p>I know I can do it row-by-row:</p>
<pre class=""lang-py prettyprint-override""><code>result = []
if X.shape[0] is not None:  # Placeholders have None everywhere -&gt; range(None) raises an exception -&gt; this condition
    for row in range(X.shape[0]):
        result.append(X[row] - y)
    result = tf.stack(result)
</code></pre>
<p>However, this works only in the eager mode - in graph mode, I get:</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: No gradients provided for any variable
</code></pre>
<p>because the code gets executed only conditionally...</p>
<p>What works is to hard-code the count of rows:</p>
<pre class=""lang-py prettyprint-override""><code>for row in range(2):
    result.append(X[row] - y)
result = tf.stack(result)
</code></pre>
<p>But that doesn't generalize well.</p>
<p>I also know I can write:</p>
<pre class=""lang-py prettyprint-override""><code>X - tf.expand_dims(y, axis=1)
</code></pre>
<p>But that returns a result for &quot;transposed&quot; y:</p>
<pre class=""lang-py prettyprint-override""><code>[[[2, 0], [2]],
 [[0], [1, 2]]]
</code></pre>
<p>I also know I can also use:</p>
<pre class=""lang-py prettyprint-override""><code>def subtract(x):
    return x - y

tf.map_fn(subtract, X)
</code></pre>
<p>But when using the result in graph mode, I get:</p>
<pre><code>ValueError: Unable to broadcast: unknown rank
</code></pre>
",824276.0,,1699075.0,,2022-09-07 01:20:21,2023-03-11 02:10:07,TensorFlow broadcasting of RaggedTensor,<python><tensorflow><tensorflow2.0>,3,0,,,,CC BY-SA 4.0
70567700,1,,,2022-01-03 15:06:22,,5,102,"<p>I'm using Electron + ReactJS and Tenserflow.</p>
<p>I want to have like a collection of 500-1000 words like 'dog', 'newline', 'cat' be recognized when i talk.</p>
<ol>
<li>How much time can it take for the model to be trained with 500 words? I used 5 words and it took a bit of time. I don't want to have a loader and to take too much time to train on client. Can i train the model on server and fetch it to the user, or do i need to train it everytime he enters the app?</li>
<li>I tried using model training but it doesn't work. Also i'm not even talking and it shows random words. I didn't find much information about model training in tenserflow javascript. If collectExample just transfers the words, how can i train the model with custom words?</li>
</ol>
<p>Also i'm quite new to Tenserflow. Here is the code:</p>
<pre><code>const loadModel = async () =&gt; {
    setLoading(true);

    // start loading model
    const recognizer = await speech.create('BROWSER_FFT');
    // check if model is loaded
    await recognizer.ensureModelLoaded();
    
    const transferRecognizer = recognizer.createTransfer('programming');
    await transferRecognizer.collectExample('cat');
    await transferRecognizer.collectExample('dog');
    await transferRecognizer.collectExample('newline');

    await transferRecognizer.collectExample('_background_noise_');
    await transferRecognizer.collectExample('newline');
    await transferRecognizer.collectExample('dog');
    await transferRecognizer.collectExample('cat');
    await transferRecognizer.collectExample('_background_noise_');

    await transferRecognizer.train({
      epochs: 25,
      callback: {
        onEpochEnd: async (epoch, logs) =&gt; {
          console.log(`Epoch ${epoch}: loss=${logs.loss}, accuracy=${logs.acc}`);
        }
      }
    });

    setModel(transferRecognizer);
    // store command word list to state
    console.log('transferRecognizer.wordLabels():', transferRecognizer.wordLabels());
    setLabels(transferRecognizer.wordLabels());

    setLoading(false);
  };

</code></pre>
<pre><code>
  const recognizeCommands = async () =&gt; {
    model?.listen(
      result =&gt; {
        // add argMax function
        setAction(labels[argMax(Object.values(result.scores))]);
      },
      { includeSpectrogram: true, probabilityThreshold: 0.9 }
    );
  };

</code></pre>
",17113659.0,,17113659.0,,2022-01-03 18:56:56,2022-01-03 18:56:56,Training Tenserflow Model for Speech Recognition in React,<tensorflow><speech-to-text>,0,1,,,,CC BY-SA 4.0
75037100,1,,,2023-01-06 23:45:38,,5,1147,"<p>I am trying to run tensorflow using my GPU and have followed the instructions at this <a href=""https://www.tensorflow.org/install/pip"" rel=""noreferrer"">link</a>. After running the commands in Step 6, I get the proper output.</p>
<p>Then, when I try to run an actual model I am trying to build, I get the following error.</p>
<pre><code>2023-01-06 18:39:14.692537: W tensorflow/compiler/xla/service/gpu/nvptx_helper.cc:56] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.2
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2023-01-06 18:39:14.693094: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-01-06 18:39:14.693196: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-01-06 18:39:14.693275: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-01-06 18:39:14.704458: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-01-06 18:39:14.704603: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
Traceback (most recent call last):
  File &quot;/home/jerry/Woodburn/Woodburn_Model/model/main/Model_Main.py&quot;, line 42, in &lt;module&gt;
    main(sys.argv[1:])
  File &quot;/home/jerry/Woodburn/Woodburn_Model/model/main/Model_Main.py&quot;, line 27, in main
    model.train()
  File &quot;/home/jerry/Woodburn/Woodburn_Model/model/main/Model_V5.py&quot;, line 99, in train
    history = self.model.fit(x, y, batch_size = batchSize, epochs = epochs)
  File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/tensorflow/python/eager/execute.py&quot;, line 52, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_10' defined at (most recent call last):
    File &quot;/home/jerry/Woodburn/Woodburn_Model/model/main/Model_Main.py&quot;, line 42, in &lt;module&gt;
      main(sys.argv[1:])
    File &quot;/home/jerry/Woodburn/Woodburn_Model/model/main/Model_Main.py&quot;, line 27, in main
      model.train()
    File &quot;/home/jerry/Woodburn/Woodburn_Model/model/main/Model_V5.py&quot;, line 99, in train
      history = self.model.fit(x, y, batch_size = batchSize, epochs = epochs)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/utils/traceback_utils.py&quot;, line 65, in error_handler
      return fn(*args, **kwargs)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/engine/training.py&quot;, line 1650, in fit
      tmp_logs = self.train_function(iterator)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/engine/training.py&quot;, line 1249, in train_function
      return step_function(self, iterator)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/engine/training.py&quot;, line 1233, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/engine/training.py&quot;, line 1222, in run_step
      outputs = model.train_step(data)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/engine/training.py&quot;, line 1027, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 527, in minimize
      self.apply_gradients(grads_and_vars)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1140, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 634, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1166, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1216, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File &quot;/home/jerry/miniconda3/envs/tensorflow_gpu/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py&quot;, line 1211, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_10'
libdevice not found at ./libdevice.10.bc
         [[{{node StatefulPartitionedCall_10}}]] [Op:__inference_train_function_8591]
</code></pre>
<p>After doing some research, it appears that the relevant errors are the following:</p>
<pre><code>2023-01-06 18:39:14.692537: W tensorflow/compiler/xla/service/gpu/nvptx_helper.cc:56] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
Searched for CUDA in the following directories:
  ./cuda_sdk_lib
  /usr/local/cuda-11.2
  /usr/local/cuda
  .
You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2023-01-06 18:39:14.693094: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-01-06 18:39:14.693196: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
2023-01-06 18:39:14.693275: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-01-06 18:39:14.704458: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-01-06 18:39:14.704603: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
Traceback (most recent call last):
</code></pre>
<p>For context, this is running in Ubuntu 20.04 and python 3.9. Any ideas on how to fix?</p>
",20124380.0,,,,,2023-01-06 23:45:38,Libdevice not found Tensorflow,<tensorflow>,0,4,,,,CC BY-SA 4.0
63212749,1,,,2020-08-02 05:38:56,,5,1541,"<p>I am interested in running deep learning code on my Ryzen 3400g system.  Online I saw:</p>
<blockquote>
<p>I trying to use Vega 11 on my Ryzen 3400G, I'm aware that I cannot run
HIP, but I possibly could run opencl on this APU, my first step is to
import tensorflow, but I got an error,</p>
</blockquote>
<p>This is from <a href=""https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/669"" rel=""noreferrer"">https://github.com/ROCmSoftwarePlatform/tensorflow-upstream/issues/669</a></p>
<p>Is it really impossible to use the ryzen 3400g with Radeon vega 11 gpu with tensorflow?</p>
<p>—————</p>
<p>Could it be made to work with opencl instead (as in <a href=""https://missinglink.ai/guides/tensorflow/tensorflow-support-opencl/"" rel=""noreferrer"">https://missinglink.ai/guides/tensorflow/tensorflow-support-opencl/</a> )?</p>
",1473517.0,,1473517.0,,2020-08-08 07:41:10,2020-08-08 07:41:10,What is HIP and why can’t you run it on the ryzen 3400g?,<tensorflow><gpu><opencl><amd-rocm>,1,0,0.0,,,CC BY-SA 4.0
70527245,1,,,2021-12-30 03:42:48,,5,275,"<p>I'm following the <a href=""https://js.tensorflow.org/api_react_native/0.2.1/#cameraWithTensors"" rel=""noreferrer"">cameraWithTensors example code</a> with React Native
but getting a weird error that the camera tensors are 'Tensor' not Tensor or TensoreLike</p>
<pre><code>  handleCameraStream(images, updatePreview, gl) {
    const loop = async () =&gt; {
      const nextImageTensor = images.next().value;

      nextImageTensor.toFloat();
      // throws [Error: Argument 'x' passed to 'cast' must be a Tensor or TensorLike, but got 'Tensor']

      nextImageTensor.expandDims(0);
      // throws [Error: Argument 'x' passed to 'expandDims' must be a Tensor or TensorLike, but got 'Tensor']

      // Solved both of it using tf.func but my model is giving a similar error now
      model.predict( tf.expandDims( tf.cast(nextImageTensor, 'float32'), 0) );
      // throws [Error: Argument 'x' passed to 'stridedSlice' must be a Tensor or TensorLike, but got 'Tensor']


      //requestAnimation(loop);
    }
    loop();
  }

return &lt;View&gt;
     &lt;TensorCamera
      // Standard Camera props
      style={styles.camera}
      type={Camera.Constants.Type.back}
      // Tensor related props
      cameraTextureHeight={textureDims.height}
      cameraTextureWidth={textureDims.width}
      resizeHeight={640}
      resizeWidth={640}
      resizeDepth={3}
      onReady={handleCameraStream}
      autorender={true}
     /&gt;
   &lt;/View&gt;
</code></pre>
<p>Not sure what this error means. I tried printing out a basic tensor and the camera tensor, they both look similar</p>
<pre><code>console.log(tf.tensor4d([[
          [[1, 3], [2, 8]],
          [[3, 9], [4, 2]]
      ]]))
// {&quot;dataId&quot;: {&quot;id&quot;: 247}, &quot;dtype&quot;: &quot;float32&quot;, &quot;id&quot;: 253, &quot;isDisposedInternal&quot;: false, &quot;kept&quot;: false, &quot;rankType&quot;: &quot;4&quot;, &quot;shape&quot;: [1, 2, 2, 2], &quot;size&quot;: 8, &quot;strides&quot;: [8, 4, 2]}

console.log( tf.expandDims( tf.cast(nextImageTensor, 'float32'), 0))
// {&quot;dataId&quot;: {&quot;id&quot;: 246}, &quot;dtype&quot;: &quot;float32&quot;, &quot;id&quot;: 255, &quot;isDisposedInternal&quot;: false, &quot;kept&quot;: false, &quot;rankType&quot;: &quot;4&quot;, &quot;scopeId&quot;: 14, &quot;shape&quot;: [1, 640, 640, 3], &quot;size&quot;: 1228800, &quot;strides&quot;: [1228800, 1920, 3]}
</code></pre>
<p>Does anyone know what's going on here?</p>
",17793371.0,,,,,2022-02-20 22:13:03,"TensorFlow.js TensorCamera Error: Argument 'x' passed to 'cast' must be a Tensor or TensorLike, but got 'Tensor'",<python><react-native><tensorflow><tensorflow2.0><tensor>,0,1,0.0,,,CC BY-SA 4.0
71459009,1,,,2022-03-13 17:09:59,,5,784,"<p>So I code a Transformers neural network that works as an ASR, it works, it trains good and saved the model as...</p>
<pre><code>model.save(&quot;savedmodel.model&quot;)
</code></pre>
<p>The problem is that when I want to predict, I do this..</p>
<pre><code>speech_model = load_model('D:\DOT\Speechrecognition\speechrecognitionE.model')

path = &quot;D:\DOT\Speechrecognition\Data\LJSpeech-1.1\wavs\LJ001-0001.wav&quot;

def path_to_audio(path):
    # spectrogram using stft
    audio = tf.io.read_file(path)
    audio, _ = tf.audio.decode_wav(audio, 1)
    audio = tf.squeeze(audio, axis=-1)
    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)
    x = tf.math.pow(tf.abs(stfts), 0.5)
    # normalisation
    means = tf.math.reduce_mean(x, 1, keepdims=True)
    stddevs = tf.math.reduce_std(x, 1, keepdims=True)
    x = (x - means) / stddevs
    audio_len = tf.shape(x)[0]
    # padding to 10 seconds
    pad_len = 2754
    paddings = tf.constant([[0, pad_len], [0, 0]])
    x = tf.pad(x, paddings, &quot;CONSTANT&quot;)[:pad_len, :]
    return x

x = path_to_audio(path)
#print(x)
speech_model.predict(x)
</code></pre>
<p>The path to audio function, converts the audio path to an spectrogram, in the training model it receive audio spectrograms as inputs, but it show this error..</p>
<pre><code>    Traceback (most recent call last):
File &quot;C:\Users\berna\Desktop\Programming\AI_ML_DL\Projects\DOT\DOT-alpha.py&quot;, line 72, in &lt;module&gt;
    speech_model.predict(x)
File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1621, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1611, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1604, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1572, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer &quot;transformer&quot; (type Transformer).

    Could not find matching concrete function to call loaded from the SavedModel. Got:
    Positional arguments (2 total):
        * Tensor(&quot;inputs:0&quot;, shape=(None, 129), dtype=float32)
        * False
    Keyword arguments: {}

    Expected these arguments to match one of the following 4 option(s):

    Option 1:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='inputs/1')]
        * False
    Keyword arguments: {}

    Option 2:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='inputs/1')]
        * True
    Keyword arguments: {}

    Option 3:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='input_2')]
        * False
    Keyword arguments: {}

    Option 4:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='input_2')]
        * True
    Keyword arguments: {}

    Call arguments received:
    • args=('tf.Tensor(shape=(None, 129), dtype=float32)',)
    • kwargs={'training': 'False'}
</code></pre>
<p>What does that means? what is wrong with the prediction?</p>
",,user18455367,,,,2022-03-13 17:09:59,"ValueError: Exception encountered when calling layer ""transformer"" (type Transformer)",<python><tensorflow><transformer-model>,0,12,0.0,,,CC BY-SA 4.0
68857401,1,,,2021-08-20 05:31:39,,5,1104,"<p>I'm a newbie to ML. When trying to complete digit recognition with TPU method, I encountered following problems.</p>
<pre><code>resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)
</code></pre>
<pre><code>with strategy.scope():
    Model = Sequential([

        InputLayer((28, 28, 1)),
        Dropout(0.1),
        Conv2D(128, 3, use_bias=False),
        LeakyReLU(0.05),
        BatchNormalization(),
        MaxPooling2D(2, 2),
        Conv2D(64, 3, use_bias=False),
        LeakyReLU(0.05),
        BatchNormalization(),
        MaxPooling2D(2, 2),
        Flatten(),
        Dense(128, use_bias=False),
        LeakyReLU(0.05),
        BatchNormalization(),
        Dense(10, activation='softmax')

    ])

with strategy.scope():
    Model.compile(optimizer='adam',
                  loss='categorical_crossentropy', metrics='accuracy') 
</code></pre>
<pre><code>CancelledError: 4 root error(s) found.
  (0) Cancelled:  Operation was cancelled
     [[node IteratorGetNextAsOptional_1 (defined at &lt;ipython-input-31-44edcf0f3ea7&gt;:3) ]]
  (1) Cancelled:  Iterator was cancelled
     [[node IteratorGetNextAsOptional_6 (defined at &lt;ipython-input-31-44edcf0f3ea7&gt;:3) ]]
  (2) Cancelled:  Operation was cancelled
     [[node IteratorGetNextAsOptional_3 (defined at &lt;ipython-input-31-44edcf0f3ea7&gt;:3) ]]
  (3) Cancelled:  Iterator was cancelled
     [[node IteratorGetNextAsOptional_5 (defined at &lt;ipython-input-31-44edcf0f3ea7&gt;:3) ]]
0 successful operations.
5 derived errors ignored. [Op:__inference_train_function_23675]

Function call stack:
train_function -&gt; train_function -&gt; train_function -&gt; train_function
</code></pre>
<p>Then I run it again</p>
<pre><code>UnavailableError: 9 root error(s) found.
  (0) Unavailable:  failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{&quot;created&quot;:&quot;@1629436055.354219684&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc&quot;,&quot;file_line&quot;:4143,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1629436055.354217763&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc&quot;,&quot;file_line&quot;:398,&quot;grpc_status&quot;:14}]}
     [[{{node MultiDeviceIteratorGetNextFromShard}}]]
     [[RemoteCall]]
     [[IteratorGetNextAsOptional]]
     [[cond_11/switch_pred/_107/_78]]
  (1) Unavailable:  failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{&quot;created&quot;:&quot;@1629436055.354219684&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc&quot;,&quot;file_line&quot;:4143,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1629436055.354217763&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc&quot;,&quot;file_line&quot;:398,&quot;grpc_status&quot;:14}]}
     [[{{node MultiDeviceIteratorGetNextFromShard}}]]
     [[RemoteCall]]
     [[IteratorGetNextAsOptional]]
     [[TPUReplicate/_compile/_7290104207349758044/_4/_178]]
  (2) Unavailable:  failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{&quot;created&quot;:&quot;@1629436055.354219684&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc&quot;,&quot;file_line&quot;:4143,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1629436055.354217763&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc&quot;,&quot;file_line&quot;:398,&quot;grpc_status&quot;:14}]}
     [[{{node MultiDeviceIteratorGetNextFromShard}}]]
     [[RemoteCall]]
     [[IteratorGetNextAsOptional]]
     [[tpu_compile_succeeded_assert/_13543899577889784813/_5/_281]]
  (3) Unavailable:  failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{&quot;created&quot;:&quot;@1629436055.354219684&quot;,&quot;description&quot;:&quot;Failed to pick subchannel&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc&quot;,&quot;file_line&quot;:4143,&quot;referenced_errors&quot;:[{&quot;created&quot;:&quot;@1629436055.354217763&quot;,&quot;description&quot;:&quot;failed to connect to all addresses&quot;,&quot;file&quot;:&quot;third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc&quot;,&quot;file_line&quot;:398,&quot;grpc_status&quot;:14}]}
     [[{{node MultiDeviceIteratorGetNextFromShard}}]]
     [[RemoteCall]]
     [[IteratorGetNextAsOptional]]
     [[strided_slice_37 ... [truncated] [Op:__inference_train_function_6939]

Function call stack:
train_function -&gt; train_function -&gt; train_function -&gt; train_function
</code></pre>
<p>Must be somewhere missing <code>strategy.scopy():</code></p>
<p>I succeeded in other notebooks but they are all <code>tf.data.Dataset</code></p>
<p>Though, I still can't figure out this out.</p>
<p>Full code is at
<a href=""https://www.kaggle.com/dacianpeng/digit-hello-world?scriptVersionId=72464286"" rel=""nofollow noreferrer"">https://www.kaggle.com/dacianpeng/digit-hello-world?scriptVersionId=72464286</a></p>
<p><code>Version 6</code> is the TPU version. And only modified from <code>Version 5</code> with codes above.</p>
",16676150.0,,16676150.0,,2023-03-07 05:52:02,2023-03-07 05:52:02,Kaggle TPU Unavailable: failed to connect to all addresses,<tensorflow><tpu>,2,0,,,,CC BY-SA 4.0
63243438,1,63244644.0,,2020-08-04 08:59:47,,5,2994,"<p>I'm trying to load the &quot;iris&quot; dataset directly from tensorflow datasets and I'm stuck.
I'm use to working with CSVs.</p>
<pre><code>import tensorflow as tf
import tensorflow_datasets as tfds

data = tfds.load(&quot;iris&quot;,split='train[:80%]', as_supervised=True)
data = data.batch(10)
features, labels = data
</code></pre>
<p>I don't know how I'm supposed to separate the features X,y. The labels are in a different tensor from the features, but I don't know how to access them to work with. I'd like to one hot encode the labels and feed them into the model, but I'm stuck here.</p>
<p>The tensorflow docs are sparse with info on how to do this. any help is much appreciated</p>
",14046687.0,,10908375.0,,2020-08-04 13:53:00,2020-08-04 13:53:00,"How to load Tensorflow Dataset ""Iris"" and change the labels into one-hot encode",<python><tensorflow><tensorflow2.0><tensorflow-datasets><iris-dataset>,1,0,0.0,,,CC BY-SA 4.0
67569421,1,,,2021-05-17 12:11:17,,5,158,"<p>I am surprised that I could not see a concise way to run a tf.data API on the GPU. I understand that the data pipelines can run on CPU so that it can happen in parallel (with pre-fetching), allowing the GPU to run actual model and train it.</p>
<p>However, my pre-processing is extremely parallel and computationally intensive. While I can technically write the pre-processing as the first layer in my model, I would really not prefer to do this to prevent training data leakage into my model.</p>
<p>Any pointers is appreciated for this. The closest I found was <a href=""https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851"" rel=""nofollow noreferrer"">https://towardsdatascience.com/overcoming-data-preprocessing-bottlenecks-with-tensorflow-data-service-nvidia-dali-and-other-d6321917f851</a>, which involves using nvidia DALI frameworks.</p>
<p>Here are some critical points:</p>
<ul>
<li>I have already tried enforcing device placement with <code>tf.device('...')</code>.</li>
<li>I dont want to prefectch the data into the device but rather run the whole data pipeline on GPU.</li>
<li>Preferably, if my computation is more, I want to save my dataset as a <code>tfrecords</code>, so that I can load it directly. This can be done with <code>tf.data.experimental.save</code> for now, but it again it uses CPU!</li>
</ul>
",12155033.0,,12155033.0,,2021-05-17 12:23:04,2021-05-17 12:23:04,Is there a way to run tf.data API on GPU in tf >= 2.4,<python><tensorflow>,0,1,,,,CC BY-SA 4.0
71573215,1,,,2022-03-22 13:49:49,,5,3507,"<p>I'm trying to run a shell script on Mac OS M1, but it keeps giving me the error:</p>
<pre><code>ModuleNotFoundError: No module named 'tensorflow'
</code></pre>
<p>I followed the instructions from here: <a href=""https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706"" rel=""noreferrer"">https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706</a>.</p>
<p>Although I now can import it manually in python:</p>
<pre><code>(mlp) sarah@Air-Sarah AIR % python 
Python 3.8.11 (default, Aug 16 2021, 12:04:33) 
[Clang 12.0.0 ] :: Anaconda, Inc. on darwin
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.__version__
'2.8.0'
</code></pre>
<p>my error in shell remains the same.</p>
<p>I also tried to install it with <a href=""https://github.com/apple/tensorflow_macos"" rel=""noreferrer"">https://github.com/apple/tensorflow_macos</a>. Nothing changed.</p>
<p>Also <a href=""https://stackoverflow.com/questions/66201657/installing-tensorflow-on-mac-m1"">here</a>
it says that Python version has to be 3.8 for TensorFlow to be imported, but mine is 3.8.</p>
<p>What else can I do?</p>
",17825011.0,,,,,2023-01-29 12:59:04,Cannot import TensorFlow on Mac M1,<macos><tensorflow>,0,2,,,,CC BY-SA 4.0
68603717,1,,,2021-07-31 15:59:46,,5,2514,"<p>I'm trying to install deepface on mac m1. I have installed <code>tensorflow-macos</code> and current version is 2.5.0. But when I try to install deepface it says that there is a conflict in dependencies versions like <code>deepface 0.0.65 depends on tensorflow&gt;=1.9.0</code>.
But as per my understanding, I've already installed <code>tensorflow-macos 2.5.0</code>
So why is there is a conflict ?</p>
<p>And please give me any solutions.</p>
<pre><code>ERROR: Cannot install deepface==0.0.1, deepface==0.0.10, deepface==0.0.11, deepface==0.0.12, deepface==0.0.13, deepface==0.0.14, deepface==0.0.15, deepface==0.0.16, deepface==0.0.18, deepface==0.0.19, deepface==0.0.2, deepface==0.0.20, deepface==0.0.21, deepface==0.0.22, deepface==0.0.23, deepface==0.0.24, deepface==0.0.25, deepface==0.0.26, deepface==0.0.3, deepface==0.0.30, deepface==0.0.31, deepface==0.0.32, deepface==0.0.33, deepface==0.0.34, deepface==0.0.35, deepface==0.0.36, deepface==0.0.37, deepface==0.0.38, deepface==0.0.39, deepface==0.0.4, deepface==0.0.40, deepface==0.0.41, deepface==0.0.43, deepface==0.0.44, deepface==0.0.45, deepface==0.0.46, deepface==0.0.47, deepface==0.0.48, deepface==0.0.49, deepface==0.0.5, deepface==0.0.50, deepface==0.0.51, deepface==0.0.52, deepface==0.0.53, deepface==0.0.54, deepface==0.0.55, deepface==0.0.56, deepface==0.0.57, deepface==0.0.58, deepface==0.0.59, deepface==0.0.6, deepface==0.0.60, deepface==0.0.61, deepface==0.0.62, deepface==0.0.63, deepface==0.0.64, deepface==0.0.65, deepface==0.0.7 and deepface==0.0.9 because these package versions have conflicting dependencies.

The conflict is caused by:
    deepface 0.0.65 depends on tensorflow&gt;=1.9.0
    deepface 0.0.64 depends on tensorflow&gt;=1.9.0
    deepface 0.0.63 depends on tensorflow&gt;=1.9.0
    deepface 0.0.62 depends on tensorflow&gt;=1.9.0
    deepface 0.0.61 depends on tensorflow&gt;=1.9.0
    deepface 0.0.60 depends on tensorflow&gt;=1.9.0
    deepface 0.0.59 depends on tensorflow&gt;=1.9.0
    deepface 0.0.58 depends on tensorflow&gt;=1.9.0
    deepface 0.0.57 depends on tensorflow&gt;=1.9.0
    deepface 0.0.56 depends on tensorflow&gt;=1.9.0
    deepface 0.0.55 depends on tensorflow&gt;=1.9.0
    deepface 0.0.54 depends on tensorflow&gt;=1.9.0
    deepface 0.0.53 depends on tensorflow&gt;=1.9.0
    deepface 0.0.52 depends on tensorflow&gt;=1.9.0
    deepface 0.0.51 depends on tensorflow&gt;=1.9.0
    deepface 0.0.50 depends on tensorflow&gt;=1.9.0
    deepface 0.0.49 depends on tensorflow&gt;=1.9.0
    deepface 0.0.48 depends on tensorflow&gt;=1.9.0
    deepface 0.0.47 depends on tensorflow&gt;=1.9.0
    deepface 0.0.46 depends on tensorflow&gt;=1.9.0
    deepface 0.0.45 depends on tensorflow&gt;=1.9.0
    deepface 0.0.44 depends on tensorflow&gt;=1.9.0
    deepface 0.0.43 depends on tensorflow&gt;=1.9.0
    deepface 0.0.41 depends on tensorflow&gt;=1.9.0
    deepface 0.0.40 depends on tensorflow&gt;=1.9.0
    deepface 0.0.39 depends on tensorflow&gt;=1.9.0
    deepface 0.0.38 depends on tensorflow&gt;=1.9.0
    deepface 0.0.37 depends on tensorflow&gt;=1.9.0
    deepface 0.0.36 depends on tensorflow&gt;=1.9.0
    deepface 0.0.35 depends on tensorflow&gt;=1.9.0
    deepface 0.0.34 depends on tensorflow&gt;=1.9.0
    deepface 0.0.33 depends on tensorflow&gt;=1.9.0
    deepface 0.0.32 depends on tensorflow&gt;=1.9.0
    deepface 0.0.31 depends on tensorflow&gt;=1.9.0
    deepface 0.0.30 depends on tensorflow&gt;=1.9.0
    deepface 0.0.26 depends on tensorflow&gt;=1.9.0
    deepface 0.0.25 depends on tensorflow&gt;=1.9.0
    deepface 0.0.24 depends on tensorflow&gt;=1.9.0
    deepface 0.0.23 depends on tensorflow&gt;=1.9.0
    deepface 0.0.22 depends on tensorflow&gt;=1.9.0
    deepface 0.0.21 depends on tensorflow&gt;=1.9.0
    deepface 0.0.20 depends on tensorflow&gt;=1.9.0
    deepface 0.0.19 depends on tensorflow&gt;=1.9.0
    deepface 0.0.18 depends on tensorflow&gt;=1.9.0
    deepface 0.0.16 depends on tensorflow&gt;=1.9.0
    deepface 0.0.15 depends on tensorflow&gt;=1.9.0
    deepface 0.0.14 depends on tensorflow&gt;=1.9.0
    deepface 0.0.13 depends on tensorflow&gt;=1.9.0
    deepface 0.0.12 depends on tensorflow&gt;=1.9.0
    deepface 0.0.11 depends on tensorflow&gt;=1.9.0
    deepface 0.0.10 depends on tensorflow&gt;=1.9.0
    deepface 0.0.9 depends on tensorflow&gt;=1.9.0
    deepface 0.0.7 depends on tensorflow&gt;=1.9.0
    deepface 0.0.6 depends on tensorflow&gt;=1.9.0
    deepface 0.0.5 depends on tensorflow&gt;=1.9.0
    deepface 0.0.4 depends on tensorflow&gt;=1.9.0
    deepface 0.0.3 depends on tensorflow&gt;=1.9.0
    deepface 0.0.2 depends on tensorflow&gt;=1.9.0
    deepface 0.0.1 depends on tensorflow&gt;=1.9.0

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies
</code></pre>
",7408131.0,,10153731.0,,2021-07-31 17:13:18,2023-06-06 16:01:13,Can not install deepface because package versions have conflict,<python><tensorflow>,3,3,,,,CC BY-SA 4.0
65358676,1,65365103.0,,2020-12-18 14:21:34,,5,1245,"<p>In the code here:
<a href=""https://www.kaggle.com/ryanholbrook/detecting-the-higgs-boson-with-tpus"" rel=""noreferrer"">https://www.kaggle.com/ryanholbrook/detecting-the-higgs-boson-with-tpus</a></p>
<p>Before the model is compiled, the model is made using this code:</p>
<pre class=""lang-py prettyprint-override""><code>with strategy.scope():
    # Wide Network
    wide = keras.experimental.LinearModel()

    # Deep Network
    inputs = keras.Input(shape=[28])
    x = dense_block(UNITS, ACTIVATION, DROPOUT)(inputs)
    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)
    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)
    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)
    x = dense_block(UNITS, ACTIVATION, DROPOUT)(x)
    outputs = layers.Dense(1)(x)
    deep = keras.Model(inputs=inputs, outputs=outputs)
    
    # Wide and Deep Network
    wide_and_deep = keras.experimental.WideDeepModel(
        linear_model=wide,
        dnn_model=deep,
        activation='sigmoid',
    )
</code></pre>
<p>I don't understand what <code>with strategy.scope()</code> does here and if it in any way affects the model. What does it do exactly?</p>
<p>In the future how could I figure out what this does? What resources would I have to look into to figure this out?</p>
",11945129.0,,,,,2020-12-18 23:15:09,What does 'with strategy.scope():' or 'with tf.distribute.experimental.TPUStrategy(tpu).scope():' do to the creation of a NN?,<tensorflow><tensorflow2.0><tpu>,1,0,,,,CC BY-SA 4.0
64325937,1,64326060.0,,2020-10-12 22:08:03,,5,3936,"<p>I have a UNet that I trained and saved the model. I want to see the model summary but when I use <code>model.summary()</code> the <code>Output Shape</code> column is cut off and cant see the full shape. The model was trained on 3D images so the output should show <code>(None, shapeX, shapeY, shapeZ, num_features)</code>. How can I show the full Output Shape?</p>
<pre><code>from tensorflow.keras.models import load_model
        
model = load_model('model.h5',compile = False)
model.summary()
</code></pre>
<p>model.summary()</p>
<p><a href=""https://i.stack.imgur.com/uVRjY.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/uVRjY.png"" alt=""model"" /></a></p>
",11156090.0,,,,,2020-10-12 22:21:45,Tensorflow show model summary,<python><tensorflow>,1,0,,,,CC BY-SA 4.0
71183231,1,,,2022-02-19 08:07:13,,5,17682,"<p>I'm using raspberry model 3B+ , I made a venv and then tried to install tensorflow but I get these 2 errors
<code>ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)</code></p>
<p><code>ERROR: No matching distribution found for tensorflow</code></p>
<p>I managed to install other libraries but only tensorflow I wasn't able to install.</p>
",15343213.0,,15343213.0,,2022-02-20 12:41:01,2022-05-05 10:48:34,ERROR : could not find a version that satisfies the requirement tensorflow,<python><tensorflow><raspberry-pi>,2,4,0.0,,,CC BY-SA 4.0
71174306,1,71179308.0,,2022-02-18 13:30:20,,5,5591,"<p>I am trying to install Tensorflow on my MacBook Pro with the M1 chip. The operating system of my MacBook is MacOS Big Sur Version 11.0.</p>
<p><a href=""https://i.stack.imgur.com/2qeZ9.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/2qeZ9.png"" alt=""enter image description here"" /></a></p>
<p>In order to install Tensorflow to use it with Python, I have followed <a href=""https://www.mrdbourke.com/setup-apple-m1-pro-and-m1-max-for-machine-learning-and-data-science/"" rel=""noreferrer"">this tutorial</a>, which says that I have to do the following:</p>
<ol>
<li><p>Install Homebrew.</p>
</li>
<li><p>Download MiniForge3 for macOS arm64 chips (link provided in the webpage).</p>
</li>
<li><p>Install MiniForge3 using:</p>
<pre><code>chmod +x ~/Downloads/Miniforge3-MacOSX-arm64.sh
sh ~/Downloads/Miniforge3-MacOSX-arm64.sh
source ~/miniforge3/bin/activate
</code></pre>
</li>
<li><p>Create a folder to set up an environment for Tensorflow.</p>
<pre><code>mkdir tensorflow-test
cd tensorflow-test
</code></pre>
</li>
<li><p>Make and activate Conda environment.</p>
<pre><code>conda create --prefix ./env python=3.9.7
conda activate ./env
</code></pre>
</li>
<li><p>Install Tensorflow dependencies.</p>
<pre><code>conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal
</code></pre>
</li>
</ol>
<p>After this, I open a Jupyter Notebook and I try to import tensorflow, but this error shows up:</p>
<pre><code>OSError: dlopen(/Users/blancoarnau/tensorflow-test/env/lib/python3.9/site-packages/tensorflow/python/platform/../../core/platform/_cpu_feature_guard.so, 6): Symbol not found: __ZNKSt3__115basic_stringbufIcNS_11char_traitsIcEENS_9allocatorIcEEE3strEv
   Referenced from: /Users/blancoarnau/tensorflow-test/env/lib/python3.9/site-packages/tensorflow/python/platform/../../core/platform/_cpu_feature_guard.so (which was built for Mac OS X 12.3)
   Expected in: /usr/lib/libc++.1.dylib
</code></pre>
<p>As you can see in this screenshot:</p>
<p><a href=""https://i.stack.imgur.com/OS2qD.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/OS2qD.jpg"" alt=""enter image description here"" /></a></p>
<p>Do you have an idea why this is happening?</p>
",,user16478428,,,,2022-02-18 20:06:58,'Expected in: /usr/lib/libc++.1.dylib': Installing Tensorflow on M1 MacBook Pro,<python><tensorflow><conda><apple-m1>,1,2,,,,CC BY-SA 4.0
68003145,1,,,2021-06-16 12:58:55,,5,157,"<p>Let us say we have some Relational data.
Making a simple example for a retail store chain:</p>
<ul>
<li>Dataset 1 --&gt; Store_id, Daily_sales</li>
<li>Dataset 2 --&gt; Customer_id, store_id, Time in, Time out</li>
</ul>
<p>Let us say the task is to predict <code>Daily_sales</code>.</p>
<p>I know how how to create data batches for one single CSV. I can use <code>tf.data.experimental.make_csv_dataset</code> and iterate over the dataset iterable that it returns to read the batches lazily.</p>
<p>However, I want to read in the batches from <code>Dataset 1</code> and <code>Dataset 2</code> described above where the common id is <code>store_id</code> such that the batch reads the rows with same <code>store_id</code>s from both the datasets. I want to do this because I will run two networks (RNN on <code>Dataset 2</code> and a single Fully connected layer on <code>Dataset 1</code>) on both datasets and then merge them in the final fully connected layer.</p>
<p>Can you please guide me on how to approach this problem in scenarios where:</p>
<ul>
<li>The datasets can fit into the memory</li>
<li>The datasets can not fit into the memory</li>
</ul>
<p>Here is a concrete example of the consistent batch creation I am looking  for:</p>
<pre><code>import pandas as pd
Dataset_1 = pd.DataFrame({'id':['a','b','c','d'],'col1':[1,2,3,4]})
print(Dataset_1)
  id  col1
0  a     1
1  b     2
2  c     3
3  d     4
Dataset_2 = pd.DataFrame({'id':['a','a','b','c','c','c','d'],'col1':[10,11,12,13,14,15,16]})
print(Dataset_2)
    id  col1
0   a   10
1   a   11
2   b   12
3   c   13
4   c   14
5   c   15
6   d   16
#Let us say i want to create 2 batches. The following dataframes are how i want my batches to look like
batch_1 = (pd.DataFrame({'id':['a','b'],'col1':[1,2]}),pd.DataFrame({'id':['a','a','b'],'col1':[10,11,12]}))
print(batch_1[0])
    id  col1
0   a   1
1   b   2
print(batch_1[1])
  id  col1
0  a    10
1  a    11
2  b    12
batch_2 = (pd.DataFrame({'id':['c','d'],'col1':[3,4]}),pd.DataFrame({'id':['c','c','c','d'],'col1':[13,14,15,16]}))
print(batch_2[0])
id  col1
0  c     3
1  d     4

print(batch_2[1])
 id  col1
0  c    13
1  c    14
2  c    15
3  d    16
</code></pre>
",6546694.0,,9215780.0,,2021-06-26 05:22:34,2021-06-26 05:22:34,How to create Tensorflow data ingestion pipeline for multiple related CSVs?,<tensorflow><relational-database><tensorflow2.0>,0,2,0.0,,,CC BY-SA 4.0
71211053,1,71211960.0,,2022-02-21 18:05:44,,5,1093,"<p>I'm following one of the online courses about time series predictions using Tensorflow. The function used to convert Numpy array (TS) into a Tensorflow dataset used is LSTM-based model is already given (with my comment lines):</p>
<pre><code>def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
     # creating a tensor from an array
     dataset = tf.data.Dataset.from_tensor_slices(series)
     # cutting the tensor into fixed-size windows
     dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)  
     # joining windows into a batch?
     dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))
     # separating row into features/label
     dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))
     dataset = dataset.batch(batch_size).prefetch(1)
     return dataset
</code></pre>
<p>This code work fine but I want to understand it better to modify/adapt it for my needs.</p>
<p>If I remove <code>dataset.flat_map(lambda window: window.batch(window_size + 1))</code> operation, I receive the <code>TypeError: '_VariantDataset' object is not subscriptable</code> pointing to the line: <code>lambda window: (window[:-1], window[-1]))</code></p>
<p>I managed to rewrite part of this code (skipping shuffling) to Numpy-based one:</p>
<pre><code>def windowed_dataset_np(series, window_size):
    values = sliding_window_view(series, window_size)
    X = values[:, :-1]
    X = tf.convert_to_tensor(np.expand_dims(X, axis=-1))
    y = values[:,-1]
    return X, y
</code></pre>
<p>Syntax of fitting of the model looks a bit differently but it works fine.</p>
<p>My two questions are:</p>
<ol>
<li>What does <code>dataset.flat_map(lambda window: window.batch(window_size + 1))</code> achieves?</li>
<li>Is the second code really equivalent to the three first operations in the original function?</li>
</ol>
",8179672.0,,9657861.0,,2022-02-21 19:26:15,2022-02-21 19:57:08,What tensorflow's flat_map + window.batch() does to a dataset/array?,<python><tensorflow><tensorflow-datasets><data-preprocessing>,1,0,0.0,,,CC BY-SA 4.0
67737186,1,,,2021-05-28 10:26:18,,5,656,"<p>I was wondering if it is possible/how easy it would be to implement a TFX pipeline (on a real dataset, with 100+ GB dataset, not a tutorial with a small dataset) in AWS?</p>
<p>For the orchestration, I might use Kubeflow. But I suppose, the major issue would be setting up a proper scalable runner for the Apache Beam. I am thinking of using Apache Flink for that.</p>
<p>Anyone with experience doing it? How would you go about putting a TF in production in AWS in general when you need to train the model on a regular basis on new data, do you write the pipeline from scratch or use some tool?</p>
",15971266.0,,,,,2021-05-28 10:26:18,Running TensorFlow Extended (TFX) on AWS,<tensorflow><apache-beam><tfx>,0,1,0.0,,,CC BY-SA 4.0
75232220,1,,,2023-01-25 09:54:29,,5,391,"<p>I'm trying to run a notebook on deepnote/colab but I keep getting the same issue, everytime tflite-model-maker tries to install it just fills the disk entirely and can't install.
Deepnote is limited at 5GB of disk storage and colab is around 100GB but I keep getting the same issue on both notebooks.
Any help would be greatly appreciated ! Thanks in advance.</p>
",20570839.0,,,,,2023-01-25 09:54:29,Is there any way to reduce the size of tflite-model-maker?,<tensorflow><data-science><google-colaboratory><tensorflow-lite>,0,0,,,,CC BY-SA 4.0
68449103,1,68449344.0,,2021-07-20 03:05:11,,5,12238,"<p>belos is my code to ensure that the folder has images, but tf.keras.preprocessing.image_dataset_from_directory returns no images found. What did I do wrong? Thanks.</p>
<pre><code>DATASET_PATH = pathlib.Path('C:\\Users\\xxx\\Documents\\images')
image_count = len(list(DATASET_PATH.glob('.\\*.jpg')))
print(image_count)
</code></pre>
<p>output = 2715</p>
<pre><code>batch_size = 4
img_height = 32
img_width = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    DATASET_PATH.name,
    validation_split=0.8,
    subset=&quot;training&quot;,
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size)
</code></pre>
<p>output:</p>
<pre><code>Found 0 files belonging to 0 classes.
Using 0 files for training.
Traceback (most recent call last):
  File &quot;.\tensorDataPreProcessed.py&quot;, line 23, in &lt;module&gt;
    batch_size=batch_size)
  File &quot;C:\Users\xxx\Anaconda3\envs\xxx\lib\site-packages\tensorflow\python\keras\preprocessing\image_dataset.py&quot;, line 200, in image_dataset_from_directory
    raise ValueError('No images found.')
ValueError: No images found.
</code></pre>
",13479017.0,,,,,2023-01-31 23:36:38,tf.keras.preprocessing.image_dataset_from_directory Value Error: No images found,<python><image><tensorflow><tensorflow2.0><tf.keras>,2,8,0.0,,,CC BY-SA 4.0
63415102,1,63450946.0,,2020-08-14 14:50:47,,5,276,"<p>I'm trying to make an Evaluator for my model. Until now every other components are fine but When I try this config:</p>
<pre><code>eval_config = tfma.EvalConfig(
    model_specs=[
        tfma.ModelSpec(label_key='Category'),
    ],
    metrics_specs=tfma.metrics.default_multi_class_classification_specs(),
    slicing_specs=[
        tfma.SlicingSpec(),
        tfma.SlicingSpec(feature_keys=['Category'])
    ])
</code></pre>
<p>to make this evaluator:</p>
<pre><code>model_resolver = ResolverNode(
      instance_name='latest_blessed_model_resolver',
      resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,
      model=Channel(type=Model),
      model_blessing=Channel(type=ModelBlessing))
context.run(model_resolver)

evaluator = Evaluator(
    examples=example_gen.outputs['examples'],
    model=trainer.outputs['model'],
    baseline_model=model_resolver.outputs['model'],
    eval_config=eval_config)
context.run(evaluator)
</code></pre>
<p>I get this:</p>
<pre><code>[...]
IndexError                                Traceback (most recent call last)
/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/apache_beam/runners/common.cpython-37m-darwin.so in apache_beam.runners.common.DoFnRunner.process()

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/apache_beam/runners/common.cpython-37m-darwin.so in apache_beam.runners.common.PerWindowInvoker.invoke_process()

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/apache_beam/runners/common.cpython-37m-darwin.so in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window()

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/apache_beam/runners/common.cpython-37m-darwin.so in apache_beam.runners.common._OutputProcessor.process_outputs()

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/apache_beam/runners/worker/operations.cpython-37m-darwin.so in apache_beam.runners.worker.operations.SingletonConsumerSet.receive()

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/apache_beam/runners/worker/operations.cpython-37m-darwin.so in apache_beam.runners.worker.operations.PGBKCVOperation.process()

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/apache_beam/runners/worker/operations.cpython-37m-darwin.so in apache_beam.runners.worker.operations.PGBKCVOperation.process()

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/tensorflow_model_analysis/evaluators/metrics_and_plots_evaluator_v2.py in add_input(self, accumulator, element)
    355     for i, (c, a) in enumerate(zip(self._combiners, accumulator)):
--&gt; 356       result = c.add_input(a, get_combiner_input(elements[0], i))
    357       for e in elements[1:]:

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/tensorflow_model_analysis/metrics/calibration_histogram.py in add_input(self, accumulator, element)
    141             flatten=True,
--&gt; 142             class_weights=self._class_weights)):
    143       example_weight = float(example_weight)

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/tensorflow_model_analysis/metrics/metric_util.py in to_label_prediction_example_weight(inputs, eval_config, model_name, output_name, sub_key, class_weights, flatten, squeeze, allow_none)
    283     elif sub_key.top_k is not None:
--&gt; 284       label, prediction = select_top_k(sub_key.top_k, label, prediction)
    285 

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/tensorflow_model_analysis/metrics/metric_util.py in select_top_k(top_k, labels, predictions, scores)
    621   if not labels.shape or labels.shape[-1] == 1:
--&gt; 622     labels = one_hot(labels, predictions)
    623 

/opt/miniconda3/envs/archiving/lib/python3.7/site-packages/tensorflow_model_analysis/metrics/metric_util.py in one_hot(tensor, target)
    671   # indexing the -1 and then removing it after.
--&gt; 672   tensor = np.delete(np.eye(target.shape[-1] + 1)[tensor], -1, axis=-1)
    673   return tensor.reshape(target.shape)

IndexError: arrays used as indices must be of integer (or boolean) type

During handling of the above exception, another exception occurred:
[...]

IndexError: arrays used as indices must be of integer (or boolean) type [while running 'ExtractEvaluateAndWriteResults/ExtractAndEvaluate/EvaluateMetricsAndPlots/ComputeMetricsAndPlots()/ComputePerSlice/ComputeUnsampledMetrics/CombinePerSliceKey/WindowIntoDiscarding']
</code></pre>
<p>I thought it was my config, but I don't get what is wrong with this.</p>
<p>I'm using this data set <a href=""https://www.kaggle.com/c/learn-ai-bbc"" rel=""noreferrer"">Kaggle - BBC News Classification</a>.
I've followed this notebook: <a href=""https://www.tensorflow.org/tfx/tutorials/tfx/components_keras"" rel=""noreferrer"">TFX - Chicago Taxi</a> in order to serve my model with Tensorflow Serving.</p>
<p>Note: The model I'm using look like this:</p>
<pre><code>def _build_keras_model(vectorize_layer: TextVectorization) -&gt; tf.keras.Model: 

  input_layer = tf.keras.layers.Input(shape=(1,), dtype=tf.string)

  deep = vectorize_layer(input_layer)
  deep = layers.Embedding(_max_features + 1, _embedding_dim)(deep)
  deep = layers.Dropout(0.5)(deep)
  deep = layers.GlobalAveragePooling1D()(deep)
  deep = layers.Dropout(0.5)(deep)

  output = layers.Dense(5, activation=tf.nn.softmax)(deep)

  model = tf.keras.Model(input_layer, output)
  model.compile(
      loss=losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer='adam', 
      metrics=['accuracy'])
  model.summary(print_fn=absl.logging.info)  
  return model
</code></pre>
",4360683.0,,4360683.0,,2020-08-17 09:00:37,2020-08-17 12:35:44,TFX IndexError on Evaluator component,<python><tensorflow><tfx>,1,0,,,,CC BY-SA 4.0
64224051,1,64966229.0,,2020-10-06 10:36:04,,5,1936,"<p>I want to associate to each person's name a list of numbers.</p>
<pre><code>keys = [&quot;Fritz&quot;, &quot;Franz&quot;, &quot;Fred&quot;]
values = [[1, 2, 3], [4, 5], [6, 7, 8, 9]]
</code></pre>
<p>If I run the following:</p>
<pre><code>import tensorflow as tf
table = tf.lookup.StaticHashTable(tf.lookup.KeyValueTensorInitializer(keys, values), default_value=0)
</code></pre>
<p>, I get a <code>ValueError: Can't convert non-rectangular Python sequence to Tensor.</code>
since the lists are not of the same size and hence cannot be converted to a <code>tf.Tensor</code>.</p>
<p><strong>Is there another way to associate the values of a tensor to lists of arbitrary shape?</strong></p>
<p>Thank you for your help :)</p>
",14147996.0,,,,,2020-11-23 10:01:37,tf.lookup.StaticHashTable with lists (of arbitrary sizes) as values,<python><tensorflow><dictionary><hashtable><lookup>,1,0,,,,CC BY-SA 4.0
71290994,1,,,2022-02-28 05:41:05,,5,1434,"<p>I have the Apple M1 Pro chip and cannot get my tensorflow project running. I followed the installation instructions from <a href=""https://developer.apple.com/metal/tensorflow-plugin/"" rel=""noreferrer"">Apple's site</a>.</p>
<p>When I run <code>pip install -r requirements.txt</code>, all my python packages install except for <code>tflite-model-maker</code>. I get the following error:</p>
<pre><code>ERROR: Cannot install -r requirements.txt (line 19) and tflite-support because these package versions have conflicting dependencies.

The conflict is caused by:
    tflite-model-maker 0.3.4 depends on tensorflow&gt;=2.6.0
    tflite-model-maker 0.3.3 depends on tensorflow&gt;=2.6.0
    tflite-model-maker 0.3.2 depends on tensorflow&gt;=2.4.0
    tflite-model-maker 0.3.1 depends on tensorflow&gt;=2.4.0
    tflite-model-maker 0.3.0 depends on tensorflow&gt;=2.4.0
    tflite-model-maker 0.2.5 depends on tensorflow&gt;=2.4.0
    The user requested tflite-support
    tflite-model-maker 0.2.4 depends on tflite-support==0.1.0rc4
    tflite-model-maker 0.2.3 depends on tf-nightly==2.4.0.dev20200902
    tflite-model-maker 0.2.2 depends on tf-nightly==2.4.0.dev20200902
    tflite-model-maker 0.2.1 depends on tf-nightly==2.4.0.dev20200811
    tflite-model-maker 0.2.0 depends on tf-nightly==2.4.0.dev20200810
    tflite-model-maker 0.1.2 depends on tf-nightly
    The user requested tflite-support
    tflite-model-maker 0.1.1 depends on tflite-support==0.1.0a0
    The user requested tflite-support
    tflite-model-maker 0.1.0 depends on tflite-support==0.1.0a0

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict
</code></pre>
<p>Any ideas?</p>
",11666359.0,,,,,2022-05-09 02:24:45,I can't install Tensorflow Model Maker on Apple Silicon,<tensorflow><pip><tensorflow-lite><apple-m1><apple-silicon>,1,1,,,,CC BY-SA 4.0
71319195,1,,,2022-03-02 07:58:07,,5,3093,"<p>I am trying to run a keras code on a GPU node within a cluster. The GPU node has 4 GPUs per node. I made sure to have all 4 GPUs within the GPU node available for my use. I run the code below to let tensorflow use the GPU:</p>
<pre><code>gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
            logical_gpus = tf.config.list_logical_devices('GPU')
            print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;)
    except RuntimeError as e:
        print(e)
        
</code></pre>
<p>The 4 GPUs available get listed in the output. However, I got the following error when running the code:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/BayesOptimization.py&quot;, line 20, in &lt;module&gt;
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
  File &quot;/.conda/envs/thesis/lib/python3.9/site-packages/tensorflow/python/framework/config.py&quot;, line 439, in list_logical_devices
    return context.context().list_logical_devices(device_type=device_type)
  File &quot;/.conda/envs/thesis/lib/python3.9/site-packages/tensorflow/python/eager/context.py&quot;, line 1368, in list_logical_devices
    self.ensure_initialized()
  File &quot;/.conda/envs/thesis/lib/python3.9/site-packages/tensorflow/python/eager/context.py&quot;, line 511, in ensure_initialized
    config_str = self.config.SerializeToString()
  File &quot;/.conda/envs/thesis/lib/python3.9/site-packages/tensorflow/python/eager/context.py&quot;, line 1015, in config
    gpu_options = self._compute_gpu_options()
  File &quot;/.conda/envs/thesis/lib/python3.9/site-packages/tensorflow/python/eager/context.py&quot;, line 1074, in _compute_gpu_options
    raise ValueError(&quot;Memory growth cannot differ between GPU devices&quot;)
ValueError: Memory growth cannot differ between GPU devices
</code></pre>
<p>Shouldn't the code list all the available gpus and set memory growth to true for each one?</p>
<p>I am currently using tensorflow libraries and python 3.97:</p>
<pre><code>tensorflow                2.4.1           gpu_py39h8236f22_0
tensorflow-base           2.4.1           gpu_py39h29c2da4_0
tensorflow-estimator      2.4.1              pyheb71bc4_0
tensorflow-gpu            2.4.1                h30adc30_0
</code></pre>
<p>Any idea what the problem is and how to solve it? Thanks in advance!</p>
",18351484.0,,18351484.0,,2022-03-06 09:06:40,2022-12-21 14:37:20,Tensorflow: Memory growth cannot differ between GPU devices | How to use multi-GPU with tensorflow,<tensorflow><gpu><tf.keras><multi-gpu>,2,4,0.0,,,CC BY-SA 4.0
75537409,1,,,2023-02-22 19:36:27,,5,976,"<p>I am trying to run tensorflow using conda in a virtual environment.
These are the steps I took:</p>
<ol>
<li><p>I created a new conda environment using <strong>conda create --name tf python=3.9</strong></p>
</li>
<li><p>I activated the environment using</p>
<pre><code>conda activate tf
</code></pre>
</li>
<li><p>I made sure cuda and cuDNN were installed</p>
<pre><code>conda install -c conda-forge cudatoolkit=11.2.2 cudnn=8.1.0
</code></pre>
</li>
<li><p>I configured the system path</p>
<pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/
</code></pre>
</li>
<li><p>I automated it</p>
<pre><code>mkdir -p $CONDA_PREFIX/etc/conda/activate.d
echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/' &gt; $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh
</code></pre>
</li>
<li><p>I made sure to install tensorflow</p>
<pre><code>pip install tensorflow==2.11.*
</code></pre>
</li>
<li><p>Then I tried to verify the install</p>
</li>
</ol>
<pre><code>    python3 -c &quot;import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))&quot;
</code></pre>
<p>This is is where I encounter an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/sbrent/research/Oman/venv/lib/python3.8/site-packages/tensorflow/__init__.py&quot;, line 37, in &lt;module&gt;
    from tensorflow.python.tools import module_util as _module_util
  File &quot;/home/sbrent/research/Oman/venv/lib/python3.8/site-packages/tensorflow/python/__init__.py&quot;, line 24, in &lt;module&gt;
    import ctypes
  File &quot;/usr/lib/python3.8/ctypes/__init__.py&quot;, line 7, in &lt;module&gt;
    from _ctypes import Union, Structure, Array
ImportError: /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so: undefined symbol: ffi_closure_alloc, version LIBFFI_CLOSURE_7.0
</code></pre>
<p>Honestly, I have no idea how to fix this, so any guidance would be appreciated.</p>
<p>I listed what I tried in the statement above</p>
",2558894.0,,,,,2023-05-24 16:54:05,ImportError with ffi_closure_alloc and LIBFFI_CLOSURE_7.0,<python-3.x><tensorflow><conda>,2,0,,,,CC BY-SA 4.0
68105073,1,,,2021-06-23 18:05:13,,5,3404,"<p>Hi i am using below as my Docker image for fastapi application</p>
<pre><code>FROM tensorflow/tensorflow:latest
</code></pre>
<p>when i run docker its running but i am getting this error</p>
<pre><code>2021-06-23 23:31:50.516749: F tensorflow/core/lib/monitoring/sampler.cc:42] Check failed: bucket_limits_[i] &gt; bucket_limits_[i - 1] (0 vs. 10)

qemu: uncaught target signal 6 (Aborted) - core dumped

[2021-06-23 23:31:50 +0530] [1] [WARNING] Worker with pid 2697 was terminated due to signal 6
</code></pre>
<p>and when i call api, i am not getting response, does it take time for api call or can you please tell me where it is wrong</p>
",16105407.0,,,,,2021-06-23 18:35:11,Tensorflow error when used as Docker baseimage,<python-3.x><docker><tensorflow><fastapi>,1,0,,,,CC BY-SA 4.0
70118623,1,70221054.0,,2021-11-26 00:57:30,,5,499,"<p>So I was trying to convert my data's timestamps from Unix timestamps to a more readable date format. I created a simple Java program to do so and write to a .csv file, and that went smoothly. I tried using it for my model by one-hot encoding it into numbers and then turning everything into normalized data. However, after my attempt to one-hot encode (which I am not sure if it even worked), my normalization process using make_column_transformer failed.</p>
<pre><code># model 4
# next model
import tensorflow as tf
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from tensorflow.keras import layers
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.model_selection import train_test_split

np.set_printoptions(precision=3, suppress=True)
btc_data = pd.read_csv(
    &quot;/content/drive/MyDrive/Science Fair/output2.csv&quot;,
    names=[&quot;Time&quot;, &quot;Open&quot;])

X_btc = btc_data[[&quot;Time&quot;]]
y_btc = btc_data[&quot;Open&quot;]

enc = OneHotEncoder(handle_unknown=&quot;ignore&quot;)
enc.fit(X_btc)

X_btc = enc.transform(X_btc)

print(X_btc)

X_train, X_test, y_train, y_test = train_test_split(X_btc, y_btc, test_size=0.2, random_state=62)

ct = make_column_transformer(
    (MinMaxScaler(), [&quot;Time&quot;])
)

ct.fit(X_train)
X_train_normal = ct.transform(X_train)
X_test_normal = ct.transform(X_test)

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

btc_model_4 = tf.keras.Sequential([
  layers.Dense(100, activation=&quot;relu&quot;),
  layers.Dense(100, activation=&quot;relu&quot;),
  layers.Dense(100, activation=&quot;relu&quot;),
  layers.Dense(100, activation=&quot;relu&quot;),
  layers.Dense(100, activation=&quot;relu&quot;),
  layers.Dense(100, activation=&quot;relu&quot;),
  layers.Dense(1, activation=&quot;linear&quot;)
])

btc_model_4.compile(loss = tf.losses.MeanSquaredError(),
                      optimizer = tf.optimizers.Adam())

history = btc_model_4.fit(X_train_normal, y_train, batch_size=8192, epochs=100, callbacks=[callback])

btc_model_4.evaluate(X_test_normal, y_test, batch_size=8192)

y_pred = btc_model_4.predict(X_test_normal)

btc_model_4.save(&quot;btc_model_4&quot;)
btc_model_4.save(&quot;btc_model_4.h5&quot;)

# plot model
def plot_evaluations(train_data=X_train_normal,
                     train_labels=y_train,
                     test_data=X_test_normal,
                     test_labels=y_test,
                     predictions=y_pred):
  print(test_data.shape)
  print(predictions.shape)

  plt.figure(figsize=(100, 15))
  plt.scatter(train_data, train_labels, c='b', label=&quot;Training&quot;)
  plt.scatter(test_data, test_labels, c='g', label=&quot;Testing&quot;)
  plt.scatter(test_data, predictions, c='r', label=&quot;Results&quot;)
  plt.legend()

plot_evaluations()

# plot loss curve
pd.DataFrame(history.history).plot()
plt.ylabel(&quot;loss&quot;)
plt.xlabel(&quot;epochs&quot;)
</code></pre>
<p>My normal data format is like so:</p>
<pre><code>2015-12-05 12:52:00,377.48
2015-12-05 12:53:00,377.5
2015-12-05 12:54:00,377.5
2015-12-05 12:56:00,377.5
2015-12-05 12:57:00,377.5
2015-12-05 12:58:00,377.5
2015-12-05 12:59:00,377.5
2015-12-05 13:00:00,377.5
2015-12-05 13:01:00,377.79
2015-12-05 13:02:00,377.5
2015-12-05 13:03:00,377.79
2015-12-05 13:05:00,377.74
2015-12-05 13:06:00,377.79
2015-12-05 13:07:00,377.64
2015-12-05 13:08:00,377.79
2015-12-05 13:10:00,377.77
2015-12-05 13:11:00,377.7
2015-12-05 13:12:00,377.77
2015-12-05 13:13:00,377.77
2015-12-05 13:14:00,377.79
2015-12-05 13:15:00,377.72
2015-12-05 13:16:00,377.5
2015-12-05 13:17:00,377.49
2015-12-05 13:18:00,377.5
2015-12-05 13:19:00,377.5
2015-12-05 13:20:00,377.8
2015-12-05 13:21:00,377.84
2015-12-05 13:22:00,378.29
2015-12-05 13:23:00,378.3
2015-12-05 13:24:00,378.3
2015-12-05 13:25:00,378.33
2015-12-05 13:26:00,378.33
2015-12-05 13:28:00,378.31
2015-12-05 13:29:00,378.68
</code></pre>
<p>The first is the date and the second value after the comma is the price of BTC at that time. Now after &quot;one-hot encoding&quot;, I added a print statement to print the value of those X values, and that gave the following value:</p>
<pre><code>  (0, 0)    1.0
  (1, 1)    1.0
  (2, 2)    1.0
  (3, 3)    1.0
  (4, 4)    1.0
  (5, 5)    1.0
  (6, 6)    1.0
  (7, 7)    1.0
  (8, 8)    1.0
  (9, 9)    1.0
  (10, 10)  1.0
  (11, 11)  1.0
  (12, 12)  1.0
  (13, 13)  1.0
  (14, 14)  1.0
  (15, 15)  1.0
  (16, 16)  1.0
  (17, 17)  1.0
  (18, 18)  1.0
  (19, 19)  1.0
  (20, 20)  1.0
  (21, 21)  1.0
  (22, 22)  1.0
  (23, 23)  1.0
  (24, 24)  1.0
  : :
  (2526096, 2526096)    1.0
  (2526097, 2526097)    1.0
  (2526098, 2526098)    1.0
  (2526099, 2526099)    1.0
  (2526100, 2526100)    1.0
  (2526101, 2526101)    1.0
  (2526102, 2526102)    1.0
  (2526103, 2526103)    1.0
  (2526104, 2526104)    1.0
  (2526105, 2526105)    1.0
  (2526106, 2526106)    1.0
  (2526107, 2526107)    1.0
  (2526108, 2526108)    1.0
  (2526109, 2526109)    1.0
  (2526110, 2526110)    1.0
  (2526111, 2526111)    1.0
  (2526112, 2526112)    1.0
  (2526113, 2526113)    1.0
  (2526114, 2526114)    1.0
  (2526115, 2526115)    1.0
  (2526116, 2526116)    1.0
  (2526117, 2526117)    1.0
  (2526118, 2526118)    1.0
  (2526119, 2526119)    1.0
  (2526120, 2526120)    1.0
</code></pre>
<p>Following fitting for normalization, I receive the following error:</p>
<pre><code>---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py in _get_column_indices(X, key)
    408         try:
--&gt; 409             all_columns = X.columns
    410         except AttributeError:

5 frames
AttributeError: columns not found

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py in _get_column_indices(X, key)
    410         except AttributeError:
    411             raise ValueError(
--&gt; 412                 &quot;Specifying the columns using strings is only &quot;
    413                 &quot;supported for pandas DataFrames&quot;
    414             )

ValueError: Specifying the columns using strings is only supported for pandas DataFrames
</code></pre>
<p>Am I one-hot encoding correctly? What is the appropriate way to do this? Should I directly implement the one-hot encoder in my normalization process?</p>
",12068687.0,,,,,2021-12-09 20:59:55,ValueError after attempting to use OneHotEncoder and then normalize values with make_column_transformer,<python><pandas><tensorflow><deep-learning><one-hot-encoding>,2,0,,,,CC BY-SA 4.0
71321954,1,,,2022-03-02 11:31:39,,5,222,"<p>I have managed to install tensorflow-macos on my Macbook with M1 chip</p>
<p><a href=""https://i.stack.imgur.com/FttnN.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/FttnN.png"" alt=""enter image description here"" /></a></p>
<p>But whenever I try to install a package that requires tensorflow I get an error, stating that tensorflow is not/can not be installed.</p>
<p><a href=""https://i.stack.imgur.com/oGQJI.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/oGQJI.png"" alt=""enter image description here"" /></a></p>
<p>How can I work around this issue? I assume tensorflow-macos is the replacement of tensorflow for Macs running arm64 chips but how can I actually make the packages know that it is?</p>
",13713586.0,,,,,2022-03-02 11:31:39,Install packages that rely on Tensorflow with 'Tensorflow-macos',<python><macos><tensorflow><apple-m1><arm64>,0,2,0.0,,,CC BY-SA 4.0
70091290,1,70091889.0,,2021-11-24 05:50:45,,5,1575,"<p>Is it possible to Crop/Resize images per batch ?</p>
<p>I'm using Tensorflow dataset API as below:</p>
<pre><code>dataset = dataset.shuffle().repeat().batch(batch_size, drop_remainder=True)
</code></pre>
<p>I want, within the batch all the images should have the same size. However across the batches it can have different sizes.</p>
<p>For example, 1st batch has all the images of shape (batch_size, 300, 300, 3). Next batch can have images of shape (batch_size, 224, 224, 3). Another batch can have images of shape (batch_size, 400, 400, 3).</p>
<p>Basically I want to have dynamically shaped batches, however all the images within the batch have static shapes.</p>
<p>If we do as follow:</p>
<pre><code>dataset = dataset.shuffle().repeat().batch(batch_size, drop_remainder=True).map(lambda x, y: map_fn(x, y))
</code></pre>
<p>Does the above .map() applies to each batch separately or over the entire dataset ?</p>
<p>If above .map() doesn't apply to each batch separately, how can we do this ? Can we define any iterator after dataset.batch(), apply tf.image.crop_and_resize() over each image per batch and later use dataset.concatenate() to combine all transformed batches ?</p>
<p>I'm creating the dataset as below:</p>
<pre><code># Dataset creation (read image data from files of COCO dataset)
dataset = tf.data.Dataset.list_files(self._file_pattern, shuffle=False)
dataset = dataset.shard(dataset_num_shards, dataset_shard_index)
dataset = dataset.shuffle(tf.cast(256 / dataset_num_shards, tf.int64))
dataset = dataset.interleave(map_func=tf.data.TFRecordDataset(filename).prefetch(1), cycle_length=32, block_length=1, num_parallel_calls=tf.data.experimental.AUTOTUNE)
dataset = dataset.map(tf_example_decoder.TfExampleDecoder().decode, num_parallel_calls=64)
dataset = dataset.shuffle(64).repeat()
# Parse each image for preprocessing
dataset = dataset.map(lambda data, _: _parse_example(data), num_parallel_calls=64)
dataset = dataset.batch(batch_size=batch_size, drop_remainder=True)

# Below code suggested by you to resize images to fixed shape in each batch
def resize_data(images, labels):
    tf.print('Original shape --&gt;', tf.shape(images))
    SIZE = (300, 300)
    return tf.image.resize(images, SIZE), labels
dataset = dataset.map(resize_data)
dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)

tf.estimator.Estimator(...).train(
        input_fn=dataset,
        steps=steps,
        hooks=train_hooks)
</code></pre>
",13679128.0,,13302.0,,2022-07-08 18:40:26,2022-07-08 18:40:26,Tensorflow Datasets: Crop/Resize images per batch after dataset.batch(),<python><tensorflow><tensorflow-datasets>,1,0,,,,CC BY-SA 4.0
71335585,1,71336842.0,,2022-03-03 10:28:24,,5,10418,"<p>Not always, but occasionally when running my code this error appears.</p>
<p>At first, I doubted it was a connectivity issue but to do with cashing issue, as discussed on an older <a href=""https://github.com/huggingface/transformers/issues/8690"" rel=""noreferrer"">Git Issue</a>.</p>
<p>Clearing cache didn't help runtime:</p>
<pre class=""lang-sh prettyprint-override""><code>$ rm ~/.cache/huggingface/transformers/ *
</code></pre>
<p>Traceback references:</p>
<ul>
<li>NLTK also gets <code>Error loading stopwords: &lt;urlopen error [Errno -2] Name or service not known</code>.</li>
<li>Last 2 lines re <code>cached_path</code> and <code>get_from_cache</code>.</li>
</ul>
<hr />
<p>Cache (before cleared):</p>
<pre class=""lang-sh prettyprint-override""><code>$ cd ~/.cache/huggingface/transformers/
(sdg) me@PF2DCSXD:~/.cache/huggingface/transformers$ ls
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.json
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.json
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.lock
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.json
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.json
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.json
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock
</code></pre>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, set_seed

generator = pipeline('text-generation', model='gpt2')  # Error
set_seed(42)
</code></pre>
<p>Traceback:</p>
<pre><code>2022-03-03 10:18:06.803989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-03-03 10:18:06.804057: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[nltk_data] Error loading stopwords: &lt;urlopen error [Errno -2] Name or
[nltk_data]     service not known&gt;
2022-03-03 10:18:09.216627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-03-03 10:18:09.216700: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-03-03 10:18:09.216751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PF2DCSXD): /proc/driver/nvidia/version does not exist
2022-03-03 10:18:09.217158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-03 10:18:09.235409: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
All model checkpoint layers were used when initializing TFGPT2LMHeadModel.

All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
Traceback (most recent call last):
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py&quot;, line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py&quot;, line 87, in _run_code
    exec(code, run_globals)
  File &quot;/mnt/c/Users/me/Documents/GitHub/project/foo/bar/__main__.py&quot;, line 26, in &lt;module&gt;
    nlp_setup()
  File &quot;/mnt/c/Users/me/Documents/GitHub/project/foo/bar/utils/Modeling.py&quot;, line 37, in nlp_setup
    generator = pipeline('text-generation', model='gpt2')
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/pipelines/__init__.py&quot;, line 590, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 463, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py&quot;, line 324, in get_tokenizer_config
    resolved_config_file = get_file_from_repo(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 2235, in get_file_from_repo
    resolved_file = cached_path(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 1846, in cached_path
    output_path = get_from_cache(
  File &quot;/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py&quot;, line 2102, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.
</code></pre>
<hr />
<p><strong>Failed Attempts</strong></p>
<ol>
<li>I closed my IDE and bash terminal. Ran <code>wsl.exe --shutdown</code> in PowerShell. Relaunched IDE and bash terminal with same error.</li>
<li>Disconnecting/ different VPN.</li>
<li>Clear cache <code>$ rm ~/.cache/huggingface/transformers/ *</code>.</li>
</ol>
",16852041.0,,16852041.0,,2022-03-03 13:51:06,2023-05-15 19:12:21,"HuggingFace | ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet con",<python-3.x><tensorflow><huggingface-transformers><valueerror><gpt-2>,4,3,,,,CC BY-SA 4.0
69031604,1,69042554.0,,2021-09-02 13:49:14,,5,8443,"<p>I am fairly new to Tensorflow and I am having trouble with Dataset. I work on Windows 10, and the Tensorflow version is 2.6.0 used with CUDA.
I have 2 numpy arrays that are X_train and X_test (already split). The train is 5Gb and the test is 1.5Gb.
The shapes are:</p>
<p>X_train: (259018, 30, 30, 3), &lt;class 'numpy.ndarray'&gt;</p>
<p>Y_train: (259018, 1), &lt;class 'numpy.ndarray'&gt;</p>
<p>I create Datasets using the following code:</p>
<pre><code>dataset_train = tf.data.Dataset.from_tensor_slices((X_train , Y_train)).batch(BATCH_SIZE)
</code></pre>
<p>And BATCH_SIZE = 32.</p>
<p>But I cannot create a Dataset, I get the following error:</p>
<pre><code>2021-09-02 15:26:35.429930: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-09-02 15:26:35.772235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3495 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6
2021-09-02 15:26:36.414627: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2700000000 exceeds 10% of free system memory.
2021-09-02 15:26:47.146977: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 607.1KiB (rounded to 621824)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2021-09-02 15:26:47.147299: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc
2021-09-02 15:26:47.147383: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.147514: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.147636: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024):     Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2021-09-02 15:26:47.147761: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.147905: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148040: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148157: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384):    Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148276: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768):    Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148402: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536):    Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148518: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148645: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148786: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.148918: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576):  Total Chunks: 1, Chunks in use: 1. 1.91MiB allocated for chunks. 1.91MiB in use in bin. 1.91MiB client-requested in use in bin.
2021-09-02 15:26:47.149079: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.149212: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.149342: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.

2021-09-02 15:26:47.149477: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.

2021-09-02 15:26:47.164471: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.164619: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.164765: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728):    Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-09-02 15:26:47.164884: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456):    Total Chunks: 2, Chunks in use: 2. 3.41GiB allocated for chunks. 3.41GiB in use in bin. 3.30GiB client-requested in use in bin.
2021-09-02 15:26:47.164982: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 607.2KiB was 512.0KiB, Chunk State: 
2021-09-02 15:26:47.165040: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 3665166336
2021-09-02 15:26:47.165106: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at b0e200000 of size 2700000000 next 1
2021-09-02 15:26:47.165159: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at baf0ebb00 of size 1280 next 2
2021-09-02 15:26:47.165208: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at baf0ec000 of size 2000128 next 3
2021-09-02 15:26:47.165250: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at baf2d4500 of size 963164928 next 18446744073709551615
2021-09-02 15:26:47.165297: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: 
2021-09-02 15:26:47.165341: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB
2021-09-02 15:26:47.165382: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 2000128 totalling 1.91MiB
2021-09-02 15:26:47.165426: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 963164928 totalling 918.54MiB
2021-09-02 15:26:47.165470: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 2700000000 totalling 2.51GiB
2021-09-02 15:26:47.165514: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 3.41GiB
2021-09-02 15:26:47.165558: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 3665166336 memory_limit_: 3665166336 available bytes: 0 curr_region_allocation_bytes_: 7330332672
2021-09-02 15:26:47.165633: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: 
Limit:                      3665166336
InUse:                      3665166336
MaxInUse:                   3665166336
NumAllocs:                           4
MaxAllocSize:               2700000000
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2021-09-02 15:26:47.165771: W tensorflow/core/common_runtime/bfc_allocator.cc:468] *************************************************************************************************xxx
Traceback (most recent call last):
  File &quot;C:/Users/headl/Documents/github projects/datascience/DL_model_deep_insight.py&quot;, line 100, in &lt;module&gt;
    dataset_train, dataset_test = prepare_tf_dataset(path_to_x_train, config.y_train_combined,
  File &quot;C:/Users/headl/Documents/github projects/datascience/DL_model_deep_insight.py&quot;, line 28, in prepare_tf_dataset
    dataset_test = tf.data.Dataset.from_tensor_slices((X_test , Y_test)).batch(BATCH_SIZE)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;, line 685, in from_tensor_slices
    return TensorSliceDataset(tensors)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;, line 3844, in __init__
    element = structure.normalize_element(element)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\data\util\structure.py&quot;, line 129, in normalize_element
    ops.convert_to_tensor(t, name=&quot;component_%d&quot; % i, dtype=dtype))
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\profiler\trace.py&quot;, line 163, in wrapped
    return func(*args, **kwargs)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 1566, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\framework\tensor_conversion_registry.py&quot;, line 52, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 271, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 283, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 308, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File &quot;C:\Users\headl\Documents\virtual_env\datascience\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 106, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.

Process finished with exit code 1
</code></pre>
<p>There seems to be a problem of running out of GPU memory, and indeed, when I follow this process in the Windows task manager I can see a peak in GPU usage just before the script dies.
I tried to use only some part of the X_train. I can create a Dataset up to X_train[:240000]. When I add more rows after that, the error appears.
I thought that the Tensorflow Dataset is a generator that was supposed to take care of the memory problem, along with batches? Also, reducing the batch size did not have any effect.
I also tried to do the suggested 'TF_GPU_ALLOCATOR=cuda_malloc_async' but it didn't work neither.</p>
<p>What can I do to load the whole data?</p>
<p>Thank you very much in advance!</p>
",15453731.0,,,,,2022-03-23 17:54:53,Tensorflow running out of GPU memory: Allocator (GPU_0_bfc) ran out of memory trying to allocate,<python><tensorflow><tensorflow2.0><tensorflow-datasets>,2,0,0.0,,,CC BY-SA 4.0
73991600,1,74016817.0,,2022-10-07 18:58:00,,5,168,"<p>I have the following code in TensorFlow 1.0. I tried to migrate it to TensorFlow 2.0 using tf_upgrade_v2 script. However, it didnt find an equivalent function in the tf-2 compact version.</p>
<p>I was recommended to use tensorflow_addons. However, I dont see an equivalent attention_decoder in the tf_addons module. Please guide me.</p>
<pre><code>decoder_outputs,decoder_state = tf.contrib.legacy_seq2seq.attention_decoder(
            decoder_inputs = decoder_inputs,
            initial_state = encoder_state,
            attention_states = encoder_outputs,
            cell = cell,
            output_size = word_embedding_dim,
            loop_function = None if mode=='pretrain' else feed_prev_loop,
            scope = scope
        )
</code></pre>
<p>The link to tf 1.0 code is here:
<a href=""https://github.com/yaushian/CycleGAN-sentiment-transfer/blob/master/lib/seq2seq.py"" rel=""nofollow noreferrer"">https://github.com/yaushian/CycleGAN-sentiment-transfer/blob/master/lib/seq2seq.py</a></p>
",16028265.0,,16028265.0,,2022-10-10 06:27:02,2022-10-10 14:54:04,Equivalent of tf.contrib.legacy_seq2seq.attention_decoder in tensorflow 2 after upgrade,<python><tensorflow><tensorflow2.0><seq2seq><tensorflow1.15>,1,1,,,,CC BY-SA 4.0
62987784,1,,,2020-07-20 02:26:17,,5,165,"<p>During a project, I have prototyped an adaptation of the K-means algorithm in octave. If it is not possible to run Octave within a HTML file, how would I turn this K-means algorithm which is written in octave into Tensorflow.js so that I would be able to run it in a browser.:</p>
<pre><code>x = [1,2,3,4,5,6;7,8,9,10,11,12;13,14,15,16,17,18]
time = [1,2,3,4,5,6]

k = size(x)
foo = 0
mu = zeros(k(1),1)

for i = 1:k(2),
  mu = [x(:,i)/pinv(time(i)), mu]
  foo = foo+1
endfor
mu(:,[foo+1]) = [];
usr = sum(mu') / numel(x)
usr = usr'


x = [x,usr]



centroids = [x(:,randi([1,size(x)(2)])), x(:,randi([1,size(x)(2)]))]


if centroids(:,1) == centroids(:,2),
  for i = 1:500,
    centroids = [x(:,randi([1,size(x)(2)])), x(:,randi([1,size(x)(2)]))]
  endfor
endif


K = size(centroids, 2);

idx = zeros(size(x,1), 1);


for c = 1:500,

for i = 1:size(x,2),
    min = Inf;
    for j = 1:K,
        diff = sum((x(:,i) - centroids(:,j)).^2);
        if min &gt; diff
            min = diff;
            idx(i) = j;
        end
    end
end

for i = 1:size(centroids,2),
  xi = x(:,idx==i)
  ck = size(xi,2);
  centroids(:,i) = [sum(xi,2) * (1/size(x(:,idx==i),2))]
endfor
endfor
</code></pre>
",12504573.0,,,,,2020-07-22 12:57:59,Turning K-means written in Octave into Tensorflow.js,<javascript><tensorflow><octave><k-means><tensorflow.js>,1,0,,,,CC BY-SA 4.0
67443545,1,,,2021-05-08 02:31:38,,5,1151,"<p>Official TF documentation [1] suggests 2 ways to control GPU memory allocation</p>
<p>Memory growth allows TF to grow memory based on usage</p>
<pre><code>tf.config.experimental.set_memory_growth(gpus[0], True)
</code></pre>
<p>Virtual device config sets limit to memory</p>
<pre><code>tf.config.experimental.set_virtual_device_configuration(
  gpus[0],
  [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])

</code></pre>
<p>In this case, can these two statements be used together in conjunction? or are these mutually exclusive and opposite?</p>
<p>Speak: Can we set memory growth to true but at the same time restrict memory limit?</p>
<p>Refer</p>
<p>[1] <a href=""https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth"" rel=""noreferrer"">https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth</a></p>
",5157515.0,,,,,2021-05-08 02:31:38,tensorflow gpu - can memory growth and memory limit be used in conjunction?,<python><tensorflow><memory><gpu>,0,0,0.0,,,CC BY-SA 4.0
65446130,1,65587301.0,,2020-12-25 07:32:27,,5,6710,"<p>Yolov3-tiny-416.tflite is a tflite model for yolov3 tiny model created from yolov3-tiny.weights
I had tried to use this from ML kit Vision module provided by google in android. In repo: <a href=""https://github.com/googlesamples/mlkit/tree/master/android/vision-quickstart"" rel=""noreferrer"">https://github.com/googlesamples/mlkit/tree/master/android/vision-quickstart</a></p>
<p>This is the way I have load and and choose detection options for yolo v3 tiny tflite model.</p>
<pre><code>LocalModel localModel = new LocalModel.Builder()
              .setAssetFilePath(&quot;yolov3-tiny-416.tflite&quot;)
              .build();
CustomObjectDetectorOptions customObjectDetectorOptions = PreferenceUtils.getCustomObjectDetectorOptionsForLivePreview(this,localModel);
cameraSource.setMachineLearningFrameProcessor(new ObjectDetectorProcessor(this,customObjectDetectorOptions));
</code></pre>
<p>Now, I have encounterd a error that says:</p>
<pre><code>E/MobileVisionBase: Error preloading model resource
b.a.d.a.a: Failed to initialize detector. Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images. 
</code></pre>
<p>As I know from error, I need to specify NormalizationOptions Metadata to process Image. So, How the problem can be solved? any Suggestion?</p>
",9670031.0,,9670031.0,,2020-12-25 07:44:38,2021-01-05 22:09:40,it requires specifying NormalizationOptions metadata to preprocess input images,<android><tensorflow><yolo><google-mlkit>,1,0,0.0,,,CC BY-SA 4.0
63138030,1,64080413.0,,2020-07-28 15:47:39,,5,9018,"<p>I am trying to predict the Global Sales from the values 'Name', 'Platform', 'Genre', 'Publisher' and 'Year' from this dataset here: <a href=""https://www.kaggle.com/gregorut/videogamesales"" rel=""noreferrer"">https://www.kaggle.com/gregorut/videogamesales</a></p>
<p>This is my code for training the model:</p>
<pre><code>from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
from six.moves import urllib

import tensorflow as tf

dftrain = pd.read_csv('./vgsales_eval.csv')
dfeval = pd.read_csv('./vgsales_train.csv')

print(dftrain[dftrain.isnull().any(axis=1)])

y_train = dftrain.pop('Global_Sales')
y_eval = dfeval.pop('Global_Sales')

CATEGORICAL_COLUMNS = ['Name', 'Platform', 'Genre', 'Publisher']
NUMERIC_COLUMNS = ['Year']

feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = dftrain[feature_name].unique()  # gets a list of all unique values from given feature column
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.int64))

print(feature_columns)

def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):
  def input_function():  
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  
    if shuffle:
      ds = ds.shuffle(1000)  
    ds = ds.batch(batch_size).repeat(num_epochs)  
    return ds
  return input_function  

train_input_fn = make_input_fn(dftrain, y_train)  
eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)
linear_est.train(train_input_fn)
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\util\structure.py&quot;, line 93, in normalize_element
    spec = type_spec_from_value(t, use_fallback=False)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\util\structure.py&quot;, line 466, in type_spec_from_value
    (element, type(element).__name__))
TypeError: Could not build a TypeSpec for 0                 Tecmo Koei
1       Nippon Ichi Software
2                    Ubisoft
3                 Activision
4                      Atari
                ...
6594                   Kemco
6595              Infogrames
6596              Activision
6597                7G//AMES
6598                 Wanadoo
Name: Publisher, Length: 6599, dtype: object with type Series

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;c:\Users\kuhn-\Documents\Github\Tensorflow_Test\VideoGameSales_Test\main.py&quot;, line 45, in &lt;module&gt;
    linear_est.train(train_input_fn)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py&quot;, line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py&quot;, line 1175, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py&quot;, line 1201, in _train_model_default
    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py&quot;, line 1037, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py&quot;, line 1130, in _call_input_fn
    return input_fn(**kwargs)
  File &quot;c:\Users\kuhn-\Documents\Github\Tensorflow_Test\VideoGameSales_Test\main.py&quot;, line 34, in input_function
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;, line 682, in from_tensor_slices
    return TensorSliceDataset(tensors)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py&quot;, line 3001, in __init__
    element = structure.normalize_element(element)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\util\structure.py&quot;, line 98, in normalize_element
    ops.convert_to_tensor(t, name=&quot;component_%d&quot; % i))
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py&quot;, line 1499, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 338, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 264, in constant
    allow_broadcast=True)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\constant_op.py&quot;, line 282, in _constant_impl
    allow_broadcast=allow_broadcast))
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py&quot;, line 563, in make_tensor_proto
    append_fn(tensor_proto, proto_values)
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py&quot;, line 155, in SlowAppendObjectArrayToTensorProto
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\tensor_util.py&quot;, line 155, in &lt;listcomp&gt;
    tensor_proto.string_val.extend([compat.as_bytes(x) for x in proto_values])
  File &quot;C:\Users\kuhn-\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\compat.py&quot;, line 87, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got nan
</code></pre>
<p>What am I doing wrong here? Is it a problem with the dataset or do I have to read the values differently?</p>
",14010684.0,,,,,2021-06-14 10:42:12,TypeError: Could not build a TypeSpec for a column,<python><python-3.x><tensorflow><machine-learning>,1,2,0.0,,,CC BY-SA 4.0
66090385,1,66091023.0,,2021-02-07 16:43:42,,5,590,"<p>I was prunning a model and came across a library TensorFlow model optimization so initially, we have
<a href=""https://i.stack.imgur.com/Ovnhq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Ovnhq.png"" alt=""in this image my model have 20410 parameters in total"" /></a></p>
<p>I trained this model on a default dataset and it gave me an accuracy of 96 percent which is good. then I saved the model in a JSON file and saved its weight in h5 file now I loaded this model into another script to prune it after applying prunning and compiling the model I got this model summary
<a href=""https://i.stack.imgur.com/7h0t7.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7h0t7.png"" alt=""enter image description here"" /></a></p>
<p>although the model is prunned well and there is a significant amount of reduction in parameters but the problem here is why parameters increased after applying the prunning and also even after rmoving non-trainable parameters still the prunned and simple model has same number of parameters can anyone explain me if this is normal or i am doing something wrong. Also please explain why this is happening. Thank you in advance to all of you :)</p>
",13006519.0,,,,,2021-02-08 13:17:24,Why parameters in prunning increases in tensorflow's tfmot,<python><tensorflow><machine-learning><deep-learning><pruning>,1,0,,,,CC BY-SA 4.0
65079318,1,,,2020-11-30 18:41:19,,5,5960,"<p>I got the following error output in the PyTorch when sent model predictions into the model. Does anyone know what's going on?</p>
<p>Following are the architecture model that I created, in the error output, it shows the issue exists in the x = self.fc1(cls_hs) line.</p>
<pre><code>class BERT_Arch(nn.Module):

    def __init__(self, bert):
      
      super(BERT_Arch, self).__init__()

      self.bert = bert 
      
      # dropout layer
      self.dropout = nn.Dropout(0.1)
      
      # relu activation function
      self.relu =  nn.ReLU()

      # dense layer 1
      self.fc1 = nn.Linear(768,512)
      
      # dense layer 2 (Output layer)
      self.fc2 = nn.Linear(512,2)

      #softmax activation function
      self.softmax = nn.LogSoftmax(dim=1)

    #define the forward pass
    def forward(self, sent_id, mask):

      #pass the inputs to the model  
      _, cls_hs = self.bert(sent_id, attention_mask=mask)
      print(mask)
      print(type(mask))
      
      x = self.fc1(cls_hs)

      x = self.relu(x)

      x = self.dropout(x)

      # output layer
      x = self.fc2(x)
      
      # apply softmax activation
      x = self.softmax(x)

      return x
</code></pre>
<pre><code>/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
   1686         if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):
   1687             return handle_torch_function(linear, tens_ops, input, weight, bias=bias)
-&gt; 1688     if input == 2 and bias is not None:
   1689         print(input)
   1690         # fused op is marginally faster
AttributeError: 'str' object has no attribute 'dim'
</code></pre>
",10799847.0,,10799847.0,,2020-11-30 19:16:58,2021-04-16 11:25:51,AttributeError: 'str' object has no attribute 'dim' in pytorch,<python><python-3.x><tensorflow><machine-learning><bert-language-model>,3,0,,,,CC BY-SA 4.0
70382083,1,,,2021-12-16 16:01:10,,5,1968,"<p>I have a conda env that I build from a requirements.yml file that I obtained from a classmate so we could work on a project together. I tried installing matplotlib and it resulted in a gigantic list of incompatibilities that I don't think I could even start tackling manually.</p>
<p>Here are the most important packages I'm using (the ones that have come up in a few other posts I've looked at and what the error looks like):</p>
<ul>
<li>python 3.9.7</li>
<li>tensorflow 2.6.0</li>
<li>anaconda 4.11</li>
<li>numpy 1.21.2</li>
<li>tornado 6.1</li>
</ul>
<p>Is there a way of adressing this without going into every line of the error?:</p>
<p>The part of the error containing matplotlib incompatibilities specifically:</p>
<pre><code>- matplotlib -&gt; cycler[version='&gt;=0.10'] -&gt; six[version='&gt;=1.5']
  - matplotlib -&gt; libpng[version='&gt;=1.6.32,&lt;1.7.0a0|&gt;=1.6.34,&lt;1.7.0a0|&gt;=1.6.35,&lt;1.7.0a0|&gt;=1.6.36,&lt;1.7.0a0|&gt;=1.6.37,&lt;1.7.0a0']
  - matplotlib -&gt; matplotlib-base[version='&gt;=3.5.0,&lt;3.5.1.0a0'] -&gt; numpy[version='&gt;=1.15.4,&lt;2.0a0|&gt;=1.16.6,&lt;2.0a0|&gt;=1.19.2,&lt;2.0a0']
  - matplotlib -&gt; matplotlib-base[version='&gt;=3.5.0,&lt;3.5.1.0a0'] -&gt; packaging[version='&gt;=20.0']
  - matplotlib -&gt; matplotlib-base[version='&gt;=3.5.0,&lt;3.5.1.0a0'] -&gt; pyparsing[version='&gt;=2.0.3,!=2.0.4,!=2.1.2,!=2.1.6|&gt;=2.2.1']
  - matplotlib -&gt; matplotlib-base[version='&gt;=3.5.0,&lt;3.5.1.0a0'] -&gt; python-dateutil[version='&gt;=2.1|&gt;=2.7']
  - matplotlib -&gt; numpy[version='&gt;=1.14.6,&lt;2.0a0'] -&gt; blas[version='*|1.0',build=mkl]
  - matplotlib -&gt; numpy[version='&gt;=1.14.6,&lt;2.0a0'] -&gt; icc_rt[version='&gt;=13.1.6|&gt;=2019.0.0|&gt;=16.0.4']
  - matplotlib -&gt; numpy[version='&gt;=1.14.6,&lt;2.0a0'] -&gt; mkl-service[version='&gt;=2,&lt;3.0a0|&gt;=2.3.0,&lt;3.0a0']
  - matplotlib -&gt; numpy[version='&gt;=1.14.6,&lt;2.0a0'] -&gt; mkl[version='&gt;=2018.0.0,&lt;2019.0a0|&gt;=2018.0.1,&lt;2019.0a0|&gt;=2018.0.2,&lt;2019.0a0|&gt;=2018.0.3,&lt;2019.0a0|&gt;=2019.1,&lt;2021.0a0|&gt;=2019.3,&lt;2021.0a0|&gt;=2019.4,&lt;2021.0a0|&gt;=2021.2.0,&lt;2022.0a0|&gt;=2021.3.0,&lt;2022.0a0|&gt;=2019.4,&lt;2020.0a0']
  - matplotlib -&gt; numpy[version='&gt;=1.14.6,&lt;2.0a0'] -&gt; mkl_random[version='&gt;=1.0.2,&lt;2.0a0|&gt;=1.2.1,&lt;2.0a0|&gt;=1.0.4,&lt;2.0a0']
  - matplotlib -&gt; numpy[version='&gt;=1.14.6,&lt;2.0a0'] -&gt; numpy-base[version='1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.11.3|1.14.3|1.14.3|1.14.3|1.14.4|1.14.4|1.14.4|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.5|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.14.6|1.15.0|1.15.0|1.15.0|1.15.0|1.15.1|1.15.1|1.15.1|1.15.1|1.15.2|1.15.2|1.15.2|1.15.2|1.15.2|1.15.3|1.15.3|1.15.3|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.15.4|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.0|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.1|1.16.2|1.16.2|1.16.2|1.16.3|1.16.3|1.16.3|1.16.4|1.16.4|1.16.5|1.16.5|1.16.5|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.16.6|1.17.2.*|1.17.3.*|1.17.4.*|1.18.1.*|1.18.5.*|1.19.1|1.19.1|1.19.1|1.19.2|1.19.2|1.19.2|1.19.2|1.20.1|1.20.1|1.20.1|1.20.2|1.20.2|1.20.2|1.20.3|1.20.3|1.20.3|1.21.2|1.17.0|1.17.0|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|1.9.3|&gt;=1.9.3,&lt;2.0a0',build='py27h0bb1d87_7|py35h5c71026_7|py36h5c71026_7|py27h0bb1d87_8|py35h4a99626_9|py27hfef472a_9|py37h8128ebf_9|py36h8128ebf_9|py35h8128ebf_9|py27h2753ae9_9|py27h2753ae9_10|py36h8128ebf_11|py37h2a9b21d_11|py36h2a9b21d_11|py27hb1d0314_11|py37hc3f5095_12|py38hc3f5095_12|py27h917549b_1|py35h5c71026_0|py27h0bb1d87_0|py35h5c71026_0|py27h0bb1d87_0|py27h0bb1d87_1|py36h5c71026_1|py37h5c71026_2|py27h0bb1d87_2|py27h0bb1d87_3|py36h5c71026_3|py27h0bb1d87_4|py37h5c71026_4|py36h5c71026_4|py35h4a99626_4|py37h8128ebf_4|py27h2753ae9_4|py35h8128ebf_4|py38hc3f5095_4|py37hc3f5095_5|py27hb1d0314_5|py36hc3f5095_5|py35h4a99626_0|py37h4a99626_0|py37h8128ebf_0|py35h8128ebf_0|py27h2753ae9_0|py36h8128ebf_0|py35h8128ebf_0|py37h8128ebf_0|py36h8128ebf_0|py37h8128ebf_0|py27h2753ae9_0|py37h8128ebf_0|py36h8128ebf_0|py36hc3f5095_0|py27hb1d0314_0|py36hc3f5095_0|py37hc3f5095_1|py27hb1d0314_1|py27hb1d0314_0|py36hc3f5095_0|py37hc3f5095_1|py27hb1d0314_1|py27hb1d0314_0|py27hb1d0314_0|py36hc3f5095_0|py36hc3f5095_0|py27hb1d0314_0|py36h5bb6eb2_3|py38h5bb6eb2_3|py37hc2deb75_0|py39h0829f74_0|py37h0829f74_0|py38h0829f74_0|py39hc2deb75_0|py38hc2deb75_0|py37hc2deb75_0|py38hc2deb75_0|py39hc2deb75_0|py38haf7ebc8_0|py39haf7ebc8_0|py37haf7ebc8_0|py39hbd0edd7_0|py36ha3acd2a_0|py37ha3acd2a_0|py38ha3acd2a_0|py36ha3acd2a_0|py37ha3acd2a_0|py38ha3acd2a_0|py39h5bb6eb2_3|py37h5bb6eb2_3|py39h2e04a8b_1|py38hc3f5095_0|py37hc3f5095_0|py37hc3f5095_0|py27hb1d0314_0|py36hc3f5095_0|py37hc3f5095_0|py36hc3f5095_0|py37hc3f5095_0|py36hc3f5095_0|py37hc3f5095_0|py36hc3f5095_1|py37hc3f5095_0|py36hc3f5095_1|py37hc3f5095_0|py37hc3f5095_0|py27hb1d0314_0|py27h2753ae9_0|py27h2753ae9_1|py36h8128ebf_0|py27h2753ae9_0|py27hfef472a_0|py36h4a99626_0|py36h8128ebf_4|py36hc3f5095_0|py37hc3f5095_0|py37h5c71026_3|py36h5c71026_2|py37h5c71026_1|py37h5c71026_0|py36h5c71026_0|py36h5c71026_0|py36h555522e_1|py35h555522e_1|py36hc3f5095_12|py27hb1d0314_12|py37h8128ebf_11|py37h8128ebf_10|py36h8128ebf_10|py35h8128ebf_10|py37h4a99626_9|py36h4a99626_9|py35h4a99626_8|py37h5c71026_8|py36h5c71026_8|py37h5c71026_7|py27h0bb1d87_7|py37h5c71026_7|py36h5c71026_7|py27h0bb1d87_6|py36h5c71026_6|py37h5c71026_6']
  - matplotlib -&gt; pyparsing
  - matplotlib -&gt; python-dateutil
  - matplotlib -&gt; python[version='&gt;=2.7,&lt;2.8.0a0'] -&gt; ca-certificates
  - matplotlib -&gt; python[version='&gt;=3.6,&lt;3.7.0a0'] -&gt; vs2015_runtime[version='&gt;=14.0.25123,&lt;15.0a0|&gt;=14.0.25420|&gt;=14.15.26706|&gt;=14.27.29016|&gt;=14.16.27012']
  - matplotlib -&gt; python[version='&gt;=3.9,&lt;3.10.0a0'] -&gt; openssl[version='&gt;=1.1.1a,&lt;1.1.2a|&gt;=1.1.1b,&lt;1.1.2a|&gt;=1.1.1c,&lt;1.1.2a|&gt;=1.1.1d,&lt;1.1.2a|&gt;=1.1.1e,&lt;1.1.2a|&gt;=1.1.1f,&lt;1.1.2a|&gt;=1.1.1g,&lt;1.1.2a|&gt;=1.1.1h,&lt;1.1.2a|&gt;=1.1.1i,&lt;1.1.2a|&gt;=1.1.1j,&lt;1.1.2a|&gt;=1.1.1k,&lt;1.1.2a|&gt;=1.1.1l,&lt;1.1.2a']
  - matplotlib -&gt; python[version='&gt;=3.9,&lt;3.10.0a0'] -&gt; pip
  - matplotlib -&gt; python[version='&gt;=3.9,&lt;3.10.0a0'] -&gt; sqlite[version='&gt;=3.25.3,&lt;4.0a0|&gt;=3.26.0,&lt;4.0a0|&gt;=3.27.2,&lt;4.0a0|&gt;=3.28.0,&lt;4.0a0|&gt;=3.29.0,&lt;4.0a0|&gt;=3.30.1,&lt;4.0a0|&gt;=3.31.1,&lt;4.0a0|&gt;=3.33.0,&lt;4.0a0|&gt;=3.35.4,&lt;4.0a0|&gt;=3.36.0,&lt;4.0a0|&gt;=3.32.3,&lt;4.0a0|&gt;=3.30.0,&lt;4.0a0|&gt;=3.35.1,&lt;4.0a0']
  - matplotlib -&gt; python[version='&gt;=3.9,&lt;3.10.0a0'] -&gt; tzdata
  - matplotlib -&gt; pytz
  - matplotlib -&gt; setuptools -&gt; wincertstore[version='&gt;=0.2']
  - matplotlib -&gt; tornado -&gt; certifi[version='&gt;=2016.09|&gt;=2016.9.26|&gt;=2020.06.20']
  - matplotlib -&gt; vc[version='14.*|&gt;=14.1,&lt;15.0a0|9.*']
  - matplotlib -&gt; vs2015_runtime[version='&gt;=14.16.27012,&lt;15.0a0']
  - matplotlib -&gt; zlib[version='&gt;=1.2.11,&lt;1.3.0a0']

</code></pre>
",15951179.0,,7758804.0,,2021-12-16 17:44:59,2022-05-10 19:54:35,conda install matplotlib results in huge list on incompatibilities,<numpy><tensorflow><matplotlib><anaconda><conda>,1,0,,,,CC BY-SA 4.0
63127784,1,63128324.0,,2020-07-28 05:47:30,,5,398,"<p>Tensorflow is a super heavy import. I want to import it only when it's needed. However, I have a model loading function like this:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Dict, Any
from keras.models import Model  # Heavy import! Takes 2 seconds or so!

# Model loading is a heavy task. Only do it once and keep it in memory
model = None  # type: Optional[Model]

def load_model(config: Dict[str, Any], shape) -&gt; Model:
    &quot;&quot;&quot;Load a model.&quot;&quot;&quot;
    if globals()['model'] is None:
        globals()['model'] = create_model(wili.n_classes, shape)
        print(globals()['model'].summary())
    return globals()['model']
</code></pre>
",562769.0,,,,,2021-10-06 11:50:32,How can I keep imports lightweight and still properly type annotate?,<python><tensorflow><mypy><python-typing>,1,0,,,,CC BY-SA 4.0
64684171,1,,,2020-11-04 16:41:18,,5,990,"<p>I am using the tensorflow <code>centernet_resnet50_v2_512x512_kpts_coco17_tpu-8</code> object detection model on a Nvidia <strong>Tesla P100</strong> to extract <strong>bounding boxes</strong> and <strong>keypoints</strong> for detecting people in a video. Using the pre-trained from tensorflow.org, I am able to process about 16 frames per second. Is there any way I can imporve the evaluation speed for this model? Here are some ideas I have been looking into:</p>
<ul>
<li><strong>Pruning the model graph</strong> since I am only detecting 1 type of object (people)
<ul>
<li>Have not been successful in doing this. Changing the <code>label_map</code> when building the model does not seem to improve performance.</li>
</ul>
</li>
<li><strong>Hard coding the input size</strong>
<ul>
<li>Have not found a good way to do this.</li>
</ul>
</li>
<li><strong>Compiling the model</strong> to an optimized form using something like <strong>TensorRT</strong>
<ul>
<li>Initial attempts to convert to TensorRT did not have any performance improvements.</li>
</ul>
</li>
<li><strong>Batching</strong> predictions
<ul>
<li>It looks like the pre-trained model has the batch size hard coded to 1, and so far when I try to change this using the <code>model_builder</code> I see a drop in performance.</li>
<li>My GPU utilization is about ~75% so I don't know if there is much to gain here.</li>
</ul>
</li>
</ul>
",2958221.0,,,,,2020-11-13 09:49:17,Optimize Tensorflow Object Detection Model V2 Centernet Model for Evaluation,<tensorflow><tensorflow2.0><object-detection><object-detection-api><tensorrt>,1,0,0.0,,,CC BY-SA 4.0
63653903,1,63660881.0,,2020-08-30 04:16:20,,5,7598,"<p>I was looking at the TensorFlow 2.0 Detection Zoo recently and found the <code>SSD MobileNet V2 FPNLite 320x320</code> pre-trained model and was wondering what the FPN part in &quot;FPNLite&quot; means/stands for.</p>
",13667786.0,,13667786.0,,2020-08-30 18:30:15,2021-09-11 07:25:45,What does FPN in SSD MobileNet V2 FPNLite 320x320 stand for?,<python><tensorflow><tensorflow2.0><tensorflow-lite><mobilenet>,2,0,,,,CC BY-SA 4.0
66765575,1,,,2021-03-23 14:50:01,,5,9423,"<p>I already installed Python 3.9.2 as it supports ARM64 as recommended in Python.org</p>
<p>I created a virtual environment after that using <code>python3 -m venv py39</code></p>
<p>Now I need to have another environment but with Python 3.8.8 as Tensorflow is supporting 3.8 only.
How could I create another virtual environment with Python 3.8 while maintaining the other 3.9 env.
In case of asking me to use <code>conda</code>, Does conda support Mac M1 ARM64 as it doesn't according to my search</p>
<p>I found same question asked many times but for windows and answers are very old like:
<a href=""https://stackoverflow.com/questions/1534210/use-different-python-version-with-virtualenv"">Use different Python version with virtualenv</a> 11 Years ago</p>
",7580912.0,,,,,2022-10-17 18:48:21,How to install multiple Python versions on Mac M1,<python><macos><tensorflow><virtualenv><python-venv>,4,1,0.0,,,CC BY-SA 4.0
68903962,1,,,2021-08-24 08:12:43,,5,21260,"<p>I have just bought a new Macbook M1, and am struggling to use Jupyter notebook in it.</p>
<p>These are the steps that I followed so far:</p>
<pre><code>
1. Installed Homebrew
2. Installed pyenv 
3. brew install miniforge
4. conda create -n new_env python=3.8.11
5. conda activate new_env
6. conda install Ipython
7. conda install ipykernel
8. conda install jupyter
9. pip install tensorflow
</code></pre>
<p>Then, I opened Jupyter notebook from this virtual env and tried to <code>import tensorflow</code> which gave me the error 'The kernel appears to have died. It will restart automatically'</p>
<p>Why does this happen? Is there anything that I missed? Could someone please help me with this?</p>
",13987643.0,,,,,2022-08-24 12:02:45,Mac M1 - The Kernel appears to have died. It will restart automatically,<macos><tensorflow><jupyter-notebook><anaconda><apple-m1>,2,2,0.0,,,CC BY-SA 4.0
66787279,1,66789185.0,,2021-03-24 18:35:49,,5,2604,"<p>I was trying to apply a deep learning algorithm(CNN) in python but after separating training-testing data and transforming time series to image step my <strong>Colab Notebook</strong> crashed and restarted itself again.</p>
<p>It gives an error like <strong>&quot;Your session crashed after using all RAM&quot;</strong> and when I checked <code>app.log</code> I saw something about <strong>tcmalloc: large alloc</strong>. I didn't find anything to fix this crashed.</p>
<p>Do you have any idea how to prevent this warning and fixed this situation?</p>
",7317549.0,,15791.0,,2021-03-25 08:13:01,2021-03-25 08:13:01,tcmalloc: large alloc python in Google Colab,<python><tensorflow><memory-management><google-colaboratory><tcmalloc>,1,2,,,,CC BY-SA 4.0
67441084,1,,,2021-05-07 20:06:44,,5,345,"<p>I am trying to train a <strong>seq-to-seq</strong> model on a simple sin wave. The target is to get <code>Nin</code> points of data and predict <code>Nout</code> next data points. Task seems simple and the model predicts well for large frequency <code>freq</code> (y = sin(freq * x)). For example, for <code>freq=4</code>, the loss is very low and the prediction is very close to the target. However, for low frequencies the prediction is bad. Any thoughts on why the model fails?</p>
<pre><code>from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed, Dense

freq = 0.25
Nin, Nout = 14, 14

# Helper function to convert 1d data to (input, target) samples
def windowed_dataset(y, input_window = 5, output_window = 1, stride = 1, num_features = 1):
    L = y.shape[0]
    num_samples = (L - input_window - output_window) // stride + 1
    X = np.zeros([input_window, num_samples, num_features])
    Y = np.zeros([output_window, num_samples, num_features])    
    for ff in np.arange(num_features):
        for ii in np.arange(num_samples):
            start_x = stride * ii
            end_x = start_x + input_window
            X[:, ii, ff] = y[start_x:end_x, ff]
            start_y = stride * ii + input_window
            end_y = start_y + output_window 
            Y[:, ii, ff] = y[start_y:end_y, ff]
    return X, Y

# The input shape is your sequence length and your token embedding size
inputs = Input(shape=(Nin, 1))
# Build a RNN encoder
encoder = LSTM(128, return_sequences=False)(inputs)
# Repeat the encoding for every input to the decoder
encoding_repeat = RepeatVector(Nout)(encoder)
# Pass your (5, 128) encoding to the decoder
decoder = LSTM(128, return_sequences=True)(encoding_repeat)
# Output each timestep into a fully connected layer
sequence_prediction = TimeDistributed(Dense(1, activation='linear'))(decoder)
model = Model(inputs, sequence_prediction)
model.compile('adam', 'mse')  # Or categorical_crossentropy
y = np.sin(freq * np.linspace(0, 10, 1000))[:, None]
Ntr = int(0.8 * y.shape[0])
y_train, y_test = y[:Ntr], y[Ntr:]
from generate_dataset import *
stride = 1
N_features = 1
Xtrain, Ytrain = windowed_dataset(y_train, input_window=Nin, output_window=Nout, stride=stride,
                                  num_features=N_features)
print(model.summary())
Xtrain, Ytrain = Xtrain.transpose(1, 0, 2), Ytrain.transpose(1, 0, 2)
print(&quot;Xtrain&quot;, Xtrain.shape)
model.fit(Xtrain, Ytrain, epochs=30)
plt.figure(); plt.plot(y, 'ro')
for Ns in arr([10, 50, 200, 400, 800, 1500, 3000]) // 10:
    ypred = model.predict(Xtrain[[Ns]])
    print(&quot;ypred&quot;, ypred.shape)
    ypred = ypred[-1]
    plt.figure()
    plt.plot(ypred, 'ro')
    plt.plot(Xtrain[Ns], 'm--')
    plt.plot(Ytrain[Ns], 'k.')
    plt.show()
exit()
</code></pre>
",1103122.0,,9215780.0,,2021-05-15 12:23:53,2021-05-15 12:24:51,Poor performance of seq-to-seq LSTM on simple sin wave with low frequency,<python><tensorflow><lstm><recurrent-neural-network>,1,0,0.0,,,CC BY-SA 4.0
63009227,1,63163366.0,,2020-07-21 07:11:38,,5,5546,"<p>colab offers free TPUs. It's easy to see how many cores are given, but I was wondering if its possible to see how much memory per core?</p>
",3259896.0,,,,,2020-07-29 22:26:41,"In google colab, is there a way to check what TPU verison is running?",<tensorflow><google-colaboratory><tpu>,1,0,0.0,,,CC BY-SA 4.0
69345540,1,,,2021-09-27 11:06:31,,5,1718,"<p>I would like to fine tune a pre-trained GAN available online using my own images. For example, BigGAN, which was trained on ImageNet, can generate realistic images. However, I do not want to generate the classes of images in ImageNet. I want to generate artificial images of my own image sets. How can I fine tune the pre-train models? Is it the same as fine-tuning other neural networks like a CNN image classification model? Is just replacing/retrain the last few layers is enough? It would be nice if I have see some examples in code of Tensorflow/Keras. Thanks so much!</p>
<p>BigGAN
<a href=""https://tfhub.dev/deepmind/biggan-deep-256/1"" rel=""noreferrer"">https://tfhub.dev/deepmind/biggan-deep-256/1</a></p>
",15064649.0,,,,,2022-01-12 01:24:15,How to fine tune a pre-trained GAN?,<tensorflow><deep-learning><generative-adversarial-network><transfer-learning><tensorflow-hub>,1,1,,,,CC BY-SA 4.0
73415571,1,,,2022-08-19 10:45:56,,5,189,"<p>I want to create content-based recommendation system with Tensorflow Recommenders, but I can’t find any resource about it. There are a few about collaborative filtering. including official tutorial, but can’t find content-based (where you recommend based on item attributes and not users interactions).</p>
<p>As I guess TFRS utilizes two tower model with user embedding and item embedding layers, then compiling it with TFRS retrieval task, but I can’t wrap my head around how I can translate it to item features without other users previous interactions (except the one who I should recommend items to).</p>
<p>I know I’m probably lacking knowledge of several important topics of neural nets, Keras layers, matrix factorization, but TFRS recommender seems to be easy to utilize for content-based filtering.</p>
<p>Please recommend what should I do, or any resources about this topic in particular or some prerequisite topics required for it.</p>
",10735631.0,,,,,2022-08-19 10:45:56,How to implement Content-Based Recommendation system with Tensorflow Recommenders (TFRS),<tensorflow><recommendation-engine><content-based-retrieval>,0,1,,,,CC BY-SA 4.0
66743171,1,,,2021-03-22 09:14:38,,5,360,"<p>I am trying to run the training of some models in tensorflow 2 object detection api.</p>
<p>I am using this command:</p>
<pre><code>gcloud ai-platform jobs submit training segmentation_maskrcnn_`date +%m_%d_%Y_%H_%M_%S` \
    --runtime-version 2.1 \
    --python-version 3.7 \
    --job-dir=gs://${MODEL_DIR} \
    --package-path ./object_detection \
    --module-name object_detection.model_main_tf2 \
    --region us-central1 \
    --scale-tier CUSTOM \
    --master-machine-type n1-highcpu-32 \
    --master-accelerator count=4,type=nvidia-tesla-p100 \
    -- \
    --model_dir=gs://${MODEL_DIR} \
    --pipeline_config_path=gs://${PIPELINE_CONFIG_PATH}
</code></pre>
<p>The training job is submitted successfully but when I look at my submitted job on AI platform I notice that it's not using the GPUs!
<a href=""https://i.stack.imgur.com/BRlbi.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/BRlbi.png"" alt=""enter image description here"" /></a></p>
<p>Also, when looking at the logs for my training job, I noticed that in some cases it couldn't open cuda. It would say something like this:</p>
<pre><code>Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib64
</code></pre>
<p>I was using AI platform for training a few months back and it was successful. I don't know what has changed now!
In fact, for my own setup, nothing has changed.</p>
<p>For the record, I am training Mask RCNN now. A few months back I trained Faster RCNN and SSD models.</p>
",4274552.0,,,,,2021-03-27 02:28:01,Training using object detection api is not running on GPUs in AI Platform,<tensorflow><object-detection><object-detection-api><gcp-ai-platform-training><google-ai-platform>,1,0,,,,CC BY-SA 4.0
66288078,1,66308458.0,,2021-02-20 04:38:23,,5,3252,"<p>In my experiment, I want to train my custom model on <code>imagenet</code> datasets. For simplicity, I am interested 10/100 class classification task. But, direct downloading <code>imagenet</code> dataset from <code>tfds</code> requires a lot of space on a hard disk. Is there any workaround we could subset <code>imagenet</code> dataset so the subsetted <code>imagenet</code> dataset could fit for 10/100 class classification task? Does anyone know any way of making this happen? any idea?</p>
<p>In general, <code>cifar10</code>, <code>cifar100</code> is quite handy to work with functional api of TensorFlow. But, in my experiment, I want to train my own model on <code>imagenet</code>. I want to avoid download <code>imagenet</code> dataset directly, instead, I want something less computational approach so I can train my custom model on subsetted <code>imagenet</code> (10 or 100 class classification). is there any way around to do this? any thoughts?</p>
<p><strong>my attempt to download <code>imagenet</code></strong></p>
<p>this is my attempt to download <code>imagenet</code> dataset locally, then train my custom model on <code>imagenet</code> dataset. But it is time-consuming to download and load data for training. But this is what I did:</p>
<pre><code>import keras
import tensorflow as tf
import tensorflow_datasets as tfds

## fetch imagenet dataset directly
imagenet = tfds.image.Imagenet2012()

## describe the dataset with DatasetInfo
C = imagenet.info.features['label'].num_classes
n_train = imagenet.info.splits['train'].num_examples
n_validation = imagenet.info.splits['validation'].num_examples

assert C == 1000
assert n_train == 1281167
assert n_validation == 50000

imagenet.download_and_prepare()   ## need more space in harddrive

# load imagenet data from disk as tf.data.Datasets
datasets = imagenet.as_dataset()
train_data, validation_data= datasets['train'], datasets['validation']
assert isinstance(train_data, tf.data.Dataset)
assert isinstance(validation_data, tf.data.Dataset)
</code></pre>
<p>If I do like this, this is time-consuming to download and needs more space on a hard-drive. Is there any easier way to subset <code>imagenet</code> dataset and get it from TensorFlow? Does anyone know an easier way of getting a smaller <code>imagenet</code> dataset for 10/100 classification task? any thoughts?</p>
<p><strong>desired output</strong></p>
<p>usually we can get <code>cifar10</code>, <code>cifar100</code> from <code>tf.keras.datasets</code>. Can we subset the imagenet dataset to something range to (200k ~ 500K)? Is there any less painful approach to get imagenet dataset for training a custom model on <code>imagenet</code> data? any idea?</p>
",7114383.0,,,,,2021-08-18 06:07:43,any easy way to get imagenet dataset for training custom model in tensorflow?,<python><tensorflow>,1,0,0.0,,,CC BY-SA 4.0
69480199,1,69526979.0,,2021-10-07 11:18:23,,5,2537,"<p>I want to download the GPT-2 model and tokeniser. For open-end generation, HuggingFace sets the padding token ID to be equal to the end-of-sentence token ID, so I configured it manually using :</p>
<pre><code>
import tensorflow as tf
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)
model = TFGPT2LMHeadModel.from_pretrained(&quot;gpt2&quot;, pad_token_id=tokenizer.eos_token_id)

</code></pre>
<p>However, it gives me the following error:</p>
<blockquote>
<p>TypeError: ('Keyword argument not understood:', 'pad_token_id')</p>
</blockquote>
<p>I haven't been able to find a solution for this nor do I understand why I am getting this error. Insights will be appreciated.</p>
",12113684.0,,13990016.0,,2021-10-11 15:09:56,2021-10-11 15:09:56,pad_token_id not working in hugging face transformers,<python><python-3.x><tensorflow><huggingface-transformers>,1,1,,,,CC BY-SA 4.0
63432215,1,63485898.0,,2020-08-16 01:08:36,,5,374,"<p>We got this 3D <code>input_tensor</code> which is a tensor representing <code>(batch_size, N, 2)</code>.</p>
<ul>
<li>Where,
<ul>
<li><code>batch_size = total batches</code></li>
<li><code>N = total predictions</code>,</li>
<li><code>2 = (label, score)</code></li>
</ul>
</li>
</ul>
<p>I want to add the score values (2nd column elements) where labels (1st column elements) are same per batch. For example, given this tensor with 3 batches, 4 predictions per batch and 2 elements; I want <code>required_output_tensor</code> as result.</p>
<p><strong>Condition:</strong> No <code>for loops</code> or <code>tf.map_fn()</code> for this answer. Reason, tf.map_fn() is SLOW on GPU with TF2.X. <a href=""https://stackoverflow.com/questions/57959305/tensorflow-how-to-combine-rows-of-tensor-with-summing-the-2nd-element-of-tensor"">You can take a look at my sample code here that is working on 2d tensor and I can use the same with tf.map_fn().</a></p>
<pre class=""lang-py prettyprint-override""><code>input_tensor = tf.constant([
    [
        [2., 0.7],
        [1., 0.1],
        [3., 0.4],
        [2., 0.8],
    ],
    [
        [2., 0.7],
        [1., 0.1],
        [1., 0.4],
        [4., 0.8],
    ],
    [
        [3., 0.7],
        [1., 0.1],
        [3., 0.4],
        [4., 0.8],
    ]
])

required_output_tensor = [
    [
        [2., 1.5],
        [1., 0.1],
        [3., 0.4],
    ],
    [
        [2., 0.7],
        [1., 0.5],
        [4., 0.8],
    ],
    [
        [3., 1.1],
        [1., 0.1],
        [4., 0.8],
    ]
]
</code></pre>
<p><strong>EDIT:</strong> I can see how we will end up with ragged tensor. In that case, I'm fine with choosing top-k elements per batch where k=min(size(smallest_batch)), or it can be hard coded to topk=2.</p>
<p><strong>EDIT 2:</strong> Adding additional input to try out the proposed solution:</p>
<pre class=""lang-py prettyprint-override""><code>additional_input_tensor = tf.constant([
    [
        [2., 0.5],
        [1., 0.1],
        [3., 0.4],
        [2., 0.5],
    ],
    [
        [22., 0.7],
        [11., 0.2],
        [11., 0.3],
        [44., 0.8],
    ],
    [
        [3333., 0.7],
        [1111., 0.1],
        [4444., 0.4],
        [5555., 0.8],
    ],
    [
        [2., 0.9],
        [1., 0.2],
        [5., 0.3],
        [2., 0.9],
    ]
])
</code></pre>
",4496896.0,,4496896.0,,2020-08-19 16:36:03,2020-08-19 16:50:54,How to combine tensor elements with similar values of specific column?,<python><tensorflow><tensorflow2.0>,2,4,,,,CC BY-SA 4.0
65718838,1,65979383.0,,2021-01-14 12:19:40,,5,3313,"<p>I have this problem I know quite a lot of people heard about. I upgraded from my laptop with GTX 1050 Ti to a PC with RTX 3060 Ti. I'm running everything in an Anaconda Virtual Environment. I've copied my env from the laptop to the PC. Now the TensorFlow GPU takes a lot of time to start up. Even if I write the 2 lines of code:</p>
<pre><code>from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
</code></pre>
<p>It takes a lot of time (more than 30 minutes). The same thing works perfectly on my laptop with GTX 1050 Ti.
I tried a lot of stuff:</p>
<ul>
<li>reinstalling every package in another environment (of course, same versions - I am using TF 2.1, cudnn 7.6.5, cudatoolkit 10.1.243).</li>
<li>putting some lines of code before my program (I tried more than 10 different possibilities).</li>
<li>reinstalling clean the GPU drivers.</li>
</ul>
<p>After TensorFlow starts up, the RTX 3060 Ti is working properly, training very fast.
I googled a lot, but I see that there are still a lot of people in my place right now, so I'm not expecting an answer pretty soon:).</p>
<p>Anyways, if someone manages to find an answer, please share it with me! Thanks in advance and have a great day!</p>
<p>P.S. If you need code or console logs, here you go. I've written a quick MNIST program:</p>
<pre><code>from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.layers import MaxPooling2D
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.optimizers import SGD
from numpy import mean
from numpy import std
from matplotlib import pyplot as plt
from sklearn.model_selection import KFold

# Load and prepare the train and test set
def load_dataset():
    # Load the dataset
    (trainX, trainY), (testX, testY) = mnist.load_data()
    # Reshape the dataset to have a single channel
    trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
    testX = testX.reshape((testX.shape[0], 28, 28, 1))
    # One hot encode target values
    trainY = to_categorical(trainY)
    testY = to_categorical(testY)
    return trainX, trainY, testX, testY

# Scale pixels
def prep_pixels(train, test):
    # Convert from integers to float
    train_norm = train.astype('float32')
    test_norm = test.astype('float32')
    # Normalize to range 0-1
    train_norm = train_norm / 255.0
    test_norm = test_norm / 255.0
    return train_norm, test_norm

# Define the CNN classifier
def define_classifier():
    # Build the structure
    classifier = Sequential()
    classifier.add(Conv2D(32, (3, 3), activation = 'relu', input_shape = (28, 28, 1)))
    classifier.add(MaxPooling2D(pool_size = (2, 2)))
    classifier.add(Conv2D(64, (3, 3), activation = 'relu'))
    classifier.add(Conv2D(64, (3, 3), activation = 'relu'))
    classifier.add(MaxPooling2D((2, 2)))
    classifier.add(Flatten())
    classifier.add(Dense(100, activation = 'relu'))
    classifier.add(Dense(10, activation = 'softmax'))
    # Compile the model
    classifier.compile(optimizer = SGD(lr = 0.01, momentum = 0.9), loss = 'categorical_crossentropy',
                       metrics = ['accuracy'])
    return classifier

# Evaluate the classifier using the K-Fold Cross-Validation
def evaluate_classifier(dataX, dataY, n_folds = 5):
    scores, histories = list(), list()
    # Prepare Cross-Validation
    kfold = KFold(n_folds, shuffle = True, random_state = 1)
    # Enumerate splits
    for trainX_i, testX_i in kfold.split(dataX):
        # Define classifier
        classifier = define_classifier()
        # Select rows for train and test
        trainX, trainY, testX, testY = dataX[trainX_i], dataY[trainX_i], dataX[testX_i], dataY[testX_i]
        # Fit the classifier
        history = classifier.fit(trainX, trainY, batch_size = 32, epochs = 10, 
                                 validation_data = (testX, testY), verbose = 1)
        # Evaluate the classifier
        _, acc = classifier.evaluate(testX, testY, verbose = 1)
        print('&gt; ACC: %.3f' % (acc * 100.0))
        # Store history, accuracy
        scores.append(acc)
        histories.append(history)
    return scores, histories

# Plot learning curves
def visualise_learning(histories):
    for i in range(len(histories)):
        plt.tight_layout()
        # Plot LOSS
        plt.subplot(2, 1, 1)
        plt.title('Cross-Entropy Loss')
        plt.plot(histories[i].history['loss'], color = 'blue', label = 'train')
        plt.plot(histories[i].history['val_loss'], color = 'orange', label = 'test')
        # Plot ACCURACY
        plt.subplot(2, 1, 2)
        plt.title('Classification Accuracy')
        plt.plot(histories[i].history['accuracy'], color = 'blue', label = 'train')
        plt.plot(histories[i].history['val_accuracy'], color = 'orange', label = 'test')
    plt.show()

# Summarize classifier performance
def summarize_performance(scores):
    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores) * 100, std(scores) * 100, len(scores)))
    
# Run all parts together
def run():
    trainX, trainY, testX, testY = load_dataset()
    trainX, testX = prep_pixels(trainX, testX)
    scores, histories = evaluate_classifier(trainX, trainY)
    visualise_learning(histories)
    summarize_performance(scores)
  
def save_model():
    trainX, trainY, testX, testY = load_dataset()
    trainX, testX = prep_pixels(trainX, testX)
    classifier = define_classifier()
    classifier.fit(trainX, trainY, epochs = 25, batch_size = 32, verbose = 1)
    classifier.save('final_classifier.h5')
    
##############################################################################################################

# make a prediction for a new image.
from tensorflow.keras.preprocessing.image import load_img
from tensorflow.keras.preprocessing.image import img_to_array
from tensorflow.keras.models import load_model

# load and prepare the image
def load_image(filename):
    # load the image
    img = load_img(filename, grayscale=True, target_size=(28, 28))
    # convert to array
    img = img_to_array(img)
    # reshape into a single sample with 1 channel
    img = img.reshape(1, 28, 28, 1)
    # prepare pixel data
    img = img.astype('float32')
    img = img / 255.0
    return img

# load an image and predict the class
def run_example():
    # load the image
    img = load_image('image.png')
    # load model
    model = load_model('final_classifier.h5')
    # predict the class
    digit = model.predict_classes(img)
    print(digit[0])

# entry point, run the example
#run_example()
run()
</code></pre>
<p>And here is the console log:</p>
<pre><code>Python 3.7.9 (default, Aug 31 2020, 17:10:11) [MSC v.1916 64 bit (AMD64)]
Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.

IPython 7.19.0 -- An enhanced Interactive Python.

runcell(0, 'C:/Python/Projects/Handwritten Digit Recognition/digit_recognizer.py')

2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll

2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.018731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-01-14 13:47:31.041720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:31.041751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll

2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.018731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-01-14 13:47:31.041720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:31.041751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.395981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:31.430370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:31.452057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:31.659034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:31.837570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.055598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.056116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0

2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.018731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-01-14 13:47:31.041720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:31.041751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.395981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:31.430370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:31.452057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:31.659034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:31.837570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.055598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.056116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:47:32.652696: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-01-14 13:47:32.655023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:32.655039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:32.655046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:32.655051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:32.655057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:32.655062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:32.655067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.655072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.655095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0

2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.018731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-01-14 13:47:31.041720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:31.041751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.395981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:31.430370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:31.452057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:31.659034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:31.837570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.055598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.056116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:47:32.652696: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-01-14 13:47:32.655023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:32.655039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:32.655046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:32.655051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:32.655057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:32.655062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:32.655067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.655072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.655095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:50:57.038023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-14 13:50:57.038040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2021-01-14 13:50:57.038045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2021-01-14 13:50:57.039526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6699 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
Train on 48000 samples, validate on 12000 samples
Epoch 1/10

2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.018731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-01-14 13:47:31.041720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:31.041751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.395981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:31.430370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:31.452057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:31.659034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:31.837570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.055598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.056116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:47:32.652696: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-01-14 13:47:32.655023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:32.655039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:32.655046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:32.655051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:32.655057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:32.655062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:32.655067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.655072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.655095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:50:57.038023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-14 13:50:57.038040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2021-01-14 13:50:57.038045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2021-01-14 13:50:57.039526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6699 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-01-14 13:50:57.563527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll

2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.018731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-01-14 13:47:31.041720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:31.041751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.395981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:31.430370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:31.452057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:31.659034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:31.837570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.055598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.056116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:47:32.652696: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-01-14 13:47:32.655023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:32.655039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:32.655046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:32.655051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:32.655057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:32.655062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:32.655067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.655072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.655095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:50:57.038023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-14 13:50:57.038040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2021-01-14 13:50:57.038045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2021-01-14 13:50:57.039526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6699 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-01-14 13:50:57.563527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:52:17.763274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
 7232/48000 [===&gt;..........................] - ETA: 1:21:26 - loss: 2.3010 - accuracy: 0.1114  
2021-01-14 13:47:28.396292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.018731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2021-01-14 13:47:31.041720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:31.041751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:31.395981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:31.430370: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:31.452057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:31.659034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:31.837570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.055598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.056116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:47:32.652696: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2021-01-14 13:47:32.655023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 3060 Ti computeCapability: 8.6
coreClock: 1.8GHz coreCount: 38 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-01-14 13:47:32.655039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2021-01-14 13:47:32.655046: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:47:32.655051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2021-01-14 13:47:32.655057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2021-01-14 13:47:32.655062: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2021-01-14 13:47:32.655067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2021-01-14 13:47:32.655072: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 13:47:32.655095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2021-01-14 13:50:57.038023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-14 13:50:57.038040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2021-01-14 13:50:57.038045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2021-01-14 13:50:57.039526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6699 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-01-14 13:50:57.563527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2021-01-14 13:52:17.763274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-01-14 14:05:23.645822: W tensorflow/stream_executor/gpu/redzone_allocator.cc:312] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation. This message will be only logged once.
48000/48000 [==============================] - 869s 18ms/sample - loss: 2.3019 - accuracy: 0.1101 - val_loss: 2.3014 - val_accuracy: 0.1144
</code></pre>
<p>It took a lot of time after the lines &quot;Adding visible gpu devices: 0&quot; and openning the dynamic libraries after that.</p>
",9040471.0,,,,,2021-01-31 13:00:36,Tensorflow GPU long startup time,<tensorflow><time><gpu><startup><rtx>,1,7,,,,CC BY-SA 4.0
65113746,1,,,2020-12-02 18:08:30,,5,391,"<p>I am preparing a <a href=""https://pypi.org/project/sagemaker-tensorflow/"" rel=""noreferrer"">sagemaker PIPE mode dataset</a> to train a time series model on <code>SageMaker</code> with <code>PIPE</code> mode. The <code>PipeModeDataset</code> is a <code>TensorFlow Dataset</code> for reading <code>SageMaker</code> Pipe Mode channels. I am using an augmented manifest file which contains image location on <code>S3</code> and the label each line. My model accept batches of images (512 x 512 x 1) with single label per batch as input. I thought of using the window function to bundle the images read from pipe. Please refer to following partial code for dataset generation.</p>
<pre><code>def _input_fn(channel):
    &quot;&quot;&quot;Returns a Dataset for reading from a SageMaker PipeMode channel.&quot;&quot;&quot;
    features = {
        'image-ref': tf.io.FixedLenFeature([], tf.string),
        'label': tf.io.FixedLenFeature([3], tf.int64),
    }
    
    def parse(record):
        parsed = tf.io.parse_single_example(record, features)
        image = tf.io.decode_png(parsed['image-ref'], channels=1, dtype=tf.uint8)
        image = tf.reshape(image, [512, 512, 1])
        label = parsed['label']
        return (image, label)

    ds = PipeModeDataset(channel, record_format='TFRecord', benchmark=True, benchmark_records_interval=100)
    ds = ds.map(parse)
    
    print (&quot;PipeModeDataset print0 = &quot; + str(ds))
    ds = ds.window(16, shift=1, drop_remainder=True)
    print (&quot;PipeModeDataset print1 = &quot; + str(ds))
    
    def window_func(window, label):
        window = window.batch(16, drop_remainder=True)
        label = label.batch(16, drop_remainder=True)
        
        print (&quot;window batch is = &quot; + str(window))
        print (&quot;label batch is = &quot; + str(label))
        
        window_np = np.stack(list(window.as_numpy_iterator()))
        label_np = np.stack(list(label.as_numpy_iterator())) # TODO: only get the last label
        
        return tf.data.Dataset.from_tensor_slices((window_np, label_np))
    
    ds = ds.flat_map(lambda window, label: window_func(window, label))
    ....
    ....
</code></pre>
<p>Getting following error at the moment. How to fix this? Recommend better ways if there is any.</p>
<pre><code>PipeModeDataset print0 = &lt;MapDataset shapes: ((512, 512, 1), (3,)), types: (tf.uint8, tf.int64)&gt;
PipeModeDataset print1 = &lt;WindowDataset shapes: (DatasetSpec(TensorSpec(shape=(512, 512, 1), dtype=tf.uint8, name=None), TensorShape([])), DatasetSpec(TensorSpec(shape=(3,), dtype=tf.int64, name=None), TensorShape([]))), types: (DatasetSpec(TensorSpec(shape=(512, 512, 1), dtype=tf.uint8, name=None), TensorShape([])), DatasetSpec(TensorSpec(shape=(3,), dtype=tf.int64, name=None), TensorShape([])))&gt;
window batch is = &lt;BatchDataset shapes: (16, 512, 512, 1), types: tf.uint8&gt;
label batch is = &lt;BatchDataset shapes: (16, 3), types: tf.int64&gt;

RuntimeError: in user code:

    /opt/ml/code/train_on_pipemode.py:104 None  *
        ds = ds.flat_map(lambda window, label: window_func(window, label))
    /opt/ml/code/train_on_pipemode.py:96 window_func  *
        window_np = np.stack(list(window.as_numpy_iterator()))
    /usr/local/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:518 as_numpy_iterator  **
        raise RuntimeError(&quot;as_numpy_iterator() is not supported while tracing &quot;

    RuntimeError: as_numpy_iterator() is not supported while tracing functions
</code></pre>
<p><a href=""https://stackoverflow.com/questions/60045971/runtimeerror-as-numpy-iterator-is-not-supported-while-tracing-functions"">This answer</a> says to enable eager execution, but it is enabled in my case when I printed <code>tf.executing_eagerly()</code>. I am training on <code>tensorflow 2.x</code>.</p>
<pre><code>Tensorflow version: 2.3.1
Eager execution: True
</code></pre>
",3363978.0,,,,,2020-12-02 18:08:30,RuntimeError: as_numpy_iterator() is not supported while tracing functions on tensorflow 2.x dataset on sagemaker,<tensorflow><runtime-error><tensorflow2.0><tensorflow-datasets><amazon-sagemaker>,0,0,,,,CC BY-SA 4.0
64506489,1,,,2020-10-23 19:52:19,,5,1808,"<p>I have been trying to understand (but miserably failing) how convolutions on images (with height, width, channels) are implemented in software.</p>
<p>I've heard people say their convolution implementation is done using GEMM, or done using &quot;Direct convolution&quot; or done using 1x1 kernels.</p>
<p>I find it very confusing and can't wrap my head around so many different ways it's described everywhere - I <em>thought</em> I understood a typical convolution like <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"" rel=""noreferrer"">pytorch conv2d</a> as a mathematical operation on an image, but what do they mean when someone says they do conv2d using one of the following ways?</p>
<ul>
<li>1x1 kernels or 1x1 convolution (what does kernel even mean here)</li>
<li>GEMM</li>
<li>&quot;direct convolution&quot;</li>
</ul>
<p>For doing Convolution using GEMM, what I understand based on <a href=""https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/"" rel=""noreferrer"">this paper</a> is that each of the input-image and filters are converted to 2d matrices using <code>im2col</code> and <code>im2row</code> ops and then these two are simply matrix-multiplied.</p>
<p>The 3d input image (height, width, input-channels) is converted to a 2d matrix, the 4-d kernel (output-channels, input-channels, kernel-height, kernel-width) is converted to a 2d matrix. Or does &quot;GEMM-based implementation of convolution&quot; mean something else? If that's what it means then how is it different than doing &quot;convolution using 1x1 kernels&quot;?</p>
",3268219.0,,3268219.0,,2020-10-23 19:58:54,2021-10-16 19:51:45,What does it mean to say convolution implementation is based on GEMM (matrix multiply) or it is based on 1x1 kernels?,<tensorflow><conv-neural-network><matrix-multiplication><convolution><tiling>,1,0,0.0,,,CC BY-SA 4.0
69752055,1,69898246.0,,2021-10-28 10:02:38,,5,572,"<p>I am trying to train a <code>seq2seq</code> model for language translation, and I am copy-pasting code from this <a href=""https://www.kaggle.com/aiswaryaramachandran/english-to-hindi-neural-machine-translation/notebook"" rel=""nofollow noreferrer"">Kaggle Notebook</a> on Google Colab. The code is working fine with CPU and GPU, but it is giving me errors while training on a TPU. This same question has been already asked <a href=""https://stackoverflow.com/questions/66189494/training-seq2seq-model-on-google-colab-tpu-with-big-dataset-keras"">here</a>.</p>
<p>Here is my code:</p>
<pre><code>    strategy = tf.distribute.experimental.TPUStrategy(resolver)
    
    with strategy.scope():
      model = create_model()
      model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy')
    
    model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),
                        steps_per_epoch = train_samples // batch_size,
                        epochs = epochs,
                        validation_data = generate_batch(X_test, y_test, batch_size = batch_size),
                        validation_steps = val_samples // batch_size)
</code></pre>
<p>Traceback:</p>
<pre><code>Epoch 1/2
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-60-940fe0ee3c8b&gt; in &lt;module&gt;()
      3                     epochs = epochs,
      4                     validation_data = generate_batch(X_test, y_test, batch_size = batch_size),
----&gt; 5                     validation_steps = val_samples // batch_size)

10 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    992           except Exception as e:  # pylint:disable=broad-except
    993             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 994               raise e.ag_error_metadata.to_exception(e)
    995             else:
    996               raise

ValueError: in user code:
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *
    return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **
    outputs = model.distribute_strategy.run(run_step, args=(data,))
...
ValueError: None values not supported.
</code></pre>
<p>I couldn't figure out the error, and I think the error is because of this <code>generate_batch</code> function:</p>
<pre><code>X, y = lines['english_sentence'], lines['hindi_sentence']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 34)

def generate_batch(X = X_train, y = y_train, batch_size = 128):
    while True:
        for j in range(0, len(X), batch_size):
 
            encoder_input_data = np.zeros((batch_size, max_length_src), dtype='float32')
            decoder_input_data = np.zeros((batch_size, max_length_tar), dtype='float32')
            decoder_target_data = np.zeros((batch_size, max_length_tar, num_decoder_tokens), dtype='float32')
            
            for i, (input_text, target_text) in enumerate(zip(X[j:j + batch_size], y[j:j + batch_size])):
                for t, word in enumerate(input_text.split()):
                    encoder_input_data[i, t] = input_token_index[word]
                for t, word in enumerate(target_text.split()):
                    if t&lt;len(target_text.split())-1:
                        decoder_input_data[i, t] = target_token_index[word]
                    if t&gt;0:

                        decoder_target_data[i, t - 1, target_token_index[word]] = 1.
            yield([encoder_input_data, decoder_input_data], decoder_target_data)
</code></pre>
<p>My Colab notebook - <a href=""https://colab.research.google.com/drive/1PMHr6dNKR8HFVLM9ngH8jBhkxEZhp-E1?usp=sharing"" rel=""nofollow noreferrer"">here</a><br>
Kaggle dataset - <a href=""https://www.kaggle.com/aiswaryaramachandran/hindienglish-corpora"" rel=""nofollow noreferrer"">here</a><br>
TensorFlow version - <code>2.6</code></p>
<p><strong>Edit</strong> - Please don't tell me to down-grade TensorFlow/Keras version to <code>1.x</code>. I can down-grade it to <code>TensorFlow 2.0, 2.1, 2.3</code> but not <code>1.x</code>. I don't understand <code>TensorFlow 1.x</code>. Also, there is no point in using a 3-year-old version.</p>
",14425501.0,,14425501.0,,2021-11-09 06:09:18,2021-11-09 12:35:34,ValueError: None values not supported. Code working properly on CPU/GPU but not on TPU,<python><tensorflow><machine-learning><deep-learning><tpu>,3,6,0.0,,,CC BY-SA 4.0
64533790,1,,,2020-10-26 08:39:14,,5,1806,"<p>I am trying to execute a tensorflow script on my local computer. But the following warning appears. It would be really helpful if someone can specify what the common cause for this warning is.</p>
<pre><code>2020-10-26 14:04:34.690753: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-26 14:04:34.707717: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fadab4339f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-26 14:04:34.707751: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
</code></pre>
<p>I tried tracing my code and this is the line that gives the error :</p>
<pre><code>model = load_model(MODEL_PATH)
</code></pre>
<p>and I import <code>load_model</code> using</p>
<pre><code>from keras.models import load_model
</code></pre>
",14520701.0,,14520701.0,,2020-10-26 19:37:44,2020-10-29 06:38:33,"**StreamExecutor device (0): Host, Default Version** in Tensorflow",<python><tensorflow><machine-learning><deep-learning>,1,0,0.0,,,CC BY-SA 4.0
62773324,1,,,2020-07-07 10:36:19,,5,1123,"<p>My question is related to this one <a href=""https://stackoverflow.com/questions/47068709/your-cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u"">here</a>, but I am using PyCharm and I set up my virtual environment with Python interpreter according to this <a href=""https://www.tensorflow.org/site-assets/downloads/marketing/cert/Setting_Up_TF_Developer_Certificate_Exam.pdf"" rel=""noreferrer"">guide</a>, page 5.</p>
<p>When I run my tensorflow code, I get the warning:</p>
<blockquote>
<p>Your CPU supports instructions that this TensorFlow binary was not
compiled to use: AVX2</p>
</blockquote>
<p>I could ignore it, but since my model fitting is quite slow, I would like to take advantage of it. However, I do not know how to update my system here in this virtual environment PyCharm setting to make use of AVX2?</p>
",2165335.0,,,,,2020-07-15 03:44:20,Update Tensorflow binary in virtual environment in PyCharm to use AVX2,<python><tensorflow><pycharm><avx2>,2,3,,,,CC BY-SA 4.0
65895454,1,65896426.0,,2021-01-26 03:19:58,,5,5384,"<p>When I try to install <code>tensorflowjs_converter</code> to convert my <code>model.h5</code> to a json file it doesn't install it, I used <code>pip3 install tensorflowjs</code> to install it and it returns this error message:</p>
<blockquote>
<p>ERROR: ResolutionImpossible: for help visit <a href=""https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies"" rel=""noreferrer"">https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies</a></p>
</blockquote>
<p>Here is what happened before the error appeared:</p>
<pre><code>Collecting tensorflowjs
  Using cached tensorflowjs-3.0.0-py3-none-any.whl (63 kB)
Collecting h5py&lt;3,&gt;=2.8.0
  Using cached h5py-2.10.0.tar.gz (301 kB)
Collecting tensorflowjs
  Using cached tensorflowjs-2.8.5-py3-none-any.whl (63 kB)
  Using cached tensorflowjs-2.8.4-py3-none-any.whl (63 kB)
  Using cached tensorflowjs-2.8.3-py3-none-any.whl (63 kB)
  Using cached tensorflowjs-2.8.2-py3-none-any.whl (63 kB)
  Using cached tensorflowjs-2.8.1-py3-none-any.whl (63 kB)
  Using cached tensorflowjs-2.8.0-py3-none-any.whl (63 kB)
  Using cached tensorflowjs-2.7.0-py3-none-any.whl (62 kB)
  Using cached tensorflowjs-2.6.0-py3-none-any.whl (61 kB)
  Using cached tensorflowjs-2.5.0-py3-none-any.whl (61 kB)
  Using cached tensorflowjs-2.4.0-py3-none-any.whl (61 kB)
Collecting h5py&gt;=2.8.0
  Using cached h5py-3.1.0-cp39-cp39-win_amd64.whl (2.7 MB)
Collecting PyInquirer==1.0.3
  Using cached PyInquirer-1.0.3.tar.gz (27 kB)
Collecting tensorflow-hub==0.7.0
  Using cached tensorflow_hub-0.7.0-py2.py3-none-any.whl (89 kB)
Collecting numpy&lt;1.19.0,&gt;=1.16.4
  Downloading numpy-1.18.5.zip (5.4 MB)
     |████████████████████████████████| 5.4 MB 930 kB/s
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Requirement already satisfied: six&gt;=1.12.0 in c:\python39\lib\site-packages (from tensorflowjs) (1.15.0)
Collecting tensorflowjs
  Using cached tensorflowjs-2.3.0-py3-none-any.whl (61 kB)
  Using cached tensorflowjs-2.1.0-py3-none-any.whl (60 kB)
  Using cached tensorflowjs-2.0.1.post1-py3-none-any.whl (60 kB)
  Using cached tensorflowjs-2.0.1-py3-none-any.whl (60 kB)
Requirement already satisfied: numpy&gt;=1.16.4 in c:\python39\lib\site-packages (from tensorflowjs) (1.19.5)
  Using cached tensorflowjs-2.0.0-py3-none-any.whl (57 kB)
  Using cached tensorflowjs-1.7.4.post1-py3-none-any.whl (57 kB)
  Using cached tensorflowjs-1.7.4-py3-none-any.whl (57 kB)
Collecting gast==0.2.2
  Using cached gast-0.2.2.tar.gz (10 kB)
Collecting tensorflowjs
  Using cached tensorflowjs-1.7.3-py3-none-any.whl (57 kB)
  Using cached tensorflowjs-1.7.2-py3-none-any.whl (57 kB)
  Using cached tensorflowjs-1.6.0-py3-none-any.whl (57 kB)
  Using cached tensorflowjs-1.5.2-py3-none-any.whl (56 kB)
  Using cached tensorflowjs-1.4.0-py3-none-any.whl (56 kB)
  Using cached tensorflowjs-1.3.2-py3-none-any.whl (53 kB)
  Using cached tensorflowjs-1.3.1.1-py3-none-any.whl (53 kB)
  Using cached tensorflowjs-1.3.1-py3-none-any.whl (53 kB)
  Using cached tensorflowjs-1.2.10.1-py3-none-any.whl (47 kB)
Collecting h5py==2.8.0
  Using cached h5py-2.8.0.tar.gz (274 kB)
Collecting tensorflowjs
  Using cached tensorflowjs-1.2.10-py3-none-any.whl (47 kB)
  Using cached tensorflowjs-1.2.9-py3-none-any.whl (46 kB)
Collecting keras==2.2.4
  Using cached Keras-2.2.4-py2.py3-none-any.whl (312 kB)
Collecting tensorflowjs
  Using cached tensorflowjs-1.2.6-py3-none-any.whl (39 kB)
  Using cached tensorflowjs-1.2.3-py3-none-any.whl (39 kB)
  Using cached tensorflowjs-1.2.2.1-py3-none-any.whl (37 kB)
Collecting tensorflow-hub==0.3.0
  Using cached tensorflow_hub-0.3.0-py2.py3-none-any.whl (73 kB)
Collecting tensorflowjs
  Using cached tensorflowjs-1.2.2-py3-none-any.whl (38 kB)
  Using cached tensorflowjs-1.2.1-py3-none-any.whl (37 kB)
Collecting numpy==1.15.1
  Using cached numpy-1.15.1.zip (4.5 MB)
Collecting tensorflowjs
  Using cached tensorflowjs-1.1.2-py3-none-any.whl (36 kB)
  Using cached tensorflowjs-1.0.1-py3-none-any.whl (35 kB)
  Using cached tensorflowjs-0.8.6-py3-none-any.whl (39 kB)
Collecting tensorflow-hub==0.1.1
  Using cached tensorflow_hub-0.1.1-py2.py3-none-any.whl (52 kB)
Collecting keras==2.2.2
  Using cached Keras-2.2.2-py2.py3-none-any.whl (299 kB)
Collecting tensorflowjs
  Using cached tensorflowjs-0.8.5-py3-none-any.whl (39 kB)
  Using cached tensorflowjs-0.8.0-py3-none-any.whl (39 kB)
  Using cached tensorflowjs-0.6.7-py3-none-any.whl (34 kB)
  Using cached tensorflowjs-0.6.5-py3-none-any.whl (33 kB)
Collecting six==1.11.0
  Using cached six-1.11.0-py2.py3-none-any.whl (10 kB)
Collecting tensorflowjs
  Using cached tensorflowjs-0.6.4-py3-none-any.whl (33 kB)
  Using cached tensorflowjs-0.6.2-py3-none-any.whl (33 kB)
  Using cached tensorflowjs-0.6.1-py3-none-any.whl (33 kB)
  Using cached tensorflowjs-0.6.0-py3-none-any.whl (33 kB)
  Using cached tensorflowjs-0.5.7-py3-none-any.whl (33 kB)
Collecting numpy==1.14.1
  Using cached numpy-1.14.1.zip (4.9 MB)
Collecting tensorflowjs
  Using cached tensorflowjs-0.5.6-py3-none-any.whl (33 kB)
  Using cached tensorflowjs-0.5.4-py3-none-any.whl (33 kB)
  Using cached tensorflowjs-0.5.2-py3-none-any.whl (31 kB)
  Using cached tensorflowjs-0.5.0-py3-none-any.whl (31 kB)
  Using cached tensorflowjs-0.4.2-py3-none-any.whl (31 kB)
  Using cached tensorflowjs-0.4.1-py3-none-any.whl (31 kB)
  Using cached tensorflowjs-0.4.0-py3-none-any.whl (31 kB)
Collecting tensorflow-hub==0.1.0
  Downloading tensorflow_hub-0.1.0-py2.py3-none-any.whl (53 kB)
     |████████████████████████████████| 53 kB 3.8 MB/s
Collecting tensorflowjs
  Using cached tensorflowjs-0.3.1-py3-none-any.whl (30 kB)
  Using cached tensorflowjs-0.3.0-py3-none-any.whl (29 kB)
  Using cached tensorflowjs-0.2.1-py3-none-any.whl (28 kB)
  Using cached tensorflowjs-0.2.0-py3-none-any.whl (28 kB)
  Using cached tensorflowjs-0.1.2-py3-none-any.whl (24 kB)
  Using cached tensorflowjs-0.1.1-py3-none-any.whl (20 kB)
  Using cached tensorflowjs-0.1.0-py3-none-any.whl (21 kB)
ERROR: Cannot install tensorflowjs==0.1.0, tensorflowjs==0.1.1, tensorflowjs==0.1.2, tensorflowjs==0.2.0, tensorflowjs==0.2.1, tensorflowjs==0.3.0, tensorflowjs==0.3.1, tensorflowjs==0.4.0, tensorflowjs==0.4.1, tensorflowjs==0.4.2, tensorflowjs==0.5.0, tensorflowjs==0.5.2, tensorflowjs==0.5.4, tensorflowjs==0.5.6, tensorflowjs==0.5.7, tensorflowjs==0.6.0, tensorflowjs==0.6.1, tensorflowjs==0.6.2, tensorflowjs==0.6.4, tensorflowjs==0.6.5, tensorflowjs==0.6.7, tensorflowjs==0.8.0, tensorflowjs==0.8.5, tensorflowjs==0.8.6, tensorflowjs==1.0.1, tensorflowjs==1.1.2, tensorflowjs==1.2.1, tensorflowjs==1.2.10, tensorflowjs==1.2.10.1, tensorflowjs==1.2.2, tensorflowjs==1.2.2.1, tensorflowjs==1.2.3, tensorflowjs==1.2.6, tensorflowjs==1.2.9, tensorflowjs==1.3.1, tensorflowjs==1.3.1.1, tensorflowjs==1.3.2, tensorflowjs==1.4.0, tensorflowjs==1.5.2, tensorflowjs==1.6.0, tensorflowjs==1.7.2, tensorflowjs==1.7.3, tensorflowjs==1.7.4, tensorflowjs==1.7.4.post1, tensorflowjs==2.0.0, tensorflowjs==2.0.1, tensorflowjs==2.0.1.post1, tensorflowjs==2.1.0, tensorflowjs==2.3.0, tensorflowjs==2.4.0, tensorflowjs==2.5.0, tensorflowjs==2.6.0, tensorflowjs==2.7.0, tensorflowjs==2.8.0, tensorflowjs==2.8.1, tensorflowjs==2.8.2, tensorflowjs==2.8.3, tensorflowjs==2.8.4, tensorflowjs==2.8.5 and tensorflowjs==3.0.0 because these package versions have conflicting dependencies.

The conflict is caused by:
    tensorflowjs 3.0.0 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.8.5 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.8.4 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.8.3 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.8.2 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.8.1 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.8.0 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.7.0 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.6.0 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.5.0 depends on tensorflow&lt;3 and &gt;=2.1.0
    tensorflowjs 2.4.0 depends on tensorflow-cpu&lt;3 and &gt;=2.1.0
    tensorflowjs 2.3.0 depends on tensorflow-cpu&lt;3 and &gt;=2.1.0
    tensorflowjs 2.1.0 depends on tensorflow-cpu&lt;3 and &gt;=2.1.0
    tensorflowjs 2.0.1.post1 depends on tensorflow-cpu&lt;3 and &gt;=2.1.0
    tensorflowjs 2.0.1 depends on tensorflow-cpu&gt;=2.1.0&lt;3
    tensorflowjs 2.0.0 depends on tensorflow-cpu&gt;=2.1.0&lt;3
    tensorflowjs 1.7.4.post1 depends on tensorflow-cpu&gt;=2.1.0&lt;3
    tensorflowjs 1.7.4 depends on tensorflow-cpu==2.1.0
    tensorflowjs 1.7.3 depends on tensorflow-cpu==2.1.0
    tensorflowjs 1.7.2 depends on tensorflow-cpu==2.1.0
    tensorflowjs 1.6.0 depends on tensorflow-cpu==2.1.0
    tensorflowjs 1.5.2 depends on tensorflow-cpu==2.1.0
    tensorflowjs 1.4.0 depends on tensorflow==1.15.0
    tensorflowjs 1.3.2 depends on tensorflow==1.15.0
    tensorflowjs 1.3.1.1 depends on tensorflow==1.15.0
    tensorflowjs 1.3.1 depends on tensorflow==1.15.0
    tensorflowjs 1.2.10.1 depends on tensorflow==1.14.0
    tensorflowjs 1.2.10 depends on tensorflow==1.14.0
    tensorflowjs 1.2.9 depends on tensorflow==1.14.0
    tensorflowjs 1.2.6 depends on tensorflow==1.14.0
    tensorflowjs 1.2.3 depends on tensorflow==1.14.0
    tensorflowjs 1.2.2.1 depends on tensorflow==1.14.0
    tensorflowjs 1.2.2 depends on tensorflow==1.14.0
    tensorflowjs 1.2.1 depends on tf-nightly-2.0-preview==2.0.0.dev20190605
    tensorflowjs 1.1.2 depends on tf-nightly-2.0-preview&gt;=2.0.0.dev20190502
    tensorflowjs 1.0.1 depends on tf-nightly-2.0-preview&gt;=2.0.0.dev20190304
    tensorflowjs 0.8.6 depends on tensorflow==1.13.1
    tensorflowjs 0.8.5 depends on tensorflow==1.13.1
    tensorflowjs 0.8.0 depends on tensorflow==1.12.0
    tensorflowjs 0.6.7 depends on tensorflow==1.12.0
    tensorflowjs 0.6.5 depends on tensorflow==1.11.0
    tensorflowjs 0.6.4 depends on tensorflow==1.11.0
    tensorflowjs 0.6.2 depends on tensorflow==1.11.0
    tensorflowjs 0.6.1 depends on tensorflow==1.10.1
    tensorflowjs 0.6.0 depends on tensorflow==1.10.1
    tensorflowjs 0.5.7 depends on tensorflow==1.9.0
    tensorflowjs 0.5.6 depends on tensorflow==1.9.0
    tensorflowjs 0.5.4 depends on tensorflow==1.9.0
    tensorflowjs 0.5.2 depends on tensorflow==1.8.0
    tensorflowjs 0.5.0 depends on tensorflow==1.8.0
    tensorflowjs 0.4.2 depends on tensorflow==1.8.0
    tensorflowjs 0.4.1 depends on tensorflow==1.8.0
    tensorflowjs 0.4.0 depends on tensorflow==1.7.0
    tensorflowjs 0.3.1 depends on tensorflow==1.7.0
    tensorflowjs 0.3.0 depends on tensorflow==1.7.0
    tensorflowjs 0.2.1 depends on tensorflow==1.7.0
    tensorflowjs 0.2.0 depends on tensorflow==1.7.0
    tensorflowjs 0.1.2 depends on tensorflow==1.7.0
    tensorflowjs 0.1.1 depends on tensorflow&gt;=1.6.0
    tensorflowjs 0.1.0 depends on tensorflow&gt;=1.6.0

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies
</code></pre>
<p>I tried using <code>pip</code> but same results, if I didn't provide enough information, please comment.</p>
",12667109.0,,,,,2021-01-26 05:45:14,ERROR: ResolutionImpossible when installing tensorflowjs_converter,<python><tensorflow><pip><tensorflow.js>,1,0,,,,CC BY-SA 4.0
65492399,1,65519691.0,,2020-12-29 12:49:11,,5,669,"<p>I'm following a machine learning course. I have a simple linear regression (LR) problem to help me get used to TensorFlow. The LR problem is to find parameters <code>a</code> and <code>b</code> such that <code>Y = a*X + b</code> approximates an <code>(x, y)</code> point cloud (which I generated myself for the sake of simplicity).</p>
<p>I am solving this LR problem using a 'fixed step size gradient descent (FSSGD)'. I implemented it using TensorFlow and it works but I noticed that it is really slow both on GPU and CPU. Because I was curious I implemented the FSSGD myself in Python/NumPy and as expected this runs much faster, about:</p>
<ul>
<li>10x faster than TF@CPU</li>
<li>20x faster than TF@GPU</li>
</ul>
<p>If TensorFlow is this slow, I cannot imagine that so many people are using this framework. So I must be doing something wrong. Can anyone help me so I can speedup my TensorFlow implementation.</p>
<p><em>I'm NOT interested in the difference between the CPU and GPU performance. Both performance indicators are merely provided for completeness and illustration.</em> <strong>I'm interested in why my TensorFlow implementation is so much slower than a raw Python/NumPy implementation.</strong></p>
<p>As reference, I add my code below.</p>
<ul>
<li>Stripped to a minimal (but fully working) example.</li>
<li>Using <code>Python v3.7.9 x64</code>.</li>
<li>Used <code>tensorflow-gpu==1.15</code> for now (because the course uses TensorFlow v1)</li>
<li>Tested to run in both Spyder and PyCharm.</li>
</ul>
<p>My FSSGD implementation using TensorFlow (execution time about 40 sec @CPU to 80 sec @GPU):</p>
<pre><code>#%% General imports
import numpy as np
import timeit
import tensorflow.compat.v1 as tf


#%% Get input data
# Generate simulated input data
x_data_input = np.arange(100, step=0.1)
y_data_input = x_data_input + 20 * np.sin(x_data_input/10) + 15


#%% Define tensorflow model
# Define data size
n_samples = x_data_input.shape[0]

# Tensorflow is finicky about shapes, so resize
x_data = np.reshape(x_data_input, (n_samples, 1))
y_data = np.reshape(y_data_input, (n_samples, 1))

# Define placeholders for input
X = tf.placeholder(tf.float32, shape=(n_samples, 1), name=&quot;tf_x_data&quot;)
Y = tf.placeholder(tf.float32, shape=(n_samples, 1), name=&quot;tf_y_data&quot;)

# Define variables to be learned
with tf.variable_scope(&quot;linear-regression&quot;, reuse=tf.AUTO_REUSE): #reuse= True | False | tf.AUTO_REUSE
    W = tf.get_variable(&quot;weights&quot;, (1, 1), initializer=tf.constant_initializer(0.0))
    b = tf.get_variable(&quot;bias&quot;, (1,), initializer=tf.constant_initializer(0.0))

# Define loss function    
Y_pred = tf.matmul(X, W) + b
loss = tf.reduce_sum((Y - Y_pred) ** 2 / n_samples)  # Quadratic loss function


# %% Solve tensorflow model
#Define algorithm parameters
total_iterations = 1e5  # Defines total training iterations

#Construct TensorFlow optimizer
with tf.variable_scope(&quot;linear-regression&quot;, reuse=tf.AUTO_REUSE): #reuse= True | False | tf.AUTO_REUSE
    opt = tf.train.GradientDescentOptimizer(learning_rate = 1e-4)
    opt_operation = opt.minimize(loss, name=&quot;GDO&quot;)

#To measure execution time
time_start = timeit.default_timer()

with tf.Session() as sess:
    #Initialize variables
    sess.run(tf.global_variables_initializer())
    
    #Train variables
    for index in range(int(total_iterations)):
        _, loss_val_tmp = sess.run([opt_operation, loss], feed_dict={X: x_data, Y: y_data})
    
    #Get final values of variables
    W_val, b_val, loss_val = sess.run([W, b, loss], feed_dict={X: x_data, Y: y_data})
      
#Print execution time      
time_end = timeit.default_timer()
print('')
print(&quot;Time to execute code: {0:0.9f} sec.&quot;.format(time_end - time_start))
print('')


# %% Print results
print('')
print('Iteration = {0:0.3f}'.format(total_iterations))
print('W_val = {0:0.3f}'.format(W_val[0,0]))
print('b_val = {0:0.3f}'.format(b_val[0]))
print('')
</code></pre>
<p>My own python FSSGD implementation  (execution time about 4 sec):</p>
<pre><code>#%% General imports
import numpy as np
import timeit


#%% Get input data
# Define input data
x_data_input = np.arange(100, step=0.1)
y_data_input = x_data_input + 20 * np.sin(x_data_input/10) + 15


#%% Define Gradient Descent (GD) model
# Define data size
n_samples = x_data_input.shape[0]

#Initialize data
W = 0.0  # Initial condition
b = 0.0  # Initial condition

# Compute initial loss
y_gd_approx = W*x_data_input+b
loss = np.sum((y_data_input - y_gd_approx)**2)/n_samples  # Quadratic loss function


#%% Execute Gradient Descent algorithm
#Define algorithm parameters
total_iterations = 1e5  # Defines total training iterations
GD_stepsize = 1e-4  # Gradient Descent fixed step size

#To measure execution time
time_start = timeit.default_timer()

for index in range(int(total_iterations)):
    #Compute gradient (derived manually for the quadratic cost function)
    loss_gradient_W = 2.0/n_samples*np.sum(-x_data_input*(y_data_input - y_gd_approx))
    loss_gradient_b = 2.0/n_samples*np.sum(-1*(y_data_input - y_gd_approx))
    
    #Update trainable variables using fixed step size gradient descent
    W = W - GD_stepsize * loss_gradient_W
    b = b - GD_stepsize * loss_gradient_b
    
    #Compute loss
    y_gd_approx = W*x_data_input+b
    loss = np.sum((y_data_input - y_gd_approx)**2)/x_data_input.shape[0]

#Print execution time 
time_end = timeit.default_timer()
print('')
print(&quot;Time to execute code: {0:0.9f} sec.&quot;.format(time_end - time_start))
print('')


# %% Print results
print('')
print('Iteration = {0:0.3f}'.format(total_iterations))
print('W_val = {0:0.3f}'.format(W))
print('b_val = {0:0.3f}'.format(b))
print('')
</code></pre>
",7568866.0,,7568866.0,,2020-12-29 15:47:14,2021-01-01 18:23:29,"Gradient descent using TensorFlow is much slower than a basic Python implementation, why?",<python><python-3.x><tensorflow><linear-regression><gradient-descent>,2,5,0.0,,,CC BY-SA 4.0
66477889,1,,,2021-03-04 15:14:01,,5,157,"<p>I use the Tensorflow Recommender (TFRS) and get recommendations for previously watched movies
<a href=""https://www.tensorflow.org/recommenders/examples/basic_retrieval"" rel=""noreferrer"">https://www.tensorflow.org/recommenders/examples/basic_retrieval</a></p>
<p>The manual says:</p>
<p>...The model is re-recommending some of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations...
..can be tackled by excluding previously seen movies from test recommendations.</p>
<p>How can I exclude previously watched movies from test recommendations?
At what stage should this be done?</p>
",12003828.0,,,,,2021-03-04 15:14:01,How to exclude predictions previously seen movies. Tensorflow Recommenders (TFRS),<tensorflow><recommendation-engine>,0,0,0.0,,,CC BY-SA 4.0
66474583,1,,,2021-03-04 11:59:43,,5,5130,"<p>I have trained a tensorflow model to predict the next word for an input text. I saved it as an <strong>.h5</strong> file.</p>
<p>I can use that model in another python code to predict word as follows:</p>
<pre><code>import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.models import load_model

model = load_model('model.h5')
model.compile(
    loss = &quot;categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = [&quot;accuracy&quot;]
)

data = open(&quot;dataset.txt&quot;).read()
corpus = data.lower().split(&quot;\n&quot;)
tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)

seed_text = input()

sequence_text = tokenizer.texts_to_sequences([seed_text])[0]
padded_sequence = np.array(pad_sequences([sequence_text], maxlen = 11 -1))
predicted = np.argmax(model.predict(padded_sequence))
</code></pre>
<blockquote>
<p><em><strong>Is there a way through which I can directly use that model inside
flutter, where I can take input from TextField() and by pressing the
button, display the predicted word??</strong></em></p>
</blockquote>
",13592012.0,,,,,2021-03-29 05:09:44,How to use trained tensorflow model in flutter?,<python><flutter><tensorflow><tensorflow2.0><flutter-web>,2,0,0.0,,,CC BY-SA 4.0
66302371,1,70889015.0,,2021-02-21 12:37:35,,5,2574,"<p>I have followed the basic example as given below, from: <a href=""https://huggingface.co/transformers/training.html"" rel=""noreferrer"">https://huggingface.co/transformers/training.html</a></p>
<pre><code>from transformers import TFBertForSequenceClassification, TFTrainer, TFTrainingArguments

model = TFBertForSequenceClassification.from_pretrained(&quot;bert-large-uncased&quot;)

training_args = TFTrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)

trainer = TFTrainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=tfds_train_dataset,    # tensorflow_datasets training dataset
    eval_dataset=tfds_test_dataset       # tensorflow_datasets evaluation dataset
)
trainer.train()
</code></pre>
<p>But there seems to be no way to specify the loss function for the classifier. For-ex if I finetune on a binary classification problem, I would use</p>
<pre><code>tf.keras.losses.BinaryCrossentropy(from_logits=True)
</code></pre>
<p>else I would use</p>
<pre><code>tf.keras.losses.CategoricalCrossentropy(from_logits=True)
</code></pre>
<p>My set up is as follows:</p>
<pre><code>transformers==4.3.2
tensorflow==2.3.1
python==3.6.12
</code></pre>
",10944913.0,,,,,2022-01-28 05:01:44,How to specify the loss function when finetuning a model using the Huggingface TFTrainer Class?,<python-3.x><tensorflow><nlp><huggingface-transformers>,2,0,0.0,,,CC BY-SA 4.0
62751221,1,62752650.0,,2020-07-06 07:30:35,,5,2210,"<p>I have trained an object detection model using Tensorflow API, following an example based on this Google Colaboratory notebook by Roboflow.
<a href=""https://colab.research.google.com/drive/1wTMIrJhYsQdq_u7ROOkf0Lu_fsX5Mu8a"" rel=""noreferrer"">https://colab.research.google.com/drive/1wTMIrJhYsQdq_u7ROOkf0Lu_fsX5Mu8a</a></p>
<p>So far so good and i have successfully extracted my trained model as an Inference graph, again following the same notebook:</p>
<pre><code>import re
import numpy as np

output_directory = './fine_tuned_model'

lst = os.listdir(model_dir)
lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]
steps=np.array([int(re.findall('\d+', l)[0]) for l in lst])
last_model = lst[steps.argmax()].replace('.meta', '')

last_model_path = os.path.join(model_dir, last_model)
print(last_model_path)
!python /content/models/research/object_detection/export_inference_graph.py \
    --input_type=image_tensor \
    --pipeline_config_path={pipeline_fname} \
    --output_directory={output_directory} \
    --trained_checkpoint_prefix={last_model_path}
</code></pre>
<p>That gives me a <code>frozen_inference_graph.pb</code>file that i can use to make my object detection program in OpenCV DNN. Also following this example <a href=""https://stackoverflow.com/a/57055266/9914815"">https://stackoverflow.com/a/57055266/9914815</a> i prepared a .pbtxt file of the model and pipeline config as the second argument for the <code>cv2.dnn.readNetFromTensorflow</code> function. Here is the code just enough to reproduce the error i'm having:</p>
<pre><code>model = cv2.dnn.readNetFromTensorflow('models/trained/frozen_inference_graph.pb', 
                                      'models/trained/output.pbtxt')
</code></pre>
<p>This code works successfully when i used the pretrained SSD MobileNet V2 COCO model, <code>ssd_mobilenet_v2_coco_2018_03_29.pbtxt</code></p>
<p>however using my trained .pbtxt file, it will throw this error:</p>
<pre><code>C:\Users\Satria\Desktop\ExploreOpencvDnn-master&gt;python trainedmodel_video.py -i test1.mp4 -o test1result.mp4
Traceback (most recent call last):                                                                                                                            
File &quot;trainedmodel_video.py&quot;, line 48, in &lt;module&gt; 'models/trained/output.pbtxt') cv2.error:
OpenCV(4.1.1) C:\projects\opencv-python\opencv\modules\dnn\src\tensorflow\tf_importer.cpp:544:error:
(-2:Unspecified error) Input layer not found: FeatureExtractor/MobilenetV2/Conv/weights in function
'cv::dnn::dnn4_v20190621::`anonymous-namespace'::TFImporter::connect' 
</code></pre>
<p>It says that Input Layer is not found. Why does this happen?
Also Notice the error message points out to a directory:</p>
<pre><code>C:\projects\opencv-python\opencv\modules\dnn\src\tensorflow\tf_importer.cpp
</code></pre>
<p>which is incredibly strange, because i <strong>do not</strong> have that directory at all in my computer.
I tried diffchecking the pbtxt and config files of my and the sample SSD mobilenet model and i cannot find any instance of that particular directory used in anywhere, nor even they have a directory path inside.</p>
<p>Is this caused by training using Google Colab?
Is there any correct way i can use Colab-trained Tensorflow models in OpenCV DNN?</p>
<p>Thanks in advance!</p>
",9914815.0,,899365.0,,2020-08-13 02:25:52,2021-12-01 08:50:05,Error Loading Tensorflow Frozen Inference Graph to OpenCV DNN,<python><tensorflow><opencv><google-colaboratory><roboflow>,1,0,0.0,,,CC BY-SA 4.0
65963026,1,,,2021-01-29 23:21:26,,5,1217,"<p>as the Title suggest is there anyway to implement early stopping on the TF object detection API?</p>
<p>i read in this thread <a href=""https://github.com/tensorflow/models/issues/5887"" rel=""noreferrer"">https://github.com/tensorflow/models/issues/5887</a> on the github of Object detection that there's a repo <a href=""https://github.com/hongym7/early_stopping"" rel=""noreferrer"">https://github.com/hongym7/early_stopping</a> for early stopping. but this is an older version of TF1 and i use TF2</p>
<p>is there any way to implement early stopping on the <code>model_main_tf2.py</code> ? i've spent few hours reading the codes but i don't see any place to implement the early stopping.</p>
<p>Here's the link to the script to train the model
<a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py"" rel=""noreferrer"">https://github.com/tensorflow/models/blob/master/research/object_detection/model_main_tf2.py</a></p>
",14639790.0,,,,,2021-02-01 09:08:39,Early stopping Tensorflow object detection API,<python><tensorflow><tensorflow2.0><object-detection><object-detection-api>,1,0,0.0,,,CC BY-SA 4.0
66873185,1,,,2021-03-30 14:42:39,,5,733,"<p>I'm new to tensorflow. I want to train a recommendation model on my dataset using the TensorFlow Recommenders library and the simple code provided at:</p>
<p><a href=""https://github.com/tensorflow/recommenders"" rel=""noreferrer"">https://github.com/tensorflow/recommenders</a></p>
<p>I want to know how can I use (load and feed to the model) my custom .csv file in the following format instead of loading the built-in Movielens dataset?</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>user_id</th>
<th>item_id</th>
<th>rating</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>8</td>
<td>3</td>
</tr>
<tr>
<td>5</td>
<td>12</td>
<td>4</td>
</tr>
<tr>
<td>6</td>
<td>4</td>
<td>2</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
</div>
<p>My Tensorflow versions:</p>
<p>tensorflow==2.4.0</p>
<p>tensorflow-datasets==4.2.0</p>
<p>tensorflow-recommenders==0.4.0</p>
",6383522.0,,,,,2021-04-27 01:20:47,How to use a custom .csv dataset in TensorFlow Recommenders library?,<python><tensorflow><tensorflow-datasets><recommendation-engine>,1,0,0.0,,,CC BY-SA 4.0
65288044,1,65306815.0,,2020-12-14 11:16:11,,5,1003,"<p>I'm trying to use <a href=""https://www.tensorflow.org/tutorials/load_data/tfrecord"" rel=""noreferrer"">TFRecord format</a> to record data from C++ and then use it in python to feed TensorFlow model.</p>
<p><strong>TLDR; Simply serializing proto messages into a stream doesn't satisfy <code>.tfrecord</code> format requirements of Python <code>TFRecordDataset</code> class. Is there an equivalent of Python <code>TfRecordWriter</code> in C++ (either in TensorFlow or in Google Protobuf libraries) to generate proper <code>.tfrecord</code> data?</strong></p>
<p>Details:</p>
<p>The simplified C++ code looks like this:</p>
<pre><code>tensorflow::Example sample;
sample.mutable_features()-&gt;mutable_feature()-&gt;operator[](&quot;a&quot;).mutable_float_list()-&gt;add_value(1.0);

std::ofstream out;
out.open(&quot;cpp_example.tfrecord&quot;, std::ios::out | std::ios::binary);
sample.SerializeToOstream(&amp;out);
</code></pre>
<p>In Python, to create a TensorFlow data I'm trying to use <a href=""https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset"" rel=""noreferrer"">TFRecordDataset</a>, but apparently it expects extra header/footer information in the .tfrecord file (rather than simple list of serialized proto messages):</p>
<pre><code>import tensorflow as tf
tfrecord_dataset = tf.data.TFRecordDataset(filenames=&quot;cpp_example.tfrecord&quot;)
next(tfrecord_dataset.as_numpy_iterator())
</code></pre>
<p>output:</p>
<pre><code>tensorflow.python.framework.errors_impl.DataLossError: corrupted record at 0 [Op:IteratorGetNext]
</code></pre>
<p>Note that there is nothing wrong with the recorded binary file, as following code prints a valid output:</p>
<pre><code>import tensorflow as tf
p = open(&quot;cpp_example.tfrecord&quot;, &quot;rb&quot;)
example = tf.train.Example.FromString(p.read())
</code></pre>
<p>output:</p>
<pre><code>features {
  feature {
    key: &quot;a&quot;
    value {
      float_list {
        value: 1.0
      }
    }
  }
}
</code></pre>
<p>By analyzing the binary output generated by my C++ example, and an output generated by using Python <code>TfRecordWriter</code>, I observed additional header and footer bytes in the content. Unfortunately, what do these extra bytes represent was an implementation detail (probably compression type and some extra info) and I couldn't track it deeper than some class in python libraries which just exposed the interface from <code>_pywrap_tfe.so</code>.</p>
<p>There was <a href=""https://www.reddit.com/r/tensorflow/comments/a7jd0e/does_anyone_know_how_to_create_a_tfrecord_file"" rel=""noreferrer"">this advice</a> saying that <code>.tfrecord</code> is just a normal google protobuf data. It might be I'm missing the knowledge where to find protobuf data writer (expect serializing proto messages into the output stream)?</p>
",874667.0,,,,,2020-12-15 13:25:35,Generating TFRecord format data from C+,<python><c++><tensorflow><protocol-buffers><tfrecord>,1,0,0.0,,,CC BY-SA 4.0
68164440,1,68165378.0,,2021-06-28 13:43:26,,5,6479,"<p>I'm using the <code>batch(8)</code> function, it modifies the shape and adds batch dimension, but only getting one image per batch. Below is my code:-</p>
<pre><code>import cv2
import numpy as np
import os
import tensorflow as tf
import random

folder_path = &quot;./real/&quot;
files = os.listdir(folder_path)

def get_image():
    index = random.randint(0,len(files)-1)
    img = cv2.imread(folder_path+files[index])
    img = cv2.resize(img,(128,128))
    img = img/255.
    #More complex transformation
    yield img

dset = tf.data.Dataset.from_generator(get_image,(tf.float32)).batch(8)

for img in dset:
    print(img.shape)
    break
</code></pre>
<p>The output still is (1, 128, 128, 3) even after using batch(8). Do I need to modify the generator to manually crate the batch? Also, how can it be wrapped in the generator in tensorflow so that it runs faster?</p>
",6277325.0,,9215780.0,,2021-06-28 14:07:27,2021-06-28 14:42:41,how to batch with tf.data.Dataset.from_generator? Do i needto modify generator,<tensorflow><tensorflow2.0><tensorflow-datasets>,1,0,,,,CC BY-SA 4.0
65298391,1,,,2020-12-15 00:28:27,,5,1462,"<p>I have tried to follow Tensorflow instructions to use BERT model: (<a href=""https://www.tensorflow.org/tutorials/text/classify_text_with_bert"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/text/classify_text_with_bert</a>)</p>
<p>However, when I run these lines:</p>
<pre><code>text_test = ['this is such an amazing movie!']
text_preprocessed = bert_preprocess_model(text_test)
</code></pre>
<p>I got the below error:</p>
<pre>
InvalidArgumentError:  Trying to access resource using the wrong type. Expected class tensorflow::lookup::LookupInterface got class tensorflow::lookup::LookupInterface
     [[{{node StatefulPartitionedCall/StatefulPartitionedCall/bert_tokenizer/StatefulPartitionedCall/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets}}]] [Op:__inference_restored_function_body_72474]
</pre>
<p>The two classes are exactly the same: &quot;tensorflow::lookup::LookupInterface&quot;. Could anyone help with this? Thank you.</p>
",14826837.0,,14826837.0,,2020-12-15 00:38:28,2021-01-11 08:33:12,Error with using BERT model from Tensorflow,<python><tensorflow><bert-language-model>,2,3,0.0,,,CC BY-SA 4.0
64769187,1,,,2020-11-10 12:44:46,,5,117,"<p>I have a deep CNN/RNN that I train on Google AI platform. I distribute the training on 8 GPUs using the <code>tf.distribute.MirroredStrategy</code>. I recently upgraded my runtime version from 1.13 to 1.15 and my training is more than 2x slower than before. I read that <code>tf.estimator.ProfilerHook</code> can be used to identify performance bottlenecks. So I collected the profiling information and rendered it at <code>chrome://tracing</code>. I got this</p>
<p><a href=""https://i.stack.imgur.com/l5hDZ.jpg"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/l5hDZ.jpg"" alt=""profiling screenshot"" /></a></p>
<p>A training step spends an entire 1 second on these <code>_Send</code> ops. What is this? I can't find any documentation on the op or why it's in my graph. What does this mean?</p>
",6078821.0,,,,,2020-11-10 12:44:46,"Tensorflow - Interpreting the tf.estimator.ProfilerHook ""_Send"" op",<tensorflow><google-cloud-ml><gcp-ai-platform-training>,0,0,,,,CC BY-SA 4.0
65548876,1,65582758.0,,2021-01-03 10:39:25,,5,380,"<p>I have a dataset (which contains both the data and the label) with the size of [299,13], and the model keeps outputting / predicting the same value. This is a binary classification task. How would I make my model predict values which are not constantly the same?</p>
<p>Here is the code (with some dummy data):</p>
<pre><code>//X is the data and y is the label

    
   var Dataset = tf.tensor([[1,0.491821360184978,9,314,0.504585169147173,542,1231,3213,1,0.267304071302649,3,0.615917680092409,0],
        [0,0.72959029133292,3,758,0.402582737085955,400,1788,4599,0,0.532702887951197,4,0.18630897965037,1],
        [1,0.198764110760428,5,787,0.65507860022684,887,192,4831,1,0.739456077544426,3,0.100068056951143,1],
        [0,0.583574833590476,5,596,0.933996451580092,631,331,811,0,0.258445986493932,7,0.811276729811182,0],
        [1,0.701499878184206,8,854,0.0326334179806069,845,470,4930,1,0.825469683527519,1,0.448086959665654,1],
        [0,0.954482878414911,2,468,0.736300149681564,557,3110,739,0,0.325783042694677,5,0.43488580142501,1],
        [1,0.384845877769,2,662,0.265402742189238,649,384,1158,1,0.484884260891815,2,0.915444292219105,0],
        [1,0.379266474923531,9,551,0.275982850450116,1022,3329,1413,1,0.237295089390298,4,0.817104709627837,1],
        [1,0.691365367558705,8,549,0.479627221800976,796,3381,495,1,0.37129382411555,9,0.332832739155564,1],
        [0,0.433042848178662,5,529,0.545178403950882,842,4768,506,0,0.386370525896832,9,0.189942077251933,0],
        [1,0.611272282663452,4,823,0.737901576655264,839,2724,1787,1,0.365032317656007,6,0.884073622694046,0],
        [0,0.0084315409129881,5,352,0.76858549557176,476,685,4796,0,0.302944943656102,1,0.849655932794213,1],
        [0,0.977380232874908,6,701,0.588833228576897,999,2897,3325,0,0.418024491281536,2,0.631872118440871,1],
        [1,0.419601058571829,10,384,0.0157052616592944,1009,4438,113,1,0.909015627566542,1,0.0297684897733232,0],
        [0,0.739471449044276,4,836,0.0430176780439737,1030,1456,3932,0,0.331426481315121,6,0.734008754824423,0],
        [1,0.00209807072438295,4,352,0.499622407429238,418,1912,4452,1,0.727130871883893,8,0.157427964683612,0],
        [1,0.956533819923862,10,681,0.196708599930969,829,4562,1718,1,0.233193195569506,7,0.60582783922237,0],
        [1,0.504637155233183,8,809,0.608861975627751,717,130,4194,1,0.134197560919101,6,0.375188428842507,0],
        [0,0.747363884375055,1,522,0.868234577182028,849,3529,1192,0,0.0322641640468155,5,0.185973206518818,0],
        [0,0.244142898027225,10,402,0.0280582030746698,315,3576,3882,0,0.724916254371562,8,0.062229775169706,1],
        [0,0.858414851618448,8,459,0.367325906336267,616,930,3892,0,0.177388425930446,10,0.859824526007041,1],
        [1,0.921555604905976,2,863,0.821166873626313,528,1624,1289,1,0.366243396916411,5,0.453840754701258,1],
        [1,0.171321120311715,1,524,0.177251413832862,468,1608,3123,1,0.192861821442111,8,0.122983286410146,0],
        [0,0.539946042901786,6,692,0.817780349862711,392,1053,4891,0,0.409578972921785,3,0.0453862502541893,1],
        [1,0.996848843212564,5,549,0.877740438211017,762,3046,843,1,0.888578696082088,8,0.877971306478434,1],
        [0,0.218116987741582,3,655,0.240496962520226,407,1001,1474,0,0.976212355833712,2,0.936396547703282,1]])
    function onBatchEnd(batch, logs) {
        console.log('Accuracy', logs.acc);
    }
    
    var x = Dataset.slice([0, 0], [-1, 12])
    const y = Dataset.slice([0, 12], [-1, 1])
    
    const model = tf.sequential({
        layers: [
            tf.layers.dense({ inputShape: [12], units: 12, activation: &quot;sigmoid&quot; }),
            tf.layers.dense({ units: 8, activation: &quot;relu&quot; }),
            tf.layers.dense({ units: 4, activation: &quot;tanh&quot; }),
            tf.layers.dense({ units: 1, activation: &quot;sigmoid&quot; })
        ]
    })
    
    model.compile({
        optimizer: tf.train.adam(0.001),
        loss: &quot;binaryCrossentropy&quot;,
        metrics: [&quot;accuracy&quot;]
    })
    
    model.fit(x, y, {
        shuffle: true,
        epochs: 100,
        //validationSplit: 0.1,
        callbacks: { onBatchEnd }
    }).then(info =&gt; {
        var predictions = model.predict(x)
        console.log('Final accuracy', info.history.acc);
        console.log(&quot;Predictions: &quot;)
        console.log(predictions.dataSync());
    })
</code></pre>
",12504573.0,,12504573.0,,2021-01-04 05:28:52,2021-01-05 16:25:53,TFJS model only predicting the same value for binary classification task,<javascript><tensorflow><machine-learning><classification><tensorflow.js>,2,5,,,,CC BY-SA 4.0
65642746,1,,,2021-01-09 12:42:13,,5,610,"<br>
I've been trying to implement this ML Linear Model into my dataset. (https://www.tensorflow.org/tutorials/estimator/linear) <br>
Language: Python 3.8.3<br>
Lİbraries: 
TensorFlow 2.4.0<br>
Numpy: 1.19.3<br>
Pandas<br>
Matplotlib<br>and the others: <br>
<pre><code>import os
import sys

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
from six.moves import urllib
</code></pre>
<pre><code>import tensorflow.compat.v2.feature_column as fc
import tensorflow as tf
</code></pre>
<p>ss1517 is the name of my dataset. It is a CSV file with 4116 rows and 20 columns and has lots of NaN values( There is no column that hasn't NaN value)</p>
<pre><code>traindata = ss1517.iloc[0:2470,:] # 60 % of my dataset is splitted by training set
evaldata = ss1517.iloc[2470:4116, :] # 40 % of my dataset is splitted by eval set
ytrain = traindata.pop(&quot;AvgOfMajor N&quot;)
yeval = evaldata.pop(&quot;AvgOfMajor N&quot;)
</code></pre>
<p>CATEGORICAL_COLUMNS are the categorical columns in my dataset. <br>
NUMERIC_COLUMNS are the numeric columns in my dataset.</p>
<pre><code>CATEGORICAL_COLUMNS = ['Location-Name', 'Location-Code', 'Borough', 'Building-Name', 'Schools-in-Building', 'ENGroupA', 'RangeA']
NUMERIC_COLUMNS = ['Geographical-District-Code', 'Register', '#-Schools', 'Major-N', 'Oth-N', 'NoCrim-N', 'Prop-N', 'Vio-N', 'AvgOfOth-N', 'AvgOfNoCrim-N', 'AvgOfProp-N', 'AvgOfVio-N']

feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = traindata[feature_name].unique()
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))
for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))
</code></pre>
<pre><code>def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):
  def input_function():# inner function, this will be returned.
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) # Create tf.data.Dataset object with data and its label
    if shuffle:
      ds = ds.shuffle(1000) # randomize order of data
    ds = ds.batch(batch_size).repeat(num_epochs)
    return ds # return a batch of dataset
  return input_function # return the input_function

train_input_fn = make_input_fn(traindata, ytrain) 
eval_input_fn = make_input_fn(evaldata, yeval, num_epochs=1, shuffle=False) 
</code></pre>
<pre><code>linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)
linear_est.train(train_input_fn) #train
result = linear_est.evaluate(eval_input_fn) #get model metrics/stats by testing on testing data

clear_output() #clears console output
print(result[&quot;accuracy&quot;]) #the result variable is simply dict of stats about our model
</code></pre>
<p><strong>I have this error</strong> <code> InvalidArgumentError: assertion failed: [Labels must be &lt;= n_classes - 1] [Condition x &lt;= y did not hold element-wise:] [x (head/losses/Cast:0) = ] [[0.28][0.28][1.69]...] [y (head/losses/check_label_range/Const:0) = ] [1]</code>  <br>
<strong>when I run this cell:</strong></p>
<pre><code>linear_est.train(train_input_fn) #train
result = linear_est.evaluate(eval_input_fn) #get model metrics/stats by testing on testing data

clear_output() #clears console output
print(result[&quot;accuracy&quot;]) #the result variable is simply dict
</code></pre>
<p><strong>Note</strong> I used <code>fillna(method=&quot;bfill&quot;)</code> and <code> fillna(method=&quot;ffill)</code> on my dataset (ss1517) to fill the Na values.<br>
How could I solve this error?</p>
",13852388.0,,,,,2021-01-16 18:50:15,InvalidArgumentError: assertion failed: [Labels must be <= n_classes - 1] [Condition x <= y did not hold element-wise:] [x (head/losses/Cast:0) = ],<python><tensorflow><invalid-argument>,0,0,,,,CC BY-SA 4.0
66356797,1,,,2021-02-24 18:30:09,,5,9128,"<p>I do object detection with tensorflow in Google Colab. I'm trying to get video from the webcam. This is the last stage. But I am getting the error below continent.How can I size the pictures?</p>
<pre><code>ValueError: in user code:

    &lt;ipython-input-49-1e7efe9130ee&gt;:11 detect_fn  *
        image, shapes = detection_model.preprocess(image)
    /usr/local/lib/python3.7/dist-packages/object_detection/meta_architectures/ssd_meta_arch.py:484 preprocess  *
        normalized_inputs, self._image_resizer_fn)
    /usr/local/lib/python3.7/dist-packages/object_detection/utils/shape_utils.py:492 resize_images_and_return_shapes  *
        outputs = static_or_dynamic_map_fn(
    /usr/local/lib/python3.7/dist-packages/object_detection/utils/shape_utils.py:246 static_or_dynamic_map_fn  *
        outputs = [fn(arg) for arg in tf.unstack(elems)]
    /usr/local/lib/python3.7/dist-packages/object_detection/core/preprocessor.py:3241 resize_image  *
        new_image = tf.image.resize_images(
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper  **
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/image_ops_impl.py:1468 resize_images
        skip_resize_if_same=True)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/image_ops_impl.py:1320 _resize_images_common
        raise ValueError('\'images\' must have either 3 or 4 dimensions.')

    ValueError: 'images' must have either 3 or 4 dimensions.
</code></pre>
<p>How can i solve?</p>
<p>All Code:</p>
<pre><code>while True: 
    ret, frame = cap.read()
    image_np = np.array(frame)
    
    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)
    detections = detect_fn(input_tensor)
    
    num_detections = int(detections.pop('num_detections'))
    detections = {key: value[0, :num_detections].numpy()
                  for key, value in detections.items()}
    detections['num_detections'] = num_detections

    # detection_classes should be ints.
    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)

    label_id_offset = 1
    image_np_with_detections = image_np.copy()

    viz_utils.visualize_boxes_and_labels_on_image_array(
                image_np_with_detections,
                detections['detection_boxes'],
                detections['detection_classes']+label_id_offset,
                detections['detection_scores'],
                category_index,
                use_normalized_coordinates=True,
                max_boxes_to_draw=5,
                min_score_thresh=.5,
                agnostic_mode=False)

    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))
    
    if cv2.waitKey(1) &amp; 0xFF == ord('q'):
        cap.release()
        break
</code></pre>
",14729646.0,,,,,2022-11-21 02:33:45,ValueError: 'images' must have either 3 or 4 dimensions. in Colab,<python><tensorflow><object-detection>,8,7,0.0,,,CC BY-SA 4.0
65681820,1,,,2021-01-12 10:11:03,,5,610,"<p>I am trying to calculate the second derivative of a simple vector function of a scalar variable <code>f(x) = (x,x^2,x^3)</code> using TF 2.3 with <code>tf.GradientTape</code>.</p>
<pre><code>def f_ab(x):
    return x, x** 2, x** 3

import tensorflow as tf
in1 = tf.cast(tf.convert_to_tensor(tf.Variable([-1,3,0,6]))[:,None],tf.float64)
with tf.GradientTape(persistent=True) as tape2:
    tape2.watch(in1)
    with tf.GradientTape(persistent=True) as tape:
        tape.watch(in1)
        f1,f2,f3 = f_ab(in1)
    df1 = tape.gradient(f1, in1)
    df2 = tape.gradient(f2, in1)
    df3 = tape.gradient(f3, in1)

d2f1_dx2 = tape2.gradient(df1, in1)
d2f2_dx2 = tape2.gradient(df2, in1)
d2f3_dx2 = tape2.gradient(df3, in1)
</code></pre>
<p>for some reason, only the last two derivative are correct while the first, <code>d2f1_dx2</code>, turned out to be <code>None</code>.</p>
<p>When I changed <code>f_ab</code> to</p>
<pre><code>def f_ab(x):
    return x** 1, x** 2, x**3   
</code></pre>
<p>I got <code>d2f1_dx2 = &lt;tf.Tensor: shape=(1, 4), dtype=float64, numpy=array([[-0.,  0., nan,  0.]])&gt;</code>
which is &quot;almost&quot; the correct result.</p>
<p>only when I changed <code>f_ab</code> to</p>
<pre><code>def f_ab(inputs_train):
    return tf.math.log(tf.math.exp(x) ), x** 2, x**3
</code></pre>
<p>I got the correct result: <code>d2f1_dx2  = &lt;tf.Tensor: shape=(1, 4), dtype=float64, numpy=array([[0., 0., 0., 0.]])&gt;</code></p>
<p>Has anyone encountered this problem before? why is the straight forward way gives <code>None</code>?</p>
",1877002.0,,11341120.0,,2021-01-12 12:15:22,2021-01-13 13:11:27,Second derivative in Tensorflow 2.0,<tensorflow><machine-learning><deep-learning><tensorflow2.0><derivative>,1,1,0.0,,,CC BY-SA 4.0
65702243,1,65702531.0,,2021-01-13 12:44:15,,5,8714,"<p>How can I install TensorFlow in the base (root) environment? The only instructions I can find to set up Tensorflow in Anaconda have me create a new environment called <code>tf</code> as shown here:</p>
<pre class=""lang-sh prettyprint-override""><code>conda create -n tf tensorflow,
conda activate tf
</code></pre>
<p>Then when I run my code, I get</p>
<pre class=""lang-sh prettyprint-override""><code> ModuleNotFoundError: No module named 'tensorflow'.
</code></pre>
<p>It seems that the Tensorflow module is not being found when it is in the <code>tf</code> environment.</p>
",14192277.0,,9730862.0,,2021-01-14 08:02:13,2021-01-14 08:02:13,How Can I Install Tensorflow in base (root) environment,<tensorflow><conda>,1,0,0.0,,,CC BY-SA 4.0
68211406,1,68256449.0,,2021-07-01 13:48:10,,5,373,"<p>I have a model with the following signature that I'm trying to invoke using <a href=""/questions/tagged/tensorflow"" class=""post-tag"" title=""show questions tagged &#39;tensorflow&#39;"" rel=""tag"">tensorflow</a> for Java:</p>
<pre><code>MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:
signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['jpegbase64_bytes'] tensor_info:
        dtype: DT_STRING
        shape: (-1)
        name: Placeholder:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['predictions'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 256)
        name: model/global_average_pooling2d/Mean:0
  Method name is: tensorflow/serving/predict
</code></pre>
<p>My code to invoke the model looks like this:</p>
<pre><code>float[] predict(byte[] imageBytes) {
    try (Tensor result = SavedModelBundle.load(&quot;model.pb&quot;, &quot;serve&quot;).session().runner()
            .feed(&quot;myinput&quot;, 0, TString.tensorOfBytes(NdArrays.scalarOfObject(imageBytes)))
            .fetch(&quot;myoutput&quot;)
            .run()
            .get(0)) {
        float[] buffer = new float[256];
        FloatNdArray floatNdArray = FloatDenseNdArray.create(RawDataBufferFactory.create(buffer, false),
                Shape.of(1, description.getNumFeatures()));
        ((TFloat32) result).copyTo(floatNdArray);
        return buffer;
    }
}
</code></pre>
<p>However, this throws the following errors:</p>
<pre><code>slice index 0 of dimension 0 out of bounds.
     [[{{node map/TensorArrayUnstack/strided_slice}}]]
org.tensorflow.exceptions.TFInvalidArgumentException: slice index 0 of dimension 0 out of bounds.
     [[{{node map/TensorArrayUnstack/strided_slice}}]]
    at org.tensorflow.internal.c_api.AbstractTF_Status.throwExceptionIfNotOK(AbstractTF_Status.java:87)
    at org.tensorflow.Session.run(Session.java:691)
    at org.tensorflow.Session.access$100(Session.java:72)
    at org.tensorflow.Session$Runner.runHelper(Session.java:381)
    at org.tensorflow.Session$Runner.run(Session.java:329)
    at com.mridang.myapp.ImageModel.predict(ImageModel.java:69)
    ...
    ...
    ...
    ...
</code></pre>
<p>From what I've understood, the model requires a dense-type string tensor while mine isn't. I found this answer on Stackoverflow <a href=""https://stackoverflow.com/questions/61389869/slice-index-0-of-dimension-0-out-of-bounds-using-java-api"">slice index 0 of dimension 0 out of bounds using Java API</a> but that seems to relate to very old version of <a href=""/questions/tagged/tensorflow"" class=""post-tag"" title=""show questions tagged &#39;tensorflow&#39;"" rel=""tag"">tensorflow</a>.</p>
<p>I'm using these dependencies:</p>
<pre><code>layer group: 'org.tensorflow', name: 'tensorflow-core-platform', version: '0.3.1'
layer group: 'org.tensorflow', name: 'tensorflow-framework', version: '0.3.1'
</code></pre>
",304151.0,,304151.0,,2021-07-01 18:58:46,2021-07-05 12:49:10,"Why do I get ""slice index 0 of dimension 0 out of bounds"" error in Tensorflow for Java?",<java><tensorflow>,1,2,,,,CC BY-SA 4.0
66158107,1,,,2021-02-11 15:52:44,,5,1095,"<p>I've been facing this issue for a while now. Whenever I import TensorFlow, I get the following:</p>
<pre class=""lang-py prettyprint-override""><code>
2021-02-11 21:05:05.855414: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2021-02-11 21:05:05.855463: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/amay428/.local/lib/python3.8/site-packages/tensorflow/__init__.py&quot;, line 41, in &lt;module&gt;
    from tensorflow.python.tools import module_util as _module_util
  File &quot;/home/amay428/.local/lib/python3.8/site-packages/tensorflow/python/__init__.py&quot;, line 45, in &lt;module&gt;
    from tensorflow.python import data
  File &quot;/home/amay428/.local/lib/python3.8/site-packages/tensorflow/python/data/__init__.py&quot;, line 25, in &lt;module&gt;
    from tensorflow.python.data import experimental
  File &quot;/home/amay428/.local/lib/python3.8/site-packages/tensorflow/python/data/experimental/__init__.py&quot;, line 126, in &lt;module&gt;
    from tensorflow.python.data.experimental.ops.prefetching_ops import copy_to_device
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 991, in _find_and_load
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 975, in _find_and_load_unlocked
  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 671, in _load_unlocked
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 779, in exec_module
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 911, in get_code
  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 580, in _compile_bytecode
EOFError: marshal data too short
</code></pre>
<p>I have looked at various answers online but all of them ask me to delete the TensorFlow _<em>pycache</em>_ folder. I have done this, but it doesn't work. I am using Manjaro, python 3.8 inside a Conda environment. I would appreciate any help.</p>
<p>Thanks!</p>
",13298841.0,,,,,2022-03-21 23:27:52,Python Tensorflow - EOFError: marshal data too short,<python><python-3.x><tensorflow>,1,1,,,,CC BY-SA 4.0
63524108,1,,,2020-08-21 13:48:09,,5,5046,"<p>how can I export a trained model to frozen_inference_graph.pb instead of saved_model.pb, because when I use the exporter_main_v2.py that comes with Tensorflow object detection v2 it gives me a folder</p>
<pre><code>├─ exported-models/
   └─ my_model/ 
      ├─ checkpoint/
      ├─ saved_model/
            └─ assets/
            ├─ variables/
            └─ saved_model.pb
      └─ pipeline.config
</code></pre>
<p>and inside the save_model I have saved_model.pb but the issue is that I can't use it alone for inference but I need to use the variable folder that comes with it. that't why I'm asking if theire is a way to export a trained model to frozen_inference_graph.pb to use it for inference without need for variables folderlike in TF1.</p>
",8912375.0,,,,,2021-01-06 11:42:49,How to export frozen_inference_graph.pb from a checkpoint in Tensorflow Objectdetection 2,<python><tensorflow><computer-vision><object-detection><object-detection-api>,2,1,,,,CC BY-SA 4.0
66910103,1,66937352.0,,2021-04-01 18:55:41,,5,1425,"<p>I'm browsing the documentation around <code>SavedModels</code>, and <code>tf.saved_model.save</code> (<a href=""https://www.tensorflow.org/api_docs/python/tf/saved_model/save"" rel=""noreferrer"">docs</a>) does:</p>
<blockquote>
<p>Exports the Trackable object obj to SavedModel format.</p>
</blockquote>
<p><strong>What is a trackable object</strong>, I have not been able to find anything regarding this. Except for my PyCharm debugger printing it.</p>
<hr />
<p>I am reading the documentation because I need to understand how SavedModels work. I've tried to load some SavedModels and subsequently convert these to TensorFlow lite/ coreML. Things are not going well in this respect, and I'm completely lost there so I won't bother mentioning it much detail in this question.</p>
",7365866.0,,7365866.0,,2021-04-01 19:02:31,2021-04-04 01:47:20,What is a trackable object?,<python><python-3.x><tensorflow><tensorflow2.0>,1,1,0.0,,,CC BY-SA 4.0
75978533,1,,,2023-04-10 15:43:12,,5,1838,"<p>I get the following warning when running the following minimal code (that will train a very simple tensorflow 2 model). Actually it is telling me that I can ignore this message, but I still got the feeling that something might be wrong and I don't like ignoring messages like this. The message still persists even if I set verbose=0.</p>
<p>I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [5000,25,1]
[[{{node Placeholder/_1}}]]</p>
<p>I am using tensorflow 2.12.0.</p>
<p><strong>Code:</strong></p>
<pre><code>import numpy as np
from keras import Model
from keras.layers import Input, Flatten, Dense, Reshape
import tensorflow as tf

x = np.random.rand(5000, 10, 7)
y = np.random.rand(5000, 25, 1)

#############################################################
# create the dataset
#############################################################
ds = tf.data.Dataset.from_tensor_slices((x, y))
ds = ds.batch(32, drop_remainder=True)

#############################################################
# construct the model
#############################################################
inputs = []

x = Input(shape=(10, 7))
inputs.append(x)

x = Flatten()(x)
x = Dense(25)(x)
x = Reshape((25, 1))(x)

model = Model(inputs=inputs, outputs=x)

model.compile(loss=&quot;mse&quot;)
model.summary()

#############################################################
# fit the model
#############################################################
model.fit(ds, batch_size=10, verbose=1, epochs=10)
</code></pre>
<p><strong>Output</strong></p>
<pre><code>2023-04-10 17:32:46.805357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-04-10 17:32:47.312460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-04-10 17:32:50.162190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2578 MB memory:  -&gt; device: 0, name: Quadro T1000, pci bus id: 0000:01:00.0, compute capability: 7.5
Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 10, 7)]           0         
                                                                 
 flatten (Flatten)           (None, 70)                0         
                                                                 
 dense (Dense)               (None, 25)                1775      
                                                                 
 reshape (Reshape)           (None, 25, 1)             0         
                                                                 
=================================================================
Total params: 1,775
Trainable params: 1,775
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
2023-04-10 17:32:50.244149: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [5000,25,1]
     [[{{node Placeholder/_1}}]]
2023-04-10 17:32:50.804972: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7f6e808f79c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2023-04-10 17:32:50.804996: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Quadro T1000, Compute Capability 7.5
2023-04-10 17:32:50.807856: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
2023-04-10 17:32:50.909253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600
2023-04-10 17:32:50.946872: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2023-04-10 17:32:50.981030: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
156/156 [==============================] - 1s 986us/step - loss: 0.2249
Epoch 2/10
156/156 [==============================] - 0s 958us/step - loss: 0.1457
Epoch 3/10
156/156 [==============================] - 0s 981us/step - loss: 0.1185
Epoch 4/10
156/156 [==============================] - 0s 929us/step - loss: 0.1026
Epoch 5/10
156/156 [==============================] - 0s 1ms/step - loss: 0.0940
Epoch 6/10
156/156 [==============================] - 0s 929us/step - loss: 0.0895
Epoch 7/10
156/156 [==============================] - 0s 960us/step - loss: 0.0872
Epoch 8/10
156/156 [==============================] - 0s 958us/step - loss: 0.0860
Epoch 9/10
156/156 [==============================] - 0s 969us/step - loss: 0.0854
Epoch 10/10
156/156 [==============================] - 0s 998us/step - loss: 0.0851

Process finished with exit code 0
</code></pre>
",871495.0,,871495.0,,2023-04-10 15:57:15,2023-05-23 16:53:00,"Tensorflow2 Warning: ""INVALID_ARGUMENT: You must feed a value for placeholder ...""",<python><tensorflow><tensorflow2.0>,1,1,,,,CC BY-SA 4.0
63746426,1,,,2020-09-04 18:34:31,,5,485,"<p>I am using eigendecomposition in Tensorflow and find that it is extremely slow. Here's the code to show Tensorflow's speed vs numpy and scipy:</p>
<pre><code>import numpy as np
import scipy as sp
import tensorflow as tf
from time import time

A = np.random.randn(400, 400)
A_tf = tf.constant(A)

cur = time()
d, v = sp.linalg.eig(A)
print(f'sp: {time() - cur:4.2f} s')

cur = time()
d, v = np.linalg.eig(A)
print(f'np: {time() - cur:4.2f} s')

cur = time()
d, v = tf.linalg.eig(A_tf)
print(f'tf: {time() - cur:4.2f} s')
</code></pre>
<p>This gives the following output:</p>
<pre><code>sp: 0.09 s
np: 0.08 s
tf: 5.04 s
</code></pre>
<p>Any ideas of what's up here?</p>
",2904731.0,,,,,2020-10-21 13:22:31,Tensorflow eigenvalue decomposition is extremely slow,<python><tensorflow><eigenvalue><eigenvector>,1,1,,,,CC BY-SA 4.0
64080154,1,,,2020-09-26 16:40:34,,5,476,"<p>I'm trying to implement a simple recurrent network using TensorFlow, but am receiving the above error. I've looked through several answers related to the:</p>
<pre><code>&quot;Failed to convert a NumPy array to a Tensor (Unsupported object type ____)&quot; 
</code></pre>
<p>error, but none so far have addressed &quot;tensorflow.python.framework.ops.EagerTensor&quot; as the unsupported type. I am receiving this error after trying to implement code from <a href=""https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/"" rel=""noreferrer"">this tutorial</a> (albeit with a different data-set).</p>
<p>The error occurs on the <em>history = model.fit</em> line:</p>
<pre><code># Define the network
epochs_qty = 50
batch_size_qty = 72
model = Sequential()
model.add(LSTM(epochs_qty, input_shape = (train_X.shape[1], train_X.shape[2])))
model.add(Dense(1))
model.compile(loss = 'mae', optimizer = 'adam')

# Fit the network
history = model.fit(train_X, train_y, epochs = epochs_qty, batch_size = batch_size_qty, validation_data = (test_X, test_y), verbose = 2, shuffle = False)
</code></pre>
<p>The data sets have the following shapes:</p>
<pre><code>print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)
&gt;&gt; (1762, 1, 2) (1762,) (588, 1, 2) (588,)
</code></pre>
<p>I am running the following versions:</p>
<ul>
<li>Python 3.7.9</li>
<li>Windows 10</li>
<li>tensorflow-gpu-2.3.1</li>
<li>CUDA Toolkit 10.1 Update 1</li>
<li>cuDNN v8.0.3 for CUDA 10.1</li>
</ul>
<p>I have tried disabling eager execution, but this leads to a pile of additional errors, and does not seem optimal for future code development.</p>
<p>Also, I have tried running this code both locally and through a jupyter notebook. Both result in the exact same error, so it seems like my software setup is not the issue.<br />
Can anyone please suggest where to look next for the cause of this error?</p>
",14345989.0,,,,,2020-09-26 16:40:34,Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor),<python><tensorflow>,0,1,0.0,,,CC BY-SA 4.0
70784509,1,,,2022-01-20 10:24:26,,5,226,"<p>I have implemented the following version of my ResNet50. I trained my model with my own data in another notebook so I just load the weights and compile the model. Now, I just want to make predictions on my new unseen data.</p>
<pre><code>def resnet50F(im_size):

    resnet = ResNet50(input_shape=(im_size, im_size, 3), weights='imagenet', include_top = False)
    headModel = AvgPool2D(pool_size=(3,3))(resnet.output)
    headModel = Flatten(name='flatten')(headModel)
    headModel = Dense(256, activation='relu')(headModel)
    headModel = Dropout(0.5)(headModel)
    headModel = Dense(1, activation='sigmoid')(headModel)

    model = Model(inputs=resnet.input, outputs=headModel)

    model.trainable = True

    return model


resnet50 = resnet50F(im_size=224)
resnet50.load_weights(PATH_MODEL_WEIGHTS)
opt = optimizers.Adam(learning_rate=1e-6)
resnet50.compile(loss='binary_crossentropy', optimizer=opt, metrics=METRICS)
predictions = resnet50.predict(X)
</code></pre>
<p>However, when I print <code>predictions</code> I get the following output:</p>
<pre><code> [[4.22752373e-06]
 [2.81104029e-10]
 [3.21204737e-02]
 [5.09007333e-12]
 [6.25871266e-08]
 [3.95518853e-08]
 [3.76289577e-09]
 [1.04685043e-07]
 [4.40788448e-01]
 [4.18029167e-09]
 [1.68976447e-04]
 [4.83552366e-03]
 [5.67837298e-01]
 [1.92822833e-02]
 [1.86168763e-04]
 [3.30054699e-11]
 [1.55285016e-01]
 [1.40850764e-12]
 [4.75460291e-02]
 [2.36899691e-08]
 [1.91837142e-04]
 [2.70789745e-03]
 [2.28864295e-07]
 [1.04725331e-08]
 [3.17185315e-15]
 [1.86515141e-08]
 [9.09119472e-03]
 [2.67773657e-06]
 [6.43107248e-03]
 [1.06139310e-14]
 [3.12786847e-01]
 [1.47488710e-04]
 [7.75789477e-09]
 [2.05256441e-03]
 [5.19017190e-11]
 [6.54808059e-02]
 [9.27565736e-04]
 [6.90304815e-26]
 [8.59875661e-14]
 [2.54806340e-01]
 [1.05227390e-02]
 [4.43476923e-02]
 [3.65121141e-02]
 [4.71908916e-13]
 [1.16901109e-02]
 [2.83952375e-07]
 [6.87847793e-01]
 [6.25556211e-08]
 [2.92979064e-03]
 [1.00091375e-08]
 [7.29291560e-06]
 [7.43216195e-16]
 [1.16142066e-04]
 [6.63836045e-06]
 [4.89238771e-12]
 [3.75503966e-08]
 [7.99435584e-05]
 [5.35736717e-06]
 [2.15524092e-11]
 [1.89218114e-14]
 [4.04082388e-02]
 [1.11348586e-09]
 [1.72054302e-03]
 [2.21202258e-11]
 [2.13359108e-08]
 [2.09557402e-05]
 [1.01457292e-04]
 [9.81324539e-03]
 [9.62927871e-08]
 [4.38750768e-03]
 [7.26699904e-02]
 [6.57562000e-16]
 [4.28197110e-13]]

</code></pre>
<p>As I understand it, it's supposed to represent the probability of my model to belong to class 1. So there's either only one sample predicted as class 1 (5.67837298e-01) or I am missing something in my methodology.</p>
",4943535.0,,,,,2022-02-12 13:46:23,Making predictions and determine class on binary tensorflow model,<python><tensorflow><prediction>,3,0,,,,CC BY-SA 4.0
69234978,1,69334450.0,,2021-09-18 13:11:18,,5,595,"<p>Following <a href=""https://radimrehurek.com/gensim/models/word2vec.html"" rel=""noreferrer"">gensim word2vec embedding tutorial</a>, I have trained a simple word2vec model:</p>
<pre><code>from gensim.test.utils import common_texts
from gensim.models import Word2Vec
model = Word2Vec(sentences=common_texts, size=100, window=5, min_count=1, workers=4)
model.save(&quot;/content/word2vec.model&quot;)
</code></pre>
<p>I would like to visualize it <a href=""https://projector.tensorflow.org/"" rel=""noreferrer"">using the Embedding Projector in TensorBoard</a>. <a href=""https://radimrehurek.com/gensim/scripts/word2vec2tensor.html"" rel=""noreferrer"">There is another straightforward tutorial in gensim documentation</a>. I did the following in Colab:</p>
<pre><code>!python3 -m gensim.scripts.word2vec2tensor -i /content/word2vec.model -o /content/my_model

Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 94, in &lt;module&gt;
    word2vec2tensor(args.input, args.output, args.binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/scripts/word2vec2tensor.py&quot;, line 68, in word2vec2tensor
    model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_model_path, binary=binary)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py&quot;, line 1438, in load_word2vec_format
    limit=limit, datatype=datatype)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py&quot;, line 172, in _load_word2vec_format
    header = utils.to_unicode(fin.readline(), encoding=encoding)
  File &quot;/usr/local/lib/python3.7/dist-packages/gensim/utils.py&quot;, line 355, in any2unicode
    return unicode(text, encoding, errors=errors)

UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte
</code></pre>
<p>Please note that I did check first this <a href=""https://stackoverflow.com/questions/50492676/visualize-gensim-word2vec-embeddings-in-tensorboard-projector"">exact same question from 2018</a> - but the accepted answer no longer works as both in gensim and tensorflow have been updated so I considered it was worth asking again in Q4 2021.</p>
",7762646.0,,7762646.0,,2021-09-19 21:40:26,2021-09-27 18:40:39,How to visualize Gensim Word2vec Embeddings in Tensorboard Projector,<python><tensorflow><gensim><word2vec><tensorboard>,1,2,,,,CC BY-SA 4.0
63907113,1,,,2020-09-15 17:32:59,,5,821,"<p><a href=""https://i.stack.imgur.com/5Awiy.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/5Awiy.png"" alt=""enter image description here"" /></a></p>
<p>** I'M AWARE OF SIMILAR QUESTIONS!! **</p>
<p>My question is for my particular situation...
I used Google Vision to train my own model to detect custom objects.
I've come across similar errors about shape in the past and I resolved them by reshaping my input image.</p>
<p>This particular error is telling me that my shape must be an empty array or empty shape. Is that even possible? If this is not a glitch, how do i resolve it?</p>
<p>This is how I resolved previous errors in other projects when it complains about shape. This solution does not work for empty array/shape</p>
<pre><code>    const model = await autoML.loadObjectDetection('./model/model.json');
 // const model = await tfjs.loadGraphModel('./model/model.json');
    await tfjs.ready();
    const tfImg = tfjs.browser.fromPixels(videoElement.current).expandDims(0);
    const smallImg = await tfjs.image.resizeBilinear(tfImg, [224, 224]);
    const resized = tfjs.cast(smallImg, 'float32');
    const t4d = tfjs.tensor4d(Array.from(resized.dataSync()), [1, 224, 224, 3]);
    const predictions = await modelRef.current.detect(tfImg, options);
</code></pre>
",1397586.0,,,,,2020-10-15 11:03:06,The shape of dict['ToFloat'] provided in model.execute(dict) must be [],<tensorflow><tensorflow2.0><tensorflow.js><google-vision><automl>,1,10,0.0,,,CC BY-SA 4.0
70466992,1,70512494.0,,2021-12-23 20:14:47,,5,504,"<p>I want to apply a partial tucker decomposition algorithm to minimize MNIST image tensor dataset of (60000,28,28), in order to conserve its features when applying another machine algorithm afterwards like SVM.
I have this code that minimizes the second and third dimension of the tensor</p>
<pre><code>i = 16
j = 10
core, factors = partial_tucker(train_data_mnist, modes=[1,2],tol=10e-5, rank=[i,j])
train_datapartial_tucker = tl.tenalg.multi_mode_dot(train_data_mnist, factors, 
                              modes=modes, transpose=True)
test_data_partial_tucker = tl.tenalg.multi_mode_dot(test_data_mnist, factors, 
                              modes=modes, transpose=True)
</code></pre>
<p>How to find the best rank <code>[i,j]</code> when I'm using <code>partial_tucker</code> in tensorly that will give the best dimension reduction for the image while conserving as much data?</p>
",1655410.0,,4685471.0,,2021-12-24 08:11:50,2021-12-28 21:06:30,partial tucker decomposition,<python><tensorflow><machine-learning><tensorly>,2,0,0.0,,,CC BY-SA 4.0
63776386,1,,,2020-09-07 11:06:51,,5,1753,"<p>This is my data. it has 7 images:</p>
<p><a href=""https://i.stack.imgur.com/GPuex.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/GPuex.png"" alt=""enter image description here"" /></a></p>
<p>I using <code>autokeras</code>  to training:</p>
<pre><code>import tensorflow as tf
import numpy as np
import autokeras as ak
from tensorflow.keras.preprocessing import image

BATCH_SIZE = 32
IMG_HEIGHT = 224
IMG_WIDTH = 224
train_data_dir = &quot;E:\\DemoTensorflow\\NhanDienDoiTuong\\Data\\Traintest&quot;


def preprocess(img):
    img = image.array_to_img(img, scale=False)
    img = img.resize((IMG_WIDTH, IMG_HEIGHT))
    img = image.img_to_array(img)
    return img / 255.0


image_generator = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1.0 / 255,
    horizontal_flip=True,
    validation_split=0.2,
    preprocessing_function=preprocess,
)

train_generator = image_generator.flow_from_directory(
    directory=train_data_dir,
    batch_size=BATCH_SIZE,
    shuffle=True,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    subset=&quot;training&quot;,
)

val_generator = image_generator.flow_from_directory(
    directory=train_data_dir,
    batch_size=BATCH_SIZE,
    shuffle=True,
    # class_mode=&quot;categorical&quot;,
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    subset=&quot;validation&quot;,
)


def callable_iterator(generator):
    for img_batch, targets_batch in generator:
        yield img_batch, targets_batch


train_dataset = tf.data.Dataset.from_generator(
    lambda: callable_iterator(train_generator),
    output_types=(tf.float32, tf.int8),
    output_shapes=(
        tf.TensorShape([None, 224, 224, 3]),
        tf.TensorShape([None, 2]),
    ),
)
val_dataset = tf.data.Dataset.from_generator(lambda: callable_iterator(val_generator),output_types=(tf.float32, tf.float32))

clf = ak.ImageClassifier(max_trials=10)
clf.fit(train_dataset, epochs=10)
print(clf.evaluate(val_dataset))
</code></pre>
<p>Result: It can't finish when execute:
It hang at this command very long:  <code>StreamExecutor device (0): Host, Default Version</code>
<a href=""https://i.stack.imgur.com/lOvpo.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/lOvpo.png"" alt=""enter image description here"" /></a></p>
<p>Why can't finish my training?</p>
<p>My OS is Win7, python 3.8, tensorflow 2.3, autokeras 1.0.8</p>
",1497597.0,,1497597.0,,2020-09-08 02:50:24,2020-09-10 02:33:21,"Why ""I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version"" can't finish",<tensorflow><machine-learning><auto-keras>,0,3,,,,CC BY-SA 4.0
73223678,1,,,2022-08-03 15:00:54,,5,384,"<p>I have used this project from Github: <a href=""https://github.com/nicknochnack/TFODCourse"" rel=""noreferrer"">https://github.com/nicknochnack/TFODCourse</a></p>
<p>The project contains a model that can detect License Plate on a given Vehicle image. The Github repo also contains code for the conversion of model into Tensorflow Lite file.</p>
<p>I used that code to generate TFLite file.</p>
<p>And then, I followed this link: <a href=""https://developers.google.com/codelabs/tflite-object-detection-android"" rel=""noreferrer"">https://developers.google.com/codelabs/tflite-object-detection-android</a></p>
<p>Where I downloaded the sample Application of Object detection model and following the instructions, I copied my TFLite files into the Android Application.</p>
<p>Now, if I run the application and take a photo, it gives me this error,</p>
<pre><code>/TaskJniUtils: Error getting native address of native library: task_vision_jni
    java.lang.RuntimeException: Error occurred when initializing ObjectDetector: Input tensor has type kTfLiteFloat32: it requires specifying NormalizationOptions metadata to preprocess input images.
        at org.tensorflow.lite.task.vision.detector.ObjectDetector
</code></pre>
<p>I understand that I have to add Metadata in my TFLite model. so, I searched about it and ended up on this link: <a href=""https://www.tensorflow.org/lite/models/convert/metadata#model_with_metadata_format"" rel=""noreferrer"">https://www.tensorflow.org/lite/models/convert/metadata#model_with_metadata_format</a></p>
<p>But I didn't understand at all what exactly should I be doing. Can anyone please help me in pointing to the right direction that for my problem specifically, what exactly do I need to do?</p>
",17866070.0,,,,,2022-08-14 02:05:29,How to add Metadata in the Tensorflow Lite for a model taken from Github?,<android-studio><tensorflow><kotlin><metadata><tensorflow-lite>,0,2,,,,CC BY-SA 4.0
63557158,1,68278748.0,,2020-08-24 08:16:13,,5,1004,"<p>Computing mean, total, etc. of each feature in a dataset seems quite trivial in <code>Pandas</code> and <code>Numpy</code>, but I couldn't find any similarly easy functions/operations for <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset"" rel=""noreferrer""><code>tf.data.Dataset</code></a>. Actually I found <a href=""https://www.tensorflow.org/api_docs/python/tf/data/Dataset#reduce"" rel=""noreferrer""><code>tf.data.Dataset.reduce</code></a> which allows me to compute running <code>sum</code>, but it's not that easy for other operation (<code>min</code>, <code>max</code>, <code>std</code>, etc.)
<br>
<br>So, my question is, is there a simple way to compute statistics for <code>tf.data.Dataset</code>? Moreover, is there a way to standardize/normalize (an entire, i.e. not in batch) <code>tf.data.Dataset</code>, especially if not using <code>tf.data.Dataset.reduce</code>?</p>
",10125672.0,,,,,2021-07-07 00:35:24,"How to compute statistics (sum, mean, variance, etc.) over an entire dataset in Tensorflow",<python><tensorflow>,1,0,,,,CC BY-SA 4.0
70624869,1,,,2022-01-07 17:15:18,,5,4400,"<p>this:</p>
<pre><code>import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification

model = TFBertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>Outputs the following error:</p>
<pre><code>ImportError: 
TFBertForSequenceClassification requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the
installation page: https://www.tensorflow.org/install and follow the ones that match your environment.
</code></pre>
<p>However it's not true I don't have the TensorFlow library imported.</p>
<pre><code>&gt; print(tf.__version__)
'2.7.0'
</code></pre>
",7227146.0,,,,,2023-05-22 08:58:25,TFBertForSequenceClassification requires the TensorFlow library but it was not found in your environment,<python><tensorflow><installation><huggingface-transformers>,4,1,,,,CC BY-SA 4.0
63562691,1,63566567.0,,2020-08-24 14:06:41,,5,2676,"<p>I am very new to TensorFlow and this might be a very beginner question. I have seen examples where custom datasets are converted to TFRecord files using the knowledge of the features one wants to use (for example-'image', 'label'). And while parsing this TFRecord file back, one has to know the features beforehand (i.e. 'image', 'label') in order to be able to use this dataset.</p>
<p>My question is- how do we parse TFRecord files where we do not know the features beforehand? Suppose someone gives me a TFRecord file and I want to decode all the associated features with this.</p>
<p>Some examples which I am referring to are: <a href=""https://stackoverflow.com/questions/37151895/tensorflow-read-all-examples-from-a-tfrecords-at-once/39376833"">Link 1</a>, <a href=""https://stackoverflow.com/questions/55378569/i-want-to-read-data-from-tfrecord"">Link 2</a></p>
",7341905.0,,,,,2020-08-24 18:19:15,Reading a TFRecord file where features that were used to encode is not known,<tensorflow><tfrecord>,1,3,0.0,,,CC BY-SA 4.0
64051293,1,,,2020-09-24 17:15:44,,5,3463,"<p>According to official documentation, in Tensorflow 2.3 <a href=""https://www.tensorflow.org/install/source#gpu"" rel=""noreferrer"">CUDA 10.1 is supported</a></p>
<p>I have Ubuntu 20.04, GPU onboard, CUDA 10.1 and CUDNN 7.6</p>
<p>I am getting the error when start using Tensorflow (2.3):
<strong>Could not load dynamic library 'libcublas.so.10';
dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64</strong></p>
<p>After some hours of investigations, it turned out that CUBLAS packaging changed in CUDA 10.1 to be <strong>outside</strong> of the toolkit installation path</p>
<pre><code>/usr/local/cuda-10.1/lib64
</code></pre>
<p>See here :
<a href=""https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/16"" rel=""noreferrer"">https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/16</a></p>
<p>In my case I searched with</p>
<pre><code>sudo find /usr -name libcublas*
</code></pre>
<p>and founded :</p>
<pre><code>            /usr/share/doc/libcublas-dev
            /usr/share/doc/libcublas10
            /usr/local/cuda-10.1/doc/man/man7/libcublas.so.7
            /usr/local/cuda-10.1/doc/man/man7/libcublas.7
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublas.so.10.2.2.214
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublasLt.so.10.2.2.214
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/stubs/libcublasLt.so
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/stubs/libcublas.so
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublas.so.10
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublas_static.a
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublasLt.so
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublasLt_static.a
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublasLt.so.10
            /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublas.so
</code></pre>
<p>Then, following some suggestions for workarounds using symlink (founded in the nvdia site), I created a symlink for the files above founded, to the :</p>
<pre><code>sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublas.so /usr/local/cuda-10.1/lib64/libcublas.so
sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcublas.so.10 /usr/local/cuda-10.1/lib64/libcublas.so.10
</code></pre>
<p>Even after the symlinks, the error persists:</p>
<p><strong>Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64</strong></p>
<p>with my <code>nano ~/.profile</code> containing :</p>
<pre><code># set PATH for cuda 10.1 installation
if [ -d &quot;/usr/local/cuda-10.1/bin/&quot; ]; then
    export PATH=/usr/local/cuda-10.1/bin${PATH:+:${PATH}}
    export LD_LIBRARY_PATH=/usr/local/cuda-10.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
fi
</code></pre>
<p>I wanted also to try and take manually the files from the toolkit available in the <em>cuda_10.1.168_418.67_linux.run</em> file as suggested <a href=""https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/16"" rel=""noreferrer"">here</a>  but have founded that the suggested command does not work.I adjusted to command :</p>
<pre><code>sh cuda_10.1.168_418.67_linux.run --extract=/extracted
</code></pre>
<p>which goes KO when finalizing with a message ..
<strong>Failed to verify gcc version. See log at /tmp/cuda-installer.log for details.</strong></p>
<p>If only that extraction could work, maybe a manual copy of the files saves the full headache.</p>
<p>It seems that this cublas step is not documented in Tensorflow official documentation for installing with CUDA 10.1</p>
<p>Any idea ?</p>
",14335441.0,,681865.0,,2020-09-24 21:06:58,2020-11-14 19:00:44,Tensorflow 2.3 and libcublas.so.10,<python><tensorflow>,3,2,,,,CC BY-SA 4.0
65396968,1,65397572.0,,2020-12-21 17:04:06,,5,4772,"<p>Short TL;DR:
I am using BERT for a sequence classification task and don't understand the output I get.</p>
<p>This is my first post, so please bear with me:
I am using bert for a sequence classification task with 3 labels. To do this, I am using huggingface transformers with tensorflow, more specifically the TFBertForSequenceClassification class with the bert-base-german-cased model (yes, using german sentences).</p>
<p>I am by no means an expert in NLP, which is why I pretty much followed this approch here: <a href=""https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333"" rel=""noreferrer"">https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333</a> (with some tweaks of course)</p>
<p>Everything seems to be working fine, but the output I receive from my model is what throws me off.
Here's just some of the output along the way for context.</p>
<p>The main difference I have to the example from the article is the number of labels. I have 3 while the article only featured 2.</p>
<p>I use a LabelEncoder from sklearn.preprocessing to process my labels</p>
<pre><code>label_encoder = LabelEncoder()
Y_integer_encoded = label_encoder.fit_transform(Y)
</code></pre>
<p>*Y here is a list of labels as strings, so something like this</p>
<pre><code>['e_3', 'e_1', 'e_2',]
</code></pre>
<p>then turns into this:</p>
<pre><code>array([0, 1, 2], dtype=int64)
</code></pre>
<p>I then use the BertTokenizer to process my text and create the input datasets (training and testing).
These are the shapes of those:</p>
<pre><code> &lt;TensorSliceDataset shapes: ({input_ids: (99,), token_type_ids: (99,), attention_mask: (99,)}, ()), types: ({input_ids: tf.int32, token_type_ids: tf.int32, attention_mask: tf.int32}, tf.int32)&gt;
</code></pre>
<p>I then train the model as per Huggingface docs.</p>
<p>The last epoch while training the model looks like this:</p>
<pre><code>Epoch 3/3
108/108 [==============================] - 24s 223ms/step - loss: 25.8196 - accuracy: 0.7963 - val_loss: 24.5137 - val_accuracy: 0.7243
</code></pre>
<p>Then I run model.predict on an example sentence and get this output (yes I tokenized the sentence accordingly just like the other article does). The output looks like this:</p>
<pre><code>array([ 3.1293588, -5.280143 ,  2.4700692], dtype=float32)
</code></pre>
<p>And lastly that's the softmax function I apply in the end and it's output:</p>
<pre><code>tf_prediction = tf.nn.softmax(tf_output, axis=0).numpy()[0]

output: 0.6590041
</code></pre>
<p>So here's my question:
I don't quite understand that output. With an accuracy of ~70% (validation accuracy), my model should be okay in predicting the labels. Yet only the logits from the direct output don't mean much to me tbh and the output after the softmax function seems to be on a linear scale, as if it came from a sigmoid function. How do I interpret this and translate it to the label I am trying to predict?</p>
<p>And also: shouldn't I feed one hot encoded labels into my bert model for it to work? I always thought Bert needs that but it seems like it doesn't.</p>
<p>Thanks a lot in advance!</p>
",8291269.0,,5561472.0,,2021-01-18 08:27:14,2021-12-04 12:00:55,How do I interpret my BERT output from Huggingface Transformers for Sequence Classification and tensorflow?,<python><tensorflow><bert-language-model><huggingface-transformers>,1,1,0.0,,,CC BY-SA 4.0
64093720,1,,,2020-09-27 22:00:39,,5,6288,"<p>If I have a dataset</p>
<pre><code>dataset = tf.keras.preprocessing.image_dataset_from_directory(
    directory,
    labels=&quot;inferred&quot;,
    label_mode=&quot;int&quot;,
    class_names=None,
    color_mode=&quot;rgb&quot;,
    batch_size=32,
    image_size=(32, 32),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation=&quot;bilinear&quot;,
    follow_links=False,
)
</code></pre>
<p>how do I separate this into x and y arrays? The x array would be the IMG array and the y array would have the category for each img.</p>
",14351469.0,,14351469.0,,2020-09-27 22:23:07,2021-09-03 21:55:01,"how to split up tf.data.Dataset into x_train, y_train, x_test, y_test for keras",<python><image><tensorflow><machine-learning><keras-2>,1,1,,,,CC BY-SA 4.0
69950509,1,,,2021-11-13 00:40:38,,5,23063,"<p>I'm trying to install TensorFlow but I keep getting a longpath error , I have Python 3.9 installed and pip 21.3.1. Whenever I run pip install tensorflow I receive the following error:</p>
<p>ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\Users\obrie\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\tensorflow\include\external\com_github_grpc_grpc\src\core\ext\filters\client_channel\lb_policy\grpclb\client_load_reporting_filter.h'
HINT: This error might have occurred since this system does not have Windows Long Path support enabled. You can find information on how to enable this at <a href=""https://pip.pypa.io/warnings/enable-long-paths"" rel=""noreferrer"">https://pip.pypa.io/warnings/enable-long-paths</a></p>
<p>This seems to clearly be an error caused by the fact that LongPathsEnabled was set to false by default. I've got into my registry editor, gone to HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem and changed LongPathsEnabled to a value of 1.</p>
<p><a href=""https://i.stack.imgur.com/iBmlh.png"" rel=""noreferrer"">LongPathsEnabled set to 1</a></p>
<p>I've restarted my computer, but still get the longpaths error. Please help me understand what I am missing.</p>
",17399755.0,,,,,2023-04-28 14:51:43,"TensorFlow install error, Windows LongPath support not enabled",<python><tensorflow><pip>,5,2,,,,CC BY-SA 4.0
63676635,1,,,2020-08-31 18:59:21,,5,1165,"<p>This <a href=""https://www.tensorflow.org/hub/tutorials/tf2_object_detection"" rel=""noreferrer"">link</a> provides a Google Colab notebook for inference of CenterNet HourGlass104 Keypoints 512x512 for object detection and pose key point detection.  Is there a similar notebook or tutorial to train object detection and key point detection on custom datasets?</p>
",3317287.0,,,,,2022-05-15 21:20:23,Is there a tutorial for using tensorflow object detection API for training object and key point detection?,<tensorflow><object><key><detection><point>,1,2,,,,CC BY-SA 4.0
63233837,1,63303652.0,,2020-08-03 17:02:45,,4,117,"<p>I created a model to recognize license plates. It is this one:</p>
<pre><code>def create_model(input_shape = (224, 224, 3)):
    input_img = Input(shape=input_shape)
    model = efnB0_model (input_img)
    model = GlobalAveragePooling2D(name='avg_pool')(model)
    model = Dropout(0.2)(model)
    backbone = model

    branches = []
    for i in range(7):
            branches.append(backbone)
            branches[i] = Dense(360, name=&quot;branch_&quot;+str(i)+&quot;_Dense_360&quot;)(branches[i])
            branches[i] = BatchNormalization()(branches[i])
            branches[i] = Activation(&quot;relu&quot;) (branches[i])
            branches[i] = Dropout(0.2)(branches[i])
                       
            branches[i] = Dense(35, activation = &quot;softmax&quot;, name=&quot;branch_&quot;+str(i)+&quot;_output&quot;)(branches[i])
        
    output = Concatenate(axis=1)(branches)
    output = Reshape((7, 35))(output)
    model = Model(input_img, output)

    return model
</code></pre>
<p>I used this DataGenerator:</p>
<pre><code>import tensorflow.keras as keras
from skimage.io import imread
from skimage.transform import resize
import numpy as np
import math

class DataGenerator(Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx*self.batch_size : (idx + 1)*self.batch_size]
        batch_x = np.array([resize(imread(file_name), (224, 224)) for file_name in batch_x])
        batch_x = batch_x * 1./255
        batch_y = self.y[idx*self.batch_size : (idx + 1)*self.batch_size]
        batch_y = np.array(batch_y)

        return batch_x, batch_y
</code></pre>
<p>Therefore, I onehot-encoded every license plate (length 7 characters and 35 possible character for every position using this code:</p>
<pre><code>#One Hot Encoding der Labels, Zielarray hat eine Shape von (7,35)
from numpy import argmax
# define input string

def my_onehot_encoded(label):
    # define universe of possible input values
    characters = '0123456789ABCDEFGHIJKLMNPQRSTUVWXYZ'
    # define a mapping of chars to integers
    char_to_int = dict((c, i) for i, c in enumerate(characters))
    int_to_char = dict((i, c) for i, c in enumerate(characters))
    # integer encode input data
    integer_encoded = [char_to_int[char] for char in label]
    # one hot encode
    onehot_encoded = list()
    for value in integer_encoded:
        character = [0 for _ in range(len(characters))]
        character[value] = 1
        onehot_encoded.append(character)

    return onehot_encoded
</code></pre>
<p>For the license with label &quot;7CT2498&quot; I get the following onehot-encoded output:</p>
<pre><code>[[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
</code></pre>
<p>Now, when running the model I created above for 10 epochs on 10.000 training data and 3.000 validation data I get a training accuracy of 0.9969 and a validation accuracy of 0.9798, so not too bad.</p>
<p>But now I tried to predict a license plate with this model (the image is from the same dataset as my training and validation data is).</p>
<p>I used this code:</p>
<pre><code>model = keras.models.load_model(
    &quot;/path/to/model.h5&quot;, compile=True)
opt = keras.optimizers.Adam(learning_rate=0.0001)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[&quot;accuracy&quot;])
img = cv2.imread('/path/to/image.png')
img = cv2.resize(img,(224,224))
img = np.reshape(img,[1,224,224,3])

classes = model.predict(img)

print(classes)
</code></pre>
<p>And I do only get one correctly predicted class. Is there something wrong with my code?</p>
",11572712.0,,11572712.0,,2020-08-07 13:48:43,2020-08-07 14:17:23,model.predict() - Model with accuracy near to 1 predicts wrong classes,<python><tensorflow><conv-neural-network><predict>,2,2,0.0,,,CC BY-SA 4.0
68168281,1,,,2021-06-28 18:16:03,,4,376,"<p>I am building an audio classifier in which the NN is going to be trained to classify audios into just two classes. The following is my &quot;train.py&quot; module:</p>
<pre><code>import os 
import tensorflow as tf
import numpy as np

from configs import PARENT_DIR_TO_SAVE_CHUNKS, VALIDATION_PERCENT_OUT_OF_TOTAL, TEST_PERCENT_OUT_OF_TOTAL, PARENT_DIR_TO_PCM_GOOD_CHUNKS, PARENT_DIR_TO_PCM_BAD_CHUNKS
from utils import decode_audio, do_process_path_of_audio, get_label, get_waveform_and_label, plot, get_spectrogram

def main():
    goods = [os.path.join(PARENT_DIR_TO_SAVE_CHUNKS, &quot;GOOD&quot;,file) for file in os.listdir(os.path.join(PARENT_DIR_TO_SAVE_CHUNKS, &quot;GOOD&quot;))]
    bads = [os.path.join(PARENT_DIR_TO_SAVE_CHUNKS, &quot;BAD&quot;,file) for file in os.listdir(os.path.join(PARENT_DIR_TO_SAVE_CHUNKS, &quot;BAD&quot;))]
    all = goods + bads
    filenames = tf.convert_to_tensor(all)
    filenames = tf.random.shuffle(filenames)

    print(filenames)
    
    length_of_val_samples = int((VALIDATION_PERCENT_OUT_OF_TOTAL / 100) * len(filenames))
    length_of_test_samples = int((TEST_PERCENT_OUT_OF_TOTAL / 100) * len(filenames))
    
    train_files = filenames[:len(filenames) - length_of_val_samples - length_of_test_samples]
    val_files = filenames[len(filenames) - length_of_val_samples - length_of_test_samples:\
                            len(filenames) - length_of_val_samples]
    test_files = filenames[-(length_of_test_samples):]

    print('Training set size:', len(train_files))
    print('Validation set size:', len(val_files))
    print('Test set size:', len(test_files))

    (files, waveforms) = do_process_path_of_audio(train_files)
    print(waveforms)
    # plot(waveforms)
    # print(waveforms.take(1))
    for waveform, label in waveforms.take(1):
        label = label.numpy().decode('utf-8')
        spectrogram = get_spectrogram(waveform)

    # print('Label:', label)
    # print('Waveform shape:', waveform.shape)
    # print('Spectrogram shape:', spectrogram.shape)
    
    
if __name__ == &quot;__main__&quot;:
    
    main()
</code></pre>
<p>And the following contains the related functions inside my &quot;preprocess.py&quot; module:</p>
<pre><code>import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os

def decode_audio(audio_binary):
  audio, _ = tf.audio.decode_wav(audio_binary)
  print(_)
  print(audio)
  return tf.squeeze(audio, axis=-1)


def get_label(file_path):
    parts = tf.strings.split(file_path, os.path.sep)
    print(file_path)

    return parts[-2] 


def get_waveform_and_label(file_path):
  label = get_label(file_path)
  audio_binary = tf.io.read_file(file_path)
  waveform = decode_audio(audio_binary)

  return waveform, label

def do_process_path_of_audio(files):
    AUTOTUNE = tf.data.AUTOTUNE
    files_ds = tf.data.Dataset.from_tensor_slices(files)
    waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)
    return (files_ds, waveform_ds)


def plot(waveforms):
    rows = 3
    cols = 3
    n = rows*cols
    fig, axes = plt.subplots(rows, cols, figsize=(10, 12))
    print(&quot;======&gt;&quot;, waveforms.take(15))
    for i, (audio, label) in enumerate(waveforms.take(n)):
        r = i // cols
        c = i % cols
        ax = axes[r][c]
        ax.plot(audio.numpy())
        ax.set_yticks(np.arange(-1.2, 1.2, 0.2))
        label = label.numpy().decode('utf-8')
        ax.set_title(label)

    plt.show()


def get_spectrogram(waveform):

  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)

  waveform = tf.cast(waveform, tf.float32)
  equal_length = tf.concat([waveform, zero_padding], 0)
  spectrogram = tf.signal.stft(
      equal_length, frame_length=255, frame_step=128)
      
  spectrogram = tf.abs(spectrogram)

  return spectrogram
</code></pre>
<p>Now when I reach the line:</p>
<pre><code>for waveform, label in waveforms.take(1)
</code></pre>
<p>I get the following error:
<em><strong>OP_REQUIRES failed at decode_wav_op.cc:55 : Invalid argument: Can only read 16-bit WAV files, but received 32</strong></em></p>
<p>I have check the sample_rate of my dataset audios and they are all in 16000. I have searched all over the Internet but found no one has asked sth like this. Any Idea how to solve this.</p>
",12514551.0,,12514551.0,,2021-06-28 18:21:06,2021-06-28 18:21:06,"OP_REQUIRES failed at decode_wav_op.cc:55 : Invalid argument: Can only read 16-bit WAV files, but received 32",<tensorflow><audio>,0,0,,,,CC BY-SA 4.0
62735623,1,,,2020-07-05 00:06:52,,4,11236,"<p>So I recently got back into machine learning, and decided to start the Kaggle course for &quot;ConnectX&quot; (<a href=""https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning"" rel=""nofollow noreferrer"">https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning</a>). I am trying to do lesson 4, in which I use stable-baselines + Tensorflow to make an AI. The problem is, I can't seem to use stable-baselines properly as it gives me an error instantly as I try to import it. Here is the error message:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
&lt;ipython-input-13-f5986851ce81&gt; in &lt;module&gt;
      1 import os
----&gt; 2 from stable_baselines.bench import Monitor
      3 from stable_baselines.common.vec_env import DummyVecEnv
      4 
      5 # Create directory for logging training information

~\Anaconda3\lib\site-packages\stable_baselines\__init__.py in &lt;module&gt;
----&gt; 1 from stable_baselines.a2c import A2C
      2 from stable_baselines.acer import ACER
      3 from stable_baselines.acktr import ACKTR
      4 from stable_baselines.deepq import DQN
      5 from stable_baselines.her import HER

~\Anaconda3\lib\site-packages\stable_baselines\a2c\__init__.py in &lt;module&gt;
----&gt; 1 from stable_baselines.a2c.a2c import A2C

~\Anaconda3\lib\site-packages\stable_baselines\a2c\a2c.py in &lt;module&gt;
      5 import tensorflow as tf
      6 
----&gt; 7 from stable_baselines import logger
      8 from stable_baselines.common import explained_variance, tf_util, ActorCriticRLModel, SetVerbosity, TensorboardWriter
      9 from stable_baselines.common.policies import ActorCriticPolicy, RecurrentActorCriticPolicy

~\Anaconda3\lib\site-packages\stable_baselines\logger.py in &lt;module&gt;
     15 from tensorflow.python.util import compat
     16 
---&gt; 17 from stable_baselines.common.misc_util import mpi_rank_or_zero
     18 
     19 DEBUG = 10

~\Anaconda3\lib\site-packages\stable_baselines\common\__init__.py in &lt;module&gt;
      2 from stable_baselines.common.console_util import fmt_row, fmt_item, colorize
      3 from stable_baselines.common.dataset import Dataset
----&gt; 4 from stable_baselines.common.math_util import discount, discount_with_boundaries, explained_variance, \
      5     explained_variance_2d, flatten_arrays, unflatten_vector
      6 from stable_baselines.common.misc_util import zipsame, set_global_seeds, boolean_flag

~\Anaconda3\lib\site-packages\stable_baselines\common\math_util.py in &lt;module&gt;
      1 import numpy as np
----&gt; 2 import scipy.signal
      3 
      4 
      5 def safe_mean(arr):

~\Anaconda3\lib\site-packages\scipy\signal\__init__.py in &lt;module&gt;
    287 
    288 &quot;&quot;&quot;
--&gt; 289 from . import sigtools, windows
    290 from .waveforms import *
    291 from ._max_len_seq import max_len_seq

~\Anaconda3\lib\site-packages\scipy\signal\windows\__init__.py in &lt;module&gt;
     39 &quot;&quot;&quot;
     40 
---&gt; 41 from .windows import *
     42 
     43 __all__ = ['boxcar', 'triang', 'parzen', 'bohman', 'blackman', 'nuttall',

~\Anaconda3\lib\site-packages\scipy\signal\windows\windows.py in &lt;module&gt;
      5 
      6 import numpy as np
----&gt; 7 from scipy import linalg, special, fft as sp_fft
      8 
      9 __all__ = ['boxcar', 'triang', 'parzen', 'bohman', 'blackman', 'nuttall',

~\Anaconda3\lib\site-packages\scipy\special\__init__.py in &lt;module&gt;
    631 from .sf_error import SpecialFunctionWarning, SpecialFunctionError
    632 
--&gt; 633 from . import _ufuncs
    634 from ._ufuncs import *
    635 

ImportError: DLL load failed: The specified module could not be found.
</code></pre>
<p>It looks like something's wrong with <code>scipy</code>, but I have no idea what I can do to fix it. This error happens even if I run <code>import stable_baselines</code>. Here is the code I run to create the virtual environment (BTW, this is in PowerShell b/c that is what Jupyter Lab gives me):</p>
<pre><code>python -m venv myenv
.\myenv\Scripts\Activate.ps1
pip install stable-baselines
</code></pre>
<p><strong>NOTE:</strong> I don't know if this is of any significance, but when I install <code>stable-baselines</code>, an error appears: <code>ERROR: gym 0.17.2 has requirement cloudpickle&lt;1.4.0,&gt;=1.2.0, but you'll have cloudpickle 1.5.0 which is incompatible.</code></p>
<p><strong>PS:</strong> I found the same issue <a href=""https://stackoverflow.com/questions/62042491/stable-baseline-with-tensorflow-issue"">here</a>, but I have no idea how they fixed it. The answer just says &quot;I used anaconda&quot;, but there is no <code>stable-baselines</code> package in anaconda! I tried installing tensorflow from anaconda and stable-baselines from pip, but it still gave the same error.</p>
<p><strong>LAST EDIT:</strong> Looks like this issue is on <code>.</code> imports and is only valid in <code>jupyter notebook</code> (doesn't have anything to do with <code>tensorflow</code> - it works fine in Python CLI). I have explained it in my new question about <code>opencv</code> <a href=""https://stackoverflow.com/questions/62786036/opencv-gives-an-error-in-the-jupyter-notebook-but-works-in-python-cli"">here</a>.</p>
<p>~ Ayush</p>
",11483682.0,,11483682.0,,2020-07-08 18:28:15,2022-08-02 18:16:33,Stable Baselines doesn't work with tensorflow,<python><tensorflow><stable-baselines>,3,1,,,,CC BY-SA 4.0
68567630,1,,,2021-07-28 18:39:26,,4,3179,"<p>I have a dictionary which has been completely preprocessed and is ready to feed in to a BERT model. However, I am struggling a lot to get it into a tf.dataset. This is what my one element of my dataset looks like:
<code>print(dataset[0])</code></p>
<pre><code>{'input_ids': &lt;tf.Tensor: shape=(128,), dtype=int64, numpy= array([  101,   171,   112,  2537, 12293,   131, 11250,   118,   118,
        2537, 12293,   131, 11250,  1110,  1126,  1237,  1778,  1326,
        1687,  1111,  5957,  1398, 11737,  1118,  8129, 14399,  1105,
        3230,  9426, 27277,   119,  1135,  1110,  1103,  1148,  1326,
        1872,  4418,  1111,  1115,  1555,   117,  1105,  1103,  1148,
        2537, 12293,  1326,  1290,  2537, 12293,   131,  9892,  4803,
        1107,  1478,   119,  9617,  4986,   170,  4967,  1196,  1103,
        1958,  1104,  1103,  1560,  2537, 12293,  1326,  1105,  2767,
        1121,  1103, 21169,  1104,  1103, 18061,  1666,  2672,  2441,
         117, 11250, 16001,  1103,  4245,   118,   118,   148,  1979,
        1320,  1594,  1229,  1378,  1103,  3039,  1104,  1103,  6684,
       11250,   119, 23886,   147,   119, 16218,  1105,  6619, 11679,
       19644,  2145,  2867,  1112,  1437, 14627,   102,   171,   112,
        1110,  1175,   170,  1207,  2851,   189, 14909,  1326,  1909,
         112,   102])&gt;, 'input_mask': &lt;tf.Tensor: shape=(128,), dtype=int64, numpy= array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])&gt;, 'segment_ids': &lt;tf.Tensor: shape=(128,), dtype=int64, numpy= array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])&gt;, 'labels': &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt;}
</code></pre>
<p>all I need to do is get it into the tf.data.Dataset() format, however, I cannot seem to figure out how to make any of the functions available <code>from_tensor_slices, from_tensors, from_generator</code> work with what I have.</p>
",9443671.0,skidjoe,,,,2021-10-06 13:33:37,Converting a list of dictionaries to a tf dataset,<python><tensorflow><dataset><preprocessor>,2,1,,,,CC BY-SA 4.0
68176888,1,68177849.0,,2021-06-29 10:24:07,,4,1701,"<p>I am trying to compile protoc files using this command:</p>
<pre><code>protoc/bin/protoc models/research/object_detection/protos/*.proto --python_out=.
</code></pre>
<p>but I am getting  this output on cmd</p>
<pre><code>object_detection/protos/flexible_grid_anchor_generator.proto: File not found.
object_detection/protos/grid_anchor_generator.proto: File not found.
object_detection/protos/multiscale_anchor_generator.proto: File not found.
object_detection/protos/ssd_anchor_generator.proto: File not found.
models/research/object_detection/protos/anchor_generator.proto:5:1: Import &quot;object_detection/protos/flexible_grid_anchor_generator.proto&quot; was not found or had errors.
models/research/object_detection/protos/anchor_generator.proto:6:1: Import &quot;object_detection/protos/grid_anchor_generator.proto&quot; was not found or had errors.
models/research/object_detection/protos/anchor_generator.proto:7:1: Import &quot;object_detection/protos/multiscale_anchor_generator.proto&quot; was not found or had errors.
models/research/object_detection/protos/anchor_generator.proto:8:1: Import &quot;object_detection/protos/ssd_anchor_generator.proto&quot; was not found or had errors.
models/research/object_detection/protos/anchor_generator.proto:14:5: &quot;GridAnchorGenerator&quot; is not defined.
models/research/object_detection/protos/anchor_generator.proto:15:5: &quot;SsdAnchorGenerator&quot; is not defined.
models/research/object_detection/protos/anchor_generator.proto:16:5: &quot;MultiscaleAnchorGenerator&quot; is not defined.
models/research/object_detection/protos/anchor_generator.proto:17:5: &quot;FlexibleGridAnchorGenerator&quot; is not defined.
</code></pre>
<p>So what can be the problem
Thank you</p>
",13184263.0,,,,,2023-03-14 06:14:27,ERROR when trying to compile protoc files: file not found or had errors,<python><tensorflow><object-detection><protoc>,3,0,,,,CC BY-SA 4.0
68181902,1,,,2021-06-29 15:42:47,,4,2100,"<p>I'm looking to export my PyTorch model into tensorflow.js and have the ability to finetune it in tensorflow.js.
To do this, I first convert PyTorch weights to ONNX, then to tensorflow, and finally use <code>tensorflowjs_converter</code> to convert to tensorflow.js. This results in an un-trainable model in TensorFlow.js. Is there any way to make this model trainable at one of these steps? The following is a minimal reproducible example.</p>
<p>First, defining a generic model and converting it in PyTorch:</p>
<pre><code>import torch
import torch.nn.functional as F


class ModelClass(torch.nn.Module):
    def __init__(self):
        super(ModelClass, self).__init__()
        self.fc1 = torch.nn.Linear(100, 10)
        self.fc2 = torch.nn.Linear(10, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.sigmoid(self.fc2(x))
        return x


model = ModelClass()

example_input = torch.randn((1, 100), requires_grad=True)
print(model(example_input))

input_names = [&quot;input0&quot;]
output_names = [&quot;output0&quot;]
dynamic_axes = {'input0': {0: 'batch'}, 'output0': {0: 'batch'}}

torch_out = torch.onnx.export(
    model, example_input, 'model.onnx', export_params=True, verbose=True, input_names=input_names,
    output_names=output_names, dynamic_axes=dynamic_axes, opset_version=10,
    operator_export_type=torch.onnx.OperatorExportTypes.ONNX)
</code></pre>
<p>Next, I use <code>onnx_tf</code> to convert from ONNX to TensorFlow.</p>
<pre><code>import onnx
import tensorflow as tf
from onnx_tf.backend import prepare

onnx_model = onnx.load('model.onnx')
tf_model = prepare(onnx_model)

tf_model.export_graph('model')

</code></pre>
<p>Finally, I use <code>tensorflowjs_converter</code> to convert to tensorflow.js with the command</p>
<pre><code>tensorflowjs_converter --input_format=tf_saved_model model model_tfjs
</code></pre>
<p>However, when loading this in tensorflow.js with <code>tf.loadGraphModel(&quot;model_tfjs/model.json&quot;)</code>, it becomes a <code>tf.FrozenModel</code> according to <a href=""https://www.tensorflow.org/js/guide/conversion"" rel=""nofollow noreferrer"">tensorflow.js documentation</a>. The only way to have a trainable model is by <code>tf.loadLayersModel</code> which requires a Keras model to be converted to tensorflow.js rather than a tensorflow savedmodel. However, I'm also unable to convert the converted tensorflow savedmodel into Keras. Is it possible to export a PyTorch model to tensorflow.js and have it still be trainable?</p>
<p>I've tried other libraries, <code>pytorch2keras</code>, <code>onnx2keras</code>, among others; they all seem to use lambda layers and therefore cannot be converted to tensorflow.js either. Thanks.</p>
<p>Edit: Here are additional details. I'm trying to convert an efficientnet from Pytorch to Tensorflow.</p>
<p>This converts the PyTorch efficientnet (from a library called geffnet) to ONNX. We can set either dynamic dimensions or static, but neither work.</p>
<pre><code>import onnx
import geffnet
import torch

efficientnet = 'efficientnet_b0'
DYNAMIC_SIZE = True

img_sizes = [224, 240, 260, 300, 380, 456, 528, 600, 672]
model_idx = int(efficientnet[-1]) # to find the correct static image size 

model = geffnet.create_model(
    efficientnet,
    in_chans=3,
    pretrained=True,
    exportable=True)

model.eval()

example_input = torch.randn((1, 3, img_sizes[model_idx], img_sizes[model_idx]), requires_grad=True)
model(example_input)

input_names = [&quot;input0&quot;]
output_names = [&quot;output0&quot;]
dynamic_axes = {'input0': {0: 'batch'}, 'output0': {0: 'batch'}}
if DYNAMIC_SIZE:
    dynamic_axes['input0'][2] = 'height'
    dynamic_axes['input0'][3] = 'width'

torch_out = torch.onnx.export(
    model, example_input, 'efficientnet_b0.onnx', export_params=True, verbose=False, input_names=input_names,
    output_names=output_names, dynamic_axes=dynamic_axes,
    opset_version=11, operator_export_type=torch.onnx.OperatorExportTypes.ONNX)

onnx_model = onnx.load('efficientnet_b0.onnx')
onnx.checker.check_model(onnx_model)
</code></pre>
<p>Next, we can convert to Tensorflow.</p>
<pre><code>import onnx
from onnx_tf.backend import prepare

onnx_model = onnx.load(onnx_path)
tf_model = prepare(onnx_model)
tf_model.export_graph('efficientnet_b0_tf')
</code></pre>
<p>Finally we convert to tensorflow.js using tfjs converter.</p>
<p><code>tensorflowjs_converter --input_format=tensorflow_saved_model efficientnet_b0_tf efficientnet_b0_tfjs</code></p>
<p>A minimal test in tensorflow.js is as follows</p>
<pre><code>const tf = require('@tensorflow/tfjs-node');

const getModel = async function () {
    const imgBase = await tf.loadGraphModel('file://./efficientnet_b0_tfjs/model.json');
    const x = tf.randomNormal([1, 224, 224, 3]);
    console.log(imgBase(x));
}
getModel();
</code></pre>
<p>The preceeding example works as a <code>tf.FrozenModel</code> in inference, but cannot be trained. To be trained, a model must be converted to tensorflow.js from keras. My attempts to convert the python tensorflow model to keras have been unsuccessful. For example,</p>
<pre><code>import tensorflow as tf

model = tf.keras.models.load_model('efficientnet_b0_tf')
print(model.summary())

model.save(savepath)
</code></pre>
<p>This results in the following traceback:</p>
<pre><code>Traceback (most recent call last):
  File &quot;graph2layers.py&quot;, line 29, in &lt;module&gt;
    graph2layers()
  File &quot;graph2layers.py&quot;, line 18, in graph2layers
    print(model.summary())
AttributeError: '_UserObject' object has no attribute 'summary'
</code></pre>
",13610744.0,,13610744.0,,2021-07-03 19:46:12,2021-07-03 19:46:12,How to train a PyTorch model in TensorFlow.js?,<python><tensorflow><machine-learning><tensorflow.js><onnx>,1,5,,,,CC BY-SA 4.0
63359268,1,,,2020-08-11 13:32:43,,4,386,"<p>I am trying to load a model saved in <a href=""https://www.tensorflow.org/guide/saved_model"" rel=""nofollow noreferrer"">SavedModel</a> format, and then apply some calculations on top of it and re-save the whole pipeline. A minimal code is the following (from <a href=""https://www.kaggle.com/camaskew/host-baseline-example"" rel=""nofollow noreferrer"">this Kaggle kernel</a>):</p>
<pre class=""lang-python prettyprint-override""><code># Load the model. After that, the `delg_model` will be of the type
# `tensorflow.python.training.tracking.tracking.AutoTrackable`
delg_model = tf.saved_model.load('path/to/saved/model/dir')

# we don't need the whole model, so we prune it. After that, the
# `global_feature_extraction_fn` will be of the type
# `tensorflow.python.eager.wrap_function.WrappedFunction`
delg_input_tensor_names = ['input_image:0', 'input_scales:0']
global_feature_extraction_fn = delg_model.prune(
    delg_input_tensor_names, ['global_descriptors:0'])
</code></pre>
<blockquote>
<p><strong>Question:</strong> Now, I want to save the <code>global_feature_extraction_fn</code>, with some other TF ops to post-process the output, in the same SavedModel format. What is the correct way to do that?</p>
</blockquote>
<hr />
<h2>What I have tried</h2>
<p>I have tried to follow the TensorFlow documentation of <a href=""https://www.tensorflow.org/guide/saved_model#saving_a_custom_model"" rel=""nofollow noreferrer"">saving custom models to SavedModel format</a> and define a <code>tf.Module</code>:</p>
<pre class=""lang-python prettyprint-override""><code>class DelgModule(tf.Module):
    def __init__(self):
        super().__init__()

    @tf.function(input_signature=[
        tf.TensorSpec(shape=[None, None, 3], name='input_image')
    ])
    def call(self, input_tensor):
        # custom function on top of the model's output
        embedding = tf.nn.l2_normalize(
            global_feature_extraction_fn(
                input_tensor,                         # input_image
                tf.convert_to_tensor([0.7, 1.0, 1.4]) # input_scales
            )[0],
            axis=1, name='l2_normalized_output')
        return output_tensors = {
            'global_descriptor': embedding
        }

delg_module = DelgModule()
</code></pre>
<p>Then I ran it on the test image to build the <code>tf.function</code> and make sure that it works correctly (produces the right output). But when I tried to save it as follows:</p>
<pre class=""lang-python prettyprint-override""><code>tf.saved_model.save(
    delg_module, export_dir='./delg_resaved',
    signatures={
        'serving_default': delg_module.call
    })
</code></pre>
<p>the resulting model was incorrect. The original one weighted 90 MB while the one in <code>delg_resaved</code> weights only 800 KB. I also tried doing <code>tf.saved_model.load</code> <strong>inside</strong> the <code>DelgModule.call</code> function, so that the graph creation and variables loading is done entirely inside that <code>tf.function</code>, but the results remained the same.</p>
",4789373.0,,,,,2021-02-03 07:02:39,TensorFlow 2: Re-saving a SavedModel?,<python><tensorflow><machine-learning><tensorflow2.0><tensorflow-serving>,0,0,,,,CC BY-SA 4.0
63159070,1,,,2020-07-29 17:19:55,,4,1901,"<p>I'm doing following tutorial where you implement a TensorFlow Lite model in an android app:
<a href=""https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#0"" rel=""nofollow noreferrer"">https://codelabs.developers.google.com/codelabs/recognize-flowers-with-tensorflow-on-android/#0</a></p>
<p>Everything works well except that following error messages shows up while testing the app and my model doesn't recognize the images.</p>
<pre><code>E/libc: Access denied finding property &quot;persist.vendor.camera.privapp.list&quot;
W/.classification: type=1400 audit(0.0:45214): avc: denied { read } for name=&quot;u:object_r:persist_camera_prop:s0&quot; dev=&quot;tmpfs&quot; ino=17052 scontext=u:r:untrusted_app_27:s0:c88,c256,c512,c768 tcontext=u:object_r:persist_camera_prop:s0 tclass=file permissive=0
I/tensorflow: CameraConnectionFragment: Opening camera preview: 640x480
W/BpBinder: Slow Binder: BpBinder transact took 213ms, interface=android.hardware.camera2.ICameraDeviceUser, code=6 oneway=false
W/Gralloc3: allocator 3.x is not supported
D/tensorflow: CameraActivity: Initializing buffer 0 at size 307200
D/tensorflow: CameraActivity: Initializing buffer 1 at size 153599
    CameraActivity: Initializing buffer 2 at size 153599
W/ImageReader_JNI: Unable to acquire a buffer item, very likely client tried to acquire more than maxImages buffers
W/ImageReader_JNI: Unable to acquire a buffer item, very likely client tried to acquire more than maxImages buffers
</code></pre>
<p>Has somebody experienced this issue before? I'm testing the app with a Xiaomi Redmi Note 9 Pro.</p>
",11469656.0,,,,,2020-07-29 17:19:55,"access denied ""persist.vendor.camera.privapp.list""",<android><tensorflow>,0,0,0.0,,,CC BY-SA 4.0
73610400,1,,,2022-09-05 13:47:56,,4,615,"<p>I have a dummy model (a linear autoencoder). When training on a dataset of 1 000 records, it works; but on a larger dataset, three orders of magnitude larger, it runs out of GPU memory; even though the batch size is fixed and the computer has enough RAM to hold.</p>
<p>Am I doing something silly?</p>
<p>Note: it works fine on TF 2.5, but crashes on TF 2.6-2.9. It always works if training on CPU.</p>
<p>The model is:</p>
<pre><code>def get_model(n_inputs: int) -&gt; models.Model:
    inp = layers.Input(shape=(n_inputs,))

    out = layers.Dense(n_inputs, activation='linear')(inp)
    m = models.Model(inputs=inp, outputs=out)
    m.compile(loss='mse', optimizer='adam')
    m.summary()
    return m
</code></pre>
<p>I am feeding the data through the <code>tf.data</code> API</p>
<pre><code>    def wrap_data(data: np.ndarray) -&gt; tf.data.Dataset:
        dataset = tf.data.Dataset.from_tensor_slices(data)
        shuffled = dataset.shuffle(buffer_size=len(data), reshuffle_each_iteration=True)
        batched = shuffled.batch(16, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)
        autoencoder = batched.map(lambda x: (x, x)).prefetch(5)

        return autoencoder
</code></pre>
<p>The full reproducing script is <a href=""https://gist.github.com/Dapid/c876d6d634ec4454edfdae5c16a4cdb2"" rel=""nofollow noreferrer"">here.</a> Running <code>python benchmark.py</code> works, but <code>python benchmark.py --big</code> doesn't.</p>
<p>I am using Python 3.9 on Fedora 36. The GPU is a Nvidia RTX 2070 with 8 GiB of RAM. The driver version is 515.48.07 and CUDA Version: 11.7. <code>nvidia-smi</code> reports most of the memory is available between runs, and the small version requires less than 800 MiB.</p>
<p>The full traceback is:</p>
<pre><code>2022-09-05 15:29:37.525261: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000000 exceeds 10% of free system memory.
2022-09-05 15:29:54.002629: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 15.26GiB (rounded to 16384000000)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2022-09-05 15:29:54.002987: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *_******____________________________________________________________________________________________
Traceback (most recent call last):
  File &quot;/home/david/[path]/benchmark.py&quot;, line 49, in &lt;module&gt;
    main(parser.parse_args().big)
  File &quot;/home/david/[path]/benchmark.py&quot;, line 40, in main
    train_data_iterator = wrap_data(train_data)
  File &quot;/home/david/[path]/benchmark.py&quot;, line 33, in wrap_data
    dataset = tf.data.Dataset.from_tensor_slices(data)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py&quot;, line 809, in from_tensor_slices
    return TensorSliceDataset(tensors, name=name)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py&quot;, line 4551, in __init__
    element = structure.normalize_element(element)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/data/util/structure.py&quot;, line 125, in normalize_element
    ops.convert_to_tensor(t, name=&quot;component_%d&quot; % i, dtype=dtype))
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/profiler/trace.py&quot;, line 183, in wrapped
    return func(*args, **kwargs)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 1640, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py&quot;, line 48, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 267, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 279, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 304, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 102, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
</code></pre>
<p>And specifying the GPU memory allocator, as suggested, doesn't help:</p>
<pre><code>2022-09-05 15:33:19.542433: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 16384000000 exceeds 10% of free system memory.
2022-09-05 15:33:25.973935: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:288] gpu_async_0 cuMemAllocAsync failed to allocate 16384000000 bytes: CUDA error: out of memory (CUDA_ERROR_OUT_OF_MEMORY)
 Reported by CUDA: Free memory/Total memory: 1115357184/8369799168
2022-09-05 15:33:25.973961: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:293] Stats: Limit:                      6221922304
InUse:                        67126312
MaxInUse:                    201327628
NumAllocs:                          13
MaxAllocSize:                 67108864
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-09-05 15:33:25.973970: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:56] Histogram of current allocation: (allocation_size_in_bytes, nb_allocation_of_that_sizes), ...;
2022-09-05 15:33:25.973974: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 4, 5
2022-09-05 15:33:25.973976: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 8, 2
2022-09-05 15:33:25.973979: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 1028, 1
2022-09-05 15:33:25.973982: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 16384, 1
2022-09-05 15:33:25.973985: E tensorflow/core/common_runtime/gpu/gpu_cudamallocasync_allocator.cc:59] 67108864, 1
Traceback (most recent call last):
  File &quot;/home/david/[path]/benchmark.py&quot;, line 48, in &lt;module&gt;
    main(parser.parse_args().big)
  File &quot;/home/david/[path]/benchmark.py&quot;, line 40, in main
    train_data_iterator = wrap_data(train_data)
  File &quot;/home/david/[path]/benchmark.py&quot;, line 33, in wrap_data
    dataset = tf.data.Dataset.from_tensor_slices(data)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py&quot;, line 809, in from_tensor_slices
    return TensorSliceDataset(tensors, name=name)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py&quot;, line 4551, in __init__
    element = structure.normalize_element(element)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/data/util/structure.py&quot;, line 125, in normalize_element
    ops.convert_to_tensor(t, name=&quot;component_%d&quot; % i, dtype=dtype))
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/profiler/trace.py&quot;, line 183, in wrapped
    return func(*args, **kwargs)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 1640, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py&quot;, line 48, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 267, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 279, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 304, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File &quot;/home/david/.virtualenvs/ainet/lib64/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 102, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
</code></pre>
<p>Update: <a href=""https://github.com/keras-team/keras/issues/16997"" rel=""nofollow noreferrer"">bug report</a></p>
",3375011.0,,3375011.0,,2023-03-15 12:12:57,2023-04-15 18:24:32,TensorFlow: out of memory depends on data size,<python><tensorflow><tensorflow-datasets>,0,6,0.0,,,CC BY-SA 4.0
66888763,1,66888885.0,,2021-03-31 13:23:31,,4,10656,"<p>I first tried to install tensorflow=2.4.0 in Conda with the channel defaults, but it caused an error:</p>
<pre><code>conda install tensorflow=2.4.0
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>Solving environment: failed
PackagesNotFoundError: The following packages are not available from current channels:
  - tensorflow=2.4.0

Current channels:

  - https://repo.continuum.io/pkgs/main/linux-64
  - https://repo.continuum.io/pkgs/main/noarch
  - https://repo.continuum.io/pkgs/free/linux-64
  - https://repo.continuum.io/pkgs/free/noarch
  - https://repo.continuum.io/pkgs/r/linux-64
  - https://repo.continuum.io/pkgs/r/noarch
  - https://repo.continuum.io/pkgs/pro/linux-64
  - https://repo.continuum.io/pkgs/pro/noarch
</code></pre>
<p>Then I'm tried using the Conda Forge channel:</p>
<pre><code>conda install -c conda-forge tensorflow=2.4.0
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>Solving environment: failed
InvalidVersionSpecError: Invalid version spec: =2.7
</code></pre>
<p>How do I resolve this error?</p>
",15164196.0,,570918.0,,2021-05-10 19:24:17,2021-05-10 19:24:17,InvalidVersionSpecError: Invalid version spec: =2.7,<python><tensorflow><conda>,1,0,,,,CC BY-SA 4.0
70516978,1,,,2021-12-29 08:37:32,,4,894,"<p>I am using Pixellib library in Python to detect a person and change its background, as shown in their example <a href=""https://pixellib.readthedocs.io/en/latest/change_image_bg.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>It works flawlessly, but takes huge processing power on my laptop, coupled with their large (~150mb) pascalvoc model, thus rendering an image in approx 4-5sec.</p>
<p>I need to be able to do the same via a mobile phone app, so certainly this cannot be run on a user's mobile. Alternative is to run this on cloud and return the processed image back. This is both costly if user requests increase and will still have noticable lag on user's app.</p>
<p>So, how do achieve this? Apps like <a href=""https://www.canva.com/pro/background-remover/"" rel=""nofollow noreferrer"">Canva Pro</a> seem to do this seamlessly in an app <a href=""https://static-cse.canva.com/video/753559/02_CANVA_ProFeatures_BackgroundRemover.mp4"" rel=""nofollow noreferrer"">fairly quickly</a>. In fact, there are many other 'free' apps on Play store claiming to do the same.</p>
<p>Thus, is there a better way to run Pixellib, to make it more performant? Or any other library that can provide similar (or better) ouptut and can be run on user's mobile?</p>
",2629420.0,,2602877.0,,2022-08-18 09:14:54,2022-08-18 09:14:54,Pixellib - removing background takes huge processing,<python><tensorflow><image-processing><pixellib>,1,1,,2022-08-13 23:20:12,,CC BY-SA 4.0
71766129,1,71769796.0,,2022-04-06 11:41:36,,4,1834,"<p>This is my bunch of code:</p>
<pre><code># I train a model, save it and then clear all with
del model
tf.keras.backend.clear_session()
gc.collect()
print(f&quot;memory usage {tf.config.experimental.get_memory_info('GPU:0')['current'] / 10 ** 9} GB&quot;)
checkpoint_model = open_saved_model()    # returns a tf.keras.Model()
print(f&quot;memory usage {tf.config.experimental.get_memory_info('GPU:0')['current'] / 10 ** 9} GB&quot;)
eval_result = checkpoint_model.evaluate(train_ds[0], train_ds[1], batch_size=30)
print(f&quot;memory usage {tf.config.experimental.get_memory_info('GPU:0')['current'] / 10 ** 9} GB&quot;)
eval_result = checkpoint_model.evaluate(train_ds[0], train_ds[1], batch_size=30)
</code></pre>
<p>The memory outputs are:</p>
<pre><code>memory usage 0.0 GB
memory usage 0.013005312 GB
memory usage 5.893292544 GB
</code></pre>
<p>And on the last line I get <code>tensorflow.python.framework.errors_impl.InternalError</code> (full message at the end)</p>
<p>My train dataset is supposed to be <code>train_ds[0].size * train_ds[0].itemsize / 10**9 = 4.395368448</code> GB.</p>
<p>My GPU available size (using <code>nvidia-smi</code> command) is <code>10481MiB / 11016MiB</code>. If I do the used memory plus the numpy array I get <code>10.27146624</code> which is borderline to the 10.48 that tensorflow decided to allocate. Even more, although it reserved 10GB, there is a message (see full error message at the end) that it has 8GB memory (weird but it explains why I'm out of memory).</p>
<p>Regardless of this borderline result, it seems super wrong that the dataset is allocated AGAIN. I should either re-use the dataset used in <code>evaluate</code> or just replace it with the new one.</p>
<p>I tried using <code>train_dataset = tf.data.Dataset.from_tensor_slices((train_ds[0], train_ds[1])).batch(32)</code> and the MWE worked (with an increase of memory usage to <code>7.35GB</code>) but if I change the second <code>evaluate</code> with a <code>predict</code> (which is actually my real goal)  I then get the same error.</p>
<hr />
<p>I read about using <code>os.environ[&quot;TF_GPU_ALLOCATOR&quot;] = &quot;cuda_malloc_async&quot;</code> and then I just get a <code>Process finished with exit code 139 (interrupted by signal 11: SIGSEGV)</code> without any error message. But comparing it to the other messages, it stops before the message <code>Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8965 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5</code> meaning that I think it fails to &quot;create device&quot;.</p>
<h2>MOTIVATION</h2>
<p>This was the MWE I managed to re-create but the truth is I want to <strong>evaluate</strong> and <strong>predict</strong> on MANY datasets that should be each 5GB size. The current solution for me should be:</p>
<ol>
<li>Clear all GPU</li>
<li>Load model</li>
<li>Evaluate</li>
<li>Clear all GPU</li>
<li>Load model again</li>
<li>Predict</li>
</ol>
<p>And then repeat steps 1 to 6 for my several datasets (Highly inefficient right?).</p>
<h2>Full error message</h2>
<pre><code>2022-04-06 13:24:49.708029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:49.713198: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:49.713526: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:49.713988: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-06 13:24:49.714414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:49.714715: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:49.715002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:50.044152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:50.044479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:50.044766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2022-04-06 13:24:50.045036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8965 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5
memory usage 0.0 GB
2022-04-06 13:25:00.250155: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.
2022-04-06 13:25:00.250170: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.
2022-04-06 13:25:00.250192: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs
2022-04-06 13:25:00.250349: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcupti.so.11.2'; dlerror: libcupti.so.11.2: cannot open shared object file: No such file or directory
2022-04-06 13:25:00.356485: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.
2022-04-06 13:25:00.356639: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed
2022-04-06 13:25:00.372969: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory.
2022-04-06 13:25:03.200488: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory.
/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
  warnings.warn('Custom mask layers require a config and must override '
2022-04-06 13:25:05.075473: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2022-04-06 13:25:07.796065: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8303
2022-04-06 13:25:08.177722: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-04-06 13:25:08.177947: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-04-06 13:25:08.177972: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version
2022-04-06 13:25:08.178231: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory
2022-04-06 13:25:08.178262: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
  1/187 [..............................] - ETA: 11:00 - loss: 0.8855 - accuracy: 0.3187 - average_accuracy: 0.2666 - precision: 0.3264 - recall: 0.00222022-04-06 13:25:08.716360: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.
2022-04-06 13:25:08.716379: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.
  2/187 [..............................] - ETA: 1:05 - loss: 0.8388 - accuracy: 0.3063 - average_accuracy: 0.2759 - precision: 0.3169 - recall: 0.0049 2022-04-06 13:25:09.011233: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.
2022-04-06 13:25:09.011432: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed
2022-04-06 13:25:09.040157: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 705 callback api events and 707 activity events. 
2022-04-06 13:25:09.049283: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.
2022-04-06 13:25:09.061327: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09

2022-04-06 13:25:09.071522: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.trace.json.gz
2022-04-06 13:25:09.096291: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09

2022-04-06 13:25:09.101018: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.memory_profile.json.gz
2022-04-06 13:25:09.101899: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09
Dumped tool data for xplane.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.xplane.pb
Dumped tool data for overview_page.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.overview_page.pb
Dumped tool data for input_pipeline.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.input_pipeline.pb
Dumped tool data for tensorflow_stats.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.tensorflow_stats.pb
Dumped tool data for kernel_stats.pb to log/2022/04April/06Wednesday/run-13h24m42/tensorboard/train/plugins/profile/2022_04_06_13_25_09/barrachina-SONDRA.kernel_stats.pb

187/187 [==============================] - 10s 37ms/step - loss: 0.8277 - accuracy: 0.5412 - average_accuracy: 0.3043 - precision: 0.5026 - recall: 0.0087 - val_loss: 0.8309 - val_accuracy: 0.6880 - val_average_accuracy: 0.2931 - val_precision: 0.6810 - val_recall: 0.0047
memory usage 6.042584576 GB
memory usage 0.006478336 GB
2022-04-06 13:25:16.022531: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory.
memory usage 0.012938752 GB
2022-04-06 13:25:18.885690: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory.
187/187 [==============================] - 4s 16ms/step - loss: 0.8138 - accuracy: 0.6999 - average_accuracy: 0.2968 - precision: 0.6710 - recall: 0.0058
memory usage 5.90003712 GB
2022-04-06 13:25:24.458057: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 4396941312 exceeds 10% of free system memory.
2022-04-06 13:25:35.851249: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.09GiB (rounded to 4396941312)requested by op _EagerConst
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2022-04-06 13:25:35.851336: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc
2022-04-06 13:25:35.851375: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256):  Total Chunks: 263, Chunks in use: 263. 65.8KiB allocated for chunks. 65.8KiB in use in bin. 15.2KiB client-requested in use in bin.
2022-04-06 13:25:35.851405: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512):  Total Chunks: 71, Chunks in use: 70. 42.2KiB allocated for chunks. 41.8KiB in use in bin. 36.0KiB client-requested in use in bin.
2022-04-06 13:25:35.851432: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024):     Total Chunks: 10, Chunks in use: 9. 15.0KiB allocated for chunks. 14.0KiB in use in bin. 12.6KiB client-requested in use in bin.
2022-04-06 13:25:35.851456: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851483: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096):     Total Chunks: 6, Chunks in use: 6. 31.5KiB allocated for chunks. 31.5KiB in use in bin. 30.4KiB client-requested in use in bin.
2022-04-06 13:25:35.851511: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192):     Total Chunks: 12, Chunks in use: 12. 123.0KiB allocated for chunks. 123.0KiB in use in bin. 121.5KiB client-requested in use in bin.
2022-04-06 13:25:35.851534: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384):    Total Chunks: 1, Chunks in use: 0. 30.2KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851560: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768):    Total Chunks: 13, Chunks in use: 11. 579.0KiB allocated for chunks. 475.5KiB in use in bin. 445.5KiB client-requested in use in bin.
2022-04-06 13:25:35.851586: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536):    Total Chunks: 1, Chunks in use: 1. 73.8KiB allocated for chunks. 73.8KiB in use in bin. 40.5KiB client-requested in use in bin.
2022-04-06 13:25:35.851610: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072):   Total Chunks: 18, Chunks in use: 18. 2.85MiB allocated for chunks. 2.85MiB in use in bin. 2.72MiB client-requested in use in bin.
2022-04-06 13:25:35.851634: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144):   Total Chunks: 2, Chunks in use: 1. 769.5KiB allocated for chunks. 283.5KiB in use in bin. 162.0KiB client-requested in use in bin.
2022-04-06 13:25:35.851658: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288):   Total Chunks: 11, Chunks in use: 10. 6.96MiB allocated for chunks. 6.33MiB in use in bin. 6.33MiB client-requested in use in bin.
2022-04-06 13:25:35.851682: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576):  Total Chunks: 2, Chunks in use: 2. 2.25MiB allocated for chunks. 2.25MiB in use in bin. 1.27MiB client-requested in use in bin.
2022-04-06 13:25:35.851704: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851725: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304):  Total Chunks: 1, Chunks in use: 0. 4.43MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851769: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608):  Total Chunks: 1, Chunks in use: 0. 12.44MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851799: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851821: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851841: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851865: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728):    Total Chunks: 1, Chunks in use: 0. 128.08MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2022-04-06 13:25:35.851889: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456):    Total Chunks: 3, Chunks in use: 2. 8.60GiB allocated for chunks. 5.48GiB in use in bin. 5.46GiB client-requested in use in bin.
2022-04-06 13:25:35.851911: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 4.09GiB was 256.00MiB, Chunk State: 
2022-04-06 13:25:35.851941: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 3.12GiB | Requested Size: 1.97MiB | in_use: 0 | bin_num: 20, prev:   Size: 512B | Requested Size: 384B | in_use: 1 | bin_num: -1
2022-04-06 13:25:35.851960: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 9401270272
2022-04-06 13:25:35.851981: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000000 of size 256 next 4
2022-04-06 13:25:35.851999: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000100 of size 256 next 6
2022-04-06 13:25:35.852016: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000200 of size 256 next 3
2022-04-06 13:25:35.852032: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000300 of size 256 next 5
2022-04-06 13:25:35.852048: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000400 of size 256 next 9
2022-04-06 13:25:35.852064: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000500 of size 256 next 7
2022-04-06 13:25:35.852080: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000600 of size 256 next 8
2022-04-06 13:25:35.852097: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000700 of size 256 next 10
2022-04-06 13:25:35.852113: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000800 of size 256 next 13
2022-04-06 13:25:35.852128: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000900 of size 256 next 14
2022-04-06 13:25:35.852144: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000a00 of size 256 next 15
2022-04-06 13:25:35.852159: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000b00 of size 256 next 83
2022-04-06 13:25:35.852174: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000c00 of size 256 next 17
2022-04-06 13:25:35.852189: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000d00 of size 256 next 18
2022-04-06 13:25:35.852204: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc942000e00 of size 256 next 21
.... Many messages like this, StackOverflow limits my max characters so I cropped it.
2022-04-06 13:25:35.858545: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fcaaace5700 of size 512 next 320
2022-04-06 13:25:35.858561: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7fcaaace5900 of size 3347949312 next 18446744073709551615
2022-04-06 13:25:35.858576: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: 
2022-04-06 13:25:35.858600: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 263 Chunks of size 256 totalling 65.8KiB
2022-04-06 13:25:35.858620: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 43 Chunks of size 512 totalling 21.5KiB
2022-04-06 13:25:35.858639: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 27 Chunks of size 768 totalling 20.2KiB
2022-04-06 13:25:35.858658: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1024 totalling 1.0KiB
2022-04-06 13:25:35.858675: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1280 totalling 2.5KiB
2022-04-06 13:25:35.858694: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 1792 totalling 10.5KiB
2022-04-06 13:25:35.858712: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 6 Chunks of size 5376 totalling 31.5KiB
2022-04-06 13:25:35.858732: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 12 Chunks of size 10496 totalling 123.0KiB
2022-04-06 13:25:35.858751: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 9 Chunks of size 41472 totalling 364.5KiB
2022-04-06 13:25:35.858770: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 51456 totalling 50.2KiB
2022-04-06 13:25:35.858789: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 62208 totalling 60.8KiB
2022-04-06 13:25:35.858807: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 75520 totalling 73.8KiB
2022-04-06 13:25:35.858826: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 147456 totalling 720.0KiB
2022-04-06 13:25:35.858845: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 11 Chunks of size 165888 totalling 1.74MiB
2022-04-06 13:25:35.858863: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 176640 totalling 172.5KiB
2022-04-06 13:25:35.858882: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 248832 totalling 243.0KiB
2022-04-06 13:25:35.858901: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 290304 totalling 283.5KiB
2022-04-06 13:25:35.858919: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 10 Chunks of size 663552 totalling 6.33MiB
2022-04-06 13:25:35.858937: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 1179648 totalling 2.25MiB
2022-04-06 13:25:35.858955: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1489978112 totalling 1.39GiB
2022-04-06 13:25:35.858973: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 4396941312 totalling 4.09GiB
2022-04-06 13:25:35.858991: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 5.49GiB
2022-04-06 13:25:35.859009: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 9401270272 memory_limit_: 9401270272 available bytes: 0 curr_region_allocation_bytes_: 18802540544
2022-04-06 13:25:35.859036: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: 
Limit:                      9401270272
InUse:                      5900037120
MaxInUse:                   6431716864
NumAllocs:                      165083
MaxAllocSize:               4396941312
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

2022-04-06 13:25:35.859127: W tensorflow/core/common_runtime/bfc_allocator.cc:468] *****************************************************************___________________________________
Traceback (most recent call last):
  File &quot;/home/barrachina/Documents/onera/PolSar/principal_simulation.py&quot;, line 524, in &lt;module&gt;
    run_wrapper(model_name=args.model[0], balance=args.balance[0], tensorflow=args.tensorflow,
  File &quot;/home/barrachina/Documents/onera/PolSar/principal_simulation.py&quot;, line 504, in run_wrapper
    df, dataset_handler, eval_df = run_model(model_name=model_name, balance=balance, tensorflow=tensorflow,
  File &quot;/home/barrachina/Documents/onera/PolSar/principal_simulation.py&quot;, line 440, in run_model
    prediction_result = checkpoint_model.predict(train_ds[0])
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/training.py&quot;, line 1720, in predict
    data_handler = data_adapter.get_data_handler(
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py&quot;, line 1383, in get_data_handler
    return DataHandler(*args, **kwargs)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py&quot;, line 1138, in __init__
    self._adapter = adapter_cls(
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py&quot;, line 230, in __init__
    x, y, sample_weights = _process_tensorlike((x, y, sample_weights))
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py&quot;, line 1031, in _process_tensorlike
    inputs = tf.nest.map_structure(_convert_numpy_and_scipy, inputs)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/util/nest.py&quot;, line 869, in map_structure
    structure[0], [func(*x) for x in entries],
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/util/nest.py&quot;, line 869, in &lt;listcomp&gt;
    structure[0], [func(*x) for x in entries],
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/keras/engine/data_adapter.py&quot;, line 1026, in _convert_numpy_and_scipy
    return tf.convert_to_tensor(x, dtype=dtype)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py&quot;, line 206, in wrapper
    return target(*args, **kwargs)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 1430, in convert_to_tensor_v2_with_dispatch
    return convert_to_tensor_v2(
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 1436, in convert_to_tensor_v2
    return convert_to_tensor(
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/profiler/trace.py&quot;, line 163, in wrapped
    return func(*args, **kwargs)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/ops.py&quot;, line 1566, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/tensor_conversion_registry.py&quot;, line 52, in _default_conversion_function
    return constant_op.constant(value, dtype, name=name)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 271, in constant
    return _constant_impl(value, dtype, shape, name, verify_shape=False,
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 283, in _constant_impl
    return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 308, in _constant_eager_impl
    t = convert_to_eager_tensor(value, ctx, dtype)
  File &quot;/home/barrachina/anaconda3/envs/tf-pip/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py&quot;, line 106, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.
</code></pre>
",5931672.0,,5931672.0,,2022-04-06 11:57:33,2022-04-07 04:00:40,Dataset gets re-copied to GPU (causing out of memory) when calling to eval twice,<tensorflow><gpu><out-of-memory>,2,0,0.0,,,CC BY-SA 4.0
70515618,1,,,2021-12-29 05:54:21,,4,93,"<p>I am building a model for classifying whether the person in the image is wearing a mask or not. I used EfficientNetB0 model with custom data augmentation (sequential) layer all stacked with the help of Functional layer. After I saved the model (in h5 format) and loaded the saved model, the accuracy on the test dataset was different.</p>
<p>The accuracy on the test dataset originally was around 98%
After loading the saved model, the accuracy on the test dataset plummeted to 91%</p>
<p>I used <code>image_dataset_from_directory</code> to load the images for train, test and validation sets.</p>
<pre><code>train_data = keras.preprocessing.image_dataset_from_directory(
    train_dir, image_size=(224, 224),
    color_mode=&quot;rgb&quot;, batch_size=64, label_mode=&quot;categorical&quot;
)
</code></pre>
<p>Then created a prefetch dataset</p>
<pre><code>train_dataset = train_data.shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.AUTOTUNE)
</code></pre>
<p>This is the custom data augmentation layer:</p>
<pre><code>data_aug = keras.Sequential([
    keras.layers.RandomRotation(0.3),
    keras.layers.RandomContrast(0.3)
])
</code></pre>
<p>The model</p>
<pre><code>inputs = keras.layers.Input(shape=(224, 224, 3))
x = data_aug(inputs)
x = eff_model(x, training=False)
x = keras.layers.GlobalAveragePooling2D()(x)
x = keras.layers.Dense(3, activation=&quot;softmax&quot;)(x)

model = keras.Model(inputs=inputs, outputs=x)

model.compile(loss=keras.losses.CategoricalCrossentropy(), metrics=[&quot;accuracy&quot;], 
              optimizer=keras.optimizers.Adam(0.001))

history = model.fit(train_dataset, steps_per_epoch=len(train_dataset), epochs=20,
                    validation_data=val_dataset, validation_steps=len(val_dataset))
</code></pre>
<p>This is the accuracy my model achieved originally on the test dataset.</p>
<p><a href=""https://i.stack.imgur.com/CE4PO.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/CE4PO.png"" alt=""enter image description here"" /></a></p>
<p>Saving the model and reloading the saved model:</p>
<pre><code>model.save(&quot;face_Detect.h5&quot;)
load_model = keras.models.load_model(&quot;/content/face_Detect.h5&quot;)
</code></pre>
<p>This is the accuracy achieved on the loaded model:</p>
<p><a href=""https://i.stack.imgur.com/MjzG4.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/MjzG4.png"" alt=""enter image description here"" /></a></p>
<p>I have no idea because the accuracy on the test dataset for the original model and loaded model should be similar but this is a huge difference. Is this a bug in Tensorflow 2.7.0 or am I making some horrendous mistake ?</p>
",13634261.0,,,,,2021-12-29 05:54:21,Why there is an accuracy drop on the test dataset (around 7% drop) after loading the saved model?,<python><tensorflow><computer-vision>,0,4,,,,CC BY-SA 4.0
63240027,1,63272142.0,,2020-08-04 03:52:38,,4,13594,"<p>I had tensorflow 2.0 workig with my RTX2070 gpu. I did a windows update so I could use tf-nightly. Did not like it so uninstalled it and reinstalled tensorflow 2.3.0. Ran previous python code that ran fine with GPU previously but it did not use the GPU. Tried lots of stuff. Finally just started over. Reinstalled Anaconda, created new environment. Uninstalled Cuda toolkit 10.1 and reinstalled it. Installed cuDnn SDK 7.6 in directory c:\Tools. Checked path env variable to include</p>
<pre><code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\bin;
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\extras\CUPTI\lib64;
C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.1\include;
C:\tools\cuda\bin;%PATH%
       #then ran this code:
import tensorflow as tf
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
print(tf.__version__)
print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU')))
tf.test.is_gpu_available()
     #I get the result
[name: &quot;/device:CPU:0&quot;
device_type: &quot;CPU&quot;
memory_limit: 268435456
locality {
}
incarnation: 15177607927005893519
, name: &quot;/device:XLA_CPU:0&quot;
device_type: &quot;XLA_CPU&quot;
memory_limit: 17179869184
locality {
}
incarnation: 4640072765546557805
physical_device_desc: &quot;device: XLA_CPU device&quot;
, name: &quot;/device:XLA_GPU:0&quot;
device_type: &quot;XLA_GPU&quot;
memory_limit: 17179869184
locality {
}
incarnation: 16675502319763286567
physical_device_desc: &quot;device: XLA_GPU device&quot;
]
2.3.0
Num GPUs Available:  0

False

tensorflow still does not use GPU. What an I missing? 

also same problem using python 3.7.0 and same problem using tensorflow 2.0.0


</code></pre>
",10798917.0,,10798917.0,,2020-08-04 21:16:08,2020-08-05 19:19:06,Tensorflow 2.3.0 CUDA Toolkit version 10.1 does not use GPU,<python><tensorflow>,1,6,,,,CC BY-SA 4.0
62734663,1,62734896.0,,2020-07-04 21:28:43,,4,931,"<p>I have 2-dimensional data with shape <code>m</code> by <code>n</code> that I want to window with size <code>w</code> along the first axis into a dataset of <code>m-w</code> many two-dimensional arrays each of size <code>w</code> by <code>n</code>. For instance if the data is:</p>
<pre><code>[[0,  1,  2 ], 
 [3,  4,  5 ], 
 [6,  7,  8 ],
 [9,  10, 11]]
</code></pre>
<p>then I want to window it into</p>
<pre><code>[[[0, 1 , 2 ], 
  [3, 4 , 5 ], 
  [6, 7 , 8 ]],
 [[3, 4 , 5 ],
  [6, 7 , 8 ],
  [9, 10, 11]]]
</code></pre>
<p>I can window the data together into the right sets:</p>
<pre><code>dataset = tf.data.Dataset.from_tensor_slices(np.arange(5*3).reshape(5,3))
dataset = dataset.window(size=3,shift=1,drop_remainder=True)
for window in dataset : print(list(window.as_numpy_iterator()))
&gt;&gt;&gt;[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8])]
&gt;&gt;&gt;[array([3, 4, 5]), array([6, 7, 8]), array([ 9, 10, 11])]
&gt;&gt;&gt;[array([6, 7, 8]), array([ 9, 10, 11]), array([12, 13, 14])]
</code></pre>
<p>but I can't figure out how to get the data back into the stacked shape again. I thought maybe tf.stack, but no dice on that. Does anybody know how to finish this?</p>
",5489889.0,,,,,2020-07-04 22:01:36,Window Multidimensional Tensorflow Dataset,<python><tensorflow><tensorflow2.0><tensorflow-datasets>,1,0,0.0,,,CC BY-SA 4.0
64203611,1,64204517.0,,2020-10-05 06:46:07,,4,6752,"<p>I am trying to find out, how exactly does BatchNormalization layer behave in TensorFlow. I came up with the following piece of code which to the best of my knowledge should be a perfectly valid keras model, however the mean and variance of BatchNormalization doesn't appear to be updated.</p>
<p>From docs <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization"" rel=""nofollow noreferrer"">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a></p>
<blockquote>
<p>in the case of the BatchNormalization layer, setting trainable = False on the layer means that the layer will be subsequently run in inference mode (meaning that it will use the moving mean and the moving variance to normalize the current batch, rather than using the mean and variance of the current batch).</p>
</blockquote>
<p>I expect the model to return a different value with each subsequent predict call.
What I see, however, are the exact same values returned 10 times.
Can anyone explain to me why does the BatchNormalization layer not update its internal values?</p>
<pre><code>import tensorflow as tf
import numpy as np

if __name__ == '__main__':

    np.random.seed(1)
    x = np.random.randn(3, 5) * 5 + 0.3

    bn = tf.keras.layers.BatchNormalization(trainable=False, epsilon=1e-9)
    z = input = tf.keras.layers.Input([5])
    z = bn(z)

    model = tf.keras.Model(inputs=input, outputs=z)

    for i in range(10):
        print(x)
        print(model.predict(x))
        print()
</code></pre>
<p>I use <strong>TensorFlow 2.1.0</strong></p>
",3399825.0,,,,,2020-10-05 07:53:58,tf.keras.layers.BatchNormalization with trainable=False appears to not update its internal moving mean and variance,<tensorflow><tensorflow2.0><batch-normalization>,1,0,0.0,,,CC BY-SA 4.0
66855559,1,,,2021-03-29 13:57:00,,4,1044,"<p>When you want to train a neural network, you need to set a batch size. The higher the batch size, the higher the GPU memory consumption. When you lack GPU memory, tensorflow will raise this kind of message :</p>
<pre><code>2021-03-29 15:45:04.185417: E tensorflow/stream_executor/cuda/cuda_driver.cc:825] failed to alloc 8589934592 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-03-29 15:45:04.229570: E tensorflow/stream_executor/cuda/cuda_driver.cc:825] failed to alloc 7730940928 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-03-29 15:45:10.776120: E tensorflow/stream_executor/cuda/cuda_driver.cc:825] failed to alloc 17179869184 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory
...
</code></pre>
<p>The solution is to decrease the batch size. I would like to be able to catch this exception when I get this message, so I can send a message to the view, or even automatically decrease the batch size in order to automate the learning behaviour.
In my case, the out of memory come from the loading of the dataset :</p>
<pre><code>try:
  features, labels = iter(input_dataset).next()
except:
  print(&quot;this is my exception&quot;) 
  raise
</code></pre>
<p>but, the cuda error oom seems not to be catchable like this. Actually, I think that the error is already caught in the <strong>next</strong> function of tf.Dataset class. what i see seems to actually be the log generate by the catch of the oom error. I don't know how to detect this log in order to react to the oom event.</p>
",8150031.0,,8150031.0,,2021-03-30 13:17:24,2021-03-31 09:26:35,Catch CUDA_ERROR_OUT_OF_MEMORY from a tensorflow script,<tensorflow><deep-learning><neural-network><tensorflow2.0><object-detection>,1,3,,,,CC BY-SA 4.0
66854994,1,,,2021-03-29 13:21:21,,4,642,"<p>I tried to implement the Spearman's rank correlation coefficient (<a href=""https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient"" rel=""nofollow noreferrer"">wiki</a>) as a custom objective function for xgboost. I'm using the fast-soft-sort (<a href=""https://github.com/google-research/fast-soft-sort"" rel=""nofollow noreferrer"">github</a>) package from google for the differentiable ranking and tensorflow to automatically calculate the gradients. You can find the code below:</p>
<pre><code>from fast_soft_sort.tf_ops import soft_rank
import tensorflow as tf
import numpy as np

def pearson_corr(x, y):
    
    xy_t = tf.concat([x, y], axis=0)
    mean_t = tf.reduce_mean(xy_t, axis=1, keepdims=True)
    cov_t = ((xy_t-mean_t) @ tf.transpose(xy_t-mean_t))/(x.shape[1]-1)
    cov2_t = tf.linalg.diag(1/tf.sqrt(tf.linalg.diag_part(cov_t)))
    corr_matrix = cov2_t @ cov_t @ cov2_t
    corr = tf.reduce_mean(corr_matrix) * 2 - 1 # equivalent to taking element [0][1] assuming the 2x2 corr matrix is symmetric and the diagonals are 1
    
    return corr

def spearman_corr(x, y):
    
    ranks = soft_rank(x, regularization_strength=0.1)
    corr = pearson_corr(ranks, y)
    
    return corr

def get_value_grad_and_hess(x, y, f):
    
    x_var = tf.Variable(x, dtype=tf.float32)
    y_var = tf.Variable(y, dtype=tf.float32)
        
    val, grad, hess = None, None, None

    with tf.GradientTape() as t2:
    
        with tf.GradientTape() as t1:
            
            val = f(x_var, y_var)
        
        grad = t1.gradient(val, x_var)    

    hess = t2.jacobian(grad, x_var)

    return val, grad, hess

# test with random input
x = np.random.rand(1, 10) # predictions
y = np.random.rand(1, 10) # labels

print('pearson:')
val, grad, hess = get_value_grad_and_hess(x, y, pearson_corr)
print(' value:',  val)
print(' gradient:', grad)
print(' hessian:', hess)

print('spearman:')
val, grad, hess = get_value_grad_and_hess(x, y, spearman_corr)
print(' value:',  val)
print(' gradient:', grad)
print(' hessian:', hess)
</code></pre>
<p>Example output:</p>
<pre><code>pearson:
 value: tf.Tensor(-0.3348779, shape=(), dtype=float32)
 gradient: tf.Tensor(
[[ 0.21893269  0.16921082  0.19409613 -0.00321923  0.07347419  0.29004234
  -0.07947832 -0.7088071   0.29586902 -0.4501205 ]], shape=(1, 10), dtype=float32)
 hessian: tf.Tensor(
[[[[ 0.04441248 -0.03097764  0.02028688 -0.20294864 -0.22516166
    -0.09771542 -0.06334648  0.42131865 -0.02681065  0.16094248]]

  [[-0.03097765  0.40132353  0.04399774 -0.07797898 -0.05632872
     0.04975905 -0.07172927 -0.17790946  0.06856277 -0.14871901]]

  [[ 0.02028689  0.04399772  0.44207606 -0.06522453 -0.03210837
     0.0911998  -0.07974204 -0.30411014  0.10508882 -0.22146425]]

  [[-0.20294863 -0.077979   -0.06522458  0.27985442 -0.12591925
    -0.13325104 -0.02723934  0.31153008 -0.10839472  0.14957213]]

  [[-0.22516167 -0.05632871 -0.03210838 -0.12591931  0.23029271
    -0.10794277 -0.04108595  0.30121914 -0.07069567  0.12773061]]

  [[-0.09771542  0.04975905  0.0911998  -0.13325103 -0.10794276
     0.4497667  -0.09163402 -0.12746409  0.11477053 -0.14748882]]

  [[-0.06334649 -0.07172926 -0.07974204 -0.02723937 -0.04108596
    -0.09163402  0.35762674  0.07487351 -0.09705587  0.03933275]]

  [[ 0.4213187  -0.17790946 -0.3041101   0.31153005  0.3012191
    -0.12746407  0.07487351 -0.09769349 -0.2807703  -0.12099396]]

  [[-0.02681071  0.06856281  0.1050889  -0.10839473 -0.07069571
     0.11477058 -0.0970559  -0.28077024  0.5259669  -0.23066193]]

  [[ 0.1609425  -0.14871901 -0.22146428  0.1495721   0.12773061
    -0.14748883  0.03933276 -0.12099396 -0.23066193  0.39175004]]]], shape=(1, 10, 1, 10), dtype=float32)

spearman:
 value: tf.Tensor(-0.3408205, shape=(), dtype=float32)
 gradient: tf.Tensor(
[[ 0.13679196  0.13627169  0.15643153 -0.10963751 -0.02715444  0.2698098
   0.20591483 -0.8303905   0.26787752 -0.20591483]], shape=(1, 10), dtype=float32)
 hessian: None
</code></pre>
<p>As you can see the code above yields both gradient and hessian for the pearson correlation function but for the Spearman correlation the hessian is None.</p>
<p>Does someone have an idea why the hessian is None for the Spearman correlation?</p>
",13672947.0,,13672947.0,,2021-03-29 13:39:28,2021-03-29 13:39:28,Hessians for Spearman Rank Correlation,<python><tensorflow><gradient><hessian>,0,0,,,,CC BY-SA 4.0
64203053,1,,,2020-10-05 05:49:49,,4,891,"<p>I am a newbie to the <code>anaconda</code> environment. A couple of days back I installed <code>keras</code> and <code>tensorflow</code> through <code>anaconda</code> prompt. <code>tensorflow</code> installed successfully but <code>keras</code> didn't. After, whenever I open the terminal the following error is showing. Please help me out.</p>
<p>my installation commands are:</p>
<pre><code>conda create -n tf tensorflow

conda install -c conda-forge keras
</code></pre>
<pre><code>C:\Users\venkatesh&gt;SET DISTUTILS_USE_SDK=1

C:\Users\venkatesh&gt;SET MSSdk=1

C:\Users\venkatesh&gt;SET &quot;VS_VERSION=15.0&quot;

C:\Users\venkatesh&gt;SET &quot;VS_MAJOR=15&quot;

C:\Users\venkatesh&gt;SET &quot;VS_YEAR=2017&quot;

C:\Users\venkatesh&gt;set &quot;MSYS2_ARG_CONV_EXCL=/AI;/AL;/OUT;/out&quot;

C:\Users\venkatesh&gt;set &quot;MSYS2_ENV_CONV_EXCL=CL&quot;

C:\Users\venkatesh&gt;set &quot;PY_VCRUNTIME_REDIST=\bin\vcruntime140.dll&quot;

C:\Users\venkatesh&gt;set &quot;CXX=cl.exe&quot;

C:\Users\venkatesh&gt;set &quot;CC=cl.exe&quot;

C:\Users\venkatesh&gt;set &quot;VSINSTALLDIR=&quot;

C:\Users\venkatesh&gt;for /F &quot;usebackq tokens=*&quot; %i in (`vswhere.exe -nologo -products * -version [15.0,16.0) -property installationPath`) do (set &quot;VSINSTALLDIR=%i\&quot; )

C:\Users\venkatesh&gt;if not exist &quot;&quot; (for /F &quot;usebackq tokens=*&quot; %i in (`vswhere.exe -nologo -products * -requires Microsoft.VisualStudio.Component.VC.v141.x86.x64 -property installationPath`) do (set &quot;VSINSTALLDIR=%i\&quot; ) )

C:\Users\venkatesh&gt;if not exist &quot;&quot; (set &quot;VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\&quot; )

C:\Users\venkatesh&gt;if not exist &quot;C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\&quot; (set &quot;VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\&quot; )

C:\Users\venkatesh&gt;if not exist &quot;C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\&quot; (set &quot;VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\&quot; )

C:\Users\venkatesh&gt;if not exist &quot;C:\Program Files (x86)\Microsoft Visual Studio\2017\BuildTools\&quot; (set &quot;VSINSTALLDIR=C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\&quot; )

C:\Users\venkatesh&gt;IF NOT &quot;&quot; == &quot;&quot; (
set &quot;INCLUDE=;&quot;
 set &quot;LIB=;&quot;
 set &quot;CMAKE_PREFIX_PATH=;&quot;
)

C:\Users\venkatesh&gt;call :GetWin10SdkDir

C:\Users\venkatesh&gt;call :GetWin10SdkDirHelper HKLM\SOFTWARE\Wow6432Node  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE\Wow6432Node  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 call :GetWin10SdkDirHelper HKLM\SOFTWARE  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 exit /B 1

C:\Users\venkatesh&gt;exit /B 0

C:\Users\venkatesh&gt;for /F %i in ('dir /ON /B &quot;\include\10.*&quot;') DO (SET WindowsSDKVer=%~i )
The system cannot find the file specified.

C:\Users\venkatesh&gt;if errorlevel 1 (echo &quot;Didn't find any windows 10 SDK. I'm not sure if things will work, but let's try...&quot; )  else (echo Windows SDK version found as: &quot;&quot; )
Windows SDK version found as: &quot;&quot;

C:\Users\venkatesh&gt;IF &quot;win-64&quot; == &quot;win-64&quot; (
set &quot;CMAKE_GEN=Visual Studio 15 2017 Win64&quot;
 set &quot;BITS=64&quot;
)  else (
set &quot;CMAKE_GEN=Visual Studio 15 2017&quot;
 set &quot;BITS=32&quot;
)

C:\Users\venkatesh&gt;pushd C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\
The system cannot find the path specified.

C:\Users\venkatesh&gt;CALL &quot;VC\Auxiliary\Build\vcvars64.bat&quot; -vcvars_ver=14.16
The system cannot find the path specified.

C:\Users\venkatesh&gt;popd

C:\Users\venkatesh&gt;IF &quot;&quot; == &quot;&quot; SET &quot;CMAKE_GENERATOR=Visual Studio 15 2017 Win64&quot;

C:\Users\venkatesh&gt;call :GetWin10SdkDirHelper HKLM\SOFTWARE\Wow6432Node  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE\Wow6432Node  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 call :GetWin10SdkDirHelper HKLM\SOFTWARE  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 call :GetWin10SdkDirHelper HKCU\SOFTWARE  1&gt;nul 2&gt;&amp;1

C:\Users\venkatesh&gt;if errorlevel 1 exit /B 1

C:\Users\venkatesh&gt;exit /B 0
</code></pre>
",14010737.0,,12479639.0,,2020-10-05 18:02:24,2022-04-03 13:18:08,Visual Studio and Anaconda Terminal Error,<windows><visual-studio><tensorflow><anaconda>,2,4,,,,CC BY-SA 4.0
72177210,1,,,2022-05-09 19:19:12,,4,4368,"<p>I'm trying to create object detection using tensorflow as an assignment.</p>
<p><code> </code> <code> ImportError: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental' (c:\Project\.venv\lib\site-packages\tensorflow\_api\v2\compat\v2\experimental\__init__.py)</code> <code> </code>
I would appreciate any help!</p>
",8400167.0,,,,,2022-06-17 13:56:56,ImportError: cannot import name 'dtensor' from 'tensorflow.compat.v2.experimental',<python><tensorflow>,2,2,,,,CC BY-SA 4.0
62757103,1,62758562.0,,2020-07-06 13:25:42,,4,2824,"<p>I stored a model using <code>model.save('model')</code> after this tutorial:</p>
<p><a href=""https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e"" rel=""nofollow noreferrer"">https://towardsdatascience.com/keras-transfer-learning-for-beginners-6c9b8b7143e</a></p>
<p>The labels are taken from the directory itself.
Now I would like to load it and do a prediction on an image using the following code:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.preprocessing import image

new_model = keras.models.load_model('./model/')

# Check its architecture
new_model.summary()

with image.load_img('testpics/mypic.jpg') as img: # , target_size=(32,32)) 
    img  = image.img_to_array(img)
    img  = img.reshape((1,) + img.shape)
    # img  = img/255
    # img = img.reshape(-1,784)
    img_class=new_model.predict(img) 
    prediction = img_class[0]
    classname = img_class[0]
    print(&quot;Class: &quot;,classname)
</code></pre>
<p>Sadly the output is just</p>
<blockquote>
<p>Class:  [1.3706615e-03 2.9885881e-03 1.6783881e-03 3.0293325e-03 2.9168031e-03
7.2344812e-04 2.0196944e-06 2.0119224e-02 2.2996603e-04 1.1960276e-05
3.0794670e-04 6.0808496e-05 1.4892215e-05 1.5410941e-02 1.2452166e-04
8.2580920e-09 2.4049083e-02 3.1140331e-05 7.4609083e-01 1.5793210e-01
2.4283256e-03 1.5755130e-04 2.4227127e-03 2.2325735e-07 7.2101393e-06
7.6298704e-03 2.0922457e-04 1.2269774e-03 5.5882465e-06 2.4516811e-04
8.5745640e-03]</p>
</blockquote>
<p>And I cannot figure out how to reload the labels... could someone help me out here :/?</p>
<p><a href=""https://i.stack.imgur.com/mx14e.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/mx14e.png"" alt=""Here is a screenshot of my saved files"" /></a></p>
",7610751.0,,,,,2020-07-06 14:46:12,Load Tensorflow model with labels,<python><tensorflow>,2,2,,,,CC BY-SA 4.0
65722484,1,65725261.0,,2021-01-14 16:03:45,,4,2244,"<p>I am tuning the hyperparameters using ray tune. The model is built in the tensorflow library, it occupies a large part of the available GPU memory. I noticed that every second call reports an out of memory error.It looks like the memory is being freed, you can see in the GPU memory usage graph, this is the moment between calls of consecutive trials, between which the OOM error occurred. I add that on smaller models I do not encounter this error and the graph looks the same.</p>
<p>How to deal with this out of memory error in every second trial ?</p>
<p><img src=""https://i.stack.imgur.com/Z6nEB.png"" alt=""Memory usage graph"" /></p>
",15006624.0,,480982.0,,2021-01-14 16:10:20,2022-06-07 00:45:27,Out of memory at every second trial using Ray Tune,<tensorflow><ray><ray-tune>,1,0,,,,CC BY-SA 4.0
65334652,1,65358899.0,,2020-12-17 04:15:19,,4,566,"<p><strong>Background</strong></p>
<p>My question is based off an example from <em>Hands-On Machine Learning by Geron, Chapter 12: Custom Models</em>.</p>
<p>The purpose of this example is to create a custom neural network model. The model has 5 <code>Dense</code> hidden layers. The custom part is that we add a <code>reconstruction</code> layer before the output. The purpose of the reconstruction layer is to reconstruct the inputs. Then we take the difference <code>reconstruction-inputs</code>, get the MSE, and apply this value to the loss function. It's supposed to be a regularization step.</p>
<p><strong>Minimum (should be) Working Example</strong></p>
<p>The following code is almost directly from the textbook, but it doesn't work.</p>
<pre><code>import numpy as np

num_training=10;
num_dim=2;

X = np.random.random((10,2))
y = np.random.random(10)

import tensorflow as tf
import tensorflow.keras as keras

class ReconstructingRegressor(keras.models.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden = [keras.layers.Dense(30, activation=&quot;selu&quot;,
                                          kernel_initializer=&quot;lecun_normal&quot;)
                       for _ in range(5)]
        self.out = keras.layers.Dense(output_dim)

    def build(self, batch_input_shape):
        n_inputs = batch_input_shape[-1]
        self.reconstruct = keras.layers.Dense(n_inputs)
        super().build(batch_input_shape)

    def call(self, inputs, training=None):
        Z = inputs
        for layer in self.hidden:
            Z = layer(Z)
        reconstruction = self.reconstruct(Z)
        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        self.add_loss(0.05 * recon_loss)
        return self.out(Z)
    
model = ReconstructingRegressor(1)
model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;)
history = model.fit(X, y, epochs=2)
</code></pre>
<p><strong>Error Message</strong></p>
<p>However, I get the following error while calling <code>model.fit()</code>:</p>
<pre><code>---------------------------------------------------------------------------
InaccessibleTensorError                   Traceback (most recent call last)
&lt;ipython-input-10-b7211d3022fa&gt; in &lt;module&gt;
     34 model = ReconstructingRegressor(1)
     35 model.compile(loss=&quot;mse&quot;, optimizer=&quot;nadam&quot;)
---&gt; 36 history = model.fit(X, y, epochs=2)
</code></pre>
<p>and, at the end of the error message:</p>
<blockquote>
<p>InaccessibleTensorError: The tensor 'Tensor(&quot;mul:0&quot;, shape=(), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=build_graph, id=140602287140624); accessed from: FuncGraph(name=train_function, id=140602287108640).</p>
</blockquote>
<p><strong>Troubleshooting</strong></p>
<p>If I comment out the code that computes the loss, i.e.,</p>
<pre><code>        #recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        #self.add_loss(0.05 * recon_loss)
</code></pre>
<p>in <code>call</code>, but I keep everything else the same, then I get the following warning</p>
<blockquote>
<p>WARNING:tensorflow:Gradients do not exist for variables ['dense/kernel:0', 'dense/bias:0'] when minimizing the loss.</p>
</blockquote>
<p>Not sure if that's relevant.</p>
",2799466.0,,,,,2020-12-18 15:56:35,"Error subclassing model in Tensorflow 2 ""InaccessibleTensorError""",<tensorflow><neural-network><tensorflow2.0>,1,2,0.0,,,CC BY-SA 4.0
72832509,1,72833226.0,,2022-07-01 16:58:22,,4,132,"<p>Is there a TensorFlow equivalent for <code>cv2.addWeighted()</code>? I need this function for image processing on my <code>tf.dataset</code> object.</p>
<p>If there isn't, how can I use the OpenCV method with TensorFlow to get the same result?</p>
<p>Here is my code below for reference.</p>
<pre><code>import tensorflow as tf
from tensorflow.image import ResizeMethod
import pathlib
import os
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import cv2
from PIL import Image

np.set_printoptions(precision=4)
filepaths = df['filepath'].values
labels = df[[0,2,4]].values


ds_train = tf.data.Dataset.from_tensor_slices((filepaths, labels))

def read_image(image_file, labels):
    img = tf.io.read_file(image_file)
    img = tf.image.decode_png(img, channels=3, dtype=tf.uint8)

    
    img = tf.image.resize(img, [512, 512], preserve_aspect_ratio=False)
    
    img2 = ...
    
    # Need something equivalent to cv2.addWeighted() to use on img and img2
  
    return img, labels

ds_train = ds_train.map(read_image).batch(32, drop_remainder = True)
</code></pre>
<p>I apologize in advance if this question has already been answered. I searched everywhere and could not find a way to replicate this.</p>
",16514772.0,,7328782.0,,2022-07-01 18:51:24,2022-07-01 18:51:24,Tensorflow equivalent for cv2.addWeighted?,<python><tensorflow><opencv><image-processing><computer-vision>,1,1,,,,CC BY-SA 4.0
71884959,1,,,2022-04-15 13:59:26,,4,5675,"<p>I have encounter &quot;I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8400
Could not load library cudnn_cnn_infer64_8.dll. Error code 193&quot;</p>
<p>will working with TensorFlow.
version:
TensorFlow 2.8
CUDA 11.6
CUDNN 8.4</p>
",11972083.0,,,,,2023-02-13 15:36:16,"error - ""I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8400 Could not load library cudnn_cnn_infer64_8.dll. Error code 193""",<python><tensorflow><deep-learning><facenet>,2,2,,,,CC BY-SA 4.0
68255860,1,,,2021-07-05 12:05:17,,4,779,"<p>I'm building a react native app which uses a custom trained model to identify objects.</p>
<p>I have done all the setup, camera is working, we exported our Model for Tensorflow (json + bin files) and hosted them on our Webserver to load them inside the app via &quot;tf.loadGraphModel&quot; (so far so good)</p>
<p>Calling a prediction, doing some transformations, gives me the array with the chances of what the model thinks is most accurat. Is it possible to show boundary boxes for the prediction using tensor flow for react native with the expo camera attached (real time at having the video feed open) and if so how.</p>
<p>Also, is it possible to get the prediction class any way easier, or is this the way to go ?</p>
<p>My Screen currently looks like this</p>
<pre class=""lang-js prettyprint-override""><code>import { Camera } from 'expo-camera';
const TensorCamera = cameraWithTensors(Camera);

function DetectionScreen() {
  const navigation = useNavigation();

  const [tfReady, setTfReady] = useState(false);
  const [focus, setFocus] = useState(false);
  const [modelReady, setModelReady] = useState(false);
  const [hasError, setHasError] = useState(false);
  const [detectionModel, setDetectionModal] = useState(null);

  let textureDims;
  if (Platform.OS === 'ios') {
    textureDims = {
      height: 1920,
      width: 1080,
    };
  } else {
    textureDims = {
      height: 1200,
      width: 1600,
    };
  }

  useFocusEffect(
    useCallback(() =&gt; {
      const loadTensor = async () =&gt; {
        await tf.ready();
        setTfReady(true);

        let model = null;
        let hasModel = false;

        try {
          model = await tf.loadGraphModel(
            asyncStorageIO('react-native-tensor-flow-model')
          );
          hasModel = true;
          setDetectionModal(model);
          setModelReady(true);

          console.log('Model loaded from storage');
        } catch (e) {
          console.log('Error loading model from storage');
          console.log(e);
        }

        if (!hasModel) {
          try {
            model = await tf.loadGraphModel(
              'https://model-web-server-domain.com/model.json'
            );

            // Save the model to async storage
            await model.save(asyncStorageIO('react-native-tensor-flow-model'));
            setDetectionModal(model);
            setModelReady(true);
          } catch (e) {
            console.log(e);
            setHasError(true);
          }
        }
      };

      setFocus(true);
      loadTensor();

      return () =&gt; {
        setFocus(false);
      };
    }, [])
  );

 const handleCameraStream = (images, updatePreview, gl) =&gt; {
    const loop = async () =&gt; {
      const nextImageTensor = images.next().value;

      if (detectionModel &amp;&amp; nextImageTensor) {
        try {
          // needs to be expanded to match the models dims or an error occurs
          const tensor4d = nextImageTensor.expandDims(0);
          // needs a cast or an error occurs
          const float32Tensor = tensor4d.cast('float32');
          // const prediction = await detectionModel.predict(nextImageTensor);
          const prediction = await detectionModel.executeAsync(float32Tensor);

          if (prediction &amp;&amp; prediction.length &gt; 0) {
            const classes = prediction[0].argMax(-1).print();
            console.log('=== PREDICTION ===');
            console.log(classes);
          }
        } catch (e) {
          console.log('ERROR PREDICTING FROM MODEL');
          console.log(e);
        }
      }

      requestAnimationFrame(loop);
    };
    loop();
  };

  return (
      &lt;View style={styles.container}&gt;
        {tfReady &amp;&amp; focus &amp;&amp; modelReady &amp;&amp; (
          &lt;&gt;
            &lt;TensorCamera
              // Standard Camera props
              style={styles.camera}
              type={Camera.Constants.Type.back}
              // Tensor related props
              cameraTextureHeight={textureDims.height}
              cameraTextureWidth={textureDims.width}
              resizeHeight={200}
              resizeWidth={150}
              resizeDepth={3}
              onReady={handleCameraStream}
              autorender={true}
            /&gt;
          &lt;/&gt;
        )}
        {!tfReady &amp;&amp; &lt;Text&gt;Loading ...&lt;/Text&gt;}
      &lt;/View&gt;
  );
}
</code></pre>
<p>*** Update ***</p>
<p>Some more Informations about the Model and the Output of the Prediction</p>
<p>moel.json</p>
<pre class=""lang-js prettyprint-override""><code>{
  &quot;format&quot;: &quot;graph-model&quot;,
  &quot;generatedBy&quot;: &quot;2.4.0&quot;,
  &quot;convertedBy&quot;: &quot;TensorFlow.js Converter v1.7.0&quot;,
  &quot;userDefinedMetadata&quot;: {
    &quot;signature&quot;: {
      &quot;inputs&quot;: {
        &quot;ToFloat:0&quot;: {
          &quot;name&quot;: &quot;ToFloat:0&quot;,
          &quot;dtype&quot;: &quot;DT_FLOAT&quot;,
          &quot;tensorShape&quot;: {
            &quot;dim&quot;: [
              { &quot;size&quot;: &quot;-1&quot; },
              { &quot;size&quot;: &quot;-1&quot; },
              { &quot;size&quot;: &quot;-1&quot; },
              { &quot;size&quot;: &quot;3&quot; }
            ]
          }
        }
      },
      &quot;outputs&quot;: {
        &quot;Postprocessor/convert_scores:0&quot;: {
          &quot;name&quot;: &quot;Postprocessor/convert_scores:0&quot;,
          &quot;dtype&quot;: &quot;DT_FLOAT&quot;,
          &quot;tensorShape&quot;: {
            &quot;dim&quot;: [{ &quot;size&quot;: &quot;-1&quot; }, { &quot;size&quot;: &quot;-1&quot; }, { &quot;size&quot;: &quot;11&quot; }]
          }
        },
        &quot;Postprocessor/Decode/transpose_1:0&quot;: {
          &quot;name&quot;: &quot;Postprocessor/Decode/transpose_1:0&quot;,
          &quot;dtype&quot;: &quot;DT_FLOAT&quot;,
          &quot;tensorShape&quot;: { &quot;dim&quot;: [{ &quot;size&quot;: &quot;-1&quot; }, { &quot;size&quot;: &quot;4&quot; }] }
        }
      }
    }
  },
  &quot;modelTopology&quot;: {/*...*/},
  &quot;weightsManifest&quot;: [/*...*/],
}
</code></pre>
<p>Output of prediction (Data Tensor)</p>
<pre><code>{&quot;dataId&quot;: {&quot;id&quot;: 22594}, &quot;dtype&quot;: &quot;int32&quot;, &quot;id&quot;: 17510, &quot;isDisposedInternal&quot;: false, &quot;kept&quot;: false, &quot;rankType&quot;: &quot;3&quot;, &quot;shape&quot;: [200, 150, 3], &quot;size&quot;: 90000, &quot;strides&quot;: [450, 3]}

Tensor
    [[[0.0002148, 0.0004637, 0.0005074, 0.0002892, 0.0006514, 0.0002825, 0.000659 , 0.0004711, 0.0006962, 0.0002513, 0.0007014],
      [0.0002115, 0.000315 , 0.0005155, 0.0002003, 0.0006719, 0.0003006, 0.000607 , 0.00035  , 0.000555 , 0.0003226, 0.0011692],
      [0.0002034, 0.0005054, 0.0007887, 0.0003393, 0.0008593, 0.0003684, 0.0009112, 0.0006189, 0.0007553, 0.0006771, 0.0009623],
      ...,
      [0.0031853, 0.0024052, 0.0078735, 0.0032234, 0.0032864, 0.0030518, 0.007637 , 0.0053635, 0.0085449, 0.0039902, 0.0059357],
      [0.0031471, 0.0018387, 0.0050392, 0.0019646, 0.0024433, 0.0026016, 0.0039139, 0.0029011, 0.0051994, 0.0027256, 0.0041809],
      [0.0032482, 0.0017414, 0.0041161, 0.0016489, 0.0021324, 0.001853 , 0.0030632, 0.0022793, 0.0032864, 0.0045204, 0.007637 ]]]
</code></pre>
",2630265.0,,2630265.0,,2021-07-05 13:49:45,2021-07-05 13:49:45,How to show Object Detection Boundary Boxes with Tensorflow and React Native,<react-native><tensorflow><tensorflow.js>,0,12,,,,CC BY-SA 4.0
70356867,1,70367407.0,,2021-12-14 23:44:43,,4,3887,"<p>when i add tensoflow with poetry (poetry add tensorflow) i get this error : <br></p>
<pre><code>Using version ^2.7.0 for tensorflow

Updating dependencies
Resolving dependencies... (0.8s)

  SolverProblemError

  The current project's Python requirement (&gt;=3.6,&lt;4.0) is not compatible with some of the required packages Python requirement:
    - tensorflow-io-gcs-filesystem requires Python &gt;=3.6, &lt;3.10, so it will not be satisfied for Python &gt;=3.10,&lt;4.0
    - tensorflow-io-gcs-filesystem requires Python &gt;=3.6, &lt;3.10, so it will not be satisfied for Python &gt;=3.10,&lt;4.0
    - tensorflow-io-gcs-filesystem requires Python &gt;=3.6, &lt;3.10, so it will not be satisfied for Python &gt;=3.10,&lt;4.0

....

    For tensorflow-io-gcs-filesystem, a possible solution would be to set the `python` property to &quot;&gt;=3.6,&lt;3.10&quot;
    For tensorflow-io-gcs-filesystem, a possible solution would be to set the `python` property to &quot;&gt;=3.6,&lt;3.10&quot;
    For tensorflow-io-gcs-filesystem, a possible solution would be to set the `python` property to &quot;&gt;=3.6,&lt;3.10&quot;

</code></pre>
",11514393.0,,3585557.0,,2022-05-26 15:06:52,2023-02-22 09:13:19,SolverProblemError on install Tensorfow with poetry,<python><tensorflow><python-poetry>,3,0,,,,CC BY-SA 4.0
68237724,1,68241328.0,,2021-07-03 15:43:21,,4,18504,"<p>Whenever I import tensorflow in my code, I get the following warning</p>
<pre><code>2021-07-03 20:43:38.432690: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
</code></pre>
<p>To check gpu availability I ran</p>
<pre><code>tf.config.list_physical_devices('GPU')
</code></pre>
<p>For which it returned <code>[]</code></p>
<p>I also ran</p>
<pre><code>tf.test.is_gpu_available()
</code></pre>
<p>Which returned some noticeable things</p>
<pre><code>2021-07-03 20:49:02.552671: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA

2021-07-03 20:49:02.584748: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2394575000 Hz

2021-07-03 20:49:02.585433: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f32e8000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:

2021-07-03 20:49:02.585504: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version

2021-07-03 20:49:02.590168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1

2021-07-03 20:49:02.737950: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected

2021-07-03 20:49:02.737996: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (user_name-HP-Pavilion-15-Notebook-PC): /proc/driver/nvidia/version does not exist
False
</code></pre>
<p>Following are my setup specifications</p>
<pre><code>Python 3.8.5
TensorFlow 2.2.0
nvcc 10.1
os Ubuntu 20.04 LTS
GPU GM108M [GeForce 830M]
</code></pre>
<p>What should I do to resolve this? I want tensorflow lib. to utilise the local GPU while running the code.</p>
<p>EDIT: libcudart,libcuda are installed.</p>
",14337775.0,,14337775.0,,2021-07-03 16:07:40,2021-07-04 02:54:57,failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected,<python><tensorflow><deep-learning>,1,4,0.0,,,CC BY-SA 4.0
64200574,1,,,2020-10-04 22:43:07,,4,1320,"<p>I am using a GTX-1660Ti on training a Convnet. I was able to use my GPU to train 3 weeks ago. Today, I tried training my model and ran in to the following error.</p>
<pre><code>RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable
</code></pre>
<p>When I run,</p>
<pre><code>tf.config.list_physical_devices(device_type='GPU')
print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU')))
</code></pre>
<p>I get</p>
<pre><code>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Num GPUs Available:  1
</code></pre>
<p>When I run</p>
<pre><code>import tensorflow as tf
if tf.test.gpu_device_name():
    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))
else:
    print(&quot;Please install GPU version of TF&quot;)
</code></pre>
<p>I also ge the same RunTimeError.</p>
<p>I'm not sure why I'm getting this issue when it was working fine couple weeks ago... Anyone have similar issue? I have tensorflow-gpu installed and my tensorflow.<strong>version</strong> is '2.2.0. I'm also using Ubuntu 20.04 on WSL</p>
",10642892.0,,10642892.0,,2020-10-04 22:48:14,2020-10-04 22:48:14,"Tensorflow-GPU Error: ""RuntimeError: CUDA runtime implicit initialization on GPU:0 failed. Status: all CUDA-capable devices are busy or unavailable""",<python><tensorflow><gpu>,0,3,0.0,,,CC BY-SA 4.0
62962183,1,62974026.0,,2020-07-17 21:47:19,,4,2365,"<p>I am switching from running TPUs in colab to running TPUs in Google cloud. I am used to running training in the colab jupyter notebook, but from the GCP TPU quickstart guide, I'll need to use the shell script, and convert my code into a script.</p>
<p><a href=""https://cloud.google.com/tpu/docs/quickstart"" rel=""nofollow noreferrer"">https://cloud.google.com/tpu/docs/quickstart</a></p>
<p>Is there way to open a Jupyter notebook version of my GCP VM?</p>
",3259896.0,,12919986.0,,2020-07-22 04:32:04,2023-05-06 04:54:21,Can you use a Jupyter notebook on my GCP VM to run TPU training in Google Cloud?,<tensorflow><google-cloud-platform><jupyter-notebook><google-cloud-storage><google-cloud-tpu>,3,0,0.0,,,CC BY-SA 4.0
73317676,1,,,2022-08-11 08:35:17,,4,8375,"<p>I install the kneed package in linux aarch64 architecture in <strong>miniconda3</strong>.
When I import kneed inside python,
I got the following error</p>
<pre><code> import kneed
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/kneed/__init__.py&quot;, line 4, in &lt;module&gt;
    from .knee_locator import KneeLocator
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/kneed/knee_locator.py&quot;, line 3, in &lt;module&gt;
    from scipy.signal import argrelextrema
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/signal/__init__.py&quot;, line 309, in &lt;module&gt;
    from . import _sigtools, windows
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/signal/windows/__init__.py&quot;, line 41, in &lt;module&gt;
    from ._windows import *
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/signal/windows/_windows.py&quot;, line 7, in &lt;module&gt;
    from scipy import linalg, special, fft as sp_fft
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/__init__.py&quot;, line 211, in __getattr__
    return _importlib.import_module(f'scipy.{name}')
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/importlib/__init__.py&quot;, line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/fft/__init__.py&quot;, line 92, in &lt;module&gt;
    from ._helper import next_fast_len
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/fft/_helper.py&quot;, line 3, in &lt;module&gt;
    from ._pocketfft import helper as _helper
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/fft/_pocketfft/__init__.py&quot;, line 3, in &lt;module&gt;
    from .basic import *
  File &quot;/home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/fft/_pocketfft/basic.py&quot;, line 6, in &lt;module&gt;
    from . import pypocketfft as pfft
ImportError: /usr/lib/aarch64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.30' not found (required by /home/su/miniconda3/envs/myenv/lib/python3.10/site-packages/scipy/fft/_pocketfft/pypocketfft.cpython-310-aarch64-linux-gnu.so)
</code></pre>
<p>When I check libstdc++.so.6,</p>
<pre><code>ls /usr/lib/aarch64-linux-gnu | grep libstdc++.so.6
libstdc++.so.6
libstdc++.so.6.0.28
</code></pre>
<p>May I know do I know to install any package to solve the <strong>ImportError</strong>?</p>
<p>The below are packages that I install</p>
<pre><code>Package                      Version
---------------------------- ------------
absl-py                      1.2.0
appdirs                      1.4.4
astunparse                   1.6.3
attrs                        22.1.0
audioread                    2.1.9
cachetools                   5.2.0
certifi                      2022.6.15
cffi                         1.15.1
charset-normalizer           2.1.0
cycler                       0.11.0
decorator                    5.1.1
distlib                      0.3.5
docopt                       0.6.2
filelock                     3.8.0
flatbuffers                  2.0
fonttools                    4.34.4
fpdf                         1.7.2
gast                         0.4.0
google-auth                  2.10.0
google-auth-oauthlib         0.4.6
google-pasta                 0.2.0
grpcio                       1.47.0
h5py                         3.7.0
hdfs                         2.7.0
idna                         3.3
joblib                       1.1.0
jsonschema                   4.9.1
keras                        2.9.0
Keras-Preprocessing          1.1.2
kiwisolver                   1.4.4
kneed                        0.8.1
libclang                     14.0.6
librosa                      0.9.2
llvmlite                     0.39.0
logger                       1.4
Markdown                     3.4.1
MarkupSafe                   2.1.1
matplotlib                   3.5.2
numba                        0.56.0
numpy                        1.22.0
oauthlib                     3.2.0
opt-einsum                   3.3.0
packaging                    21.3
pandas                       1.4.3
Pillow                       9.2.0
pip                          22.2.2
platformdirs                 2.5.2
pooch                        1.6.0
protobuf                     3.19.4
pyasn1                       0.4.8
pyasn1-modules               0.2.8
pycparser                    2.21
pyparsing                    3.0.9
pyrsistent                   0.18.1
python-dateutil              2.8.2
python-Levenshtein           0.12.2
pytz                         2022.1
PyYAML                       6.0
rdp                          0.8
requests                     2.28.1
requests-oauthlib            1.3.1
resampy                      0.4.0
rsa                          4.9
scikit-learn                 1.1.2
scipy                        1.9.0
seaborn                      0.11.2
setuptools                   63.4.3
six                          1.16.0
SoundFile                    0.10.3.post1
tensorboard                  2.9.1
tensorboard-data-server      0.6.1
tensorboard-plugin-wit       1.8.1
tensorflow                   2.10.0rc0
tensorflow-cpu-aws           2.10.0rc0
tensorflow-estimator         2.9.0
tensorflow-io-gcs-filesystem 0.26.0
termcolor                    1.1.0
threadpoolctl                3.1.0
typing_extensions            4.3.0
urllib3                      1.26.11
virtualenv                   20.16.3
watchdog                     2.1.9
Werkzeug                     2.2.2
wheel                        0.37.1
wrapt                        1.14.1
</code></pre>
<p>Moreover the packages <code>kears, kneed, librosa, seaborn, sklearn and tensorflow</code> also give the same error. I am not sure where and how to check the dependency of package version. May I know how can I know which versions are compatible with numpy version? Which versions should I install for those packages <code>kears, kneed, librosa, seaborn, sklearn,  tensorflow and numpy</code> using pip install in <strong>miniconda3</strong>.</p>
",17779615.0,,17779615.0,,2022-08-11 09:19:48,2022-11-13 23:29:30,ImportError: /usr/lib/aarch64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.30' not found,<python><numpy><tensorflow>,3,14,,,,CC BY-SA 4.0
69734214,1,,,2021-10-27 07:08:55,,4,1922,"<p>I am trying to install tensorflow on my Mac M1 and I am getting the following error:</p>
<p>ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow</p>
<p>I have python3- 3.9.7, pip- 21.3.1</p>
",10289449.0,,,,,2022-08-18 15:45:43,Not able to install tensorflow in Mac M1,<python><django><tensorflow><apple-m1>,2,1,0.0,,,CC BY-SA 4.0
62892405,1,,,2020-07-14 09:49:41,,4,6983,"<p>I'm working on an object detection project. I followed <a href=""https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10?fbclid=IwAR36wdN6VAQ1lrOjvVCYPSEi1cOhAhwPDO0fUR-EqR8LRLLNoCDr8u3OyYM#1-install-anaconda-cuda-and-cudnn"" rel=""nofollow noreferrer"">the instruction from Github.</a></p>
<p>But I used a different model.</p>
<p>I run this command</p>
<pre><code>python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config
</code></pre>
<p>The error is</p>
<pre><code>ValueError: ssd_inception_v2 is not supported. See `model_builder.py` for features extractors compatible with different versions of Tensorflow
</code></pre>
<p>I don't know why. I tried to change the model version but still error.</p>
<p>Please guide me. How to solve it?</p>
",12947351.0,,6228891.0,,2020-07-14 18:04:58,2021-01-05 12:08:18,ssd_inception_v2 is not supported. See `model_builder.py` for features extractors compatible with different versions of Tensorflow,<python><tensorflow><object-detection>,2,0,0.0,,,CC BY-SA 4.0
73748939,1,73774656.0,,2022-09-16 18:25:00,,4,13086,"<p>I have the following code:</p>
<pre><code>import tensorflow as tf

print(&quot;Hello&quot;)
</code></pre>
<p>And the output is:</p>
<pre><code>This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Hello # This is printed about 5 seconds after the message
</code></pre>
<p>I had looked into the meaning of the message in <a href=""https://stackoverflow.com/questions/65298241/what-does-this-tensorflow-message-mean-any-side-effect-was-the-installation-su"">this</a> thread and in <a href=""https://stackoverflow.com/questions/35911252/disable-tensorflow-debugging-information/42121886#42121886"">this</a> one, but failed to make it disappear (and to run any program in less than 5 seconds). Any help would be greatly appreciated.</p>
",16069481.0,,,,,2022-09-19 14:03:04,This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical,<python><tensorflow>,1,0,,,,CC BY-SA 4.0
70465064,1,70496742.0,,2021-12-23 16:46:21,,4,314,"<p>I have the following code that runs two TensorFlow trainings in parallel using Dask workers implemented in Docker containers.</p>
<p>I need to launch two processes, using the same dask client, where each will train their respective models with N workers.</p>
<p>To that end, I do the following:</p>
<ul>
<li>I use <code>joblib.delayed</code> to spawn the two processes.</li>
<li>Within each process I run <code>with joblib.parallel_backend('dask'):</code> to execute the fit/training logic. Each training process triggers N dask workers.</li>
</ul>
<p>The problem is that I don't know if the entire process is thread safe, are  there any concurrency elements that I'm missing?</p>
<pre><code># First, submit the function twice using joblib delay
delayed_funcs = [joblib.delayed(train)(sub_task) for sub_task in [123, 456]]
parallel_pool = joblib.Parallel(n_jobs=2)
parallel_pool(delayed_funcs)

# Second, submit each training process
def train(sub_task):

    global client
    if client is None:
        print('connecting')
        client = Client()

    data = some_data_to_train

    # Third, process the training itself with N workers
    with joblib.parallel_backend('dask'):
        X = data[columns] 
        y = data[label]

        niceties = dict(verbose=False)
        model = KerasClassifier(build_fn=build_layers,
                loss=tf.keras.losses.MeanSquaredError(), **niceties)
        model.fit(X, y, epochs=500, verbose = 0)
</code></pre>
",1362485.0,,1362485.0,,2021-12-28 03:37:33,2021-12-28 15:50:59,Running two Tensorflow trainings in parallel using joblib and dask,<python><tensorflow><dask><dask-distributed><joblib>,2,2,,,,CC BY-SA 4.0
62951782,1,,,2020-07-17 10:07:43,,4,1212,"<p>I am trying to form a mask model r cnn with new data. I have my tf.record and I launch the following command</p>
<pre><code>python D:\Projet\CV-CommonTools\model_main.py 
--model_dir training 
--pipeline_config_path training\mask_rcnn_inception_v2.config 
--num_train_steps 20000 
--sample_1_of_n_eval_examples=0 
--alsologtostderr
</code></pre>
<p>But I get the following error.</p>
<pre><code>Traceback (most recent call last):
  File &quot;D:\Projet\model_main.py&quot;, line 110, in &lt;module&gt;
    tf.app.run()
  File &quot;C:\Users\Ibrahim\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\platform\app.py&quot;, line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File &quot;C:\Users\Ibrahim\AppData\Local\Programs\Python\Python36\lib\site-packages\absl\app.py&quot;, line 299, in run
    _run_main(main, args)
  File &quot;C:\Users\Ibrahim\AppData\Local\Programs\Python\Python36\lib\site-packages\absl\app.py&quot;, line 250, in _run_main
    sys.exit(main(argv))
  File &quot;D:\Projet\model_main.py&quot;, line 71, in main
    FLAGS.sample_1_of_n_eval_on_train_examples))
TypeError: create_estimator_and_inputs() missing 1 required positional argument: 'hparams'
</code></pre>
<p>Could someone guide me please ?</p>
<p>ps:why don't I have an answer? Did I ask my question wrong?</p>
",13947346.0,,13947346.0,,2020-07-21 13:44:02,2021-02-14 06:51:23,TypeError: create_estimator_and_inputs() missing 1 required positional argument: 'hparams',<python><tensorflow>,2,1,0.0,,,CC BY-SA 4.0
68902851,1,69013981.0,,2021-08-24 06:41:48,,4,2088,"<p>I am trying to run a tensorflow project and I am encountering memory problems on the university HPC cluster. I have to run a prediction job for hundreds of inputs, with differing lengths. We have GPU nodes with different amounts of vmem, so I am trying to set up the scripts in a way that will not crash in any combination of GPU node - input length.</p>
<p>After searching the net for solutions, I played around with TF_FORCE_UNIFIED_MEMORY, XLA_PYTHON_CLIENT_MEM_FRACTION, XLA_PYTHON_CLIENT_PREALLOCATE, and TF_FORCE_GPU_ALLOW_GROWTH, and also with tensorflow's <code>set_memory_growth</code>. As I understood, with unified memory, I should be able to use more memory than a GPU has in itself.</p>
<p>This was my final solution (only relevant parts)</p>
<pre><code>os.environ['TF_FORCE_UNIFIED_MEMORY']='1'
os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION']='2.0'
#os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'
os.environ['TF_FORCE_GPU_ALLOW_GROWTH ']='true' # as I understood, this is redundant with the set_memory_growth part :)

import tensorflow as tf    
gpus = tf.config.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      print(gpu)
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.list_logical_devices('GPU')
    print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;)
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)
</code></pre>
<p>and I submit it on the cluster with <code>--mem=30G</code> (slurm job scheduler) and <code>--gres=gpu:1</code>.</p>
<p>And this is the error my code crashes with. As I understand, it does try to use the unified memory but is failing for some reason.</p>
<pre><code>Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX TITAN Black, pci bus id: 0000:02:00.0, compute capability: 3.5)
2021-08-24 09:22:02.053935: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 12758286336 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-08-24 09:22:03.738635: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 11482457088 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-08-24 09:22:05.418059: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 10334211072 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-08-24 09:22:07.102411: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 9300789248 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-08-24 09:22:08.784349: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 8370710016 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-08-24 09:22:10.468644: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 7533638656 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-08-24 09:22:12.150588: E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to alloc 6780274688 bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-08-24 09:23:10.326528: W external/org_tensorflow/tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.33GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.


Traceback (most recent call last):
  File &quot;scripts/script.py&quot;, line 654, in &lt;module&gt;
    prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed), &quot;cpu&quot;)
  File &quot;env/lib/python3.7/site-packages/alphafold/model/model.py&quot;, line 134, in predict
    result, recycles = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)
  File &quot;env/lib/python3.7/site-packages/jax/_src/traceback_util.py&quot;, line 183, in reraise_with_filtered_traceback
    return fun(*args, **kwargs)
  File &quot;env/lib/python3.7/site-packages/jax/_src/api.py&quot;, line 402, in cache_miss
    donated_invars=donated_invars, inline=inline)
  File &quot;env/lib/python3.7/site-packages/jax/core.py&quot;, line 1561, in bind
    return call_bind(self, fun, *args, **params)
  File &quot;env/lib/python3.7/site-packages/jax/core.py&quot;, line 1552, in call_bind
    outs = primitive.process(top_trace, fun, tracers, params)
  File &quot;env/lib/python3.7/site-packages/jax/core.py&quot;, line 1564, in process
    return trace.process_call(self, fun, tracers, params)
  File &quot;env/lib/python3.7/site-packages/jax/core.py&quot;, line 607, in process_call
    return primitive.impl(f, *tracers, **params)
  File &quot;env/lib/python3.7/site-packages/jax/interpreters/xla.py&quot;, line 608, in _xla_call_impl
    *unsafe_map(arg_spec, args))
  File &quot;env/lib/python3.7/site-packages/jax/linear_util.py&quot;, line 262, in memoized_fun
    ans = call(fun, *args)
  File &quot;env/lib/python3.7/site-packages/jax/interpreters/xla.py&quot;, line 758, in _xla_callable
    compiled = compile_or_get_cached(backend, built, options)
  File &quot;env/lib/python3.7/site-packages/jax/interpreters/xla.py&quot;, line 76, in compile_or_get_cached
    return backend_compile(backend, computation, compile_options)
  File &quot;env/lib/python3.7/site-packages/jax/interpreters/xla.py&quot;, line 373, in backend_compile
    return backend.compile(built_c, compile_options=options)
jax._src.traceback_util.UnfilteredStackTrace: RuntimeError: Resource exhausted: Out of memory while trying to allocate 4649385984 bytes.

The stack trace below excludes JAX-internal frames.
The preceding is the original exception that occurred, unmodified.

--------------------

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;scripts/script.py&quot;, line 654, in &lt;module&gt;
    prediction_result, (r, t) = cf.to(model_runner.predict(processed_feature_dict, random_seed=seed), &quot;cpu&quot;)
  File &quot;env/lib/python3.7/site-packages/alphafold/model/model.py&quot;, line 134, in predict
    result, recycles = self.apply(self.params, jax.random.PRNGKey(random_seed), feat)
  File &quot;env/lib/python3.7/site-packages/jax/interpreters/xla.py&quot;, line 373, in backend_compile
    return backend.compile(built_c, compile_options=options)
RuntimeError: Resource exhausted: Out of memory while trying to allocate 4649385984 bytes.
</code></pre>
<p>I would be glad for any ideas on how to get it to work and use all the available memory.</p>
<p>Thank you!</p>
",4126888.0,,9215780.0,,2021-08-27 14:56:11,2021-09-01 12:43:34,failed to alloc X bytes unified memory; result: CUDA_ERROR_OUT_OF_MEMORY: out of memory,<tensorflow><out-of-memory><gpu><slurm>,2,2,0.0,,,CC BY-SA 4.0
69426006,1,69532366.0,,2021-10-03 14:54:48,,4,453,"<p>I have been reading the official guide here (<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a>) to try and recreate the Vanilla Transformer in Tensorflow. I notice the dataset used is quite specific, and at the end of the guide, it says to try with a different dataset.</p>
<p>But that is where I have been stuck for a long time! I am trying to use the WMT14 dataset (as used in the original paper, Vaswani et. al.) here: <a href=""https://www.tensorflow.org/datasets/catalog/wmt14_translate#wmt14_translatede-en"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/wmt14_translate#wmt14_translatede-en</a> .</p>
<p>I have also tried Multi30k and IWSLT dataset from Spacy, but are there any guides on how I can fit the dataset to what the model requires? Specifically, to tokenize it. The official TF guide uses a pretrained tokenizer, which is specific to the PR-EN dataset given.</p>
<pre><code>model_name = &quot;ted_hrlr_translate_pt_en_converter&quot;
</code></pre>
<p>I am wondering, how I can use the TF (bert) tokenizer to tokenize the Spacy dataset? I have the code for PyTorch, unfortunately I do not know how to adapt it for Tensorflow. Any help would be greatly appreciated!</p>
<pre><code>import spacy

spacy_de = spacy.load('de')
spacy_en = spacy.load('en')

def tokenize_de(text):
    return [tok.text for tok in spacy_de.tokenizer(text)]

def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

BOS_WORD = '&lt;s&gt;'
EOS_WORD = '&lt;/s&gt;'
BLANK_WORD = &quot;&lt;blank&gt;&quot;
SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)
TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, 
                 eos_token = EOS_WORD, pad_token=BLANK_WORD)

MAX_LEN = 100
train, val, test = datasets.IWSLT.splits(
    exts=('.de', '.en'), fields=(SRC, TGT), 
    filter_pred=lambda x: len(vars(x)['src']) &lt;= MAX_LEN and 
        len(vars(x)['trg']) &lt;= MAX_LEN)
MIN_FREQ = 2
SRC.build_vocab(train.src, min_freq=MIN_FREQ)
TGT.build_vocab(train.trg, min_freq=MIN_FREQ)
</code></pre>
",11644523.0,,,,,2021-10-11 23:08:06,"Tensorflow ""Transformer model for language understanding"" with another Dataset?",<python><tensorflow><translation><transformer-model><opennmt>,2,0,,,,CC BY-SA 4.0
72311337,1,,,2022-05-19 21:37:56,,4,1292,"<p>Very simple question. I am using tensorflow probability package to use a bijector to form a trainable_distribution from a simple distribution (let's say gaussian)</p>
<p>Everything works properly in jupyter notebook but as I bring it to terminal (where I am running my code file) it gives me this error:</p>
<p>TypeError: Cannot interpret '&lt;KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'input_1')&gt;' as a data type</p>
<p>Here is my code for this part (X_data is (m,3) where m is the number of samples and trainable_distribution is already built using tensorflow_probability.distributions.TransformedDistribution(base_dist, bijector):</p>
<p>def train_dist_routine(X_data, trainable_distribution, n_epochs=200, batch_size=None):</p>
<pre><code>    x_ = tensorflow.keras.layers.Input(shape=(3,), dtype=tf.float32)
    print(x_)
    log_prob_ = trainable_distribution.log_prob(x_)
    model = tensorflow.keras.models.Model(x_, log_prob_)

    model.compile(optimizer=tf.optimizers.Adam(),
                  loss=lambda _, log_prob: -log_prob)

    ns = X_data.shape[0]
    if batch_size is None:
        batch_size = ns



    history = model.fit(x=X_data,
                        y=np.zeros((ns, 0), dtype=np.float32),
                        batch_size=batch_size,
                        epochs=n_epochs,
                        validation_split=0.2,
                        shuffle=True,
                        verbose=False)
    return history
</code></pre>
",8593315.0,,,,,2022-05-19 21:37:56,"TypeError: Cannot interpret '<KerasTensor: shape=(None, 3) dtype=float32 (created by layer 'input_1')>' as a data type",<python><tensorflow><tf.keras><keras-layer><tensorflow-probability>,0,0,,,,CC BY-SA 4.0
69782818,1,69789522.0,,2021-10-30 23:11:55,,4,1108,"<p>I am interested about training a neural network using JAX. I had a look on <code>tf.data.Dataset</code>, but it provides exclusively tf tensors. I looked for a way to change the dataset into JAX numpy array and I found a lot of implementations that use <code>Dataset.as_numpy_generator()</code> to turn the tf tensors to numpy arrays. However I wonder if it is a good practice, as numpy arrays are stored in CPU memory and it is not what I want for my training (I use the GPU). So the last idea I found is to manually recast the arrays by calling <code>jnp.array</code> but it is not really elegant (I am afraid about the copy in GPU memory). Does anyone have a better idea for that?</p>
<p>Quick code to illustrate:</p>
<pre class=""lang-py prettyprint-override""><code>import os
import jax.numpy as jnp
import tensorflow as tf

def generator():
    for _ in range(2):
        yield tf.random.uniform((1, ))

ds = tf.data.Dataset.from_generator(generator, output_types=tf.float32,
                                    output_shapes=tf.TensorShape([1]))

ds1 = ds.take(1).as_numpy_iterator()
ds2 = ds.skip(1)

for i, batch in enumerate(ds1):
    print(type(batch))

for i, batch in enumerate(ds2):
    print(type(jnp.array(batch)))

# returns:

&lt;class 'numpy.ndarray'&gt; # not good
&lt;class 'jaxlib.xla_extension.DeviceArray'&gt; # good but not elegant
</code></pre>
",17289463.0,,4685471.0,,2021-10-31 22:42:12,2022-07-28 09:28:08,Turn a tf.data.Dataset to a jax.numpy iterator,<python><tensorflow><numpy-ndarray><jax>,2,0,0.0,,,CC BY-SA 4.0
71927074,1,72017306.0,,2022-04-19 14:57:12,,4,899,"<p>I need to avoid this error: <code>tensorflow.python error.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized</code>. It is connected with the acquisition of my 3060 memory, in order to avoid it, I have to do <code>Embedding</code> layer calculations on the <strong>CPU</strong>, but how? I tried run full model on <strong>CPU</strong>, and its works fine, but very slow. For example, if I reduce the number of neurons in all layers to 128, then I can use 8000 sentences (<code>data_list[:8000]</code> instead of 6000 for example below) for training, but I have ~ 20000 of them.</p>
<p>My model:</p>
<pre class=""lang-py prettyprint-override""><code>class CPUEmbedding(Embedding):
    @tf_utils.shape_type_conversion
    def build(self, input_shape):
        with ops.device('cpu:0'):
            self.embeddings = self.add_weight(
                shape=(self.input_dim, self.output_dim),
                initializer=self.embeddings_initializer,
                name='embeddings',
                regularizer=self.embeddings_regularizer,
                constraint=self.embeddings_constraint)

        self.built = True

        print('Embedding starts on cpu')

model = Sequential()
model.add(CPUEmbedding(19260, 256, input_length=163))
model.add(LSTM(256, return_sequences=True))  # the output will be a sequence of the same length
model.add(Dropout(0.2))
model.add(LSTM(512))
model.add(Dropout(0.2))
model.add(Dense(self.total_words, activation='softmax'))
adam = Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])
</code></pre>
<p>Model summary:</p>
<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 cpu_embedding (CPUEmbedding  (None, 163, 256)         4930560   
 )                                                               
                                                                 
 lstm (LSTM)                 (None, 163, 256)          525312    
                                                                 
 dropout (Dropout)           (None, 163, 256)          0         
                                                                 
 lstm_1 (LSTM)               (None, 512)               1574912   
                                                                 
 dropout_1 (Dropout)         (None, 512)               0         
                                                                 
 dense (Dense)               (None, 19260)             9880380   
                                                                 
=================================================================
Total params: 16,911,164
Trainable params: 16,911,164
Non-trainable params: 0
</code></pre>
<p>A model that you can run, but first you will need to download some big book</p>
<pre class=""lang-py prettyprint-override""><code>from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding, LSTM, Dense, Dropout
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical

import numpy as np

tokenizer = Tokenizer()

# Book with len &gt; 1 000 000 words
with open('text.txt', encoding='utf-8') as f:
    data = f.read().replace('\ufeff', '')

data_list = data.lower().split(&quot;\n&quot;)
tokenizer.fit_on_texts(data_list)
total_words = len(tokenizer.word_index) + 1

print('Words number:', total_words)

input_sequences = []

for line in data_list:
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i + 1]
        input_sequences.append(n_gram_sequence)

max_sequence_len = max([len(x) for x in input_sequences])

input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

X, labels = input_sequences[:, :-1], input_sequences[:, -1]
Y = to_categorical(labels, num_classes=total_words)

model = Sequential()
model.add(Embedding(total_words, 256, input_length=max_sequence_len - 1))
model.add(LSTM(256, return_sequences=True))
model.add(Dropout(0.1))
model.add(LSTM(512))
model.add(Dropout(0.1))
model.add(Dense(total_words, activation='softmax'))
adam = Adam(lr=0.001)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['acc'])

history = model.fit(x=X, y=Y, batch_size=128, epochs=1000)
</code></pre>
<p><strong>Versions:</strong></p>
<p>1)</p>
<ul>
<li>OS: Windows 10</li>
<li>Cuda: 11.6 (latest, from nvidia site)</li>
<li>python: 3.9</li>
<li>tensorflow: 2.8</li>
<li>starts in: cmd</li>
<li>GPU: 3060</li>
</ul>
<ol start=""2"">
<li></li>
</ol>
<ul>
<li>OS: Windows 11</li>
<li>Cuda: downloaded by conda</li>
<li>python: 3.8</li>
<li>tensorflow: 2.6</li>
<li>starts in: conda</li>
<li>GPU: 1060 Ti</li>
</ul>
",12938901.0,,12938901.0,,2022-04-23 05:08:28,2022-04-28 07:54:58,How to use CPU only for Embedding?,<python><tensorflow><tensorflow2.0>,1,6,,,,CC BY-SA 4.0
73499814,1,,,2022-08-26 10:45:51,,4,103,"<p>I have seen the question multiple times but never got a straight forward answer. In one node of our cluster we have 7 gpus. I want to use two specific gpus in <code>tensorflow2</code>, say <code>GPU 3</code> and <code>GPU 5</code>. How do I select these two GPUs?
I have tried:</p>
<pre><code>physical_devices = tf.config.list_physical_devices('GPU')
tf.config.set_visible_devices([physical_devices[3],physical_devices[5]],'GPU')
</code></pre>
<p>Still it is using <code>GPU 0</code> and I am getting error because someone else is already using it.</p>
<p>If parallel use could be an issue, please suggest me how to use a specific one, say GPU 3? And how will I check that only GPU 3 is being used?</p>
",3142840.0,,681865.0,,2022-08-27 02:21:53,2022-08-27 02:21:53,Tensorflow use particular gpus,<tensorflow><gpu><tensorflow2.0>,1,1,,,,CC BY-SA 4.0
72045836,1,,,2022-04-28 14:49:47,,4,1548,"<p>I am getting this weird error about output shape for lstm layer. I have tried several things but not sure where I am doing mistake.</p>
<p>This question is from deep learning specialization from courser</p>
<p>'''
def music_inference_model(LSTM_cell, densor, Ty=100):</p>
<pre><code>    n_values = densor.units
    n_a = LSTM_cell.units
    
    x0 = Input(shape=(1, n_values))


    a0 = Input(shape=(n_a,), name='a0')
    c0 = Input(shape=(n_a,), name='c0')
    a = a0
    c = c0
    x = x0

    outputs = []

    for t in range(Ty):
        a, _, c = LSTM_cell(x, initial_state=[a, c])
        out = densor(a)
        outputs.append(out)
        x = tf.math.argmax(out)
        x = tf.one_hot(x,  depth=n_values)
        x = RepeatVector(1)(x)
    
    inference_model = Model([x0,a0,c0],outputs)



    return inference_model

inference_model = music_inference_model(LSTM_cell, densor, Ty = 50)

inference_summary = summary(inference_model) 
comparator(inference_summary, music_inference_model_out)
</code></pre>
<p>'''</p>
<p>But I am getting this error.</p>
<h2>'''</h2>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-21-c395f100af16&gt; in &lt;module&gt;
      1 # UNIT TEST
----&gt; 2 inference_summary = summary(inference_model)
      3 comparator(inference_summary, music_inference_model_out)

~/work/W1A3/test_utils.py in summary(model)
     34     result = []
     35     for layer in model.layers:
---&gt; 36         descriptors = [layer.__class__.__name__, layer.output_shape,             layer.count_params()]
     37         if (type(layer) == Conv2D):
     38             descriptors.append(layer.padding)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in     output_shape(self)
   2190                            'ill-defined for the layer. '
   2191                            'Use `get_output_shape_at(node_index)` '
-&gt; 2192                            'instead.' % self.name)
   2193 
   2194   @property

AttributeError: The layer &quot;lstm&quot; has multiple inbound nodes, with different output shapes.     Hence the notion of &quot;output shape&quot; is ill-defined for the layer. Use `get_output_shape_at(node_index)` instead.
</code></pre>
<p>'''</p>
",9664490.0,,,,,2022-12-20 09:22:47,"Tensorflow: Shape error in LSTM, The layer ""lstm"" has multiple inbound nodes, with different output shapes",<python><tensorflow><lstm><recurrent-neural-network>,2,0,,,,CC BY-SA 4.0
63178526,1,63191658.0,,2020-07-30 17:41:09,,4,910,"<p>Following <a href=""https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/programming-exercise"" rel=""nofollow noreferrer"">this Colab exeercise</a> from Google's ML Crash Course, I generated a model in Python for the MNIST database. The code looks as follows:</p>
<pre><code>import pandas as pd
import tensorflow as tf


def create_model(my_learning_rate):
    model = tf.keras.models.Sequential()
    model.add(tf.keras.Input(shape=(28, 28), name='input'))
    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))
    model.add(tf.keras.layers.Dense(units=256, activation='relu'))
    model.add(tf.keras.layers.Dense(units=128, activation='relu'))
    model.add(tf.keras.layers.Dropout(rate=0.2))
    model.add(tf.keras.layers.Dense(units=10, activation='softmax', name='output'))
    model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model


def train_model(model, train_features, train_label, epochs,
                batch_size=None, validation_split=0.1):
    history = model.fit(x=train_features, y=train_label, batch_size=batch_size,
                        epochs=epochs, shuffle=True,
                        validation_split=validation_split)
    epochs = history.epoch
    hist = pd.DataFrame(history.history)
    return epochs, hist


if __name__ == '__main__':
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train_normalized = x_train / 255.0
    x_test_normalized = x_test / 255.0

    learning_rate = 0.003
    epochs = 50
    batch_size = 4000
    validation_split = 0.2

    my_model = create_model(learning_rate)
    epochs, hist = train_model(my_model, x_train_normalized, y_train,
                               epochs, batch_size, validation_split)

    my_model.save('my_model')
</code></pre>
<p>The model is saved to the &quot;my_model&quot; folder, as it should. Now I load it again in my Java program:</p>
<pre><code>public class HelloTensorFlow {
    public static void main(final String[] args) {
        final String filePath = Paths.get(&quot;my_model&quot;).toAbsolutePath().toString();
        try (final SavedModelBundle b = SavedModelBundle.load(filePath, &quot;serve&quot;)) {
            final Session sess = b.session();

            final Tensor&lt;Float&gt; x = Tensor.create(new float[1][28 * 28], Float.class);
            final List&lt;Tensor&lt;?&gt;&gt; run = sess.runner()
                    .feed(&quot;input&quot;, x)
                    .fetch(&quot;output&quot;)
                    .run();

            final float[] y = run.get(0).copyTo(new float[1]);
            System.out.println(y[0]);
        }
    }
}
</code></pre>
<p>The model is loaded but the runner does not work. When I execute the program, I get &quot;No Operation named [input] in the Graph&quot;, even though my Input has this name. What am I doing wrong. I have the newest TensorFlow versions: 2.3.0 (Python) and 1.15.0 (Java).</p>
",14023361.0,,,,,2020-07-31 12:15:35,"""No Operation named [input] in the Graph"" in Java",<java><python><tensorflow><machine-learning><tensorflow2.0>,1,1,,,,CC BY-SA 4.0
63405834,1,,,2020-08-14 02:51:15,,4,2147,"<p>Recently I am trying Tensorflow Object Detection API V2 with TF2, and I've sucessfully trained a network.</p>
<p>However, I found little information about how to save the best model.</p>
<p>I've found some simple tutorials that don't talk much about these details.</p>
<p>I've found a similar question <a href=""https://stackoverflow.com/questions/49685923/how-to-store-best-models-checkpoints-not-only-newest-5-in-tensorflow-object-de"">here</a>, however the best answer is to modify <code>legacy/trainer.py</code>, which seems to be outdated.</p>
<p>I've also found another similar question <a href=""https://stackoverflow.com/questions/55005432/is-there-some-way-to-save-best-model-only-with-tensorflow-estimator-train-and-ev"">here</a>, however the answers are talking about <code>exporters</code>, which is in <code>model_lib.py</code>, but I can't find similar parameter in <code>model_lib_v2.py</code></p>
<p><strong>So, for V2, which file should I modify to save the best model?</strong></p>
",10253771.0,,10253771.0,,2020-08-20 01:58:13,2020-08-26 18:04:35,How to save best models in latest Tensorflow Object Detection API V2 with TF2?,<tensorflow><tensorflow2.0><object-detection-api><tensorflow-model-garden>,1,1,,,,CC BY-SA 4.0
68312483,1,,,2021-07-09 06:57:16,,4,2497,"<p>As described in jypyter notebook I tried:</p>
<p>import tensorflow</p>
<p>then my kernel is killed, like &quot;The kernel appears to have died. It will restart automatically.&quot;</p>
<p>Tried to reinstall anaconda, numpy, tensorflow, didn't work.</p>
",16348967.0,,,,,2022-12-29 08:39:08,simply importing tensorflow kills my Jupiter notebook kernel,<tensorflow><jupyter-notebook><tensorflow2.0>,1,2,,,,CC BY-SA 4.0
63182524,1,63220663.0,,2020-07-30 22:54:18,,4,10063,"<p>I am new to tensorflow and trying to learn it. Trying to run an estimator LinearClassifier in Tensorflow 2.2.0.</p>
<ol>
<li>Imported all the modules and read in tfRecords</li>
</ol>
<pre><code>import tensorflow as tf
print(tf.version.VERSION)
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
print (tf.executing_eagerly())
tf.executing_eagerly()
tf.compat.v1.enable_eager_execution()

path = 'train.tfrecord'
filenames = [(path + &quot;/&quot; + name) for name in os.listdir(path) if name.startswith(&quot;part&quot;)]
print (filenames)
</code></pre>
<ol start=""2"">
<li>Define the parse function</li>
</ol>
<pre><code>def _parse_function(example_proto):
    features = {
        'Age': tf.io.FixedLenFeature([], tf.string),
        'EstimatedSalary': tf.io.FixedLenFeature([], tf.string),
        'Purchased': tf.io.FixedLenFeature([], tf.string)
    }
    tf_records = tf.io.parse_single_example(example_proto, features)
    features_dict = {
        'Age': tf_records['Age'],
        'EstimatedSalary': tf_records['EstimatedSalary']
    }
    return features_dict, tf_records['Purchased']
</code></pre>
<ol start=""3"">
<li>Define the input function to pass in the estimator</li>
</ol>
<pre><code>def input_fn():
    dataset = tf.data.TFRecordDataset(filenames = filenames)
    
    dataset = dataset.map(_parse_function)
    iterator = iter(dataset)
    next_element = iterator.get_next()
    return next_element
</code></pre>
<ol start=""4"">
<li>Initializing estimator</li>
</ol>
<pre><code>feature_columns = [
    tf.feature_column.numeric_column('Age'),
    tf.feature_column.numeric_column('EstimatedSalary')
]

estimator = tf.estimator.LinearClassifier(feature_columns = feature_columns)
estimator.train(
    input_fn = input_fn
)
</code></pre>
<p>Running the following code gives an error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;linear_classification.py&quot;, line 42, in &lt;module&gt;
    input_fn = input_fn
  File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 1182, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 1208, in _train_model_default
    self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))
  File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 1044, in _get_features_and_labels_from_input_fn
    self._call_input_fn(input_fn, mode))
  File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py&quot;, line 1137, in _call_input_fn
    return input_fn(**kwargs)
  File &quot;linear_classification.py&quot;, line 31, in input_fn
    iterator = iter(dataset)
  File &quot;/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py&quot;, line 406, in __iter__
    raise RuntimeError(&quot;__iter__() is only supported inside of tf.function &quot;
RuntimeError: __iter__() is only supported inside of tf.function or when eager execution is enabled.
</code></pre>
<p>Things I tried:</p>
<ol>
<li>forcing eager execution (even tho in tf 2 it is done by default).</li>
<li>Trying to search existing StackOverflow: <a href=""https://stackoverflow.com/questions/55576133/tensorflow-2-0-dataset-iter-is-only-supported-when-eager-execution-is-enab/56736763#56736763"">TensorFlow 2.0 dataset.__iter__() is only supported when eager execution is enabled</a></li>
<li>Put print statements in actual tf source code to understand why <strong>context.executing_eagerly()</strong> is setting to False. The <strong>default_execution_mode</strong> in context.py is initialized by EAGER_MODE, so I am confused to why it becomes False</li>
</ol>
<pre><code>/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py
</code></pre>
<p>This is my first StackOverflow question, so please excuse if I have not followed any guidelines or rules. Any help is much appreciated. Thank you.</p>
",13463483.0,,,,,2020-08-02 20:22:08,RuntimeError: __iter__() is only supported inside of tf.function or when eager execution is enabled,<tensorflow><tensorflow2.0><tensorflow-datasets>,1,0,,,,CC BY-SA 4.0
63411128,1,,,2020-08-14 10:35:47,,4,10978,"<p>I'm trying to run <code>pip install tensorflow</code> from a <strong>jupyter notebook</strong> (trying to run it with <strong>cmd</strong> gives out an even stranger <a href=""https://stackoverflow.com/questions/63404192/pip-install-tensorflow-cannot-find-file-called-client-load-reporting-filter-h?noredirect=1#comment112115098_63404192"">error</a>), and my free disk space (currently 1.2 GB left) seems to dwindle during installation and then I'm presented with <code>[Errno 28] No space left on device</code>. Then it goes back to what it was.</p>
",13977239.0,,,,,2022-03-11 19:33:47,How much space is necessary to install tensorflow?,<python><tensorflow><installation><pip><jupyter>,2,0,,,,CC BY-SA 4.0
68304309,1,68365044.0,,2021-07-08 15:13:53,,4,345,"<p>I have used PCA with the 'Sphereize data' option on the following page successfully: <a href=""https://projector.tensorflow.org/"" rel=""nofollow noreferrer"">https://projector.tensorflow.org/</a></p>
<p>I wonder how to run the same computation locally using the TensorFlow API. I found the <a href=""https://www.tensorflow.org/tfx/transform/api_docs/python/tft/pca"" rel=""nofollow noreferrer"">PCA documentation in the API documentation</a>, but I am not sure if sphereizing the data is available somewhere in the API too?</p>
",463977.0,,,,,2021-07-13 15:10:18,How to use the 'sphereize data' option with PCA in TensorFlow,<tensorflow><pca><dimensionality-reduction><projector>,1,0,,,,CC BY-SA 4.0
63411142,1,,,2020-08-14 10:36:56,,4,1696,"<p>I have some code in TensorFlow which takes a base model, fine-tunes (trains) it with some data, and then uses the model to <code>predict()</code> using some other data. All this is encapsulated in a <code>main()</code> method of a module and works fine.</p>
<p>When I run this code in a loop over different base models, however, I end up with an OOM after, e.g., 7 base models. Is this expected? I would expect that Python cleans up after each <code>main()</code> call. Does TensorFlow not do that? How can I force it to?</p>
<p><strong>Edit:</strong> here's an MWE showing not the OOM crashes, but increasing memory consumption:</p>
<pre><code>import gc
import os

import numpy as np
import psutil
import tensorflow as tf

tf.get_logger().setLevel(&quot;ERROR&quot;)  # Suppress &quot;tf.function retracing&quot; warnings
process = psutil.Process(os.getpid())
for i in range(100):
    (model := tf.keras.applications.mobilenet.MobileNet()).compile(loss=&quot;mse&quot;)
    history = model.fit(
        x=(x := tf.zeros((1, *model.input.shape[1:]))),
        y=(y := tf.zeros((1, *model.output.shape[1:]))),
        verbose=0,
    )
    prediction = model.predict(x)
    _ = gc.collect()
    # tf.keras.backend.clear_session()
    print(f&quot;rss {i}: {process.memory_info().rss &gt;&gt; 20} MB&quot;)
</code></pre>
<p>On my computer (CPU), it prints</p>
<pre><code>rss 0: 374 MB
rss 1: 438 MB
rss 2: 478 MB
rss 3: 517 MB
rss 4: 554 MB
rss 5: 588 MB
rss 6: 634 MB
rss 7: 669 MB
rss 8: 686 MB
rss 9: 726 MB
...
rss 30: 1386 MB
rss 31: 1413 MB
rss 32: 1445 MB
rss 33: 1476 MB
rss 34: 1506 MB
rss 35: 1536 MB
rss 36: 1568 MB
rss 37: 1597 MB
rss 38: 1630 MB
rss 39: 1662 MB
...
</code></pre>
<p>With <code>tf.keras.backend.clear_session()</code> uncommented, it's better, but not perfect yet:</p>
<pre><code>rss 0: 374 MB
rss 1: 420 MB
rss 2: 418 MB
rss 3: 450 MB
rss 4: 447 MB
rss 5: 469 MB
rss 6: 469 MB
rss 7: 475 MB
rss 8: 487 MB
rss 9: 494 MB
...
rss 40: 519 MB
rss 41: 516 MB
rss 42: 517 MB
rss 43: 520 MB
rss 44: 519 MB
rss 45: 519 MB
rss 46: 521 MB
rss 47: 517 MB
rss 48: 521 MB
rss 49: 521 MB
...
rss 90: 531 MB
rss 91: 531 MB
rss 92: 531 MB
rss 93: 531 MB
rss 94: 532 MB
rss 95: 532 MB
rss 96: 533 MB
rss 97: 534 MB
rss 98: 533 MB
rss 99: 533 MB
</code></pre>
<p>Switching the order of <code>gc.collect()</code> and <code>tf.keras.backend.clear_session()</code> did not help, either.</p>
",880783.0,,880783.0,,2020-08-14 12:39:26,2020-08-14 12:39:26,How to avoid OOM errors in repeated training and prediction in TensorFlow?,<python><tensorflow><out-of-memory>,0,4,0.0,,,CC BY-SA 4.0
72308983,1,,,2022-05-19 17:44:20,,4,191,"<p>I have a loop where I am creating tensorflow datasets and then saving to directories for later use.</p>
<p>I found that as the loop progresses, the memory being used greatly increases, until the process eventually crashes.</p>
<p>In this first example, I do not save the file, and the memory stays at the same level throughout:</p>
<pre><code>import tensorflow as tf
import numpy as np
from humanize import naturalsize
import psutil

for i in range(10000):
    if not i % 100:
        print(naturalsize(psutil.Process().memory_info().rss))
    data = tf.data.Dataset.from_tensors(np.array([0, 1, 2]))

# prints 567.7 MB throughout
</code></pre>
<p>However, if I save each dataset, the memory used increases each time.</p>
<pre><code>import tensorflow as tf
import numpy as np
from humanize import naturalsize
import psutil

for i in range(10000):
    if not i % 100:
        print(naturalsize(psutil.Process().memory_info().rss))
    data = tf.data.Dataset.from_tensors(np.array([0, 1, 2]))
    tf.data.experimental.save(data, path='~/Desktop/1')
# Grows by about 5 MB each 100 runs.
</code></pre>
<p>Adding <code>tf.keras.backend.clear_session()</code> after each save appears to slow down the memory growth but doesn't fully stop it.</p>
<p>Thank you in advance for any help.</p>
",9909857.0,,9909857.0,,2022-05-19 17:51:11,2022-05-19 17:51:11,Memory Leak with Tensorflow Experimental Save,<python><tensorflow><memory><dataset>,0,0,,,,CC BY-SA 4.0
72107119,1,,,2022-05-04 01:42:54,,4,8106,"<p>I tried a lot of things before I could finally figure out this approach. There are a lot of videos and blogs asking to install the Cuda toolkit and cuDNN from the website. Checking the compatible version. But this is not required anymore all you have to do is the following</p>
<pre><code>pip install tensorflow-gpu

pip install cuda

pip install cudnn
</code></pre>
<p>then use the following code to check if your GPU is active in the current notebook</p>
<pre><code>print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices('GPU')))

tf.config.list_physical_devices('GPU')

from tensorflow.python.client import device_lib

device_lib.list_local_devices()

tf.test.is_built_with_cuda()

tf.debugging.set_log_device_placement(True)
</code></pre>
<p>I just want to confirm, if these steps are enough to enable GPU in jupyter notebook or am I missing something here?</p>
",17544627.0,,681865.0,,2022-05-04 01:48:47,2023-06-18 20:00:35,How to use system GPU in Jupyter notebook?,<tensorflow><jupyter-notebook><gpu>,1,6,0.0,,,CC BY-SA 4.0
68945096,1,69109009.0,,2021-08-26 20:42:43,,4,3485,"<p>Machine: <strong>MacBook Air M1 2020</strong></p>
<p>OS: <strong>macOs BigSur 11.4</strong></p>
<p>Python    version of venv: <strong>Python 3.8.6</strong></p>
<p>Tensorflow version: <strong>ATF Apple    Tensorflow 0.1a3</strong></p>
<p>Pip version: <strong>21.2.4</strong></p>
<p>I have installed Tensorflow from <a href=""https:////github.com/apple/tensorflow_macos"" rel=""nofollow noreferrer"">github</a> using <a href=""https://towardsdatascience.com/installing-tensorflow-on-the-m1-mac-410bb36b776"" rel=""nofollow noreferrer"">this guide</a>.</p>
<hr />
<p>Now, my pip list is this.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>Package                 Version
----------------------- ---------
absl-py                 0.13.0
appnope                 0.1.2
astunparse              1.6.3
backcall                0.2.0
cached-property         1.5.2
cachetools              4.2.2
certifi                 2021.5.30
charset-normalizer      2.0.4
cycler                  0.10.0
Cython                  0.29.24
debugpy                 1.4.1
decorator               5.0.9
entrypoints             0.3
flatbuffers             2.0
gast                    0.5.2
google-auth             1.35.0
google-auth-oauthlib    0.4.5
google-pasta            0.2.0
grpcio                  1.33.2
h5py                    2.10.0
idna                    3.2
ipykernel               6.2.0
ipython                 7.26.0
ipython-genutils        0.2.0
jedi                    0.18.0
jupyter-client          7.0.1
jupyter-core            4.7.1
Keras-Preprocessing     1.1.2
kiwisolver              1.3.1
Markdown                3.3.4
matplotlib              3.4.3
matplotlib-inline       0.1.2
nest-asyncio            1.5.1
numpy                   1.18.5
oauthlib                3.1.1
opt-einsum              3.3.0
packaging               21.0
parso                   0.8.2
pexpect                 4.8.0
pickleshare             0.7.5
Pillow                  8.3.1
pip                     21.2.4
prompt-toolkit          3.0.20
protobuf                3.17.3
ptyprocess              0.7.0
pyasn1                  0.4.8
pyasn1-modules          0.2.8
Pygments                2.10.0
pyparsing               2.4.7
python-dateutil         2.8.2
pyzmq                   22.2.1
requests                2.26.0
requests-oauthlib       1.3.0
rsa                     4.7.2
setuptools              57.4.0
six                     1.16.0
tensorboard             2.6.0
tensorboard-data-server 0.6.1
tensorboard-plugin-wit  1.8.0
tensorflow-addons       0.1a3
tensorflow-estimator    2.6.0
tensorflow-hub          0.12.0
tensorflow              0.1a3
termcolor               1.1.0
tornado                 6.1
traitlets               5.0.5
typeguard               2.12.1
typing-extensions       3.10.0.0
urllib3                 1.26.6
wcwidth                 0.2.5
Werkzeug                2.0.1
wheel                   0.37.0
wrapt                   1.12.1</code></pre>
</div>
</div>
</p>
<p>I want install Object Detection Api from Tensorflow in that <a href=""https://github.com/tensorflow/models"" rel=""nofollow noreferrer"">link</a>.</p>
<hr />
<p>I cloned the repo and them I follow the <a href=""https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md"" rel=""nofollow noreferrer"">guide</a>. (Python Package Installation)</p>
<hr />
<p>When I execute this command</p>
<pre><code>python -m pip install --use-feature=2020-resolver .
</code></pre>
<p>It starts to download, and start a print very long errors.</p>
<p>At the end of the operations, it gives me this error.</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-html lang-html prettyprint-override""><code>  Using cached scipy-1.2.3.tar.gz (23.3 MB)
Collecting pandas
  Using cached pandas-1.3.2-cp38-cp38-macosx_11_0_arm64.whl
Collecting tf-models-official&gt;=2.5.1
  Using cached tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)
Collecting kaggle&gt;=1.3.9
  Using cached kaggle-1.5.12-py3-none-any.whl
Collecting py-cpuinfo&gt;=3.3.0
  Using cached py_cpuinfo-8.0.0-py3-none-any.whl
Requirement already satisfied: numpy&gt;=1.15.4 in /Users/stefan/Desktop/Studio/TFOD/tf-m1/lib/python3.8/site-packages (from tf-models-official&gt;=2.5.1-&gt;object-detection==0.1) (1.18.5)
Collecting opencv-python-headless
  Using cached opencv_python_headless-4.5.3.56-cp38-cp38-macosx_11_0_arm64.whl (10.7 MB)
Collecting tf-models-official&gt;=2.5.1
  Using cached tf_models_official-2.5.1-py2.py3-none-any.whl (1.6 MB)
Collecting tensorflow-datasets
  Using cached tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)
Collecting google-api-python-client&gt;=1.6.7
  Downloading google_api_python_client-2.18.0-py2.py3-none-any.whl (7.4 MB)
     |████████████████████████████████| 7.4 MB 3.4 MB/s 
Collecting oauth2client
  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)
Collecting tensorflow-model-optimization&gt;=0.4.1
  Using cached tensorflow_model_optimization-0.6.0-py2.py3-none-any.whl (211 kB)
Collecting pyyaml&gt;=5.1
  Downloading PyYAML-5.4.1.tar.gz (175 kB)
     |████████████████████████████████| 175 kB 31.3 MB/s 
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
    Preparing wheel metadata ... done
Collecting gin-config
  Using cached gin_config-0.4.0-py2.py3-none-any.whl (46 kB)
Collecting sacrebleu
  Using cached sacrebleu-2.0.0-py3-none-any.whl (90 kB)
INFO: pip is looking at multiple versions of &lt;Python from Requires-Python&gt; to determine which version is compatible with other requirements. This could take a while.
INFO: pip is looking at multiple versions of object-detection to determine which version is compatible with other requirements. This could take a while.
ERROR: Cannot install object-detection because these package versions have conflicting dependencies.

The conflict is caused by:
    tf-models-official 2.6.0 depends on tensorflow-text&gt;=2.5.0
    tf-models-official 2.5.1 depends on tensorflow-addons

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies</code></pre>
</div>
</div>
</p>
",13272417.0,,,,,2021-11-12 08:20:34,Tensorflow Object Detection Api M1 Macbook Conflict Error,<python><tensorflow><dependencies><object-detection-api><apple-m1>,3,0,0.0,,,CC BY-SA 4.0
68262503,1,69683209.0,,2021-07-05 22:05:07,,4,11627,"<p>I am trying to save a tensor array that is necessarily computed into a function with the decorator <code>@tf.function</code>, this makes all the tensors inside the function into tensor graphs, and hence, non-iterable objects. For instance, in the following minimal code, I would like to know if it is possible to save the tensor into a file using code inside the function <code>foo()</code>.</p>
<pre><code>@tf.function
def foo(x):
    # code for saving x


a=tf.constant([1,2,3])
foo(a)
</code></pre>
",15505332.0,,15505332.0,,2021-07-05 22:28:37,2021-10-22 22:28:26,How to save the value of a tensor in Tensorflow,<python><tensorflow><tensorflow2.0><tensorflow-datasets>,2,1,,,,CC BY-SA 4.0
70492191,1,,,2021-12-27 06:07:13,,4,1138,"<p>I have created a simple notebook on SageMaker Studio using the Image &quot;Tensorflow 2.6 Python 3.8 GPU optimized&quot;. But when I try to run simple statement viz. &quot;import tensorflow&quot;, I am getting the error &quot;no module named 'tensorflow'&quot;.</p>
<p>I tried to install 'tensorflow' package using pip from the terminal attached to the image. But it shows the message &quot;requirement already satisfied&quot;.</p>
<p>Am I missing anything here? Please help.</p>
<p>Thanks in advance,
Surya Praveen</p>
",2118607.0,,,,,2022-02-04 10:09:26,"Sagemaker Studio notebook - no module named 'tensorflow' when chosen image type ""Tensorflow 2.6 Python 3.8 GPU optimized""",<tensorflow><amazon-sagemaker>,1,3,,,,CC BY-SA 4.0
70559051,1,70559656.0,,2022-01-02 19:22:20,,4,130,"<p>Since I am working with TensorFlow, I would like to know how to map my rows from a tensor C to the index of its corresponding row in  matrix B.</p>
<p>Here is the code I wrote:</p>
<pre><code>
B= tf.constant([[ 0.,  5.,  2.],[ 0.,  0.,  0.], [ 0.,  0.,  3.],[1.,5.,6.],[2.,5.,7.]])

def embeding_to_index(a_vector):
    return np.where(np.all(a_vector==B,axis=1))[0].tolist()[0]


c = tf.constant([[0.,  0.,  3.],[2.,5.,7.]])
arr =  tf.map_fn(fn=embeding_to_index,elems=c)
</code></pre>
<p>My expected result is to get a tensor [2 4], where 2 refers to index of the vector [0.,  0.,  3.] in the rows of the tensor B, and 4 refers to the index of the vector [2.,5.,7.] in the rows of the tensor B.</p>
<p>I got the following error:</p>
<pre><code>---------------------------------------------------------------------------
AxisError                                 Traceback (most recent call last)
&lt;ipython-input-8-de82c21bac35&gt; in &lt;module&gt;
     79 
     80 arr=tf.map_fn(fn=embeding_to_index,  # input &amp; output have different dtypes
---&gt; 81                     elems=c)
     82 # arr =  tf.vectorized_map(fn=embeding_to_index_,elems=U)

~\.conda\envs\test\lib\site-packages\tensorflow\python\ops\map_fn.py in map_fn(fn, elems, dtype, parallel_iterations, back_prop, swap_memory, infer_shape, name)
    266         back_prop=back_prop,
    267         swap_memory=swap_memory,
--&gt; 268         maximum_iterations=n)
    269     results_flat = [r.stack() for r in r_a]
    270 

...

AxisError: axis 1 is out of bounds for array of dimension 0
</code></pre>
<p>How can I solve this issue using the TensorFlow library? Is there any alternative to the <code>fn_map</code> method in TensorFlow?</p>
",14210738.0,,14210738.0,,2022-01-02 19:55:18,2022-01-03 18:53:20,How to use fn_map to map each row in an array C to its coresponding one in the array B,<python><python-3.x><numpy><tensorflow><anaconda>,1,2,,,,CC BY-SA 4.0
62926022,1,62932610.0,,2020-07-16 01:22:41,,4,919,"<p>I would like to try PEGASUS to summarize article. <a href=""https://github.com/google-research/pegasus"" rel=""nofollow noreferrer"">https://github.com/google-research/pegasus</a></p>
<p>The original repo's README suggests to use Google Cloud Compute Engine, but I use Colaboratory notebook. I'm an English teacher in Japan and I hope my students to try this software easily. They can experience both Machine Learning and English passage summarization.</p>
<p>I followed this instruction. <a href=""https://github.com/google-research/pegasus/tree/f76b63c2886748f7f5c6c9fb547456d8c6002562#setup"" rel=""nofollow noreferrer"">https://github.com/google-research/pegasus/tree/f76b63c2886748f7f5c6c9fb547456d8c6002562#setup</a></p>
<p>This is my colab notebook.
<a href=""https://colab.research.google.com/drive/1p95tZcjhfuCLYh23X3S_gZqRoWVhpIlE?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1p95tZcjhfuCLYh23X3S_gZqRoWVhpIlE?usp=sharing</a></p>
<p>This is my code in the notebook.</p>
<pre><code>%tensorflow_version 1.x

!git clone https://github.com/google-research/pegasus

!export PYTHONPATH=/content/pegasus

%pip install -r /content/pegasus/requirements.txt

!mkdir /content/pegasus/ckpt

!gsutil cp -r gs://pegasus_ckpt/ /content/pegasus/ckpt/

!python /content/pegasus/pegasus/bin/train.py --params=aeslc_transformer \
--param_overrides=vocab_filename=ckpt/pegasus_ckpt/c4.unigram.newline.10pct.96000.model \
--train_init_checkpoint=ckpt/pegasus_ckpt/model.ckpt-1500000 \
--model_dir=ckpt/pegasus_ckpt/aeslc
</code></pre>
<p>Then, I get this error message.</p>
<pre><code>Traceback (most recent call last):
  File &quot;/content/pegasus/pegasus/bin/train.py&quot;, line 17, in &lt;module&gt;
    from pegasus.data import infeed
ModuleNotFoundError: No module named 'pegasus'
</code></pre>
<p>This error message says that python can't import 'pegasus' module, but I made python path with <code>!export PYTHONPATH=/content/pegasus</code> this command.</p>
<p>Could you give me any advice, please?</p>
",7397906.0,,,,,2021-04-02 11:37:27,ModuleNotFoundError: No module named 'pegasus',<python><tensorflow><machine-learning><nlp><google-colaboratory>,1,0,,,,CC BY-SA 4.0
63511960,1,,,2020-08-20 19:26:29,,4,1309,"<p>(Novice question; sorry in advance.)</p>
<p>I have a pretrained (BigGAN) tensorflow model that another user developed to create images, and I would like to use it to generate new images. (If it helps to simplify the process, I do not need to train the model further; I just want to sample from the last checkpoint.) The model consists of the following four files:</p>
<pre><code>model.ckpt-607250.data-00000-of-00001
model.ckpt-607250.index
model.ckpt-607250.meta
operative_config-603500.gin
</code></pre>
<p>I've been trying to use Google Colab (with TPU processing enabled) to restore the model using the following code:</p>
<pre><code>import tensorflow as tf
import os

tf.compat.v1.enable_eager_execution()

model_path = &quot;/content/drive/My Drive/biggan/&quot;

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)

tf.compat.v1.disable_eager_execution()

model_path = &quot;/content/drive/My Drive/biggan/&quot;
saver = tf.compat.v1.train.import_meta_graph(model_path+'model.ckpt-607250.meta')
with tf.device('/TPU:0'):
  with tf.compat.v1.Session() as sess:
    saver.restore(sess, model_path+'model.ckpt-607250')
</code></pre>
<p>However, this triggers the following error:</p>
<pre><code>InvalidArgumentError                      Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1364     try:
-&gt; 1365       return fn(*args)
   1366     except errors.OpError as e:

9 frames

InvalidArgumentError: No OpKernel was registered to support Op 'TPUReplicatedInput' used by {{node input0}} with these attrs: [is_mirrored_variable=false, index=-1, T=DT_INT32, N=128, is_packed=false]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  &lt;no registered kernels&gt;

     [[input0]]


During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)


During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in restore(self, sess, save_path)
   1333       # We add a more reasonable error message here to help users (b/110263146)
   1334       raise _wrap_restore_error_with_msg(
-&gt; 1335           err, &quot;a mismatch between the current graph and the graph&quot;)
   1336 
   1337   @staticmethod

InvalidArgumentError: Restoring from checkpoint failed. This is most likely due to a mismatch between the current graph and the graph from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:

No OpKernel was registered to support Op 'TPUReplicatedInput' used by node input0 (defined at &lt;ipython-input-2-009a7264c288&gt;:11)  with these attrs: [is_mirrored_variable=false, index=-1, T=DT_INT32, N=128, is_packed=false]
Registered devices: [CPU, XLA_CPU]
Registered kernels:
  &lt;no registered kernels&gt;

     [[input0]]
</code></pre>
<p>No matter what I do, I keep running into the same error with <code>TPUReplicatedInput</code>, and I'm not sure how to resolve it. I've been struggling with this for a few weeks, so any help that you can offer would be greatly appreciated.</p>
",14085954.0,,,,,2022-01-14 22:57:30,"tensorflow error: ""No OpKernel was registered to support Op 'TPUReplicatedInput'"" when restoring model",<python><tensorflow><google-colaboratory><generative-adversarial-network><tpu>,1,1,0.0,,,CC BY-SA 4.0
67267305,1,67268537.0,,2021-04-26 13:11:57,,4,1270,"<p>I have a custom training loop that can be simplified as follow</p>
<pre><code>inputs = tf.keras.Input(dtype=tf.float32, shape=(None, None, 3))
model = tf.keras.Model({&quot;inputs&quot;: inputs}, {&quot;loss&quot;: f(inputs)})
optimizer = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=True)

for inputs in batches:
    with tf.GradientTape() as tape:
        results = model(inputs, training=True)
    grads = tape.gradient(results[&quot;loss&quot;], model.trainable_weights)
    optimizer.apply_gradients(zip(grads, model.trainable_weights))
</code></pre>
<p>The <a href=""https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage"" rel=""nofollow noreferrer"">TensorFlow documentation of ExponentialMovingAverage</a> is not clear on how it should be used in <a href=""https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch"" rel=""nofollow noreferrer"">from-scratch training loop</a>. As anyone worked with this?</p>
<p>Additionally, how should the shadow variable be restored into the model if both are still in memory, and how can I check that that training variables were correctly updated?</p>
",1782553.0,,1782553.0,,2021-04-27 08:42:55,2021-04-28 00:16:29,How should Exponential Moving Average be used in custom TF2.4 training loop,<tensorflow><tensorflow2.0>,2,0,0.0,,,CC BY-SA 4.0
74050233,1,,,2022-10-13 03:38:44,,4,17212,"<p>I am running the example Tensorflow convolutional neural network (CNN) code from &quot;Hands-on Machine Learning with Scikit-Learn, Keras &amp; TensorFlow&quot; (<a href=""https://github.com/ageron/handson-ml3"" rel=""nofollow noreferrer"">https://github.com/ageron/handson-ml3</a>). I run it on VS code on Windows 11. When I run the code of Chapter 14 and step it to</p>
<pre><code>fmaps = conv_layer(images)
</code></pre>
<p>The kernel crashed, prompting:</p>
<pre><code>Canceled future for execute_request message before replies were done
The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click here for more info. View Jupyter log for further details.
warn 20:31:46.130: StdErr from Kernel Process 2022-10-12 20:31:46.130634: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8301

error 20:31:46.685: Disposing session as kernel process died ExitCode: 3221226505, Reason: c:\ProgramData\Anaconda3\lib\site-packages\traitlets\traitlets.py:2202: FutureWarning: Supporting extra quotes around strings is deprecated in traitlets 5.0. You can use 'hmac-sha256' instead of '&quot;hmac-sha256&quot;' if you require traitlets &gt;=5.
  warn(
c:\ProgramData\Anaconda3\lib\site-packages\traitlets\traitlets.py:2157: FutureWarning: Supporting extra quotes around Bytes is deprecated in traitlets 5.0. Use 'c780d88a-4eda-4d9c-96ee-78c547d489d5' instead of 'b&quot;c780d88a-4eda-4d9c-96ee-78c547d489d5&quot;'.
  warn(
2022-10-12 20:30:39.777271: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-10-12 20:30:40.158222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21670 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6
2022-10-12 20:31:46.130634: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8301

info 20:31:46.685: Dispose Kernel process 17032.
error 20:31:46.685: Raw kernel process exited code: 3221226505
error 20:31:46.686: Error in waiting for cell to complete [Error: Canceled future for execute_request message before replies were done
</code></pre>
<p>The CUDA and GPU drivers should have been successfully installed on my Windows system. For instance, when running</p>
<pre><code>N=20000
x1=tf.random.Generator.from_seed(123).normal(shape=(N,N))
x2=tf.random.Generator.from_seed(124).normal(shape=(N,N))
x3=tf.matmul(x1,x2)
y1=np.random.rand(N,N)
y2=np.random.rand(N,N)
y3=np.matmul(y1,y2)
</code></pre>
<p>I can see from Windows Task Manager that the GPU is running and the calculation of x3 takes ~2 seconds while the calculation of y3 takes up to minutes.</p>
",10163506.0,,4685471.0,,2022-10-13 10:22:21,2023-04-06 12:43:46,Canceled future for execute_request message before replies were done,<tensorflow><machine-learning><computer-vision><conv-neural-network>,3,1,,,,CC BY-SA 4.0
66029426,1,,,2021-02-03 14:22:41,,4,5736,"<pre><code>2021-02-03 19:46:53.571084: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Invoking GPU asm compilation is supported on Cuda non-Windows platforms only
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
</code></pre>
<p>I am getting the above error when I am trying to train my deep nueral network. It doestn't stop training but the first iteration is not working fine. See below:</p>
<pre><code>Epoch 1/10
   1/1875 [..............................] - ETA: 0s - loss: 2.3001 - accuracy: 0.1250
WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0116s). Check your callbacks.
</code></pre>
<p>Help me to fix this error.</p>
<p>I am using tensorflow verson 2.3.0, cuda tool kit version 10.1.243, cudnn version 7.6.5 and my pc has NVDIA GEFORCE RTX 2070 with Super Max-Q design.</p>
<p>Some solutions say to downgrade tensorflow version, but for some projects I need 2.3.0, so please any one solve this error.</p>
",15138171.0,,4685471.0,,2021-02-03 14:59:01,2022-01-08 07:23:49,"Anaconda showing this error , can't train model properly",<python><tensorflow>,2,1,0.0,,,CC BY-SA 4.0
64707562,1,,,2020-11-06 01:15:26,,4,7615,"<p>I am trying to train my model using the RTX 3090 GPU.<br />
In order to be able to use it at all, i had to install <code>TensorFlow==2.4.0-rc0</code>, however, there is a problem with actually using that GPU.</p>
<p>(Yes, i have downclocked memory as it is getting really toasty while running at stock 19,5 Ghz, that is why memory bandwidth is 60 Gbps lower)</p>
<p>First of all, it detects GPU but then saying:</p>
<pre><code>tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s
</code></pre>
<p>Then it says:</p>
<pre><code>Adding visible gpu devices: 0
</code></pre>
<p>But a couple of lines below that message, this message is displayed:</p>
<pre><code>Created TensorFlow device 
(/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -&gt; 
physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
</code></pre>
<p>And then it just continues to hammer CPU and not actually using GPU at all. The most important part, when training is done purely on CPU, time to complete one epoch is around 80 seconds, however, when GPU is used, it wont be able to complete even a single epoch.</p>
<p><a href=""https://i.stack.imgur.com/GFMYV.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/GFMYV.png"" alt=""enter image description here"" /></a></p>
<p><strong>This is the complete text output of my Jupyter Notebook (when it is running)</strong></p>
<pre class=""lang-sh prettyprint-override""><code>[I 04:06:47.194 NotebookApp] Kernel started: e4bec12d-3d85-4019-9b5a-67d34a45acfc
[I 04:06:50.799 NotebookApp] Starting buffering for e4bec12d-3d85-4019-9b5a-67d34a45acfc:591585a545fe4d33977dac034060b33c
[I 04:06:51.031 NotebookApp] Kernel restarted: e4bec12d-3d85-4019-9b5a-67d34a45acfc
[I 04:06:51.557 NotebookApp] Restoring connection for e4bec12d-3d85-4019-9b5a-67d34a45acfc:591585a545fe4d33977dac034060b33c
[I 04:06:51.558 NotebookApp] Replaying 3 buffered messages
2020-11-06 04:06:53.766169: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-11-06 04:07:01.412837: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-11-06 04:07:01.420283: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2020-11-06 04:07:01.438547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s
2020-11-06 04:07:01.438675: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-11-06 04:07:01.450544: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-11-06 04:07:01.450698: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-11-06 04:07:01.453610: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-11-06 04:07:01.454496: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-11-06 04:07:01.457436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-11-06 04:07:01.459702: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-11-06 04:07:01.460296: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-11-06 04:07:01.460439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-11-06 04:07:01.461093: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-11-06 04:07:01.461751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce RTX 3090 computeCapability: 8.6
coreClock: 1.695GHz coreCount: 82 deviceMemorySize: 24.00GiB deviceMemoryBandwidth: 871.81GiB/s
2020-11-06 04:07:01.461854: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2020-11-06 04:07:01.462144: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-11-06 04:07:01.462407: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-11-06 04:07:01.462690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2020-11-06 04:07:01.462941: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2020-11-06 04:07:01.464597: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2020-11-06 04:07:01.464843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2020-11-06 04:07:01.465087: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2020-11-06 04:07:01.465348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2020-11-06 04:07:01.838515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-11-06 04:07:01.838596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2020-11-06 04:07:01.838999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2020-11-06 04:07:01.839431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21821 MB memory) -&gt; physical GPU (device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6)
2020-11-06 04:07:01.842196: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-11-06 04:07:10.441807: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2020-11-06 04:07:11.435159: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2020-11-06 04:07:12.026347: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2020-11-06 04:07:12.044635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
[I 04:08:47.169 NotebookApp] Saving file at /train_model.ipynb
2020-11-06 04:13:24.212460: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
</code></pre>
<p><strong>P.S. Update #1</strong><br />
It took 579 seconds to complete single epoch using GPU, while it used to take only 80 seconds to complete it on CPU</p>
",2057806.0,,,,,2022-09-18 01:06:30,TensorFlow ignores the RTX 3000 series GPU,<python><tensorflow>,4,3,0.0,,,CC BY-SA 4.0
64696478,1,,,2020-11-05 11:42:19,,4,500,"<p>I saw in many tutorials in the Tensorflow 2 documentation, that they use two different functions to deal with categorical features:</p>
<ul>
<li>tf.keras.layers.experimental.preprocessing.<strong>StringLookup</strong></li>
<li>tf.feature_column.<strong>categorical_column_with_vocabulary_list</strong></li>
</ul>
<p>From what I've seen, they work very similarly and they are used for the same things... And I have also seen that <strong>categorical_column_with_vocabulary_list</strong> existed in Tensorflow 1, but <strong>StringLookUp</strong> didn't.</p>
<p>Is anyone experienced with these two functions and able to explain their advantages/disadvantages from each other?</p>
<p>Thank you very much.</p>
",7788098.0,,,,,2020-11-05 11:42:19,Tensorflow 2: StringLookUp or categorical_column_with_vocabulary_list,<tensorflow><machine-learning><data-science><tensorflow2.0><categorical-data>,0,0,0.0,,,CC BY-SA 4.0
65067397,1,,,2020-11-30 02:29:39,,4,3674,"<p>I have been struggling with this problem for five days and read <a href=""https://github.com/DeepLabCut/DeepLabCut/issues/354"" rel=""nofollow noreferrer"">several posts</a> on StackOverflow, but still cannot get a clear clue of how to solve this problem.  <a href=""https://github.com/tensorflow/tensorflow/issues/33536"" rel=""nofollow noreferrer"">People who solved this issue</a> just recommended trying different NVIDIA driver versions until you find a lucky one that matches a CUDA version (10.1 mostly) for a specific GPU card.</p>
<p>I have an <strong>NVIDIA GeForce GTX 1015 Ti</strong> on one desktop (windows 10, 64-bit OS), and one <strong>NVIDIA GeForce RTX 2080Ti</strong> on another desktop (Windows 10, 64-bit system). I followed the hardware requirements on the <a href=""https://www.tensorflow.org/install/gpu"" rel=""nofollow noreferrer"">TensorFlow official website</a> to install GPU drivers (tried version 418.81 and 457.09 for the 1050 Ti GPU, and 432.00, 457.30 for the 2080 Ti GPU), CUDA Toolkit (10.1 for both desktops), and cuDNN (7.6.0 for both desktops) and modified the <code>PATH</code> environment variable finally. The TensorFlow version is 2.3.0, and the Python version is 3.7.9.</p>
<p>This works fun for an MNIST training dataset with this <a href=""https://www.tensorflow.org/datasets/keras_example"" rel=""nofollow noreferrer"">example code</a> from the TensorFlow website. But I always got below errors for both PCs when I run some custom code (I have a custom model inherited from Keras. Model):</p>
<p>I'm not using TensorFlow for traditional neural network training, but just taking advantage of the auto-differentiation mechanism for an optimization problem.</p>
<p>I don't think my custom code has a problem because it runs well on <strong>Google Colab</strong>. And the same code runs well on my friend's Linux system.</p>
<p>The code to reproduce the error (no problem running on Google Colab):</p>
<pre><code># -*- coding: utf-8 -*-
## This code runs well in the Google Colab GPU runtime
## Yuanhang Zhang &amp; Zheyuan Zhu, 12/1/2020, CREOL, UCF, Copyright reserved
## please contact yuanhangzhang@knights.ucf.edu if you want to use the code for research or publications
## all length units are in mm

import tensorflow as tf
import numpy as np
print('tensorflow version:',tf.__version__)

#%% ASM method
dx=np.float32(5e-3) # pixel size
N_obj= 64 # 512 

def tf_fft2d(x):
    with tf.name_scope('tf_fft2d'): # add name_scope, check in tensorboard
      x_shift = tf.signal.ifftshift(x)
      x_fft=tf.signal.fft2d(x_shift)
      y = tf.signal.fftshift(x_fft)
      return y

def tf_ifft2d(x):
    with tf.name_scope('tf_ifft2d'):
      x_shift = tf.signal.ifftshift(x)
      x_ifft=tf.signal.ifft2d(x_shift)
      y = tf.signal.fftshift(x_ifft)
      return y

# angular spectrum method (ASM), not band-limited
# @tf.function
def prop_ASM(Ein,z,wavelength,N_obj,dx):
    freq_obj = np.arange(-N_obj//2,N_obj//2,1)*(1/(dx*N_obj))
    kx = 2*np.pi*freq_obj
    ky = kx.copy()
    KX,KY = np.meshgrid(kx,ky)
    k0 = 2*np.pi/wavelength
    KZ_square = k0**2-KX**2-KY**2
    KZ_square[KZ_square&lt;0] = 0
    Q = np.exp(-1j*z*np.sqrt(KZ_square)) # transfer function of freespace
    with tf.name_scope('prop_ASM'):
      FFT_obj = tf_fft2d(Ein)
      Q_tf = tf.constant(Q,dtype=tf.complex64)
      Eout = tf_ifft2d(FFT_obj*Q_tf)
      return Eout

print('N_obj:',N_obj)

import matplotlib.pyplot as plt
import shutil
shutil.rmtree('__pycache__',ignore_errors=True) # Delete an entire directory tree
import os
os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;]='0' 

save_model_path='./models' 
save_mat_folder='./results' 
log_path='./tensorboard_log' # path to log training process
load_model_path = save_model_path

#%% inputs/ouputs for the optimization
x = (np.arange(N_obj,dtype = np.float32)-N_obj/2)*dx
y = (np.arange(N_obj,dtype = np.float32)-N_obj/2)*dx
x_c, y_c = np.meshgrid(x,y)

# input: Gaussian mode
e_in = np.zeros((N_obj, N_obj),dtype = np.float32)  # initialize input field
w_in = np.float32(5e-2)   # beam width

e = np.exp(-((x_c)**2+(y_c)**2)/w_in**2) # Gaussian beam spots array
I = np.sum(np.abs(e)**2)
e_in = e/np.sqrt(I) # normalize power

fig, ax = plt.subplots()
im=ax.imshow(e_in)
cbar=plt.colorbar(im)  
print('e_in shape:',e_in.shape)

# output: Hermite mode
e_out = np.zeros((N_obj, N_obj),dtype = np.float32)
w_out = np.float32(5e-2) # 30e-2
c = np.array([[0,0],[0,1]])
e = np.polynomial.hermite.hermgrid2d(np.sqrt(2)*x/w_out, np.sqrt(2)*y/w_out, c)*np.exp(-(x_c**2+y_c**2)/w_out**2)
e = np.float32(e)
I = np.sum(np.abs(e)**2)
e_out = e/np.sqrt(I) # power normalized

fig, ax = plt.subplots()
im=ax.imshow(e_out)
cbar=plt.colorbar(im)

print('e_out shape:',e_out.shape)

#%% optimization by GradientTape
z = 20 # propagating distance
lambda_design_list = np.array([1.550e-3],dtype = np.float32)

Ein = tf.constant(e_in, name = 'Ein', dtype = tf.complex64) # a 2D tensor
Eout = tf.constant(e_out, name = 'Eout', dtype = tf.complex64)

phi1 = tf.Variable(np.float32(np.ones((N_obj,N_obj))),name='phi1') # dtype: float32
phi2 = tf.Variable(np.float32(np.ones((N_obj,N_obj))),name='phi2')


def forward_propagate(Ein,z,lambda_design_list,N_obj,dx):
    E1_1 = prop_ASM(Ein,z,lambda_design_list[0],N_obj,dx) # used tf.signal.fft2d
    E1_mod_1 = E1_1*tf.exp(tf.complex(real=tf.zeros_like(phi1,dtype='float32'),imag=phi1))
    # E1_mod_1 = tf.math.multiply(E1_1,tf.exp(1j*phi1)) # element-wise muliply ?? not working !!
    E2_1 = prop_ASM(E1_mod_1,z,lambda_design_list[0],N_obj,dx)
    E2_mod_1 = E2_1*tf.exp(tf.complex(real=tf.zeros_like(phi2,dtype='float32'),imag=phi2)) 
    E_out = prop_ASM(E2_mod_1,z,lambda_design_list[0],N_obj,dx)
    # E_out = tf.math.multiply(E2_1,tf.exp(1j*phi2))
    return E_out

def loss_single(E_out, Eout): 
    coupling_eff = tf.sqrt(
        (tf.square(tf.reduce_sum(tf.math.real(E_out)*tf.math.real(Eout)+tf.math.imag(E_out)*tf.math.imag(Eout))) +
         tf.square(tf.reduce_sum(tf.math.imag(E_out)*tf.math.real(Eout)-tf.math.real(E_out)*tf.math.imag(Eout))) ))
    # or something simpler:
    # coupling_eff = tf.abs(tf.reduce_sum((tf.math.multiply(E_out,Eout))))
    loss = - coupling_eff
    return loss

variables = [phi1, phi2] # write variables in a list to optimize

# define optimizer
optimizer =  tf.keras.optimizers.Adam(learning_rate= 1e-2)
epoch_num = 20

for ii in tf.range(epoch_num):
  with tf.GradientTape() as tape:
    # this forward_propagate() function must be in the tape context! otherwise grads is None !!
    # the tape need to record the complete forward propagation 
    E_out = forward_propagate(Ein,z,lambda_design_list,N_obj,dx) 
    loss = loss_single(E_out, Eout)  
    tf.print('ii =:',ii,'coupling_eff =:',-loss)
    # print('watched variables in tape:',[var.name for var in tape.watched_variables()])

  # print(&quot;\n ===== calculate gradients now ====ERROR in NEXT LINE!!======\n\n&quot;)
  grads = tape.gradient(loss, variables) ## auto-differentiation
  # print(grads)

  # TensorFlow will update parameters automatically
  optimizer.apply_gradients(grads_and_vars=zip(grads, variables))
</code></pre>
<p>The kernel dies at <code>grads = tape.gradient(loss, variables)</code></p>
<p>errors for both PCs:</p>
<pre><code>2020-11-29 20:41:57.457271: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-11-29 20:41:57.457480: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1
[I 20:42:05.512 NotebookApp] KernelRestarter: restarting kernel (1/5), keep random ports
</code></pre>
<p>Could anyone tell me how to solve this issue? Is trying different versions of drivers blindly the only way to make it work?</p>
<p><strong>The weird thing</strong> is there is no such error if I run a neural network training with Keras API <a href=""https://www.tensorflow.org/datasets/keras_example"" rel=""nofollow noreferrer"">this example</a> on the PC. And if I write some very simple code with <code>GradientTape</code> to calculate gradients <a href=""https://tf.wiki/en/basic/basic.html"" rel=""nofollow noreferrer"">this linear regression example</a>, there is no error either... In this way, it seems the driver is installed correctly ...Really confusing</p>
",14230467.0,,681865.0,,2020-12-24 11:21:38,2022-09-29 21:41:03,Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure,<python><tensorflow><gradienttape>,1,9,0.0,,,CC BY-SA 4.0
66118532,1,,,2021-02-09 11:56:27,,4,641,"<p>I recently moved to tensorflow 2 from tensorflow 1.x -gpu. But TF is not recognizing the GPU anymore.
I use Spyder with a NON-ANACONDA interpreter. I use pip to manage my packages. OS is Windows 10. The GPU is a GTX 1660 Ti, so it does support CUDA.</p>
<ul>
<li>Py version -- Python 3.7</li>
<li>Current TF version -- 2.4.0</li>
<li>Current CUDA version -- 11.1</li>
<li>Current CUDNN version -- 8.0.4.30</li>
</ul>
<p>-I have copied all the CUDNN dlls after extraction as specified here -- <a href=""https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html"" rel=""nofollow noreferrer"">https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html</a></p>
<p>-All the necessary path variables have been added as well. I have Visual Studio 2019 as well as Visual Code installed from before (when
I used to run tensorflow on my GPU)</p>
<pre><code>NVIDIA SMI output :-
C:\WINDOWS\system32&gt;nvidia-smi
Tue Feb  9 17:08:11 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 461.40       Driver Version: 461.40       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 166... WDDM  | 00000000:01:00.0 Off |                  N/A |
| N/A   76C    P0    26W /  N/A |    273MiB /  6144MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     15272    C+G   ...n64\EpicGamesLauncher.exe    N/A      |
|    0   N/A  N/A     16500      C   ...iles\Python37\pythonw.exe    N/A      |
+-----------------------------------------------------------------------------+


nvcc --version :-
C:\WINDOWS\system32&gt;nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2020 NVIDIA Corporation
Built on Mon_Oct_12_20:54:10_Pacific_Daylight_Time_2020
Cuda compilation tools, release 11.1, V11.1.105
Build cuda_11.1.relgpu_drvr455TC455_06.29190527_0

tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)
Out[9]: False
</code></pre>
<p>As per <a href=""https://www.tensorflow.org/install/source"" rel=""nofollow noreferrer"">https://www.tensorflow.org/install/source</a> I have the correct combination of CUDNN and CUDA installed. Yet, tf doesn't recognize my gpu.
Thanks in advance for your time :)</p>
<p>Edit 1 -- Added the log received after importing tensorflow in cmd
Edit 2 -- Added log level as specified in <a href=""https://stackoverflow.com/questions/44853059/tensorflow-logging-messages-do-not-appear/49756653"">Tensorflow logging messages do not appear</a></p>
<pre><code>&gt;&gt;&gt; os.environ['TF_CPP_MIN_VLOG_LEVEL']='3'
&gt;&gt;&gt; os.environ['TF_CPP_MIN_LOG_LEVEL']='0'
&gt;&gt;&gt; import tensorflow as tf
2021-02-09 18:10:16.489113:Itensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
    2021-02-09 18:10:16.558283: I tensorflow/core/platform/cloud/gcs_file_system.cc:804] GCS cache max size = 0 ; block size = 67108864 ; max staleness = 0
    2021-02-09 18:10:16.562188: I .\tensorflow/core/platform/cloud/ram_file_block_cache.h:64] GCS file block cache is disabled
    2021-02-09 18:10:16.565031: I tensorflow/core/platform/cloud/gcs_file_system.cc:844] GCS DNS cache is disabled, because GCS_RESOLVE_REFRESH_SECS = 0 (or is not set)
    2021-02-09 18:10:16.568337: I tensorflow/core/platform/cloud/gcs_file_system.cc:874] GCS additional header DISABLED. No environment variable set.
</code></pre>
<p>Edit 2 -- Adding evidence where PyTorch registers the GPU as mentioned in <a href=""https://stackoverflow.com/questions/48152674/how-to-check-if-pytorch-is-using-the-gpu"">How to check if pytorch is using the GPU?</a></p>
<pre><code>import torch

torch.cuda.current_device()
Out[2]: 0

torch.cuda.device(0)
Out[3]: &lt;torch.cuda.device at 0x222be69a8c8&gt;

torch.cuda.device_count()
Out[4]: 1

torch.cuda.get_device_name(0)
Out[5]: 'GeForce GTX 1660 Ti'

torch.cuda.is_available()
Out[6]: True
</code></pre>
",7940921.0,,7940921.0,,2021-02-09 13:59:26,2021-07-18 20:17:50,Tensorflow 2.4.0 not detecting GPU despite using compatible versions,<python><python-3.x><tensorflow>,0,7,0.0,,,CC BY-SA 4.0
66118638,1,,,2021-02-09 12:03:48,,4,1860,"<p>I wonder if I can use a Tensorflow Dataset for training scikit-learn and other ML frameworks.</p>
<p>So, for example, can I take a <code>tf.data.dataset</code> for training xgboost, LogisticReg, RandomForest classifier etc?
i.e. Can I pass the <code>tf.data.dataset</code> object into the <code>.fit()</code> method of these models, for training?</p>
<p>I tried out:</p>
<pre class=""lang-py prettyprint-override""><code>    xs=np.asarray([i for i in range(10000)]).reshape(-1, 1)
    ys=np.asarray([int(i%2==0)for i in range(10000)])
    
    xs = tf.data.Dataset.from_tensor_slices(xs)
    ys = tf.data.Dataset.from_tensor_slices(ys)
    cls.fit(xs, ys)
</code></pre>
<p>I'm getting the following error:</p>
<pre class=""lang-sh prettyprint-override""><code>    TypeError: float() argument must be a string or a number, not 'TensorSliceDataset'
</code></pre>
",14584019.0,,11652623.0,,2021-02-12 07:51:27,2021-02-12 07:51:27,How to train sklearn models using tensorflow dataset?,<tensorflow><scikit-learn><tensorflow2.0><tensorflow-datasets>,1,2,0.0,,,CC BY-SA 4.0
64380076,1,,,2020-10-15 21:43:17,,4,235,"<p>Did anyone had saved tf tokenzied data on Mlflow and loaded it for prediction in production code? I know there is a method to write as json or pickle but I have no idea how to send it to Mlflow and load it back from there... Any suggestions how to resolve this issue? Or maybe you found completely differnet solution?</p>
<p>Thank you</p>
",3964758.0,,,,,2020-10-15 21:43:17,How to save Tensorflow tokenized text on Mlflow for predictions?,<tensorflow><tokenize><mlflow>,0,2,0.0,,,CC BY-SA 4.0
66705900,1,,,2021-03-19 09:49:46,,4,922,"<p>I want install Tensorflow Federated in a macOS with Apple Silicon M1.</p>
<p>I have tried installing Python 3.9.1 with pyenv and create a virtual environment. Then I installed the package.</p>
<pre class=""lang-sh prettyprint-override""><code>pip install tensorflow-federated
</code></pre>
<p>Some errors raised because of dependency conflict when installed Tensorflow Federated. You can see partial of error logs below:</p>
<pre><code>...

    clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
    clang: error: the clang compiler does not support 'faltivec', please use -maltivec and include altivec.h explicitly
    error: Command &quot;clang -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include -I/opt/homebrew/opt/openssl/include -I/opt/homebrew/opt/readline/include -DNPY_INTERNAL_BUILD=1 -DHAVE_NPY_CONFIG_H=1 -D_FILE_OFFSET_BITS=64 -D_LARGEFILE_SOURCE=1 -D_LARGEFILE64_SOURCE=1 -DNO_ATLAS_INFO=3 -DHAVE_CBLAS -Ibuild/src.macosx-11.2-arm64-3.9/numpy/core/src/umath -Ibuild/src.macosx-11.2-arm64-3.9/numpy/core/src/npymath -Ibuild/src.macosx-11.2-arm64-3.9/numpy/core/src/common -Inumpy/core/include -Ibuild/src.macosx-11.2-arm64-3.9/numpy/core/include/numpy -Inumpy/core/src/common -Inumpy/core/src -Inumpy/core -Inumpy/core/src/npymath -Inumpy/core/src/multiarray -Inumpy/core/src/umath -Inumpy/core/src/npysort -I/Users/user/projects/ucloud/federated/venv/include -I/Users/user/.pyenv/versions/3.9.1/include/python3.9 -Ibuild/src.macosx-11.2-arm64-3.9/numpy/core/src/common -Ibuild/src.macosx-11.2-arm64-3.9/numpy/core/src/npymath -c numpy/core/src/multiarray/array_assign_scalar.c -o build/temp.macosx-11.2-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o -MMD -MF build/temp.macosx-11.2-arm64-3.9/numpy/core/src/multiarray/array_assign_scalar.o.d -faltivec -I/System/Library/Frameworks/vecLib.framework/Headers&quot; failed with exit status 1
    ----------------------------------------
    ERROR: Failed building wheel for numpy
  Failed to build numpy
  ERROR: Could not build wheels for numpy which use PEP 517 and cannot be installed directly
  ----------------------------------------
WARNING: Discarding https://files.pythonhosted.org/packages/15/fb/86d26128a5ea42d20f402109e76a63e59845d73171887a08a43a28b847dc/h5py-3.0.0.tar.gz#sha256=7d3803be1b530c68c2955faba726dc0f591079b68941a0c0269b5384a42ab519 (from https://pypi.org/simple/h5py/) (requires-python:&gt;=3.6). Command errored out with exit status 1: /Users/user/projects/ucloud/federated/venv/bin/python /Users/user/projects/ucloud/federated/venv/lib/python3.9/site-packages/pip install --ignore-installed --no-user --prefix /private/var/folders/14/3fhr7hps0yvfd43qp60rbw480000gn/T/pip-build-env-9ws6euzt/normal --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- 'numpy==1.12; python_version == &quot;3.6&quot;' pkgconfig 'numpy==1.19.3; python_version &gt;= &quot;3.9&quot;' 'numpy==1.14.5; python_version == &quot;3.7&quot;' 'numpy==1.17.5; python_version == &quot;3.8&quot;' 'Cython&gt;=0.29.14; python_version &gt;= &quot;3.8&quot;' 'Cython&gt;=0.29; python_version &lt; &quot;3.8&quot;' Check the logs for full command output.
ERROR: Cannot install tensorflow-federated==0.1.0, tensorflow-federated==0.10.0, tensorflow-federated==0.10.1, tensorflow-federated==0.11.0, tensorflow-federated==0.12.0, tensorflow-federated==0.13.0, tensorflow-federated==0.13.1, tensorflow-federated==0.14.0, tensorflow-federated==0.15.0, tensorflow-federated==0.16.0, tensorflow-federated==0.16.1, tensorflow-federated==0.17.0, tensorflow-federated==0.18.0, tensorflow-federated==0.2.0, tensorflow-federated==0.3.0, tensorflow-federated==0.4.0, tensorflow-federated==0.5.0, tensorflow-federated==0.6.0, tensorflow-federated==0.7.0 and tensorflow-federated==0.9.0 because these package versions have conflicting dependencies.

The conflict is caused by:
    tensorflow-federated 0.18.0 depends on tensorflow~=2.4.0
    tensorflow-federated 0.17.0 depends on tensorflow-addons~=0.11.1
    tensorflow-federated 0.16.1 depends on tensorflow~=2.2.0
    tensorflow-federated 0.16.0 depends on tensorflow~=2.2.0
    tensorflow-federated 0.15.0 depends on tensorflow~=2.2.0
    tensorflow-federated 0.14.0 depends on tensorflow~=2.2.0
    tensorflow-federated 0.13.1 depends on tensorflow~=2.1.0
    tensorflow-federated 0.13.0 depends on tensorflow~=2.1.0
    tensorflow-federated 0.12.0 depends on tensorflow-addons~=0.7.0
    tensorflow-federated 0.11.0 depends on tensorflow~=2.0.0
    tensorflow-federated 0.10.1 depends on tensorflow-addons~=0.6.0
    tensorflow-federated 0.10.0 depends on tensorflow-addons~=0.6.0
    tensorflow-federated 0.9.0 depends on tfa-nightly
    tensorflow-federated 0.7.0 depends on tf-nightly
    tensorflow-federated 0.6.0 depends on tf-nightly
    tensorflow-federated 0.5.0 depends on tf-nightly
    tensorflow-federated 0.4.0 depends on tensorflow~=1.13
    tensorflow-federated 0.3.0 depends on tensorflow~=1.13
    tensorflow-federated 0.2.0 depends on tensorflow~=1.13
    tensorflow-federated 0.1.0 depends on tensorflow&gt;=1.13.0rc2

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies
</code></pre>
<p>I guess it happened because some required packages is not ready to satisfy the Apple Silicon M1 with the ARM architecture? Am I right?</p>
<p>Is there any solution to install Tensorflow Federated in a macOS with Apple Silicon M1 now?</p>
<hr />
<p>Edited on Mar 20, 2021:</p>
<p>Thanks for the <a href=""https://stackoverflow.com/questions/66705900/can-tensorflow-federated-be-installed-on-apple-silicon-m1#comment117928552_66705900"">answer</a> from the user <a href=""https://stackoverflow.com/users/7976758/phd"">phd</a>.</p>
<p>The problem of numpy is solved by following <a href=""https://github.com/numpy/numpy/issues/17807#issuecomment-731014921"" rel=""nofollow noreferrer"">this commend</a>.</p>
<p>But the dependency conflict is still here.</p>
<pre><code>    Preparing wheel metadata ... done
Collecting h5py
  Using cached h5py-3.2.1.tar.gz (368 kB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... done
  Installing backend dependencies ... |^[[-^[[done
    Preparing wheel metadata ... done
ERROR: Cannot install tensorflow-federated==0.1.0, tensorflow-federated==0.10.0, tensorflow-federated==0.10.1, tensorflow-federated==0.11.0, tensorflow-federated==0.12.0, tensorflow-federated==0.13.0, tensorflow-federated==0.13.1, tensorflow-federated==0.14.0, tensorflow-federated==0.15.0, tensorflow-federated==0.16.0, tensorflow-federated==0.16.1, tensorflow-federated==0.17.0, tensorflow-federated==0.18.0, tensorflow-federated==0.2.0, tensorflow-federated==0.3.0, tensorflow-federated==0.4.0, tensorflow-federated==0.5.0, tensorflow-federated==0.6.0, tensorflow-federated==0.7.0 and tensorflow-federated==0.9.0 because these package versions have conflicting dependencies.

The conflict is caused by:
    tensorflow-federated 0.18.0 depends on tensorflow~=2.4.0
    tensorflow-federated 0.17.0 depends on tensorflow~=2.3.0
    tensorflow-federated 0.16.1 depends on tensorflow-addons~=0.10.0
    tensorflow-federated 0.16.0 depends on tensorflow-addons~=0.10.0
    tensorflow-federated 0.15.0 depends on tensorflow-addons~=0.10.0
    tensorflow-federated 0.14.0 depends on tensorflow-addons~=0.9.1
    tensorflow-federated 0.13.1 depends on tensorflow~=2.1.0
    tensorflow-federated 0.13.0 depends on tensorflow~=2.1.0
    tensorflow-federated 0.12.0 depends on tensorflow~=2.1.0
    tensorflow-federated 0.11.0 depends on tensorflow-addons~=0.6.0
    tensorflow-federated 0.10.1 depends on tensorflow-addons~=0.6.0
    tensorflow-federated 0.10.0 depends on tensorflow-addons~=0.6.0
    tensorflow-federated 0.9.0 depends on tfa-nightly
    tensorflow-federated 0.7.0 depends on tf-nightly
    tensorflow-federated 0.6.0 depends on tf-nightly
    tensorflow-federated 0.5.0 depends on tf-nightly
    tensorflow-federated 0.4.0 depends on tensorflow~=1.13
    tensorflow-federated 0.3.0 depends on tensorflow~=1.13
    tensorflow-federated 0.2.0 depends on tensorflow~=1.13
    tensorflow-federated 0.1.0 depends on tensorflow&gt;=1.13.0rc2

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/user_guide/#fixing-conflicting-dependencies
</code></pre>
",6522746.0,,6522746.0,,2021-03-20 14:43:55,2022-05-31 19:26:27,Can Tensorflow Federated be installed on Apple Silicon M1?,<python-3.x><numpy><tensorflow><pip><tensorflow-federated>,1,3,0.0,,,CC BY-SA 4.0
64347802,1,,,2020-10-14 06:34:44,,4,203,"<p>Explicit data sets such as <a href=""https://grouplens.org/datasets/movielens/"" rel=""noreferrer"">MovieLens</a> work according to a ranking, for example a user gives one to five stars for a film.
Implicit data like <a href=""https://www.kaggle.com/retailrocket/ecommerce-dataset"" rel=""noreferrer"">RetailRocket</a>, for example, work with events for example user looks at the article, adds it to the shopping cart, pays for the article.</p>
<p>I am currently working with the <a href=""https://github.com/tensorflow/models/tree/master/official/recommendation"" rel=""noreferrer""><code>neuMF</code> model from TensorFlow</a>.
This works with MovieLens. How do I put my implicit data set (RetailRocket) in the <code>neuMF</code> model ?</p>
<p>Do I have to convert the implicit data set to an explicit data set first?
(So ​​I give each event an assignment, for example addtoCart = 3, this would then be a ranking again)</p>
<p>If I don't need to convert it, how do I put these events (strings) into the model?</p>
",,user7597016,,,,2020-10-16 09:37:06,Working with an implicit data set in the neuMF model,<python><tensorflow><deep-learning><neural-network><dataset>,0,0,,,,CC BY-SA 4.0
66711706,1,70174417.0,,2021-03-19 16:07:35,,4,1519,"<p>The <a href=""https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html#to-jit-or-not-to-jit"" rel=""nofollow noreferrer"">documentation for JAX</a> says,</p>
<blockquote>
<p>Not all JAX code can be JIT compiled, as it requires array shapes to be static &amp; known at compile time.</p>
</blockquote>
<p>Now I am somewhat surprised because tensorflow has operations like <code>tf.boolean_mask</code> that does what JAX seems incapable of doing when compiled.</p>
<ol>
<li>Why is there such a regression from Tensorflow? I was under the assumption that the underlying XLA representation was shared between the two frameworks, but I may be mistaken. I don't recall Tensorflow ever having troubles with dynamic shapes, and functions such as <code>tf.boolean_mask</code> have been around forever.</li>
<li>Can we expect this gap to close in the future? If not, why makes it impossible to do in JAX' jit what Tensorflow (among others) enables?</li>
</ol>
<p><strong>EDIT</strong></p>
<p>The gradient passes through <code>tf.boolean_mask</code> (obviously not on mask values, which are discrete); case in point here using TF1-style graphs where values are unknown, so TF cannot rely on them:</p>
<pre class=""lang-py prettyprint-override""><code>import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()

x1 = tf.placeholder(tf.float32, (3,))
x2 = tf.placeholder(tf.float32, (3,))
y = tf.boolean_mask(x1, x2 &gt; 0)
print(y.shape)  # prints &quot;(?,)&quot;
dydx1, dydx2 = tf.gradients(y, [x1, x2])
assert dydx1 is not None and dydx2 is None
</code></pre>
",9973879.0,,9973879.0,,2021-05-06 07:03:07,2021-11-30 18:36:06,"Jax, jit and dynamic shapes: a regression from Tensorflow?",<python><tensorflow><jax>,2,4,,,,CC BY-SA 4.0
63641000,1,63677058.0,,2020-08-28 21:21:35,,4,3328,"<p>I'm currently working on a desktop tool in .NET Framework 4.8 that takes in a list of images with potential cracks and uses a model trained with ML.Net (C#) to perform crack detection. Ideally, I'd like the prediction to take less than 100ms on 10 images (Note: a single image prediction takes between 36-41ms).</p>
<p>At first, I tried performing multiple predictions in different threads using a list of PredictionEngines and a Parallel.For-loop (using a list of threads since there is no PredictionEnginePool implementation for .Net Framework). I later learned that using an ITransformer to do predictions is a recommended, thread-safe, approach for .Net Framework and moved to using that, but in both cases it did not give me the performance I was hoping for.</p>
<p>It takes around 255-281ms (267.1ms on average) to execute the following code:</p>
<pre><code>    MLContext mlContext = new MLContext();
    IDataView inputData = mlContext.Data.LoadFromEnumerable(inputDataEnumerable);
    IDataView results = _LoadedModel.Transform(inputData);
    var imageClassificationPredictions = mlContext.Data.CreateEnumerable&lt;ImageClassificationPrediction&gt;(results, false).ToList();
</code></pre>
<p>Where <em>_LoadedModel</em> is an ITransformer representing the previously trained and loaded model, and <em>inputDataEnumerable</em> is a list of <em>ModelInput</em> which contains two properties: ImageData (byte[] of image data extracted from a png image) and Label (string type, set to null).</p>
<p>I tried to speed up this process by switching the TensorFlow package dependency from <code>SciSharp.TensorFlow.Redist</code> to
<code>SciSharp.TensorFlow.Redist-Windows-GPU</code>
as described in this <a href=""https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/getting-started/DeepLearning_ImageClassification_Training"" rel=""nofollow noreferrer"">tutorial</a>.</p>
<p>However, the execution time remained pretty much the same (average of 262.4ms for 10 images). I also tried comparing the training times on a small data set of 5760 images and couldn't see much of a difference (both took about 7min 21s).</p>
<p>From these results it seemed like it wasn't using the GPU, so I first tried deleting the bin folders of my projects and removing the old CPU-oriented tensorflow package (in case it was a simple build issue).
When that didn't help, I reinstalled CUDA 10.0, following instructions described <a href=""https://github.com/dotnet/machinelearning/blob/master/docs/api-reference/tensorflow-usage.md"" rel=""nofollow noreferrer"">here</a>. I also double checked that CUDA was working properly with my graphics card by running a few of the sample projects (DeviceQuery, DeviceQueryDrv, and bandwidthTest) just to be sure the card is actually compatible, and those ran just fine.</p>
<p>At this point it seems like I've set something up wrong or the GPU is just not applicable for my particular use case, but I can't pin-point which it is. According to the <a href=""https://github.com/dotnet/machinelearning-samples/tree/master/samples/csharp/getting-started/DeepLearning_ImageClassification_Training"" rel=""nofollow noreferrer"">tutorial</a> I was following, GPU acceleration should be available for predictions, but I'm not seeing any significant differences in execution time after trying to use the GPU.</p>
<p>If anyone has any suggests for further troubleshooting steps I can take, or if they have an idea about where I went wrong, or if they think this is the wrong use case, I'd greatly appreciate any help/feedback.</p>
<p>If it helps, here are some system specs:</p>
<ul>
<li>OS: Windows 10 Pro</li>
<li>CPU: Intel(R) Xeon(R) CPU E3-1275 v5 @ 3.60GHz</li>
<li>RAM: 16.0 GB</li>
<li>GPU: Quadro P1000 (Installed Driver: version 452.06)</li>
</ul>
<p>here are the ML.Packages (Version) I'm running:</p>
<ul>
<li>Microsoft.ML (v1.5.0)</li>
<li>Microsoft.ML.ImageAnalytics (v1.5.0)</li>
<li>Microsoft.ML.TensorFlow (v1.5.0)</li>
<li>Microsoft.ML.Vision (v1.5.0)</li>
<li>SciSharp.TensorFlow.Redist-Windows-GPU (v1.15.1)</li>
</ul>
<p>and for GPU support I've installed <a href=""https://developer.nvidia.com/cuda-10.0-download-archive"" rel=""nofollow noreferrer"">CUDA v10.0</a> along with <a href=""https://developer.nvidia.com/rdp/cudnn-download"" rel=""nofollow noreferrer"">CUDNN v7.6.4</a>.</p>
<p><strong>Edit</strong></p>
<p>The issue turned out to not be ML.Net specific, but rather related to TensorFlow.Net. After I updated the SciSharp.TensorFlow.Redist-Windows-GPU to version 2.3.0 (released 8/31/2020), I updated CUDA to 10.1, and followed guidance from the TensorFlow.Net <a href=""https://github.com/SciSharp/TensorFlow.NET/wiki/Using-GPU-with-Tensorflow.NET"" rel=""nofollow noreferrer"">GitHub</a> which had some slightly different steps for getting GPU support to work. I can now get the 10 predictions in less than 50ms which is even better than my target.</p>
",14182918.0,,14182918.0,,2020-08-31 21:20:47,2021-01-22 14:19:33,C# ML.Net Image classification: Does GPU acceleration help improve the performance of predictions and how can I tell if it is?,<c#><windows><tensorflow><prediction><ml.net>,1,0,,,,CC BY-SA 4.0
66718587,1,66724881.0,,2021-03-20 05:41:50,,4,233,"<p>I have a matrix which looks like this:</p>
<pre><code>[[5,2],
[4,3],
[3,4]]
</code></pre>
<p>Using the command</p>
<pre><code>tf.tensor([...])
</code></pre>
<p>And I would like to the matrix by the column with index 0, so that it will look like this:</p>
<pre><code>[[3,4],
[4,3],
[5,2]]
</code></pre>
<p>How would I do that using Tensorflow.js?</p>
",12504573.0,,13302.0,,2021-03-20 10:51:22,2022-09-25 09:37:16,Sorting a matrix by column,<javascript><tensorflow><matrix><tensorflow.js>,3,0,,,,CC BY-SA 4.0
64659643,1,65689204.0,,2020-11-03 09:01:54,,4,1816,"<p>I am following the Tensorflow notebook for Few shot learning ( <a href=""https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb#scrollTo=RW1FrT2iNnpy"" rel=""noreferrer"">https://colab.research.google.com/github/tensorflow/models/blob/master/research/object_detection/colab_tutorials/eager_few_shot_od_training_tf2_colab.ipynb#scrollTo=RW1FrT2iNnpy</a> )</p>
<p>In it, I saw that they were annotating the images using colab_utils.annotate(). I can't understand the annotation format they are using (like YOLO or COCO format). Another problem is that we can't specify the classes at the time when we are drawing the bounding boxes and I have to remember the order in which I annotate the different images and classes so I can add them by code later on.</p>
<p>If someone can tell me what's that format so I can annotate the images on my PC locally rather than on COLAB which will save a lot of time.</p>
<p>Any help would be appreciated.
Regards</p>
",12636204.0,,,,,2021-01-12 17:45:48,"colab_utils.annotate(), annotation format",<tensorflow><deep-learning><google-colaboratory><tensorflow-model-garden>,1,0,0.0,,,CC BY-SA 4.0
69718141,1,,,2021-10-26 06:07:15,,4,207,"<p>I am little new in this domain. So, I apologize in advance if it's something very basic.
I am currently trying to follow this link <a href=""https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_detection_qat_tf1.ipynb#scrollTo=6RxtslKJf2td"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_detection_qat_tf1.ipynb#scrollTo=6RxtslKJf2td</a>
for training a quantization aware model using TensorFlow object detection API. I am using mobilenet_v2_quant_aware as my model. Images are 1000 with 800 train and 200 train. <strong>Problem that I am facing is that the loss is not converging</strong>.  This is the pipeline:</p>
<pre><code>    model {
  SSD {
    num_classes: 1
    image_resizer {
      fixed_shape_resizer {
        height: 300
        width: 300
      }
    }
    feature_extractor {
      type: &quot;ssd_mobilenet_v2&quot;
      depth_multiplier: 1.0
      min_depth: 16
      conv_hyperparams {
        regularizer {
          l2_regularizer {
            weight: 3.99999989895e-05
          }
        }
        initializer {
          truncated_normal_initializer {
            mean: 0.0
            stddev: 0.0299999993294
          }
        }
        activation: RELU_6
        batch_norm {
          decay: 0.999700009823
          center: true
          scale: true
          epsilon: 0.0010000000475
          train: true
        }
      }
    }
    box_coder {
      faster_rcnn_box_coder {
        y_scale: 10.0
        x_scale: 10.0
        height_scale: 5.0
        width_scale: 5.0
      }
    }
    matcher {
      argmax_matcher {
        matched_threshold: 0.5
        unmatched_threshold: 0.5
        ignore_thresholds: false
        negatives_lower_than_unmatched: true
        force_match_for_each_row: true
      }
    }
    similarity_calculator {
      iou_similarity {
      }
    }
    box_predictor {
      convolutional_box_predictor {
        conv_hyperparams {
          regularizer {
            l2_regularizer {
              weight: 3.99999989895e-05
            }
          }
          initializer {
            truncated_normal_initializer {
              mean: 0.0
              stddev: 0.0299999993294
            }
          }
          activation: RELU_6
          batch_norm {
            decay: 0.999700009823
            center: true
            scale: true
            epsilon: 0.0010000000475
            train: true
          }
        }
        min_depth: 0
        max_depth: 0
        num_layers_before_predictor: 0
        use_dropout: false
        dropout_keep_probability: 0.800000011921
        kernel_size: 1
        box_code_size: 4
        apply_sigmoid_to_scores: false
      }
    }
    anchor_generator {
      ssd_anchor_generator {
        num_layers: 6
        min_scale: 0.20000000298
        max_scale: 0.949999988079
        aspect_ratios: 1.0
        aspect_ratios: 2.0
        aspect_ratios: 0.5
        aspect_ratios: 3.0
        aspect_ratios: 0.333299994469
      }
    }
    post_processing {
      batch_non_max_suppression {
        score_threshold: 9.99999993923e-09
        iou_threshold: 0.600000023842
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SIGMOID
    }
    normalize_loss_by_num_matches: true
    loss {
      localization_loss {
        weighted_smooth_l1 {
        }
      }
      classification_loss {
        weighted_sigmoid {
        }
      }
      hard_example_miner {
        num_hard_examples: 3000
        iou_threshold: 0.990000009537
        loss_type: CLASSIFICATION
        max_negatives_per_positive: 3
        min_negatives_per_image: 3
      }
      classification_weight: 1.0
      localization_weight: 1.0
    }
  }
}
train_config {
  batch_size: 8
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
  optimizer {
    rms_prop_optimizer {
      learning_rate {
        exponential_decay_learning_rate { 
          initial_learning_rate: 0.00500000018999
          decay_steps: 10000
          decay_factor: 0.949999988079
        }
      }
      momentum_optimizer_value: 0.899999976158
      decay: 0.899999976158
      epsilon: 1.0
    }
  }
  fine_tune_checkpoint: &quot;/home/syslab3/Documents/TF_9/tod_tf1/pre_trained/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt&quot;
  from_detection_checkpoint: true
  num_steps: 50000
}
train_input_reader {
  label_map_path: &quot;/home/syslab3/Documents/TF_9/tod_tf1/training/objectDetection/fire_label.pbtxt&quot;
  tf_record_input_reader {
    input_path: &quot;/home/syslab3/Documents/TF_9/tod_tf1/training/objectDetection/train.tfrecord&quot;
  }
}
eval_config {
  num_examples: 8000
  metrics_set: &quot;coco_detection_metrics&quot;
  use_moving_averages: true
  include_metrics_per_category: true
   
}
eval_input_reader {
  label_map_path: &quot;/home/syslab3/Documents/TF_9/tod_tf1/training/objectDetection/fire_label.pbtxt&quot;
  shuffle: false
  num_readers: 1
  tf_record_input_reader {
    input_path: &quot;/home/syslab3/Documents/TF_9/tod_tf1/training/objectDetection/test.tfrecord&quot;
  }
 
}
graph_rewriter {
  quantization {
    delay: 48000
    weight_bits: 8
    activation_bits: 8
  }
}
</code></pre>
<p>Can you please suggest some changes that I can do or should I increase my dataset if that is causing the problem?</p>
<p>I tried different batch sizes, learning rates, decay steps, and decay factors but It didn't help!</p>
<p>TensorFlow-gpu 1.15
GPU = Nvidia gtx 780</p>
<p>Below is the loss graphs:
<a href=""https://i.stack.imgur.com/fLElk.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/fLElk.png"" alt=""enter image description here"" /></a></p>
",11846587.0,,11846587.0,,2021-10-26 08:28:36,2021-10-26 08:28:36,Quantization aware training,<tensorflow><object-detection-api><google-coral><mobilenet><quantization-aware-training>,0,0,,,,CC BY-SA 4.0
69353325,1,,,2021-09-27 21:14:15,,4,932,"<p>I'm doing sentiment analysis of Spanish tweets.</p>
<p>After reviewing some of the recent literature, I've seen that there's been a most recent effort to train a RoBERTa model exclusively on Spanish text (<code>roberta-base-bne</code>). It seems to perform better than the current state-of-the-art model for Spanish language modeling so far, <a href=""https://huggingface.co/finiteautomata/beto-sentiment-analysis#beto-sentiment-analysis"" rel=""nofollow noreferrer"">BETO</a>.</p>
<p>The RoBERTa model has been trained for a variety of tasks, which do not include text classification.
I want to take this <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">RoBERTa model</a> and fine-tune it for text classification, more specifically, sentiment analysis.</p>
<p>I've done all the preprocessing and created the dataset objects, and want to natively train the model.</p>
<p><strong>Code</strong> <br></p>
<pre><code># Training with native TensorFlow 

from transformers import TFRobertaForSequenceClassification

model = TFRobertaForSequenceClassification.from_pretrained(&quot;BSC-TeMU/roberta-base-bne&quot;)

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)
</code></pre>
<p><strong>Question</strong><br>
My questions is regarding the <code>TFRobertaForSequenceClassification</code>: <br>
Is it correct to use this, since it's not specified in the <a href=""https://huggingface.co/BSC-TeMU/roberta-base-bne"" rel=""nofollow noreferrer"">model card</a>? Instead of the <code>AutoModelForMaskedLM </code> specified in the model card. <br></p>
<p>Do we, by simply applying <code>TFRobertaForSequenceClassification</code>, imply that it will automatically apply the trained (and pretrained) knowledge to the new task, namely text classification?</p>
",14513812.0,,14513812.0,,2021-09-28 20:32:22,2023-06-27 10:02:02,"Fine-tuning a pretrained Spanish RoBERTa model for a different task, sentiment analysis",<python><tensorflow><sentiment-analysis><text-classification><huggingface-transformers>,2,0,,,,CC BY-SA 4.0
72481195,1,72481282.0,,2022-06-02 19:08:17,,4,3323,"<p>When the programme has been started in the console outputting:</p>
<pre><code>WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.checkpoint_management has been moved to tensorflow.python.checkpoint.checkpoint_management. The old module will be deleted in version 2.9.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.resource has been moved to tensorflow.python.trackable.resource. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.util has been moved to tensorflow.python.checkpoint.checkpoint. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base_delegate has been moved to tensorflow.python.trackable.base_delegate. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.graph_view has been moved to tensorflow.python.checkpoint.graph_view. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.python_state has been moved to tensorflow.python.trackable.python_state. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.functional_saver has been moved to tensorflow.python.checkpoint.functional_saver. The old module will be deleted in version 2.11.
WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.saving.checkpoint_options has been moved to tensorflow.python.checkpoint.checkpoint_options. The old module will be deleted in version 2.11.
</code></pre>
<p>How to fix this?</p>
<p>update
<a href=""https://i.stack.imgur.com/aIawt.png"" rel=""nofollow noreferrer"">there</a></p>
",18747479.0,,18747479.0,,2022-06-02 19:44:09,2022-11-03 15:32:55,How can I fix this error with tensorflow warning?,<tensorflow>,3,3,,,,CC BY-SA 4.0
72493437,1,,,2022-06-03 17:55:56,,4,1323,"<p>I am trying to find the very best configuration of hiperparameters for my LSTM using HYDRA. The problem is that there are several houndreds of possible permutations and I want to test them all. So I am trying to use HYDRA with JobLib Launcher to do this but I just cant get it right.</p>
<p>My config folder contains 15 parameters and every single one has at least 3 values. My default configuration file is made like this:</p>
<pre><code>defaults:
  - currency: ???
  - network: ???
  - optimizer: ???
  - lstm_layers: ???
  - lstm_neurons: ???
  - learning_rate: ???
  - momentum: ???
  - dropout: ???
  - activation: ???
  - loss: ???
  - epochs: ???
  - batch_size: ???
  - window: ???
  - time_period: ???
  - exchange: ???
  - _self_
  - override hydra/launcher: joblib

hydra:
  sweeper:
    params:
       currency: glob(*)
       network: glob(*)
       optimizer: glob(*)
       lstm_layers: glob(*)
       lstm_neurons: glob(*)
       learning_rate: glob(*)
       momentum: glob(*)
       dropout: glob(*)
       activation: glob(*)
       loss: glob(*)
       epochs: glob(*)
       batch_size: glob(*)
       window: glob(*)
       time_period: glob(*)
       exchange: glob(*)
</code></pre>
<p>After calling <code>python hydra_framework.py --multirun</code> The program freezes on:</p>
<pre><code>/home/*****/scpxd/hydra_framework.py:23: UserWarning:
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path=&quot;./conf&quot;, config_name=&quot;config&quot;)
</code></pre>
<p>One more thing about the hardware. It is CUDA accelerated crypto miner with 5 GTX 1080ti GPU's. I know that it cannot run all of them in parallel, but I am looking for a solution where there will be 10 models learning at the same time and after one completes, the next starts its learning process. Is it achievable with HYDRA? Thank You in advance for your help.</p>
",11350718.0,,4256346.0,,2022-10-20 00:54:44,2023-05-02 12:24:46,Hydra multirun all permutations in parallel,<python><tensorflow><neural-network><fb-hydra><hydra>,1,1,,,,CC BY-SA 4.0
72499414,1,72624666.0,,2022-06-04 11:36:47,,4,8049,"<p>Windows Version: Windows 10 Pro 21H2 19044.1706
GPU: rtx2070</p>
<pre><code>import tensorflow as tf
import torch
print(torch.__version__) #1.10.1+cu113
print(torch.version.cuda) #11.3
print(tf.__version__) #2.9.1
</code></pre>
<p>and i run</p>
<pre><code>python .\object_detection\builders\model_builder_tf2_test.py
</code></pre>
<p>i can get 'Ran 24 tests in 18.279s OK (skipped=1)' result;</p>
<p>But when I want to train my model, i use</p>
<pre><code>feature_extractor {
   type: 'faster_rcnn_inception_resnet_v2_keras'
}
</code></pre>
<p>in my pipeline_config, and i run</p>
<pre><code>python .\object_detection\model_main_tf2.py --logtostderr --pipeline_config_path=LOCATION_OF_MY_PIPECONFIG --model_dir=LOCATION_OF_MY_MODEL_DIR
</code></pre>
<p>And then i get the following error
<a href=""https://i.stack.imgur.com/Hm5I6.png"" rel=""nofollow noreferrer""><img src=""https://i.stack.imgur.com/Hm5I6.png"" alt=""enter image description here"" /></a>
In my system environment variable , 'CUDA_DIR' is variable and can be accessed</p>
",9250391.0,,,,,2023-02-13 15:30:21,i got an error about error: Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice in tensorflow object_detection api,<python><tensorflow><cuda><tensorflow2.0>,3,0,0.0,,,CC BY-SA 4.0
